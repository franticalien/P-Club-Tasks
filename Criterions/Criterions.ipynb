{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Criterions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypOq3bABv1yW"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "np.random.seed(42069)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQzC3WhVwNAW"
      },
      "source": [
        "# The ground truth functions\n",
        "\n",
        "A0, A1, A2 = 0.4, -1.7, 0.9\n",
        "C0, C1 = 12, 4\n",
        "lin = lambda x : C1*x + C0\n",
        "quad = lambda x : A2*(x**2) + A1*x + A0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMwhvupXxYeM"
      },
      "source": [
        "# Creating the data sets\n",
        "\n",
        "train_size, test_size = 100, 50\n",
        "rng = 5\n",
        "train_x, test_x = np.random.ranf(train_size), np.random.ranf(test_size)\n",
        "train_x, test_x = (train_x)*rng, (test_x)*rng\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLXm-X0SrmW4"
      },
      "source": [
        "lin_data = {'train_x' : train_x, 'train_y' : lin(train_x), \n",
        "             'test_x' : test_x ,  'test_y' : lin(test_x)}\n",
        "quad_data = {'train_x' : train_x, 'train_y' : quad(train_x), \n",
        "              'test_x' : test_x,   'test_y' : quad(test_x)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ0K-QQrXrMe"
      },
      "source": [
        "# Adding Guassian Noise (in the direction perpendicular to the curves)\n",
        "\n",
        "noise = np.random.normal(0,0.07,1000)\n",
        "\n",
        "lin_slope = lambda x : C1 + 0*x\n",
        "quad_slope = lambda x : A2*x + A1\n",
        "f = lambda d : np.reciprocal(np.sqrt(d**2 + 1))\n",
        "\n",
        "lin_data.update({'train_slope' : lin_slope(lin_data['train_x']),\n",
        "                  'test_slope' : lin_slope(lin_data['test_x'])})\n",
        "quad_data.update({'train_slope' : quad_slope(quad_data['train_x']),\n",
        "                   'test_slope' : quad_slope(quad_data['test_x'])})\n",
        "\n",
        "for data in [lin_data, quad_data]:\n",
        "  for typ,size in [('train_',train_size), ('test_',test_size)]:\n",
        "    noise_y = f(data[typ+'slope'])*noise[0:size]\n",
        "    noise_x = -noise_y*data[typ+'slope']\n",
        "    data[typ+'x'] = data[typ+'x'] + noise_x\n",
        "    data[typ+'y'] = data[typ+'y'] + noise_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCdOaDof_X6B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "82ec177f-4f26-4082-d0ca-504ba2b2e938"
      },
      "source": [
        "# Visualizing Data \n",
        "\n",
        "x = np.arange(0,rng,0.01)\n",
        "y_lin, y_quad = lin(x), quad(x)\n",
        "\n",
        "plt.plot(lin_data['train_x'], lin_data['train_y'], 'rp')\n",
        "plt.plot(x, y_lin,'b')\n",
        "plt.legend(['train_data', 'ground_truth'])\n",
        "plt.title('Linear Training Data')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(quad_data['train_x'], quad_data['train_y'], 'rp')\n",
        "plt.plot(x, y_quad,'b')\n",
        "plt.legend(['train_data', 'ground_truth'])\n",
        "plt.title('Quadratic Training Data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vvixT_D6bii"
      },
      "source": [
        "# Forward Propagation\n",
        "\n",
        "def cost(z,y,m):\n",
        "  delta = abs(z-y)\n",
        "  loss = (1/np.size(z))*np.sum(delta**m)\n",
        "  dZ = m*(delta**(m-1))*np.sign(z-y)\n",
        "  return loss,dZ\n",
        "\n",
        "def lin_for_prop(w,b,x,y,m):\n",
        "  z = w*x + b\n",
        "  return cost(z,y,m)\n",
        "\n",
        "def quad_for_prop(a,b,c,x,y,m):\n",
        "  z = a*(x**2) + b*x + c\n",
        "  return cost(z,y,m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY0HkJksHs_H"
      },
      "source": [
        "# Backward Propagation\n",
        "def lin_back_prop(w,b,x,dZ):\n",
        "  dw = (1/np.size(x))*np.sum(dZ*x)\n",
        "  db = (1/np.size(x))*np.sum(dZ)\n",
        "  return dw,db\n",
        "\n",
        "def quad_back_prop(a,b,c,x,dZ):\n",
        "  da = (1/np.size(x))*np.sum(dZ*(x**2))\n",
        "  db = (1/np.size(x))*np.sum(dZ*x)\n",
        "  dc = (1/np.size(x))*np.sum(dZ)\n",
        "  return da,db,dc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Gt8V8qJyNS"
      },
      "source": [
        "# Train Functions\n",
        "\n",
        "def lin_train(data,w,b,m,initialize_weights=False,learning_rate=0.0001,epochs=300000):\n",
        "  print(f\"Training Linear Regression Model using COST = (y-z)**{m}\")\n",
        "\n",
        "  if initialize_weights:\n",
        "    w,b = np.random.randn(),0\n",
        "\n",
        "  train_loss, test_loss = [],[]\n",
        "  min_loss = float('inf')\n",
        "\n",
        "  x,y = data['train_x'], data['train_y']\n",
        "  xtst, ytst = data['test_x'], data['test_y']\n",
        "\n",
        "  for i in range(epochs):\n",
        "    loss1, dZ = lin_for_prop(w,b,x,y,m)\n",
        "    loss2, _ = lin_for_prop(w,b,xtst,ytst,m)\n",
        "\n",
        "    print(f\"Cost after {i+1} iterations : Training Loss =  {loss1}; Validation Loss = {loss2}\")\n",
        "\n",
        "    train_loss.append(loss1)\n",
        "    test_loss.append(loss2)\n",
        "\n",
        "    if loss1 == float('inf'):\n",
        "      w,b = np.random.randn(0),0\n",
        "    if loss2 < min_loss:\n",
        "      min_loss = loss2\n",
        "      W,B = w,b\n",
        "\n",
        "    dw,db = lin_back_prop(w,b,x,dZ)\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db \n",
        "\n",
        "  print(f\"Training Complete : min_loss_achieved = {min_loss}; W,B = {W,B}\")\n",
        "\n",
        "  return W,B,train_loss,test_loss\n",
        "\n",
        "\n",
        "def quad_train(data,a,b,c,m,initialize_weights=False,learning_rate=0.0001,epochs=300000):\n",
        "  print(f\"Training Quadratic Regression Model using COST = (y-z)**{m}\")\n",
        "\n",
        "  if initialize_weights:\n",
        "    a,b,c = np.random.randn(2),0\n",
        "\n",
        "  train_loss, test_loss = [],[]\n",
        "  min_loss = float('inf') \n",
        "\n",
        "  x, y = data['train_x'], data['train_y']\n",
        "  xtst, ytst = data['test_x'], data['test_y']\n",
        "\n",
        "  for i in range(epochs):\n",
        "    loss1, dZ = quad_for_prop(a,b,c,x,y,m)\n",
        "    loss2, _ = quad_for_prop(a,b,c,xtst,ytst,m)\n",
        "\n",
        "    print(f\"Cost after {i+1} iterations : Training Loss =  {loss1}; Validation Loss = {loss2}\")\n",
        "    \n",
        "    train_loss.append(loss1)\n",
        "    test_loss.append(loss2)\n",
        "\n",
        "    if loss1 == float('inf'):\n",
        "      a,b,c = np.random.randn(3)\n",
        "    if loss2 < min_loss:\n",
        "      min_loss = loss2\n",
        "      A,B,C = a,b,c\n",
        "\n",
        "    da,db,dc = quad_back_prop(a,b,c,x,dZ)\n",
        "    a = a - learning_rate*da\n",
        "    b = b - learning_rate*db\n",
        "    c = c - learning_rate*dc \n",
        "    \n",
        "  print(f\"Training Complete : min_loss_achieved = {min_loss}; A,B,C = {A,B,C}\")\n",
        "\n",
        "  return A,B,C,train_loss,test_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZBLUidCi4sj"
      },
      "source": [
        "# Initializing weights\n",
        "L1,Q2,Q1 = np.random.randn(3)\n",
        "L0,Q0 = 0,0\n",
        "\n",
        "lin_results, quad_results = [],[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxuLWPNplvkE",
        "outputId": "a535b9b6-5655-4377-c01a-db3e4c65befd"
      },
      "source": [
        "# Model-1 Training\n",
        "lin_results.append(lin_train(lin_data,L1,L0,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Cost after 295002 iterations : Training Loss =  0.31302096577734234; Validation Loss = 0.3485481183066402\n",
            "Cost after 295003 iterations : Training Loss =  0.31302117145107217; Validation Loss = 0.3485476417176194\n",
            "Cost after 295004 iterations : Training Loss =  0.3130210041461106; Validation Loss = 0.3485478895679272\n",
            "Cost after 295005 iterations : Training Loss =  0.3130210802136631; Validation Loss = 0.3485481374182346\n",
            "Cost after 295006 iterations : Training Loss =  0.31302105054640705; Validation Loss = 0.3485476608292138\n",
            "Cost after 295007 iterations : Training Loss =  0.3130209809447251; Validation Loss = 0.34854790867952135\n",
            "Cost after 295008 iterations : Training Loss =  0.31302109694670366; Validation Loss = 0.3485474320905009\n",
            "Cost after 295009 iterations : Training Loss =  0.3130209296417419; Validation Loss = 0.3485476799408079\n",
            "Cost after 295010 iterations : Training Loss =  0.31302109538104556; Validation Loss = 0.34854792779111576\n",
            "Cost after 295011 iterations : Training Loss =  0.3130209760420387; Validation Loss = 0.34854745120209474\n",
            "Cost after 295012 iterations : Training Loss =  0.3130209961121076; Validation Loss = 0.34854769905240246\n",
            "Cost after 295013 iterations : Training Loss =  0.31302102244233515; Validation Loss = 0.34854722246338155\n",
            "Cost after 295014 iterations : Training Loss =  0.3130208968431698; Validation Loss = 0.3485474703136892\n",
            "Cost after 295015 iterations : Training Loss =  0.3130210688426318; Validation Loss = 0.34854699372466835\n",
            "Cost after 295016 iterations : Training Loss =  0.31302090153767004; Validation Loss = 0.34854724157497613\n",
            "Cost after 295017 iterations : Training Loss =  0.3130210112794903; Validation Loss = 0.34854748942528363\n",
            "Cost after 295018 iterations : Training Loss =  0.313020947937967; Validation Loss = 0.34854701283626255\n",
            "Cost after 295019 iterations : Training Loss =  0.3130209120105523; Validation Loss = 0.3485472606865696\n",
            "Cost after 295020 iterations : Training Loss =  0.31302099433826336; Validation Loss = 0.34854678409754913\n",
            "Cost after 295021 iterations : Training Loss =  0.31302082703330164; Validation Loss = 0.34854703194785686\n",
            "Cost after 295022 iterations : Training Loss =  0.3130210264468731; Validation Loss = 0.3485472797981642\n",
            "Cost after 295023 iterations : Training Loss =  0.3130208734335982; Validation Loss = 0.34854680320914333\n",
            "Cost after 295024 iterations : Training Loss =  0.3130209271779351; Validation Loss = 0.3485470510594506\n",
            "Cost after 295025 iterations : Training Loss =  0.3130209198338949; Validation Loss = 0.3485465744704301\n",
            "Cost after 295026 iterations : Training Loss =  0.31302082790899705; Validation Loss = 0.3485468223207376\n",
            "Cost after 295027 iterations : Training Loss =  0.3130209662341915; Validation Loss = 0.34854634573171667\n",
            "Cost after 295028 iterations : Training Loss =  0.31302079892923; Validation Loss = 0.34854659358202456\n",
            "Cost after 295029 iterations : Training Loss =  0.31302094234531763; Validation Loss = 0.3485468414323319\n",
            "Cost after 295030 iterations : Training Loss =  0.3130208453295266; Validation Loss = 0.3485463648433108\n",
            "Cost after 295031 iterations : Training Loss =  0.3130208430763799; Validation Loss = 0.34854661269361825\n",
            "Cost after 295032 iterations : Training Loss =  0.3130208917298231; Validation Loss = 0.348546136104598\n",
            "Cost after 295033 iterations : Training Loss =  0.3130207438074418; Validation Loss = 0.34854638395490545\n",
            "Cost after 295034 iterations : Training Loss =  0.3130209381301197; Validation Loss = 0.3485459073658842\n",
            "Cost after 295035 iterations : Training Loss =  0.313020770825158; Validation Loss = 0.348546155216192\n",
            "Cost after 295036 iterations : Training Loss =  0.3130208582437626; Validation Loss = 0.34854640306649953\n",
            "Cost after 295037 iterations : Training Loss =  0.31302081722545455; Validation Loss = 0.34854592647747895\n",
            "Cost after 295038 iterations : Training Loss =  0.3130207589748246; Validation Loss = 0.3485461743277861\n",
            "Cost after 295039 iterations : Training Loss =  0.3130208636257513; Validation Loss = 0.34854569773876565\n",
            "Cost after 295040 iterations : Training Loss =  0.31302069632078955; Validation Loss = 0.3485459455890728\n",
            "Cost after 295041 iterations : Training Loss =  0.31302087341114515; Validation Loss = 0.3485461934393808\n",
            "Cost after 295042 iterations : Training Loss =  0.3130207427210864; Validation Loss = 0.34854571685035973\n",
            "Cost after 295043 iterations : Training Loss =  0.31302077414220725; Validation Loss = 0.34854596470066723\n",
            "Cost after 295044 iterations : Training Loss =  0.31302078912138276; Validation Loss = 0.34854548811164676\n",
            "Cost after 295045 iterations : Training Loss =  0.31302067487326946; Validation Loss = 0.3485457359619538\n",
            "Cost after 295046 iterations : Training Loss =  0.31302083552167964; Validation Loss = 0.3485452593729329\n",
            "Cost after 295047 iterations : Training Loss =  0.3130206682167179; Validation Loss = 0.3485455072232405\n",
            "Cost after 295048 iterations : Training Loss =  0.3130207893095899; Validation Loss = 0.3485457550735484\n",
            "Cost after 295049 iterations : Training Loss =  0.31302071461701453; Validation Loss = 0.34854527848452704\n",
            "Cost after 295050 iterations : Training Loss =  0.3130206900406519; Validation Loss = 0.3485455263348349\n",
            "Cost after 295051 iterations : Training Loss =  0.31302076101731136; Validation Loss = 0.3485450497458142\n",
            "Cost after 295052 iterations : Training Loss =  0.3130205937123496; Validation Loss = 0.34854529759612163\n",
            "Cost after 295053 iterations : Training Loss =  0.3130208044769725; Validation Loss = 0.34854554544642896\n",
            "Cost after 295054 iterations : Training Loss =  0.3130206401126459; Validation Loss = 0.3485450688574082\n",
            "Cost after 295055 iterations : Training Loss =  0.3130207052080346; Validation Loss = 0.34854531670771594\n",
            "Cost after 295056 iterations : Training Loss =  0.3130206865129426; Validation Loss = 0.3485448401186953\n",
            "Cost after 295057 iterations : Training Loss =  0.31302060593909675; Validation Loss = 0.3485450879690022\n",
            "Cost after 295058 iterations : Training Loss =  0.3130207329132395; Validation Loss = 0.3485446113799817\n",
            "Cost after 295059 iterations : Training Loss =  0.3130205656082777; Validation Loss = 0.3485448592302893\n",
            "Cost after 295060 iterations : Training Loss =  0.31302072037541734; Validation Loss = 0.34854510708059644\n",
            "Cost after 295061 iterations : Training Loss =  0.31302061200857434; Validation Loss = 0.348544630491576\n",
            "Cost after 295062 iterations : Training Loss =  0.31302062110647944; Validation Loss = 0.34854487834188363\n",
            "Cost after 295063 iterations : Training Loss =  0.31302065840887106; Validation Loss = 0.3485444017528627\n",
            "Cost after 295064 iterations : Training Loss =  0.3130205218375417; Validation Loss = 0.3485446496031701\n",
            "Cost after 295065 iterations : Training Loss =  0.31302070480916755; Validation Loss = 0.3485441730141494\n",
            "Cost after 295066 iterations : Training Loss =  0.3130205375042058; Validation Loss = 0.3485444208644565\n",
            "Cost after 295067 iterations : Training Loss =  0.31302063627386195; Validation Loss = 0.34854466871476436\n",
            "Cost after 295068 iterations : Training Loss =  0.3130205839045025; Validation Loss = 0.34854419212574345\n",
            "Cost after 295069 iterations : Training Loss =  0.3130205370049239; Validation Loss = 0.34854443997605095\n",
            "Cost after 295070 iterations : Training Loss =  0.31302063030479915; Validation Loss = 0.34854396338703025\n",
            "Cost after 295071 iterations : Training Loss =  0.3130204629998374; Validation Loss = 0.34854421123733803\n",
            "Cost after 295072 iterations : Training Loss =  0.3130206514412445; Validation Loss = 0.3485444590876453\n",
            "Cost after 295073 iterations : Training Loss =  0.3130205094001343; Validation Loss = 0.34854398249862456\n",
            "Cost after 295074 iterations : Training Loss =  0.3130205521723068; Validation Loss = 0.3485442303489317\n",
            "Cost after 295075 iterations : Training Loss =  0.3130205558004306; Validation Loss = 0.3485437537599117\n",
            "Cost after 295076 iterations : Training Loss =  0.31302045290336883; Validation Loss = 0.3485440016102186\n",
            "Cost after 295077 iterations : Training Loss =  0.3130206022007273; Validation Loss = 0.3485435250211979\n",
            "Cost after 295078 iterations : Training Loss =  0.3130204348957654; Validation Loss = 0.3485437728715056\n",
            "Cost after 295079 iterations : Training Loss =  0.3130205673396893; Validation Loss = 0.3485440207218129\n",
            "Cost after 295080 iterations : Training Loss =  0.3130204812960622; Validation Loss = 0.3485435441327918\n",
            "Cost after 295081 iterations : Training Loss =  0.31302046807075146; Validation Loss = 0.3485437919830997\n",
            "Cost after 295082 iterations : Training Loss =  0.31302052769635874; Validation Loss = 0.3485433153940787\n",
            "Cost after 295083 iterations : Training Loss =  0.3130203688018135; Validation Loss = 0.3485435632443861\n",
            "Cost after 295084 iterations : Training Loss =  0.3130205740966553; Validation Loss = 0.3485430866553652\n",
            "Cost after 295085 iterations : Training Loss =  0.3130204067916937; Validation Loss = 0.3485433345056731\n",
            "Cost after 295086 iterations : Training Loss =  0.31302048323813414; Validation Loss = 0.34854358235598093\n",
            "Cost after 295087 iterations : Training Loss =  0.31302045319199023; Validation Loss = 0.34854310576696\n",
            "Cost after 295088 iterations : Training Loss =  0.3130203839691963; Validation Loss = 0.34854335361726757\n",
            "Cost after 295089 iterations : Training Loss =  0.31302049959228695; Validation Loss = 0.34854287702824693\n",
            "Cost after 295090 iterations : Training Loss =  0.31302033228732534; Validation Loss = 0.3485431248785541\n",
            "Cost after 295091 iterations : Training Loss =  0.3130204984055166; Validation Loss = 0.34854337272886154\n",
            "Cost after 295092 iterations : Training Loss =  0.313020378687622; Validation Loss = 0.3485428961398408\n",
            "Cost after 295093 iterations : Training Loss =  0.31302039913657864; Validation Loss = 0.3485431439901485\n",
            "Cost after 295094 iterations : Training Loss =  0.31302042508791866; Validation Loss = 0.3485426674011272\n",
            "Cost after 295095 iterations : Training Loss =  0.31302029986764074; Validation Loss = 0.34854291525143516\n",
            "Cost after 295096 iterations : Training Loss =  0.31302047148821505; Validation Loss = 0.3485424386624145\n",
            "Cost after 295097 iterations : Training Loss =  0.3130203041832534; Validation Loss = 0.3485426865127219\n",
            "Cost after 295098 iterations : Training Loss =  0.31302041430396144; Validation Loss = 0.34854293436302924\n",
            "Cost after 295099 iterations : Training Loss =  0.31302035058355; Validation Loss = 0.3485424577740082\n",
            "Cost after 295100 iterations : Training Loss =  0.3130203150350234; Validation Loss = 0.3485427056243161\n",
            "Cost after 295101 iterations : Training Loss =  0.3130203969838467; Validation Loss = 0.3485422290352953\n",
            "Cost after 295102 iterations : Training Loss =  0.31302022967888476; Validation Loss = 0.34854247688560264\n",
            "Cost after 295103 iterations : Training Loss =  0.31302042947134423; Validation Loss = 0.34854272473590997\n",
            "Cost after 295104 iterations : Training Loss =  0.31302027607918176; Validation Loss = 0.3485422481468895\n",
            "Cost after 295105 iterations : Training Loss =  0.31302033020240627; Validation Loss = 0.3485424959971968\n",
            "Cost after 295106 iterations : Training Loss =  0.3130203224794784; Validation Loss = 0.3485420194081763\n",
            "Cost after 295107 iterations : Training Loss =  0.3130202309334683; Validation Loss = 0.3485422672584838\n",
            "Cost after 295108 iterations : Training Loss =  0.31302036887977486; Validation Loss = 0.3485417906694627\n",
            "Cost after 295109 iterations : Training Loss =  0.31302020157481314; Validation Loss = 0.34854203851977017\n",
            "Cost after 295110 iterations : Training Loss =  0.31302034536978873; Validation Loss = 0.3485422863700778\n",
            "Cost after 295111 iterations : Training Loss =  0.3130202479751097; Validation Loss = 0.34854180978105687\n",
            "Cost after 295112 iterations : Training Loss =  0.31302024610085094; Validation Loss = 0.34854205763136464\n",
            "Cost after 295113 iterations : Training Loss =  0.3130202943754065; Validation Loss = 0.3485415810423438\n",
            "Cost after 295114 iterations : Training Loss =  0.3130201468319131; Validation Loss = 0.3485418288926513\n",
            "Cost after 295115 iterations : Training Loss =  0.3130203407757032; Validation Loss = 0.3485413523036306\n",
            "Cost after 295116 iterations : Training Loss =  0.31302017347074124; Validation Loss = 0.34854160015393804\n",
            "Cost after 295117 iterations : Training Loss =  0.3130202612682335; Validation Loss = 0.3485418480042455\n",
            "Cost after 295118 iterations : Training Loss =  0.313020219871038; Validation Loss = 0.34854137141522484\n",
            "Cost after 295119 iterations : Training Loss =  0.3130201619992956; Validation Loss = 0.34854161926553207\n",
            "Cost after 295120 iterations : Training Loss =  0.3130202662713348; Validation Loss = 0.34854114267651154\n",
            "Cost after 295121 iterations : Training Loss =  0.31302009896637295; Validation Loss = 0.3485413905268189\n",
            "Cost after 295122 iterations : Training Loss =  0.3130202764356161; Validation Loss = 0.3485416383771267\n",
            "Cost after 295123 iterations : Training Loss =  0.3130201453666697; Validation Loss = 0.34854116178810585\n",
            "Cost after 295124 iterations : Training Loss =  0.3130201771666784; Validation Loss = 0.34854140963841335\n",
            "Cost after 295125 iterations : Training Loss =  0.3130201917669662; Validation Loss = 0.34854093304939227\n",
            "Cost after 295126 iterations : Training Loss =  0.3130200778977405; Validation Loss = 0.34854118089970015\n",
            "Cost after 295127 iterations : Training Loss =  0.3130202381672629; Validation Loss = 0.3485407043106792\n",
            "Cost after 295128 iterations : Training Loss =  0.3130200708623012; Validation Loss = 0.3485409521609866\n",
            "Cost after 295129 iterations : Training Loss =  0.3130201923340608; Validation Loss = 0.34854120001129446\n",
            "Cost after 295130 iterations : Training Loss =  0.3130201172625976; Validation Loss = 0.34854072342227344\n",
            "Cost after 295131 iterations : Training Loss =  0.31302009306512313; Validation Loss = 0.3485409712725814\n",
            "Cost after 295132 iterations : Training Loss =  0.3130201636628946; Validation Loss = 0.34854049468356\n",
            "Cost after 295133 iterations : Training Loss =  0.3130199963579326; Validation Loss = 0.34854074253386785\n",
            "Cost after 295134 iterations : Training Loss =  0.3130202075014436; Validation Loss = 0.34854099038417546\n",
            "Cost after 295135 iterations : Training Loss =  0.3130200427582294; Validation Loss = 0.3485405137951548\n",
            "Cost after 295136 iterations : Training Loss =  0.3130201082325058; Validation Loss = 0.3485407616454617\n",
            "Cost after 295137 iterations : Training Loss =  0.31302008915852597; Validation Loss = 0.34854028505644097\n",
            "Cost after 295138 iterations : Training Loss =  0.3130200089635677; Validation Loss = 0.3485405329067487\n",
            "Cost after 295139 iterations : Training Loss =  0.31302013555882247; Validation Loss = 0.34854005631772794\n",
            "Cost after 295140 iterations : Training Loss =  0.31301996825386097; Validation Loss = 0.34854030416803533\n",
            "Cost after 295141 iterations : Training Loss =  0.3130201233998883; Validation Loss = 0.34854055201834283\n",
            "Cost after 295142 iterations : Training Loss =  0.3130200146541577; Validation Loss = 0.34854007542932197\n",
            "Cost after 295143 iterations : Training Loss =  0.3130200241309503; Validation Loss = 0.3485403232796294\n",
            "Cost after 295144 iterations : Training Loss =  0.3130200610544544; Validation Loss = 0.34853984669060867\n",
            "Cost after 295145 iterations : Training Loss =  0.31301992486201236; Validation Loss = 0.34854009454091633\n",
            "Cost after 295146 iterations : Training Loss =  0.31302010745475095; Validation Loss = 0.34853961795189564\n",
            "Cost after 295147 iterations : Training Loss =  0.313019940149789; Validation Loss = 0.3485398658022029\n",
            "Cost after 295148 iterations : Training Loss =  0.313020039298333; Validation Loss = 0.34854011365251025\n",
            "Cost after 295149 iterations : Training Loss =  0.3130199865500858; Validation Loss = 0.34853963706348956\n",
            "Cost after 295150 iterations : Training Loss =  0.31301994002939504; Validation Loss = 0.3485398849137976\n",
            "Cost after 295151 iterations : Training Loss =  0.3130200329503823; Validation Loss = 0.3485394083247763\n",
            "Cost after 295152 iterations : Training Loss =  0.3130198656454206; Validation Loss = 0.3485396561750838\n",
            "Cost after 295153 iterations : Training Loss =  0.31302005446571574; Validation Loss = 0.34853990402539153\n",
            "Cost after 295154 iterations : Training Loss =  0.3130199120457173; Validation Loss = 0.3485394274363706\n",
            "Cost after 295155 iterations : Training Loss =  0.3130199551967778; Validation Loss = 0.34853967528667823\n",
            "Cost after 295156 iterations : Training Loss =  0.313019958446014; Validation Loss = 0.3485391986976573\n",
            "Cost after 295157 iterations : Training Loss =  0.31301985592783993; Validation Loss = 0.3485394465479649\n",
            "Cost after 295158 iterations : Training Loss =  0.31302000484631054; Validation Loss = 0.3485389699589445\n",
            "Cost after 295159 iterations : Training Loss =  0.31301983754134877; Validation Loss = 0.3485392178092513\n",
            "Cost after 295160 iterations : Training Loss =  0.3130199703641602; Validation Loss = 0.34853946565955907\n",
            "Cost after 295161 iterations : Training Loss =  0.31301988394164537; Validation Loss = 0.3485389890705383\n",
            "Cost after 295162 iterations : Training Loss =  0.31301987109522267; Validation Loss = 0.34853923692084576\n",
            "Cost after 295163 iterations : Training Loss =  0.31301993034194214; Validation Loss = 0.3485387603318248\n",
            "Cost after 295164 iterations : Training Loss =  0.3130197718262847; Validation Loss = 0.34853900818213246\n",
            "Cost after 295165 iterations : Training Loss =  0.31301997674223886; Validation Loss = 0.34853853159311127\n",
            "Cost after 295166 iterations : Training Loss =  0.3130198094372772; Validation Loss = 0.34853877944341916\n",
            "Cost after 295167 iterations : Training Loss =  0.313019886262605; Validation Loss = 0.348539027293727\n",
            "Cost after 295168 iterations : Training Loss =  0.31301985583757364; Validation Loss = 0.348538550704706\n",
            "Cost after 295169 iterations : Training Loss =  0.3130197869936672; Validation Loss = 0.34853879855501363\n",
            "Cost after 295170 iterations : Training Loss =  0.31301990223787024; Validation Loss = 0.34853832196599244\n",
            "Cost after 295171 iterations : Training Loss =  0.3130197349329086; Validation Loss = 0.3485385698163003\n",
            "Cost after 295172 iterations : Training Loss =  0.31301990142998765; Validation Loss = 0.34853881766660766\n",
            "Cost after 295173 iterations : Training Loss =  0.3130197813332052; Validation Loss = 0.34853834107758674\n",
            "Cost after 295174 iterations : Training Loss =  0.31301980216105; Validation Loss = 0.34853858892789447\n",
            "Cost after 295175 iterations : Training Loss =  0.3130198277335019; Validation Loss = 0.34853811233887344\n",
            "Cost after 295176 iterations : Training Loss =  0.3130197028921121; Validation Loss = 0.34853836018918105\n",
            "Cost after 295177 iterations : Training Loss =  0.31301987413379834; Validation Loss = 0.34853788360016047\n",
            "Cost after 295178 iterations : Training Loss =  0.31301970682883684; Validation Loss = 0.34853813145046786\n",
            "Cost after 295179 iterations : Training Loss =  0.3130198173284328; Validation Loss = 0.34853837930077547\n",
            "Cost after 295180 iterations : Training Loss =  0.31301975322913356; Validation Loss = 0.34853790271175444\n",
            "Cost after 295181 iterations : Training Loss =  0.3130197180594947; Validation Loss = 0.34853815056206217\n",
            "Cost after 295182 iterations : Training Loss =  0.3130197996294302; Validation Loss = 0.34853767397304114\n",
            "Cost after 295183 iterations : Training Loss =  0.31301963232446844; Validation Loss = 0.34853792182334886\n",
            "Cost after 295184 iterations : Training Loss =  0.3130198324958151; Validation Loss = 0.3485381696736566\n",
            "Cost after 295185 iterations : Training Loss =  0.31301967872476494; Validation Loss = 0.3485376930846358\n",
            "Cost after 295186 iterations : Training Loss =  0.31301973322687743; Validation Loss = 0.3485379409349434\n",
            "Cost after 295187 iterations : Training Loss =  0.3130197251250616; Validation Loss = 0.3485374643459224\n",
            "Cost after 295188 iterations : Training Loss =  0.31301963395793936; Validation Loss = 0.34853771219622987\n",
            "Cost after 295189 iterations : Training Loss =  0.31301977152535837; Validation Loss = 0.3485372356072091\n",
            "Cost after 295190 iterations : Training Loss =  0.3130196042203965; Validation Loss = 0.34853748345751684\n",
            "Cost after 295191 iterations : Training Loss =  0.31301974839425994; Validation Loss = 0.348537731307824\n",
            "Cost after 295192 iterations : Training Loss =  0.3130196506206931; Validation Loss = 0.34853725471880365\n",
            "Cost after 295193 iterations : Training Loss =  0.3130196491253219; Validation Loss = 0.3485375025691104\n",
            "Cost after 295194 iterations : Training Loss =  0.3130196970209899; Validation Loss = 0.3485370259800902\n",
            "Cost after 295195 iterations : Training Loss =  0.3130195498563841; Validation Loss = 0.34853727383039773\n",
            "Cost after 295196 iterations : Training Loss =  0.3130197434212864; Validation Loss = 0.34853679724137665\n",
            "Cost after 295197 iterations : Training Loss =  0.3130195761163247; Validation Loss = 0.34853704509168437\n",
            "Cost after 295198 iterations : Training Loss =  0.3130196642927047; Validation Loss = 0.34853729294199154\n",
            "Cost after 295199 iterations : Training Loss =  0.31301962251662135; Validation Loss = 0.3485368163529707\n",
            "Cost after 295200 iterations : Training Loss =  0.31301956502376693; Validation Loss = 0.34853706420327835\n",
            "Cost after 295201 iterations : Training Loss =  0.3130196689169181; Validation Loss = 0.34853658761425776\n",
            "Cost after 295202 iterations : Training Loss =  0.3130195016119563; Validation Loss = 0.3485368354645652\n",
            "Cost after 295203 iterations : Training Loss =  0.3130196794600874; Validation Loss = 0.3485370833148726\n",
            "Cost after 295204 iterations : Training Loss =  0.31301954801225296; Validation Loss = 0.34853660672585185\n",
            "Cost after 295205 iterations : Training Loss =  0.3130195801911496; Validation Loss = 0.3485368545761597\n",
            "Cost after 295206 iterations : Training Loss =  0.31301959441254973; Validation Loss = 0.3485363779871386\n",
            "Cost after 295207 iterations : Training Loss =  0.3130194809222115; Validation Loss = 0.3485366258374462\n",
            "Cost after 295208 iterations : Training Loss =  0.3130196408128464; Validation Loss = 0.34853614924842524\n",
            "Cost after 295209 iterations : Training Loss =  0.3130194735078846; Validation Loss = 0.3485363970987328\n",
            "Cost after 295210 iterations : Training Loss =  0.3130195953585321; Validation Loss = 0.34853664494903996\n",
            "Cost after 295211 iterations : Training Loss =  0.31301951990818105; Validation Loss = 0.3485361683600193\n",
            "Cost after 295212 iterations : Training Loss =  0.3130194960895942; Validation Loss = 0.348536416210327\n",
            "Cost after 295213 iterations : Training Loss =  0.3130195663084777; Validation Loss = 0.3485359396213064\n",
            "Cost after 295214 iterations : Training Loss =  0.3130193990035161; Validation Loss = 0.3485361874716138\n",
            "Cost after 295215 iterations : Training Loss =  0.31301961052591465; Validation Loss = 0.34853643532192136\n",
            "Cost after 295216 iterations : Training Loss =  0.31301944540381266; Validation Loss = 0.34853595873290033\n",
            "Cost after 295217 iterations : Training Loss =  0.3130195112569766; Validation Loss = 0.34853620658320794\n",
            "Cost after 295218 iterations : Training Loss =  0.3130194918041095; Validation Loss = 0.3485357299941873\n",
            "Cost after 295219 iterations : Training Loss =  0.31301941198803906; Validation Loss = 0.34853597784449447\n",
            "Cost after 295220 iterations : Training Loss =  0.31301953820440614; Validation Loss = 0.34853550125547356\n",
            "Cost after 295221 iterations : Training Loss =  0.31301937089944426; Validation Loss = 0.34853574910578156\n",
            "Cost after 295222 iterations : Training Loss =  0.3130195264243594; Validation Loss = 0.34853599695608917\n",
            "Cost after 295223 iterations : Training Loss =  0.3130194172997408; Validation Loss = 0.3485355203670683\n",
            "Cost after 295224 iterations : Training Loss =  0.3130194271554215; Validation Loss = 0.34853576821737564\n",
            "Cost after 295225 iterations : Training Loss =  0.3130194637000376; Validation Loss = 0.34853529162835456\n",
            "Cost after 295226 iterations : Training Loss =  0.31301932788648346; Validation Loss = 0.34853553947866245\n",
            "Cost after 295227 iterations : Training Loss =  0.3130195101003343; Validation Loss = 0.3485350628896414\n",
            "Cost after 295228 iterations : Training Loss =  0.3130193427953725; Validation Loss = 0.3485353107399493\n",
            "Cost after 295229 iterations : Training Loss =  0.3130194423228041; Validation Loss = 0.34853555859025664\n",
            "Cost after 295230 iterations : Training Loss =  0.31301938919566896; Validation Loss = 0.3485350820012358\n",
            "Cost after 295231 iterations : Training Loss =  0.3130193430538661; Validation Loss = 0.3485353298515434\n",
            "Cost after 295232 iterations : Training Loss =  0.3130194355959656; Validation Loss = 0.3485348532625224\n",
            "Cost after 295233 iterations : Training Loss =  0.31301926829100385; Validation Loss = 0.34853510111283004\n",
            "Cost after 295234 iterations : Training Loss =  0.3130194574901867; Validation Loss = 0.3485353489631373\n",
            "Cost after 295235 iterations : Training Loss =  0.3130193146913005; Validation Loss = 0.348534872374117\n",
            "Cost after 295236 iterations : Training Loss =  0.31301935822124893; Validation Loss = 0.3485351202244243\n",
            "Cost after 295237 iterations : Training Loss =  0.3130193610915973; Validation Loss = 0.34853464363540354\n",
            "Cost after 295238 iterations : Training Loss =  0.313019258952311; Validation Loss = 0.3485348914857109\n",
            "Cost after 295239 iterations : Training Loss =  0.31301940749189366; Validation Loss = 0.3485344148966904\n",
            "Cost after 295240 iterations : Training Loss =  0.31301924018693195; Validation Loss = 0.348534662746998\n",
            "Cost after 295241 iterations : Training Loss =  0.3130193733886313; Validation Loss = 0.3485349105973053\n",
            "Cost after 295242 iterations : Training Loss =  0.31301928658722894; Validation Loss = 0.34853443400828454\n",
            "Cost after 295243 iterations : Training Loss =  0.3130192741196935; Validation Loss = 0.3485346818585917\n",
            "Cost after 295244 iterations : Training Loss =  0.3130193329875254; Validation Loss = 0.3485342052695711\n",
            "Cost after 295245 iterations : Training Loss =  0.31301917485075575; Validation Loss = 0.34853445311987835\n",
            "Cost after 295246 iterations : Training Loss =  0.3130193793878221; Validation Loss = 0.34853397653085777\n",
            "Cost after 295247 iterations : Training Loss =  0.31301921208286004; Validation Loss = 0.3485342243811651\n",
            "Cost after 295248 iterations : Training Loss =  0.31301928928707623; Validation Loss = 0.34853447223147266\n",
            "Cost after 295249 iterations : Training Loss =  0.3130192584831569; Validation Loss = 0.3485339956424522\n",
            "Cost after 295250 iterations : Training Loss =  0.31301919001813827; Validation Loss = 0.3485342434927596\n",
            "Cost after 295251 iterations : Training Loss =  0.313019304883454; Validation Loss = 0.34853376690373905\n",
            "Cost after 295252 iterations : Training Loss =  0.313019137578492; Validation Loss = 0.3485340147540461\n",
            "Cost after 295253 iterations : Training Loss =  0.3130193044544591; Validation Loss = 0.34853426260435394\n",
            "Cost after 295254 iterations : Training Loss =  0.31301918397878875; Validation Loss = 0.34853378601533297\n",
            "Cost after 295255 iterations : Training Loss =  0.313019205185521; Validation Loss = 0.3485340338656408\n",
            "Cost after 295256 iterations : Training Loss =  0.3130192303790853; Validation Loss = 0.3485335572766198\n",
            "Cost after 295257 iterations : Training Loss =  0.313019105916583; Validation Loss = 0.34853380512692705\n",
            "Cost after 295258 iterations : Training Loss =  0.313019276779382; Validation Loss = 0.34853332853790614\n",
            "Cost after 295259 iterations : Training Loss =  0.3130191094744199; Validation Loss = 0.3485335763882139\n",
            "Cost after 295260 iterations : Training Loss =  0.3130192203529034; Validation Loss = 0.34853382423852153\n",
            "Cost after 295261 iterations : Training Loss =  0.3130191558747166; Validation Loss = 0.3485333476495008\n",
            "Cost after 295262 iterations : Training Loss =  0.3130191210839658; Validation Loss = 0.34853359549980795\n",
            "Cost after 295263 iterations : Training Loss =  0.3130192022750135; Validation Loss = 0.3485331189107875\n",
            "Cost after 295264 iterations : Training Loss =  0.31301903497005185; Validation Loss = 0.3485333667610951\n",
            "Cost after 295265 iterations : Training Loss =  0.31301923552028627; Validation Loss = 0.348533614611403\n",
            "Cost after 295266 iterations : Training Loss =  0.3130190813703485; Validation Loss = 0.34853313802238167\n",
            "Cost after 295267 iterations : Training Loss =  0.3130191362513483; Validation Loss = 0.34853338587268917\n",
            "Cost after 295268 iterations : Training Loss =  0.31301912777064483; Validation Loss = 0.3485329092836686\n",
            "Cost after 295269 iterations : Training Loss =  0.31301903698241057; Validation Loss = 0.3485331571339759\n",
            "Cost after 295270 iterations : Training Loss =  0.31301917417094177; Validation Loss = 0.3485326805449549\n",
            "Cost after 295271 iterations : Training Loss =  0.3130190068659798; Validation Loss = 0.34853292839526245\n",
            "Cost after 295272 iterations : Training Loss =  0.313019151418731; Validation Loss = 0.3485331762455698\n",
            "Cost after 295273 iterations : Training Loss =  0.31301905326627655; Validation Loss = 0.3485326996565493\n",
            "Cost after 295274 iterations : Training Loss =  0.3130190521497932; Validation Loss = 0.34853294750685665\n",
            "Cost after 295275 iterations : Training Loss =  0.31301909966657304; Validation Loss = 0.3485324709178358\n",
            "Cost after 295276 iterations : Training Loss =  0.31301895288085535; Validation Loss = 0.3485327187681434\n",
            "Cost after 295277 iterations : Training Loss =  0.3130191460668698; Validation Loss = 0.348532242179123\n",
            "Cost after 295278 iterations : Training Loss =  0.3130189787619081; Validation Loss = 0.34853249002943015\n",
            "Cost after 295279 iterations : Training Loss =  0.31301906731717566; Validation Loss = 0.34853273787973804\n",
            "Cost after 295280 iterations : Training Loss =  0.31301902516220476; Validation Loss = 0.34853226129071707\n",
            "Cost after 295281 iterations : Training Loss =  0.31301896804823787; Validation Loss = 0.34853250914102435\n",
            "Cost after 295282 iterations : Training Loss =  0.31301907156250136; Validation Loss = 0.3485320325520037\n",
            "Cost after 295283 iterations : Training Loss =  0.3130189042575395; Validation Loss = 0.3485322804023113\n",
            "Cost after 295284 iterations : Training Loss =  0.31301908248455845; Validation Loss = 0.3485325282526188\n",
            "Cost after 295285 iterations : Training Loss =  0.3130189506578362; Validation Loss = 0.34853205166359785\n",
            "Cost after 295286 iterations : Training Loss =  0.3130189832156205; Validation Loss = 0.34853229951390563\n",
            "Cost after 295287 iterations : Training Loss =  0.3130189970581329; Validation Loss = 0.3485318229248847\n",
            "Cost after 295288 iterations : Training Loss =  0.3130188839466826; Validation Loss = 0.3485320707751923\n",
            "Cost after 295289 iterations : Training Loss =  0.3130190434584296; Validation Loss = 0.34853159418617125\n",
            "Cost after 295290 iterations : Training Loss =  0.3130188761534678; Validation Loss = 0.3485318420364786\n",
            "Cost after 295291 iterations : Training Loss =  0.31301899838300307; Validation Loss = 0.3485320898867863\n",
            "Cost after 295292 iterations : Training Loss =  0.31301892255376434; Validation Loss = 0.3485316132977654\n",
            "Cost after 295293 iterations : Training Loss =  0.3130188991140652; Validation Loss = 0.3485318611480733\n",
            "Cost after 295294 iterations : Training Loss =  0.313018968954061; Validation Loss = 0.34853138455905247\n",
            "Cost after 295295 iterations : Training Loss =  0.31301880164909923; Validation Loss = 0.34853163240935986\n",
            "Cost after 295296 iterations : Training Loss =  0.3130190135503857; Validation Loss = 0.3485318802596672\n",
            "Cost after 295297 iterations : Training Loss =  0.31301884804939595; Validation Loss = 0.3485314036706464\n",
            "Cost after 295298 iterations : Training Loss =  0.3130189142814479; Validation Loss = 0.348531651520954\n",
            "Cost after 295299 iterations : Training Loss =  0.3130188944496926; Validation Loss = 0.34853117493193325\n",
            "Cost after 295300 iterations : Training Loss =  0.31301881501250994; Validation Loss = 0.3485314227822406\n",
            "Cost after 295301 iterations : Training Loss =  0.3130189408499894; Validation Loss = 0.3485309461932202\n",
            "Cost after 295302 iterations : Training Loss =  0.31301877354502755; Validation Loss = 0.34853119404352734\n",
            "Cost after 295303 iterations : Training Loss =  0.31301892944883014; Validation Loss = 0.34853144189383506\n",
            "Cost after 295304 iterations : Training Loss =  0.31301881994532416; Validation Loss = 0.3485309653048141\n",
            "Cost after 295305 iterations : Training Loss =  0.3130188301798926; Validation Loss = 0.34853121315512153\n",
            "Cost after 295306 iterations : Training Loss =  0.3130188663456211; Validation Loss = 0.34853073656610095\n",
            "Cost after 295307 iterations : Training Loss =  0.3130187309109547; Validation Loss = 0.34853098441640856\n",
            "Cost after 295308 iterations : Training Loss =  0.3130189127459174; Validation Loss = 0.34853050782738754\n",
            "Cost after 295309 iterations : Training Loss =  0.31301874544095587; Validation Loss = 0.3485307556776951\n",
            "Cost after 295310 iterations : Training Loss =  0.31301884534727503; Validation Loss = 0.348531003528003\n",
            "Cost after 295311 iterations : Training Loss =  0.3130187918412523; Validation Loss = 0.3485305269389821\n",
            "Cost after 295312 iterations : Training Loss =  0.31301874607833713; Validation Loss = 0.34853077478928945\n",
            "Cost after 295313 iterations : Training Loss =  0.313018838241549; Validation Loss = 0.34853029820026865\n",
            "Cost after 295314 iterations : Training Loss =  0.31301867093658736; Validation Loss = 0.3485305460505762\n",
            "Cost after 295315 iterations : Training Loss =  0.31301886051465777; Validation Loss = 0.3485307939008837\n",
            "Cost after 295316 iterations : Training Loss =  0.313018717336884; Validation Loss = 0.3485303173118628\n",
            "Cost after 295317 iterations : Training Loss =  0.31301876124571976; Validation Loss = 0.3485305651621702\n",
            "Cost after 295318 iterations : Training Loss =  0.31301876373718046; Validation Loss = 0.3485300885731493\n",
            "Cost after 295319 iterations : Training Loss =  0.3130186619767821; Validation Loss = 0.34853033642345693\n",
            "Cost after 295320 iterations : Training Loss =  0.3130188101374771; Validation Loss = 0.3485298598344362\n",
            "Cost after 295321 iterations : Training Loss =  0.31301864283251546; Validation Loss = 0.3485301076847437\n",
            "Cost after 295322 iterations : Training Loss =  0.3130187764131026; Validation Loss = 0.34853035553505124\n",
            "Cost after 295323 iterations : Training Loss =  0.3130186892328122; Validation Loss = 0.3485298789460305\n",
            "Cost after 295324 iterations : Training Loss =  0.3130186771441645; Validation Loss = 0.34853012679633805\n",
            "Cost after 295325 iterations : Training Loss =  0.31301873563310856; Validation Loss = 0.34852965020731747\n",
            "Cost after 295326 iterations : Training Loss =  0.31301857787522697; Validation Loss = 0.34852989805762474\n",
            "Cost after 295327 iterations : Training Loss =  0.3130187820334055; Validation Loss = 0.3485294214686039\n",
            "Cost after 295328 iterations : Training Loss =  0.31301861472844367; Validation Loss = 0.34852966931891144\n",
            "Cost after 295329 iterations : Training Loss =  0.3130186923115473; Validation Loss = 0.34852991716921894\n",
            "Cost after 295330 iterations : Training Loss =  0.3130186611287402; Validation Loss = 0.34852944058019836\n",
            "Cost after 295331 iterations : Training Loss =  0.3130185930426094; Validation Loss = 0.3485296884305059\n",
            "Cost after 295332 iterations : Training Loss =  0.31301870752903715; Validation Loss = 0.3485292118414847\n",
            "Cost after 295333 iterations : Training Loss =  0.3130185402240754; Validation Loss = 0.34852945969179217\n",
            "Cost after 295334 iterations : Training Loss =  0.31301870747892996; Validation Loss = 0.34852970754209994\n",
            "Cost after 295335 iterations : Training Loss =  0.3130185866243718; Validation Loss = 0.34852923095307875\n",
            "Cost after 295336 iterations : Training Loss =  0.3130186082099922; Validation Loss = 0.34852947880338647\n",
            "Cost after 295337 iterations : Training Loss =  0.3130186330246684; Validation Loss = 0.3485290022143657\n",
            "Cost after 295338 iterations : Training Loss =  0.31301850894105415; Validation Loss = 0.3485292500646734\n",
            "Cost after 295339 iterations : Training Loss =  0.3130186794249653; Validation Loss = 0.3485287734756523\n",
            "Cost after 295340 iterations : Training Loss =  0.3130185121200036; Validation Loss = 0.3485290213259603\n",
            "Cost after 295341 iterations : Training Loss =  0.3130186233773745; Validation Loss = 0.3485292691762672\n",
            "Cost after 295342 iterations : Training Loss =  0.3130185585203001; Validation Loss = 0.34852879258724684\n",
            "Cost after 295343 iterations : Training Loss =  0.3130185241084369; Validation Loss = 0.3485290404375543\n",
            "Cost after 295344 iterations : Training Loss =  0.3130186049205965; Validation Loss = 0.3485285638485335\n",
            "Cost after 295345 iterations : Training Loss =  0.31301843761563514; Validation Loss = 0.348528811698841\n",
            "Cost after 295346 iterations : Training Loss =  0.31301863854475737; Validation Loss = 0.3485290595491483\n",
            "Cost after 295347 iterations : Training Loss =  0.31301848401593163; Validation Loss = 0.3485285829601276\n",
            "Cost after 295348 iterations : Training Loss =  0.3130185392758195; Validation Loss = 0.3485288308104354\n",
            "Cost after 295349 iterations : Training Loss =  0.31301853041622835; Validation Loss = 0.34852835422141437\n",
            "Cost after 295350 iterations : Training Loss =  0.3130184400068817; Validation Loss = 0.348528602071722\n",
            "Cost after 295351 iterations : Training Loss =  0.313018576816525; Validation Loss = 0.34852812548270123\n",
            "Cost after 295352 iterations : Training Loss =  0.31301840951156334; Validation Loss = 0.3485283733330084\n",
            "Cost after 295353 iterations : Training Loss =  0.31301855444320204; Validation Loss = 0.3485286211833162\n",
            "Cost after 295354 iterations : Training Loss =  0.3130184559118598; Validation Loss = 0.34852814459429526\n",
            "Cost after 295355 iterations : Training Loss =  0.31301845517426413; Validation Loss = 0.3485283924446027\n",
            "Cost after 295356 iterations : Training Loss =  0.3130185023121564; Validation Loss = 0.3485279158555822\n",
            "Cost after 295357 iterations : Training Loss =  0.31301835590532634; Validation Loss = 0.34852816370588957\n",
            "Cost after 295358 iterations : Training Loss =  0.31301854871245305; Validation Loss = 0.34852768711686893\n",
            "Cost after 295359 iterations : Training Loss =  0.3130183814074912; Validation Loss = 0.34852793496717666\n",
            "Cost after 295360 iterations : Training Loss =  0.31301847034164687; Validation Loss = 0.3485281828174838\n",
            "Cost after 295361 iterations : Training Loss =  0.31301842780778805; Validation Loss = 0.348527706228463\n",
            "Cost after 295362 iterations : Training Loss =  0.3130183710727092; Validation Loss = 0.34852795407877063\n",
            "Cost after 295363 iterations : Training Loss =  0.3130184742080846; Validation Loss = 0.34852747748974966\n",
            "Cost after 295364 iterations : Training Loss =  0.3130183069031228; Validation Loss = 0.3485277253400574\n",
            "Cost after 295365 iterations : Training Loss =  0.3130184855090294; Validation Loss = 0.3485279731903647\n",
            "Cost after 295366 iterations : Training Loss =  0.3130183533034196; Validation Loss = 0.348527496601344\n",
            "Cost after 295367 iterations : Training Loss =  0.3130183862400914; Validation Loss = 0.3485277444516514\n",
            "Cost after 295368 iterations : Training Loss =  0.3130183997037162; Validation Loss = 0.3485272678626308\n",
            "Cost after 295369 iterations : Training Loss =  0.31301828697115375; Validation Loss = 0.3485275157129384\n",
            "Cost after 295370 iterations : Training Loss =  0.31301844610401275; Validation Loss = 0.34852703912391764\n",
            "Cost after 295371 iterations : Training Loss =  0.31301827879905114; Validation Loss = 0.3485272869742249\n",
            "Cost after 295372 iterations : Training Loss =  0.31301840140747417; Validation Loss = 0.34852753482453225\n",
            "Cost after 295373 iterations : Training Loss =  0.31301832519934786; Validation Loss = 0.3485270582355115\n",
            "Cost after 295374 iterations : Training Loss =  0.3130183021385362; Validation Loss = 0.3485273060858192\n",
            "Cost after 295375 iterations : Training Loss =  0.3130183715996444; Validation Loss = 0.34852682949679825\n",
            "Cost after 295376 iterations : Training Loss =  0.31301820429468286; Validation Loss = 0.34852707734710636\n",
            "Cost after 295377 iterations : Training Loss =  0.31301841657485674; Validation Loss = 0.34852732519741336\n",
            "Cost after 295378 iterations : Training Loss =  0.3130182506949793; Validation Loss = 0.3485268486083926\n",
            "Cost after 295379 iterations : Training Loss =  0.31301831730591856; Validation Loss = 0.34852709645870006\n",
            "Cost after 295380 iterations : Training Loss =  0.31301829709527607; Validation Loss = 0.34852661986967926\n",
            "Cost after 295381 iterations : Training Loss =  0.313018218036981; Validation Loss = 0.34852686771998703\n",
            "Cost after 295382 iterations : Training Loss =  0.31301834349557267; Validation Loss = 0.3485263911309662\n",
            "Cost after 295383 iterations : Training Loss =  0.3130181761906109; Validation Loss = 0.34852663898127334\n",
            "Cost after 295384 iterations : Training Loss =  0.3130183324733016; Validation Loss = 0.348526886831581\n",
            "Cost after 295385 iterations : Training Loss =  0.3130182225909075; Validation Loss = 0.34852641024256015\n",
            "Cost after 295386 iterations : Training Loss =  0.3130182332043636; Validation Loss = 0.34852665809286804\n",
            "Cost after 295387 iterations : Training Loss =  0.3130182689912039; Validation Loss = 0.3485261815038471\n",
            "Cost after 295388 iterations : Training Loss =  0.31301813393542577; Validation Loss = 0.3485264293541546\n",
            "Cost after 295389 iterations : Training Loss =  0.313018315391501; Validation Loss = 0.3485259527651337\n",
            "Cost after 295390 iterations : Training Loss =  0.313018148086539; Validation Loss = 0.3485262006154413\n",
            "Cost after 295391 iterations : Training Loss =  0.31301824837174613; Validation Loss = 0.34852644846574876\n",
            "Cost after 295392 iterations : Training Loss =  0.31301819448683577; Validation Loss = 0.3485259718767284\n",
            "Cost after 295393 iterations : Training Loss =  0.31301814910280845; Validation Loss = 0.3485262197270359\n",
            "Cost after 295394 iterations : Training Loss =  0.3130182408871323; Validation Loss = 0.3485257431380148\n",
            "Cost after 295395 iterations : Training Loss =  0.3130180735821706; Validation Loss = 0.34852599098832215\n",
            "Cost after 295396 iterations : Training Loss =  0.3130182635391291; Validation Loss = 0.3485262388386295\n",
            "Cost after 295397 iterations : Training Loss =  0.31301811998246726; Validation Loss = 0.348525762249609\n",
            "Cost after 295398 iterations : Training Loss =  0.31301816427019113; Validation Loss = 0.34852601009991646\n",
            "Cost after 295399 iterations : Training Loss =  0.313018166382764; Validation Loss = 0.3485255335108957\n",
            "Cost after 295400 iterations : Training Loss =  0.31301806500125323; Validation Loss = 0.348525781361203\n",
            "Cost after 295401 iterations : Training Loss =  0.31301821278306036; Validation Loss = 0.34852530477218246\n",
            "Cost after 295402 iterations : Training Loss =  0.31301804547809914; Validation Loss = 0.3485255526224902\n",
            "Cost after 295403 iterations : Training Loss =  0.3130181794375736; Validation Loss = 0.34852580047279785\n",
            "Cost after 295404 iterations : Training Loss =  0.31301809187839535; Validation Loss = 0.3485253238837768\n",
            "Cost after 295405 iterations : Training Loss =  0.31301808016863586; Validation Loss = 0.34852557173408416\n",
            "Cost after 295406 iterations : Training Loss =  0.31301813827869235; Validation Loss = 0.34852509514506325\n",
            "Cost after 295407 iterations : Training Loss =  0.31301798089969796; Validation Loss = 0.3485253429953706\n",
            "Cost after 295408 iterations : Training Loss =  0.3130181846789888; Validation Loss = 0.34852486640635005\n",
            "Cost after 295409 iterations : Training Loss =  0.31301801737402696; Validation Loss = 0.34852511425665755\n",
            "Cost after 295410 iterations : Training Loss =  0.31301809533601843; Validation Loss = 0.34852536210696516\n",
            "Cost after 295411 iterations : Training Loss =  0.3130180637743235; Validation Loss = 0.34852488551794414\n",
            "Cost after 295412 iterations : Training Loss =  0.3130179960670806; Validation Loss = 0.3485251333682517\n",
            "Cost after 295413 iterations : Training Loss =  0.31301811017462006; Validation Loss = 0.34852465677923095\n",
            "Cost after 295414 iterations : Training Loss =  0.3130179428696587; Validation Loss = 0.3485249046295384\n",
            "Cost after 295415 iterations : Training Loss =  0.3130181105034011; Validation Loss = 0.348525152479846\n",
            "Cost after 295416 iterations : Training Loss =  0.31301798926995533; Validation Loss = 0.34852467589082525\n",
            "Cost after 295417 iterations : Training Loss =  0.3130180112344632; Validation Loss = 0.3485249237411327\n",
            "Cost after 295418 iterations : Training Loss =  0.31301803567025177; Validation Loss = 0.3485244471521116\n",
            "Cost after 295419 iterations : Training Loss =  0.3130179119655252; Validation Loss = 0.3485246950024195\n",
            "Cost after 295420 iterations : Training Loss =  0.31301808207054854; Validation Loss = 0.34852421841339853\n",
            "Cost after 295421 iterations : Training Loss =  0.31301791476558666; Validation Loss = 0.3485244662637058\n",
            "Cost after 295422 iterations : Training Loss =  0.3130180264018458; Validation Loss = 0.3485247141140137\n",
            "Cost after 295423 iterations : Training Loss =  0.3130179611658836; Validation Loss = 0.3485242375249926\n",
            "Cost after 295424 iterations : Training Loss =  0.3130179271329079; Validation Loss = 0.34852448537530023\n",
            "Cost after 295425 iterations : Training Loss =  0.3130180075661801; Validation Loss = 0.3485240087862795\n",
            "Cost after 295426 iterations : Training Loss =  0.3130178402612182; Validation Loss = 0.3485242566365872\n",
            "Cost after 295427 iterations : Training Loss =  0.3130180415692283; Validation Loss = 0.348524504486895\n",
            "Cost after 295428 iterations : Training Loss =  0.3130178866615148; Validation Loss = 0.3485240278978739\n",
            "Cost after 295429 iterations : Training Loss =  0.31301794230029056; Validation Loss = 0.34852427574818146\n",
            "Cost after 295430 iterations : Training Loss =  0.31301793306181175; Validation Loss = 0.3485237991591602\n",
            "Cost after 295431 iterations : Training Loss =  0.31301784303135266; Validation Loss = 0.3485240470094682\n",
            "Cost after 295432 iterations : Training Loss =  0.31301797946210835; Validation Loss = 0.348523570420447\n",
            "Cost after 295433 iterations : Training Loss =  0.3130178121571464; Validation Loss = 0.34852381827075507\n",
            "Cost after 295434 iterations : Training Loss =  0.31301795746767297; Validation Loss = 0.34852406612106207\n",
            "Cost after 295435 iterations : Training Loss =  0.31301785855744313; Validation Loss = 0.3485235895320415\n",
            "Cost after 295436 iterations : Training Loss =  0.31301785819873523; Validation Loss = 0.34852383738234893\n",
            "Cost after 295437 iterations : Training Loss =  0.31301790495773985; Validation Loss = 0.3485233607933282\n",
            "Cost after 295438 iterations : Training Loss =  0.31301775892979744; Validation Loss = 0.3485236086436356\n",
            "Cost after 295439 iterations : Training Loss =  0.3130179513580363; Validation Loss = 0.34852313205461516\n",
            "Cost after 295440 iterations : Training Loss =  0.3130177840530747; Validation Loss = 0.3485233799049226\n",
            "Cost after 295441 iterations : Training Loss =  0.3130178733661178; Validation Loss = 0.34852362775522994\n",
            "Cost after 295442 iterations : Training Loss =  0.31301783045337145; Validation Loss = 0.3485231511662093\n",
            "Cost after 295443 iterations : Training Loss =  0.3130177740971801; Validation Loss = 0.3485233990165169\n",
            "Cost after 295444 iterations : Training Loss =  0.3130178768536681; Validation Loss = 0.3485229224274957\n",
            "Cost after 295445 iterations : Training Loss =  0.3130177095487063; Validation Loss = 0.34852317027780333\n",
            "Cost after 295446 iterations : Training Loss =  0.3130178885335005; Validation Loss = 0.34852341812811105\n",
            "Cost after 295447 iterations : Training Loss =  0.3130177559490028; Validation Loss = 0.3485229415390902\n",
            "Cost after 295448 iterations : Training Loss =  0.31301778926456264; Validation Loss = 0.3485231893893973\n",
            "Cost after 295449 iterations : Training Loss =  0.31301780234929927; Validation Loss = 0.34852271280037683\n",
            "Cost after 295450 iterations : Training Loss =  0.3130176899956248; Validation Loss = 0.34852296065068417\n",
            "Cost after 295451 iterations : Training Loss =  0.31301784874959593; Validation Loss = 0.3485224840616634\n",
            "Cost after 295452 iterations : Training Loss =  0.31301768144463454; Validation Loss = 0.34852273191197114\n",
            "Cost after 295453 iterations : Training Loss =  0.31301780443194516; Validation Loss = 0.3485229797622783\n",
            "Cost after 295454 iterations : Training Loss =  0.3130177278449311; Validation Loss = 0.34852250317325767\n",
            "Cost after 295455 iterations : Training Loss =  0.3130177051630075; Validation Loss = 0.348522751023565\n",
            "Cost after 295456 iterations : Training Loss =  0.31301777424522775; Validation Loss = 0.3485222744345446\n",
            "Cost after 295457 iterations : Training Loss =  0.3130176069402659; Validation Loss = 0.34852252228485187\n",
            "Cost after 295458 iterations : Training Loss =  0.3130178195993279; Validation Loss = 0.3485227701351596\n",
            "Cost after 295459 iterations : Training Loss =  0.31301765334056264; Validation Loss = 0.34852229354613856\n",
            "Cost after 295460 iterations : Training Loss =  0.3130177203303898; Validation Loss = 0.3485225413964462\n",
            "Cost after 295461 iterations : Training Loss =  0.3130176997408592; Validation Loss = 0.3485220648074255\n",
            "Cost after 295462 iterations : Training Loss =  0.3130176210614522; Validation Loss = 0.348522312657733\n",
            "Cost after 295463 iterations : Training Loss =  0.31301774614115585; Validation Loss = 0.3485218360687125\n",
            "Cost after 295464 iterations : Training Loss =  0.3130175788361942; Validation Loss = 0.3485220839190197\n",
            "Cost after 295465 iterations : Training Loss =  0.3130177354977726; Validation Loss = 0.34852233176932707\n",
            "Cost after 295466 iterations : Training Loss =  0.3130176252364909; Validation Loss = 0.3485218551803064\n",
            "Cost after 295467 iterations : Training Loss =  0.31301763622883455; Validation Loss = 0.3485221030306137\n",
            "Cost after 295468 iterations : Training Loss =  0.3130176716367876; Validation Loss = 0.3485216264415928\n",
            "Cost after 295469 iterations : Training Loss =  0.313017536959897; Validation Loss = 0.3485218742919006\n",
            "Cost after 295470 iterations : Training Loss =  0.313017718037084; Validation Loss = 0.34852139770287976\n",
            "Cost after 295471 iterations : Training Loss =  0.31301755073212234; Validation Loss = 0.3485216455531872\n",
            "Cost after 295472 iterations : Training Loss =  0.31301765139621734; Validation Loss = 0.348521893403495\n",
            "Cost after 295473 iterations : Training Loss =  0.31301759713241917; Validation Loss = 0.34852141681447424\n",
            "Cost after 295474 iterations : Training Loss =  0.3130175521272794; Validation Loss = 0.3485216646647817\n",
            "Cost after 295475 iterations : Training Loss =  0.31301764353271566; Validation Loss = 0.34852118807576077\n",
            "Cost after 295476 iterations : Training Loss =  0.31301747622775394; Validation Loss = 0.3485214359260682\n",
            "Cost after 295477 iterations : Training Loss =  0.31301766656359997; Validation Loss = 0.3485216837763761\n",
            "Cost after 295478 iterations : Training Loss =  0.3130175226280508; Validation Loss = 0.3485212071873552\n",
            "Cost after 295479 iterations : Training Loss =  0.3130175672946623; Validation Loss = 0.34852145503766224\n",
            "Cost after 295480 iterations : Training Loss =  0.31301756902834726; Validation Loss = 0.3485209784486415\n",
            "Cost after 295481 iterations : Training Loss =  0.3130174680257243; Validation Loss = 0.3485212262989491\n",
            "Cost after 295482 iterations : Training Loss =  0.3130176154286436; Validation Loss = 0.3485207497099283\n",
            "Cost after 295483 iterations : Training Loss =  0.31301744812368215; Validation Loss = 0.34852099756023597\n",
            "Cost after 295484 iterations : Training Loss =  0.3130175824620448; Validation Loss = 0.34852124541054336\n",
            "Cost after 295485 iterations : Training Loss =  0.3130174945239788; Validation Loss = 0.3485207688215228\n",
            "Cost after 295486 iterations : Training Loss =  0.3130174831931069; Validation Loss = 0.34852101667183005\n",
            "Cost after 295487 iterations : Training Loss =  0.31301754092427536; Validation Loss = 0.3485205400828093\n",
            "Cost after 295488 iterations : Training Loss =  0.3130173839241692; Validation Loss = 0.3485207879331171\n",
            "Cost after 295489 iterations : Training Loss =  0.3130175873245721; Validation Loss = 0.348520311344096\n",
            "Cost after 295490 iterations : Training Loss =  0.3130174200196104; Validation Loss = 0.34852055919440345\n",
            "Cost after 295491 iterations : Training Loss =  0.3130174983604894; Validation Loss = 0.3485208070447112\n",
            "Cost after 295492 iterations : Training Loss =  0.31301746641990724; Validation Loss = 0.34852033045569036\n",
            "Cost after 295493 iterations : Training Loss =  0.3130173990915518; Validation Loss = 0.34852057830599775\n",
            "Cost after 295494 iterations : Training Loss =  0.3130175128202036; Validation Loss = 0.34852010171697684\n",
            "Cost after 295495 iterations : Training Loss =  0.31301734551524196; Validation Loss = 0.34852034956728445\n",
            "Cost after 295496 iterations : Training Loss =  0.31301751352787216; Validation Loss = 0.3485205974175922\n",
            "Cost after 295497 iterations : Training Loss =  0.31301739191553857; Validation Loss = 0.3485201208285713\n",
            "Cost after 295498 iterations : Training Loss =  0.3130174142589343; Validation Loss = 0.3485203686788789\n",
            "Cost after 295499 iterations : Training Loss =  0.31301743831583506; Validation Loss = 0.3485198920898583\n",
            "Cost after 295500 iterations : Training Loss =  0.31301731498999624; Validation Loss = 0.34852013994016545\n",
            "Cost after 295501 iterations : Training Loss =  0.31301748471613194; Validation Loss = 0.34851966335114454\n",
            "Cost after 295502 iterations : Training Loss =  0.31301731741117006; Validation Loss = 0.34851991120145215\n",
            "Cost after 295503 iterations : Training Loss =  0.313017429426317; Validation Loss = 0.34852015905176004\n",
            "Cost after 295504 iterations : Training Loss =  0.3130173638114669; Validation Loss = 0.3485196824627391\n",
            "Cost after 295505 iterations : Training Loss =  0.313017330157379; Validation Loss = 0.34851993031304646\n",
            "Cost after 295506 iterations : Training Loss =  0.31301741021176344; Validation Loss = 0.3485194537240258\n",
            "Cost after 295507 iterations : Training Loss =  0.31301724290680155; Validation Loss = 0.34851970157433343\n",
            "Cost after 295508 iterations : Training Loss =  0.3130174445936994; Validation Loss = 0.3485199494246406\n",
            "Cost after 295509 iterations : Training Loss =  0.31301728930709843; Validation Loss = 0.34851947283561996\n",
            "Cost after 295510 iterations : Training Loss =  0.31301734532476155; Validation Loss = 0.3485197206859278\n",
            "Cost after 295511 iterations : Training Loss =  0.3130173357073949; Validation Loss = 0.3485192440969064\n",
            "Cost after 295512 iterations : Training Loss =  0.313017246055824; Validation Loss = 0.348519491947214\n",
            "Cost after 295513 iterations : Training Loss =  0.31301738210769153; Validation Loss = 0.3485190153581931\n",
            "Cost after 295514 iterations : Training Loss =  0.31301721480272987; Validation Loss = 0.34851926320850074\n",
            "Cost after 295515 iterations : Training Loss =  0.31301736049214435; Validation Loss = 0.34851951105880835\n",
            "Cost after 295516 iterations : Training Loss =  0.31301726120302675; Validation Loss = 0.34851903446978755\n",
            "Cost after 295517 iterations : Training Loss =  0.3130172612232062; Validation Loss = 0.348519282320095\n",
            "Cost after 295518 iterations : Training Loss =  0.3130173076033231; Validation Loss = 0.34851880573107435\n",
            "Cost after 295519 iterations : Training Loss =  0.3130171619542684; Validation Loss = 0.3485190535813819\n",
            "Cost after 295520 iterations : Training Loss =  0.31301735400361963; Validation Loss = 0.3485185769923608\n",
            "Cost after 295521 iterations : Training Loss =  0.3130171866986581; Validation Loss = 0.3485188248426684\n",
            "Cost after 295522 iterations : Training Loss =  0.31301727639058907; Validation Loss = 0.348519072692976\n",
            "Cost after 295523 iterations : Training Loss =  0.31301723309895485; Validation Loss = 0.3485185961039551\n",
            "Cost after 295524 iterations : Training Loss =  0.3130171771216512; Validation Loss = 0.3485188439542628\n",
            "Cost after 295525 iterations : Training Loss =  0.3130172794992514; Validation Loss = 0.34851836736524194\n",
            "Cost after 295526 iterations : Training Loss =  0.3130171121942895; Validation Loss = 0.3485186152155495\n",
            "Cost after 295527 iterations : Training Loss =  0.3130172915579716; Validation Loss = 0.34851886306585683\n",
            "Cost after 295528 iterations : Training Loss =  0.31301715859458634; Validation Loss = 0.34851838647683614\n",
            "Cost after 295529 iterations : Training Loss =  0.31301719228903374; Validation Loss = 0.3485186343271438\n",
            "Cost after 295530 iterations : Training Loss =  0.3130172049948829; Validation Loss = 0.3485181577381229\n",
            "Cost after 295531 iterations : Training Loss =  0.3130170930200958; Validation Loss = 0.3485184055884304\n",
            "Cost after 295532 iterations : Training Loss =  0.3130172513951795; Validation Loss = 0.3485179289994096\n",
            "Cost after 295533 iterations : Training Loss =  0.31301708409021767; Validation Loss = 0.3485181768497174\n",
            "Cost after 295534 iterations : Training Loss =  0.31301720745641654; Validation Loss = 0.348518424700025\n",
            "Cost after 295535 iterations : Training Loss =  0.3130171304905142; Validation Loss = 0.34851794811100417\n",
            "Cost after 295536 iterations : Training Loss =  0.3130171081874784; Validation Loss = 0.34851819596131123\n",
            "Cost after 295537 iterations : Training Loss =  0.31301717689081116; Validation Loss = 0.3485177193722907\n",
            "Cost after 295538 iterations : Training Loss =  0.3130170095858494; Validation Loss = 0.3485179672225981\n",
            "Cost after 295539 iterations : Training Loss =  0.313017222623799; Validation Loss = 0.3485182150729058\n",
            "Cost after 295540 iterations : Training Loss =  0.313017055986146; Validation Loss = 0.3485177384838846\n",
            "Cost after 295541 iterations : Training Loss =  0.3130171233548611; Validation Loss = 0.34851798633419223\n",
            "Cost after 295542 iterations : Training Loss =  0.3130171023864424; Validation Loss = 0.3485175097451715\n",
            "Cost after 295543 iterations : Training Loss =  0.3130170240859233; Validation Loss = 0.3485177575954789\n",
            "Cost after 295544 iterations : Training Loss =  0.31301714878673936; Validation Loss = 0.34851728100645857\n",
            "Cost after 295545 iterations : Training Loss =  0.3130169814817772; Validation Loss = 0.34851752885676573\n",
            "Cost after 295546 iterations : Training Loss =  0.3130171385222439; Validation Loss = 0.3485177767070733\n",
            "Cost after 295547 iterations : Training Loss =  0.3130170278820742; Validation Loss = 0.3485173001180523\n",
            "Cost after 295548 iterations : Training Loss =  0.31301703925330615; Validation Loss = 0.34851754796836004\n",
            "Cost after 295549 iterations : Training Loss =  0.31301707428237097; Validation Loss = 0.34851707137933946\n",
            "Cost after 295550 iterations : Training Loss =  0.313016939984368; Validation Loss = 0.3485173192296466\n",
            "Cost after 295551 iterations : Training Loss =  0.3130171206826676; Validation Loss = 0.34851684264062555\n",
            "Cost after 295552 iterations : Training Loss =  0.31301695337770563; Validation Loss = 0.3485170904909337\n",
            "Cost after 295553 iterations : Training Loss =  0.3130170544206884; Validation Loss = 0.34851733834124093\n",
            "Cost after 295554 iterations : Training Loss =  0.3130169997780022; Validation Loss = 0.34851686175222013\n",
            "Cost after 295555 iterations : Training Loss =  0.3130169551517505; Validation Loss = 0.3485171096025278\n",
            "Cost after 295556 iterations : Training Loss =  0.3130170461782989; Validation Loss = 0.348516633013507\n",
            "Cost after 295557 iterations : Training Loss =  0.31301687887333723; Validation Loss = 0.3485168808638146\n",
            "Cost after 295558 iterations : Training Loss =  0.31301706958807113; Validation Loss = 0.34851712871412177\n",
            "Cost after 295559 iterations : Training Loss =  0.31301692527363406; Validation Loss = 0.34851665212510113\n",
            "Cost after 295560 iterations : Training Loss =  0.3130169703191332; Validation Loss = 0.3485168999754086\n",
            "Cost after 295561 iterations : Training Loss =  0.31301697167393056; Validation Loss = 0.348516423386388\n",
            "Cost after 295562 iterations : Training Loss =  0.3130168710501954; Validation Loss = 0.34851667123669516\n",
            "Cost after 295563 iterations : Training Loss =  0.3130170180742271; Validation Loss = 0.3485161946476745\n",
            "Cost after 295564 iterations : Training Loss =  0.3130168507692656; Validation Loss = 0.3485164424979821\n",
            "Cost after 295565 iterations : Training Loss =  0.31301698548651585; Validation Loss = 0.34851669034828964\n",
            "Cost after 295566 iterations : Training Loss =  0.3130168971695621; Validation Loss = 0.34851621375926883\n",
            "Cost after 295567 iterations : Training Loss =  0.3130168862175779; Validation Loss = 0.34851646160957644\n",
            "Cost after 295568 iterations : Training Loss =  0.31301694356985876; Validation Loss = 0.34851598502055536\n",
            "Cost after 295569 iterations : Training Loss =  0.3130167869486401; Validation Loss = 0.348516232870863\n",
            "Cost after 295570 iterations : Training Loss =  0.31301698997015553; Validation Loss = 0.3485157562818422\n",
            "Cost after 295571 iterations : Training Loss =  0.3130168226651938; Validation Loss = 0.34851600413214995\n",
            "Cost after 295572 iterations : Training Loss =  0.3130169013849604; Validation Loss = 0.34851625198245745\n",
            "Cost after 295573 iterations : Training Loss =  0.31301686906549003; Validation Loss = 0.34851577539343653\n",
            "Cost after 295574 iterations : Training Loss =  0.31301680211602273; Validation Loss = 0.348516023243744\n",
            "Cost after 295575 iterations : Training Loss =  0.3130169154657871; Validation Loss = 0.34851554665472334\n",
            "Cost after 295576 iterations : Training Loss =  0.3130167481608251; Validation Loss = 0.3485157945050307\n",
            "Cost after 295577 iterations : Training Loss =  0.31301691655234326; Validation Loss = 0.3485160423553383\n",
            "Cost after 295578 iterations : Training Loss =  0.3130167945611223; Validation Loss = 0.34851556576631754\n",
            "Cost after 295579 iterations : Training Loss =  0.31301681728340547; Validation Loss = 0.348515813616625\n",
            "Cost after 295580 iterations : Training Loss =  0.3130168409614185; Validation Loss = 0.34851533702760407\n",
            "Cost after 295581 iterations : Training Loss =  0.3130167180144671; Validation Loss = 0.3485155848779117\n",
            "Cost after 295582 iterations : Training Loss =  0.31301688736171523; Validation Loss = 0.34851510828889126\n",
            "Cost after 295583 iterations : Training Loss =  0.31301672005675324; Validation Loss = 0.3485153561391982\n",
            "Cost after 295584 iterations : Training Loss =  0.31301683245078776; Validation Loss = 0.348515603989506\n",
            "Cost after 295585 iterations : Training Loss =  0.3130167664570502; Validation Loss = 0.3485151274004849\n",
            "Cost after 295586 iterations : Training Loss =  0.31301673318185025; Validation Loss = 0.34851537525079246\n",
            "Cost after 295587 iterations : Training Loss =  0.3130168128573467; Validation Loss = 0.3485148986617716\n",
            "Cost after 295588 iterations : Training Loss =  0.31301664555238473; Validation Loss = 0.3485151465120791\n",
            "Cost after 295589 iterations : Training Loss =  0.3130168476181705; Validation Loss = 0.3485153943623864\n",
            "Cost after 295590 iterations : Training Loss =  0.31301669195268167; Validation Loss = 0.3485149177733661\n",
            "Cost after 295591 iterations : Training Loss =  0.3130167483492325; Validation Loss = 0.34851516562367335\n",
            "Cost after 295592 iterations : Training Loss =  0.3130167383529785; Validation Loss = 0.3485146890346526\n",
            "Cost after 295593 iterations : Training Loss =  0.31301664908029475; Validation Loss = 0.34851493688496005\n",
            "Cost after 295594 iterations : Training Loss =  0.31301678475327477; Validation Loss = 0.34851446029593947\n",
            "Cost after 295595 iterations : Training Loss =  0.3130166174483131; Validation Loss = 0.34851470814624663\n",
            "Cost after 295596 iterations : Training Loss =  0.31301676351661556; Validation Loss = 0.34851495599655435\n",
            "Cost after 295597 iterations : Training Loss =  0.3130166638486099; Validation Loss = 0.34851447940753344\n",
            "Cost after 295598 iterations : Training Loss =  0.3130166642476774; Validation Loss = 0.34851472725784105\n",
            "Cost after 295599 iterations : Training Loss =  0.31301671024890654; Validation Loss = 0.3485142506688203\n",
            "Cost after 295600 iterations : Training Loss =  0.3130165649787396; Validation Loss = 0.34851449851912797\n",
            "Cost after 295601 iterations : Training Loss =  0.313016756649203; Validation Loss = 0.348514021930107\n",
            "Cost after 295602 iterations : Training Loss =  0.3130165893442411; Validation Loss = 0.3485142697804146\n",
            "Cost after 295603 iterations : Training Loss =  0.31301667941506045; Validation Loss = 0.34851451763072194\n",
            "Cost after 295604 iterations : Training Loss =  0.31301663574453775; Validation Loss = 0.3485140410417016\n",
            "Cost after 295605 iterations : Training Loss =  0.3130165801461221; Validation Loss = 0.3485142888920092\n",
            "Cost after 295606 iterations : Training Loss =  0.31301668214483463; Validation Loss = 0.34851381230298784\n",
            "Cost after 295607 iterations : Training Loss =  0.3130165148398729; Validation Loss = 0.3485140601532956\n",
            "Cost after 295608 iterations : Training Loss =  0.3130166945824428; Validation Loss = 0.34851430800360306\n",
            "Cost after 295609 iterations : Training Loss =  0.31301656124016963; Validation Loss = 0.3485138314145823\n",
            "Cost after 295610 iterations : Training Loss =  0.31301659531350495; Validation Loss = 0.3485140792648898\n",
            "Cost after 295611 iterations : Training Loss =  0.31301660764046596; Validation Loss = 0.3485136026758691\n",
            "Cost after 295612 iterations : Training Loss =  0.313016496044567; Validation Loss = 0.3485138505261763\n",
            "Cost after 295613 iterations : Training Loss =  0.31301665404076273; Validation Loss = 0.34851337393715576\n",
            "Cost after 295614 iterations : Training Loss =  0.3130164867358009; Validation Loss = 0.3485136217874631\n",
            "Cost after 295615 iterations : Training Loss =  0.3130166104808874; Validation Loss = 0.34851386963777103\n",
            "Cost after 295616 iterations : Training Loss =  0.3130165331360979; Validation Loss = 0.3485133930487503\n",
            "Cost after 295617 iterations : Training Loss =  0.31301651121194946; Validation Loss = 0.34851364089905756\n",
            "Cost after 295618 iterations : Training Loss =  0.31301657953639433; Validation Loss = 0.34851316431003687\n",
            "Cost after 295619 iterations : Training Loss =  0.3130164122314327; Validation Loss = 0.3485134121603439\n",
            "Cost after 295620 iterations : Training Loss =  0.31301662564827026; Validation Loss = 0.3485136600106516\n",
            "Cost after 295621 iterations : Training Loss =  0.3130164586317292; Validation Loss = 0.3485131834216307\n",
            "Cost after 295622 iterations : Training Loss =  0.3130165263793321; Validation Loss = 0.34851343127193846\n",
            "Cost after 295623 iterations : Training Loss =  0.3130165050320259; Validation Loss = 0.34851295468291754\n",
            "Cost after 295624 iterations : Training Loss =  0.3130164271103943; Validation Loss = 0.34851320253322515\n",
            "Cost after 295625 iterations : Training Loss =  0.31301655143232254; Validation Loss = 0.34851272594420407\n",
            "Cost after 295626 iterations : Training Loss =  0.3130163841273609; Validation Loss = 0.34851297379451185\n",
            "Cost after 295627 iterations : Training Loss =  0.3130165415467148; Validation Loss = 0.34851322164481935\n",
            "Cost after 295628 iterations : Training Loss =  0.3130164305276575; Validation Loss = 0.3485127450557986\n",
            "Cost after 295629 iterations : Training Loss =  0.31301644227777703; Validation Loss = 0.3485129929061061\n",
            "Cost after 295630 iterations : Training Loss =  0.31301647692795415; Validation Loss = 0.34851251631708524\n",
            "Cost after 295631 iterations : Training Loss =  0.31301634300883907; Validation Loss = 0.34851276416739296\n",
            "Cost after 295632 iterations : Training Loss =  0.3130165233282509; Validation Loss = 0.3485122875783722\n",
            "Cost after 295633 iterations : Training Loss =  0.3130163560232891; Validation Loss = 0.3485125354286794\n",
            "Cost after 295634 iterations : Training Loss =  0.3130164574451596; Validation Loss = 0.348512783278987\n",
            "Cost after 295635 iterations : Training Loss =  0.3130164024235856; Validation Loss = 0.3485123066899661\n",
            "Cost after 295636 iterations : Training Loss =  0.3130163581762214; Validation Loss = 0.34851255454027374\n",
            "Cost after 295637 iterations : Training Loss =  0.3130164488238823; Validation Loss = 0.3485120779512529\n",
            "Cost after 295638 iterations : Training Loss =  0.3130162815189207; Validation Loss = 0.3485123258015604\n",
            "Cost after 295639 iterations : Training Loss =  0.3130164726125422; Validation Loss = 0.34851257365186783\n",
            "Cost after 295640 iterations : Training Loss =  0.31301632791921724; Validation Loss = 0.3485120970628472\n",
            "Cost after 295641 iterations : Training Loss =  0.31301637334360416; Validation Loss = 0.34851234491315436\n",
            "Cost after 295642 iterations : Training Loss =  0.3130163743195139; Validation Loss = 0.3485118683241339\n",
            "Cost after 295643 iterations : Training Loss =  0.31301627407466653; Validation Loss = 0.3485121161744415\n",
            "Cost after 295644 iterations : Training Loss =  0.3130164207198106; Validation Loss = 0.3485116395854203\n",
            "Cost after 295645 iterations : Training Loss =  0.3130162534148488; Validation Loss = 0.34851188743572814\n",
            "Cost after 295646 iterations : Training Loss =  0.31301638851098723; Validation Loss = 0.3485121352860356\n",
            "Cost after 295647 iterations : Training Loss =  0.3130162998151456; Validation Loss = 0.3485116586970148\n",
            "Cost after 295648 iterations : Training Loss =  0.313016289242049; Validation Loss = 0.34851190654732234\n",
            "Cost after 295649 iterations : Training Loss =  0.3130163462154423; Validation Loss = 0.3485114299583013\n",
            "Cost after 295650 iterations : Training Loss =  0.3130161899731113; Validation Loss = 0.3485116778086089\n",
            "Cost after 295651 iterations : Training Loss =  0.3130163926157388; Validation Loss = 0.348511201219588\n",
            "Cost after 295652 iterations : Training Loss =  0.313016225310777; Validation Loss = 0.3485114490698959\n",
            "Cost after 295653 iterations : Training Loss =  0.31301630440943173; Validation Loss = 0.34851169692020334\n",
            "Cost after 295654 iterations : Training Loss =  0.3130162717110736; Validation Loss = 0.34851122033118265\n",
            "Cost after 295655 iterations : Training Loss =  0.31301620514049366; Validation Loss = 0.3485114681814902\n",
            "Cost after 295656 iterations : Training Loss =  0.3130163181113702; Validation Loss = 0.3485109915924693\n",
            "Cost after 295657 iterations : Training Loss =  0.3130161508064084; Validation Loss = 0.34851123944277673\n",
            "Cost after 295658 iterations : Training Loss =  0.3130163195768143; Validation Loss = 0.34851148729308407\n",
            "Cost after 295659 iterations : Training Loss =  0.3130161972067052; Validation Loss = 0.34851101070406315\n",
            "Cost after 295660 iterations : Training Loss =  0.3130162203078765; Validation Loss = 0.34851125855437093\n",
            "Cost after 295661 iterations : Training Loss =  0.31301624360700175; Validation Loss = 0.3485107819653504\n",
            "Cost after 295662 iterations : Training Loss =  0.31301612103893867; Validation Loss = 0.34851102981565774\n",
            "Cost after 295663 iterations : Training Loss =  0.3130162900072984; Validation Loss = 0.34851055322663704\n",
            "Cost after 295664 iterations : Training Loss =  0.3130161227023366; Validation Loss = 0.3485108010769441\n",
            "Cost after 295665 iterations : Training Loss =  0.31301623547525875; Validation Loss = 0.3485110489272519\n",
            "Cost after 295666 iterations : Training Loss =  0.3130161691026335; Validation Loss = 0.34851057233823085\n",
            "Cost after 295667 iterations : Training Loss =  0.313016136206321; Validation Loss = 0.34851082018853874\n",
            "Cost after 295668 iterations : Training Loss =  0.31301621550293; Validation Loss = 0.3485103435995177\n",
            "Cost after 295669 iterations : Training Loss =  0.31301604819796813; Validation Loss = 0.348510591449825\n",
            "Cost after 295670 iterations : Training Loss =  0.3130162506426417; Validation Loss = 0.3485108393001326\n",
            "Cost after 295671 iterations : Training Loss =  0.3130160945982648; Validation Loss = 0.34851036271111197\n",
            "Cost after 295672 iterations : Training Loss =  0.31301615137370375; Validation Loss = 0.34851061056141935\n",
            "Cost after 295673 iterations : Training Loss =  0.3130161409985615; Validation Loss = 0.3485101339723988\n",
            "Cost after 295674 iterations : Training Loss =  0.3130160521047657; Validation Loss = 0.34851038182270644\n",
            "Cost after 295675 iterations : Training Loss =  0.3130161873988584; Validation Loss = 0.3485099052336855\n",
            "Cost after 295676 iterations : Training Loss =  0.31301602009389634; Validation Loss = 0.34851015308399286\n",
            "Cost after 295677 iterations : Training Loss =  0.3130161665410865; Validation Loss = 0.3485104009343006\n",
            "Cost after 295678 iterations : Training Loss =  0.31301606649419317; Validation Loss = 0.34850992434527983\n",
            "Cost after 295679 iterations : Training Loss =  0.31301606727214837; Validation Loss = 0.348510172195587\n",
            "Cost after 295680 iterations : Training Loss =  0.3130161128944897; Validation Loss = 0.3485096956065661\n",
            "Cost after 295681 iterations : Training Loss =  0.3130159680032106; Validation Loss = 0.34850994345687375\n",
            "Cost after 295682 iterations : Training Loss =  0.3130161592947865; Validation Loss = 0.34850946686785345\n",
            "Cost after 295683 iterations : Training Loss =  0.31301599198982494; Validation Loss = 0.34850971471816056\n",
            "Cost after 295684 iterations : Training Loss =  0.3130160824395311; Validation Loss = 0.3485099625684683\n",
            "Cost after 295685 iterations : Training Loss =  0.31301603839012127; Validation Loss = 0.3485094859794473\n",
            "Cost after 295686 iterations : Training Loss =  0.3130159831705933; Validation Loss = 0.348509733829755\n",
            "Cost after 295687 iterations : Training Loss =  0.3130160847904181; Validation Loss = 0.3485092572407342\n",
            "Cost after 295688 iterations : Training Loss =  0.31301591748545615; Validation Loss = 0.34850950509104195\n",
            "Cost after 295689 iterations : Training Loss =  0.3130160976069141; Validation Loss = 0.3485097529413491\n",
            "Cost after 295690 iterations : Training Loss =  0.313015963885753; Validation Loss = 0.3485092763523285\n",
            "Cost after 295691 iterations : Training Loss =  0.3130159983379758; Validation Loss = 0.3485095242026356\n",
            "Cost after 295692 iterations : Training Loss =  0.3130160102860495; Validation Loss = 0.34850904761361534\n",
            "Cost after 295693 iterations : Training Loss =  0.3130158990690381; Validation Loss = 0.3485092954639225\n",
            "Cost after 295694 iterations : Training Loss =  0.31301605668634636; Validation Loss = 0.34850881887490176\n",
            "Cost after 295695 iterations : Training Loss =  0.3130158893813845; Validation Loss = 0.34850906672520937\n",
            "Cost after 295696 iterations : Training Loss =  0.3130160135053586; Validation Loss = 0.34850931457551665\n",
            "Cost after 295697 iterations : Training Loss =  0.313015935781681; Validation Loss = 0.34850883798649607\n",
            "Cost after 295698 iterations : Training Loss =  0.3130159142364207; Validation Loss = 0.3485090858368037\n",
            "Cost after 295699 iterations : Training Loss =  0.3130159821819776; Validation Loss = 0.3485086092477828\n",
            "Cost after 295700 iterations : Training Loss =  0.31301581496748276; Validation Loss = 0.34850885709809043\n",
            "Cost after 295701 iterations : Training Loss =  0.3130160285822744; Validation Loss = 0.3485083805090693\n",
            "Cost after 295702 iterations : Training Loss =  0.31301586127731257; Validation Loss = 0.3485086283593769\n",
            "Cost after 295703 iterations : Training Loss =  0.3130159294038033; Validation Loss = 0.34850887620968446\n",
            "Cost after 295704 iterations : Training Loss =  0.31301590767760945; Validation Loss = 0.3485083996206636\n",
            "Cost after 295705 iterations : Training Loss =  0.3130158301348653; Validation Loss = 0.3485086474709712\n",
            "Cost after 295706 iterations : Training Loss =  0.313015954077906; Validation Loss = 0.34850817088195046\n",
            "Cost after 295707 iterations : Training Loss =  0.3130157867729439; Validation Loss = 0.34850841873225835\n",
            "Cost after 295708 iterations : Training Loss =  0.31301594457118614; Validation Loss = 0.34850866658256574\n",
            "Cost after 295709 iterations : Training Loss =  0.31301583317324094; Validation Loss = 0.3485081899935448\n",
            "Cost after 295710 iterations : Training Loss =  0.3130158453022481; Validation Loss = 0.3485084378438523\n",
            "Cost after 295711 iterations : Training Loss =  0.3130158795735375; Validation Loss = 0.3485079612548316\n",
            "Cost after 295712 iterations : Training Loss =  0.3130157460333102; Validation Loss = 0.348508209105139\n",
            "Cost after 295713 iterations : Training Loss =  0.3130159259738341; Validation Loss = 0.3485077325161182\n",
            "Cost after 295714 iterations : Training Loss =  0.31301575866887255; Validation Loss = 0.3485079803664257\n",
            "Cost after 295715 iterations : Training Loss =  0.31301586046963054; Validation Loss = 0.34850822821673305\n",
            "Cost after 295716 iterations : Training Loss =  0.3130158050691692; Validation Loss = 0.34850775162771214\n",
            "Cost after 295717 iterations : Training Loss =  0.3130157612006928; Validation Loss = 0.3485079994780197\n",
            "Cost after 295718 iterations : Training Loss =  0.31301585146946553; Validation Loss = 0.34850752288899906\n",
            "Cost after 295719 iterations : Training Loss =  0.3130156841645039; Validation Loss = 0.34850777073930644\n",
            "Cost after 295720 iterations : Training Loss =  0.31301587563701333; Validation Loss = 0.34850801858961405\n",
            "Cost after 295721 iterations : Training Loss =  0.3130157305648006; Validation Loss = 0.348507542000593\n",
            "Cost after 295722 iterations : Training Loss =  0.31301577636807537; Validation Loss = 0.3485077898509006\n",
            "Cost after 295723 iterations : Training Loss =  0.3130157769650971; Validation Loss = 0.3485073132618801\n",
            "Cost after 295724 iterations : Training Loss =  0.31301567709913747; Validation Loss = 0.34850756111218745\n",
            "Cost after 295725 iterations : Training Loss =  0.31301582336539385; Validation Loss = 0.34850708452316653\n",
            "Cost after 295726 iterations : Training Loss =  0.31301565606043236; Validation Loss = 0.3485073323734741\n",
            "Cost after 295727 iterations : Training Loss =  0.31301579153545805; Validation Loss = 0.34850758022378164\n",
            "Cost after 295728 iterations : Training Loss =  0.3130157024607287; Validation Loss = 0.3485071036347607\n",
            "Cost after 295729 iterations : Training Loss =  0.31301569226652015; Validation Loss = 0.3485073514850682\n",
            "Cost after 295730 iterations : Training Loss =  0.3130157488610255; Validation Loss = 0.34850687489604765\n",
            "Cost after 295731 iterations : Training Loss =  0.3130155929975824; Validation Loss = 0.34850712274635515\n",
            "Cost after 295732 iterations : Training Loss =  0.31301579526132206; Validation Loss = 0.34850664615733434\n",
            "Cost after 295733 iterations : Training Loss =  0.31301562795636023; Validation Loss = 0.3485068940076417\n",
            "Cost after 295734 iterations : Training Loss =  0.3130157074339027; Validation Loss = 0.34850714185794945\n",
            "Cost after 295735 iterations : Training Loss =  0.31301567435665695; Validation Loss = 0.34850666526892843\n",
            "Cost after 295736 iterations : Training Loss =  0.3130156081649648; Validation Loss = 0.34850691311923593\n",
            "Cost after 295737 iterations : Training Loss =  0.31301572075695366; Validation Loss = 0.34850643653021535\n",
            "Cost after 295738 iterations : Training Loss =  0.3130155534519919; Validation Loss = 0.34850668438052285\n",
            "Cost after 295739 iterations : Training Loss =  0.3130157226012855; Validation Loss = 0.3485069322308304\n",
            "Cost after 295740 iterations : Training Loss =  0.31301559985228844; Validation Loss = 0.34850645564180954\n",
            "Cost after 295741 iterations : Training Loss =  0.3130156233323475; Validation Loss = 0.3485067034921174\n",
            "Cost after 295742 iterations : Training Loss =  0.3130156462525852; Validation Loss = 0.34850622690309624\n",
            "Cost after 295743 iterations : Training Loss =  0.3130155240634097; Validation Loss = 0.3485064747534038\n",
            "Cost after 295744 iterations : Training Loss =  0.3130156926528818; Validation Loss = 0.3485059981643827\n",
            "Cost after 295745 iterations : Training Loss =  0.3130155253479199; Validation Loss = 0.3485062460146906\n",
            "Cost after 295746 iterations : Training Loss =  0.3130156384997303; Validation Loss = 0.3485064938649981\n",
            "Cost after 295747 iterations : Training Loss =  0.3130155717482166; Validation Loss = 0.34850601727597735\n",
            "Cost after 295748 iterations : Training Loss =  0.31301553923079223; Validation Loss = 0.3485062651262847\n",
            "Cost after 295749 iterations : Training Loss =  0.3130156181485134; Validation Loss = 0.3485057885372641\n",
            "Cost after 295750 iterations : Training Loss =  0.3130154508435514; Validation Loss = 0.3485060363875712\n",
            "Cost after 295751 iterations : Training Loss =  0.31301565366711287; Validation Loss = 0.3485062842378788\n",
            "Cost after 295752 iterations : Training Loss =  0.3130154972438483; Validation Loss = 0.3485058076488582\n",
            "Cost after 295753 iterations : Training Loss =  0.3130155543981748; Validation Loss = 0.34850605549916575\n",
            "Cost after 295754 iterations : Training Loss =  0.3130155436441448; Validation Loss = 0.3485055789101449\n",
            "Cost after 295755 iterations : Training Loss =  0.31301545512923673; Validation Loss = 0.3485058267604522\n",
            "Cost after 295756 iterations : Training Loss =  0.3130155900444416; Validation Loss = 0.34850535017143175\n",
            "Cost after 295757 iterations : Training Loss =  0.31301542273947985; Validation Loss = 0.34850559802173947\n",
            "Cost after 295758 iterations : Training Loss =  0.3130155695655577; Validation Loss = 0.3485058458720469\n",
            "Cost after 295759 iterations : Training Loss =  0.31301546913977635; Validation Loss = 0.3485053692830259\n",
            "Cost after 295760 iterations : Training Loss =  0.31301547029661947; Validation Loss = 0.3485056171333332\n",
            "Cost after 295761 iterations : Training Loss =  0.31301551554007295; Validation Loss = 0.3485051405443126\n",
            "Cost after 295762 iterations : Training Loss =  0.3130153710276816; Validation Loss = 0.34850538839462003\n",
            "Cost after 295763 iterations : Training Loss =  0.31301556194036956; Validation Loss = 0.34850491180559906\n",
            "Cost after 295764 iterations : Training Loss =  0.313015394635408; Validation Loss = 0.348505159655907\n",
            "Cost after 295765 iterations : Training Loss =  0.31301548546400243; Validation Loss = 0.3485054075062146\n",
            "Cost after 295766 iterations : Training Loss =  0.31301544103570467; Validation Loss = 0.34850493091719337\n",
            "Cost after 295767 iterations : Training Loss =  0.3130153861950643; Validation Loss = 0.348505178767501\n",
            "Cost after 295768 iterations : Training Loss =  0.31301548743600144; Validation Loss = 0.3485047021784804\n",
            "Cost after 295769 iterations : Training Loss =  0.3130153201310394; Validation Loss = 0.348504950028788\n",
            "Cost after 295770 iterations : Training Loss =  0.31301550063138495; Validation Loss = 0.3485051978790955\n",
            "Cost after 295771 iterations : Training Loss =  0.3130153665313361; Validation Loss = 0.3485047212900745\n",
            "Cost after 295772 iterations : Training Loss =  0.3130154013624471; Validation Loss = 0.3485049691403818\n",
            "Cost after 295773 iterations : Training Loss =  0.31301541293163304; Validation Loss = 0.3485044925513611\n",
            "Cost after 295774 iterations : Training Loss =  0.31301530209350914; Validation Loss = 0.3485047404016683\n",
            "Cost after 295775 iterations : Training Loss =  0.3130154593319295; Validation Loss = 0.3485042638126478\n",
            "Cost after 295776 iterations : Training Loss =  0.31301529202696765; Validation Loss = 0.34850451166295526\n",
            "Cost after 295777 iterations : Training Loss =  0.31301541652982967; Validation Loss = 0.3485047595132629\n",
            "Cost after 295778 iterations : Training Loss =  0.3130153384272643; Validation Loss = 0.3485042829242424\n",
            "Cost after 295779 iterations : Training Loss =  0.3130153172608916; Validation Loss = 0.34850453077455\n",
            "Cost after 295780 iterations : Training Loss =  0.3130153848275611; Validation Loss = 0.34850405418552877\n",
            "Cost after 295781 iterations : Training Loss =  0.3130152179919539; Validation Loss = 0.3485043020358365\n",
            "Cost after 295782 iterations : Training Loss =  0.31301543122785747; Validation Loss = 0.34850382544681524\n",
            "Cost after 295783 iterations : Training Loss =  0.3130152639228961; Validation Loss = 0.34850407329712313\n",
            "Cost after 295784 iterations : Training Loss =  0.3130153324282744; Validation Loss = 0.3485043211474306\n",
            "Cost after 295785 iterations : Training Loss =  0.3130153103231927; Validation Loss = 0.34850384455840966\n",
            "Cost after 295786 iterations : Training Loss =  0.3130152331593366; Validation Loss = 0.34850409240871716\n",
            "Cost after 295787 iterations : Training Loss =  0.3130153567234892; Validation Loss = 0.34850361581969624\n",
            "Cost after 295788 iterations : Training Loss =  0.31301518941852763; Validation Loss = 0.3485038636700037\n",
            "Cost after 295789 iterations : Training Loss =  0.31301534759565686; Validation Loss = 0.3485041115203115\n",
            "Cost after 295790 iterations : Training Loss =  0.3130152358188241; Validation Loss = 0.3485036349312909\n",
            "Cost after 295791 iterations : Training Loss =  0.31301524832671923; Validation Loss = 0.34850388278159844\n",
            "Cost after 295792 iterations : Training Loss =  0.3130152822191207; Validation Loss = 0.34850340619257725\n",
            "Cost after 295793 iterations : Training Loss =  0.31301514905778133; Validation Loss = 0.34850365404288514\n",
            "Cost after 295794 iterations : Training Loss =  0.3130153286194174; Validation Loss = 0.34850317745386444\n",
            "Cost after 295795 iterations : Training Loss =  0.31301516131445567; Validation Loss = 0.34850342530417183\n",
            "Cost after 295796 iterations : Training Loss =  0.3130152634941015; Validation Loss = 0.3485036731544794\n",
            "Cost after 295797 iterations : Training Loss =  0.3130152077147525; Validation Loss = 0.3485031965654582\n",
            "Cost after 295798 iterations : Training Loss =  0.31301516422516396; Validation Loss = 0.3485034444157655\n",
            "Cost after 295799 iterations : Training Loss =  0.313015254115049; Validation Loss = 0.3485029678267453\n",
            "Cost after 295800 iterations : Training Loss =  0.31301508681008733; Validation Loss = 0.3485032156770524\n",
            "Cost after 295801 iterations : Training Loss =  0.3130152786614845; Validation Loss = 0.3485034635273604\n",
            "Cost after 295802 iterations : Training Loss =  0.3130151332103839; Validation Loss = 0.34850298693833937\n",
            "Cost after 295803 iterations : Training Loss =  0.3130151793925468; Validation Loss = 0.34850323478864714\n",
            "Cost after 295804 iterations : Training Loss =  0.3130151796106806; Validation Loss = 0.34850275819962606\n",
            "Cost after 295805 iterations : Training Loss =  0.3130150801236086; Validation Loss = 0.3485030060499338\n",
            "Cost after 295806 iterations : Training Loss =  0.31301522601097725; Validation Loss = 0.34850252946091276\n",
            "Cost after 295807 iterations : Training Loss =  0.31301505870601565; Validation Loss = 0.3485027773112205\n",
            "Cost after 295808 iterations : Training Loss =  0.3130151945599291; Validation Loss = 0.34850302516152804\n",
            "Cost after 295809 iterations : Training Loss =  0.3130151051063119; Validation Loss = 0.3485025485725072\n",
            "Cost after 295810 iterations : Training Loss =  0.3130150952909913; Validation Loss = 0.34850279642281423\n",
            "Cost after 295811 iterations : Training Loss =  0.3130151515066089; Validation Loss = 0.34850231983379387\n",
            "Cost after 295812 iterations : Training Loss =  0.31301499602205335; Validation Loss = 0.34850256768410104\n",
            "Cost after 295813 iterations : Training Loss =  0.3130151979069054; Validation Loss = 0.34850209109508046\n",
            "Cost after 295814 iterations : Training Loss =  0.31301503060194374; Validation Loss = 0.34850233894538774\n",
            "Cost after 295815 iterations : Training Loss =  0.3130151104583738; Validation Loss = 0.34850258679569535\n",
            "Cost after 295816 iterations : Training Loss =  0.3130150770022402; Validation Loss = 0.3485021102066748\n",
            "Cost after 295817 iterations : Training Loss =  0.313015011189436; Validation Loss = 0.3485023580569821\n",
            "Cost after 295818 iterations : Training Loss =  0.313015123402537; Validation Loss = 0.3485018814679613\n",
            "Cost after 295819 iterations : Training Loss =  0.3130149560975751; Validation Loss = 0.3485021293182686\n",
            "Cost after 295820 iterations : Training Loss =  0.31301512562575645; Validation Loss = 0.34850237716857635\n",
            "Cost after 295821 iterations : Training Loss =  0.31301500249787173; Validation Loss = 0.3485019005795557\n",
            "Cost after 295822 iterations : Training Loss =  0.31301502635681844; Validation Loss = 0.3485021484298633\n",
            "Cost after 295823 iterations : Training Loss =  0.3130150488981684; Validation Loss = 0.3485016718408423\n",
            "Cost after 295824 iterations : Training Loss =  0.3130149270878807; Validation Loss = 0.34850191969114974\n",
            "Cost after 295825 iterations : Training Loss =  0.3130150952984651; Validation Loss = 0.3485014431021293\n",
            "Cost after 295826 iterations : Training Loss =  0.31301492799350344; Validation Loss = 0.34850169095243644\n",
            "Cost after 295827 iterations : Training Loss =  0.3130150415242013; Validation Loss = 0.3485019388027439\n",
            "Cost after 295828 iterations : Training Loss =  0.3130149743937999; Validation Loss = 0.3485014622137234\n",
            "Cost after 295829 iterations : Training Loss =  0.31301494225526316; Validation Loss = 0.34850171006403075\n",
            "Cost after 295830 iterations : Training Loss =  0.31301502079409677; Validation Loss = 0.3485012334750103\n",
            "Cost after 295831 iterations : Training Loss =  0.3130148534891349; Validation Loss = 0.34850148132531755\n",
            "Cost after 295832 iterations : Training Loss =  0.31301505669158397; Validation Loss = 0.34850172917562505\n",
            "Cost after 295833 iterations : Training Loss =  0.3130148998894317; Validation Loss = 0.34850125258660397\n",
            "Cost after 295834 iterations : Training Loss =  0.31301495742264585; Validation Loss = 0.3485015004369115\n",
            "Cost after 295835 iterations : Training Loss =  0.3130149462897281; Validation Loss = 0.3485010238478908\n",
            "Cost after 295836 iterations : Training Loss =  0.31301485815370816; Validation Loss = 0.34850127169819844\n",
            "Cost after 295837 iterations : Training Loss =  0.3130149926900248; Validation Loss = 0.34850079510917753\n",
            "Cost after 295838 iterations : Training Loss =  0.31301482538506314; Validation Loss = 0.34850104295948525\n",
            "Cost after 295839 iterations : Training Loss =  0.3130149725900283; Validation Loss = 0.34850129080979264\n",
            "Cost after 295840 iterations : Training Loss =  0.3130148717853598; Validation Loss = 0.3485008142207721\n",
            "Cost after 295841 iterations : Training Loss =  0.31301487332109057; Validation Loss = 0.34850106207107956\n",
            "Cost after 295842 iterations : Training Loss =  0.3130149181856566; Validation Loss = 0.34850058548205864\n",
            "Cost after 295843 iterations : Training Loss =  0.3130147740521529; Validation Loss = 0.3485008333323663\n",
            "Cost after 295844 iterations : Training Loss =  0.3130149645859531; Validation Loss = 0.34850035674334556\n",
            "Cost after 295845 iterations : Training Loss =  0.3130147972809914; Validation Loss = 0.34850060459365284\n",
            "Cost after 295846 iterations : Training Loss =  0.31301488848847353; Validation Loss = 0.3485008524439604\n",
            "Cost after 295847 iterations : Training Loss =  0.31301484368128796; Validation Loss = 0.3485003758549398\n",
            "Cost after 295848 iterations : Training Loss =  0.3130147892195355; Validation Loss = 0.3485006237052471\n",
            "Cost after 295849 iterations : Training Loss =  0.31301489008158473; Validation Loss = 0.34850014711622607\n",
            "Cost after 295850 iterations : Training Loss =  0.31301472277662273; Validation Loss = 0.34850039496653396\n",
            "Cost after 295851 iterations : Training Loss =  0.3130149036558558; Validation Loss = 0.3485006428168413\n",
            "Cost after 295852 iterations : Training Loss =  0.31301476917691956; Validation Loss = 0.3485001662278207\n",
            "Cost after 295853 iterations : Training Loss =  0.3130148043869181; Validation Loss = 0.34850041407812804\n",
            "Cost after 295854 iterations : Training Loss =  0.31301481557721617; Validation Loss = 0.34849993748910735\n",
            "Cost after 295855 iterations : Training Loss =  0.3130147051179804; Validation Loss = 0.3485001853394145\n",
            "Cost after 295856 iterations : Training Loss =  0.31301486197751294; Validation Loss = 0.3484997087503935\n",
            "Cost after 295857 iterations : Training Loss =  0.3130146946725511; Validation Loss = 0.3484999566007012\n",
            "Cost after 295858 iterations : Training Loss =  0.31301481955430066; Validation Loss = 0.34850020445100927\n",
            "Cost after 295859 iterations : Training Loss =  0.3130147410728476; Validation Loss = 0.3484997278619878\n",
            "Cost after 295860 iterations : Training Loss =  0.31301472028536276; Validation Loss = 0.3484999757122958\n",
            "Cost after 295861 iterations : Training Loss =  0.31301478747314415; Validation Loss = 0.34849949912327466\n",
            "Cost after 295862 iterations : Training Loss =  0.3130146210164251; Validation Loss = 0.34849974697358216\n",
            "Cost after 295863 iterations : Training Loss =  0.31301483387344115; Validation Loss = 0.34849927038456147\n",
            "Cost after 295864 iterations : Training Loss =  0.3130146665684792; Validation Loss = 0.34849951823486935\n",
            "Cost after 295865 iterations : Training Loss =  0.3130147354527454; Validation Loss = 0.34849976608517635\n",
            "Cost after 295866 iterations : Training Loss =  0.31301471296877603; Validation Loss = 0.3484992894961561\n",
            "Cost after 295867 iterations : Training Loss =  0.3130146361838076; Validation Loss = 0.34849953734646333\n",
            "Cost after 295868 iterations : Training Loss =  0.3130147593690725; Validation Loss = 0.3484990607574426\n",
            "Cost after 295869 iterations : Training Loss =  0.31301459206411075; Validation Loss = 0.3484993086077502\n",
            "Cost after 295870 iterations : Training Loss =  0.31301475062012796; Validation Loss = 0.3484995564580574\n",
            "Cost after 295871 iterations : Training Loss =  0.31301463846440736; Validation Loss = 0.3484990798690366\n",
            "Cost after 295872 iterations : Training Loss =  0.31301465135119; Validation Loss = 0.3484993277193445\n",
            "Cost after 295873 iterations : Training Loss =  0.3130146848647042; Validation Loss = 0.34849885113032314\n",
            "Cost after 295874 iterations : Training Loss =  0.3130145520822524; Validation Loss = 0.3484990989806312\n",
            "Cost after 295875 iterations : Training Loss =  0.31301473126500073; Validation Loss = 0.3484986223916101\n",
            "Cost after 295876 iterations : Training Loss =  0.31301456396003896; Validation Loss = 0.3484988702419177\n",
            "Cost after 295877 iterations : Training Loss =  0.31301466651857296; Validation Loss = 0.3484991180922255\n",
            "Cost after 295878 iterations : Training Loss =  0.3130146103603358; Validation Loss = 0.3484986415032043\n",
            "Cost after 295879 iterations : Training Loss =  0.3130145672496351; Validation Loss = 0.34849888935351203\n",
            "Cost after 295880 iterations : Training Loss =  0.3130146567606322; Validation Loss = 0.34849841276449134\n",
            "Cost after 295881 iterations : Training Loss =  0.31301448945567045; Validation Loss = 0.3484986606147987\n",
            "Cost after 295882 iterations : Training Loss =  0.3130146816859555; Validation Loss = 0.34849890846510606\n",
            "Cost after 295883 iterations : Training Loss =  0.31301453585596717; Validation Loss = 0.34849843187608526\n",
            "Cost after 295884 iterations : Training Loss =  0.3130145824170174; Validation Loss = 0.34849867972639265\n",
            "Cost after 295885 iterations : Training Loss =  0.31301458225626383; Validation Loss = 0.3484982031373723\n",
            "Cost after 295886 iterations : Training Loss =  0.3130144831480797; Validation Loss = 0.34849845098767984\n",
            "Cost after 295887 iterations : Training Loss =  0.31301462865656066; Validation Loss = 0.3484979743986591\n",
            "Cost after 295888 iterations : Training Loss =  0.31301446135159894; Validation Loss = 0.3484982222489662\n",
            "Cost after 295889 iterations : Training Loss =  0.31301459758440026; Validation Loss = 0.3484984700992739\n",
            "Cost after 295890 iterations : Training Loss =  0.3130145077518955; Validation Loss = 0.34849799351025323\n",
            "Cost after 295891 iterations : Training Loss =  0.3130144983154623; Validation Loss = 0.34849824136056073\n",
            "Cost after 295892 iterations : Training Loss =  0.3130145541521923; Validation Loss = 0.3484977647715398\n",
            "Cost after 295893 iterations : Training Loss =  0.3130143990465243; Validation Loss = 0.348498012621847\n",
            "Cost after 295894 iterations : Training Loss =  0.31301460055248864; Validation Loss = 0.34849753603282635\n",
            "Cost after 295895 iterations : Training Loss =  0.313014433247527; Validation Loss = 0.3484977838831341\n",
            "Cost after 295896 iterations : Training Loss =  0.3130145134828448; Validation Loss = 0.34849803173344157\n",
            "Cost after 295897 iterations : Training Loss =  0.31301447964782325; Validation Loss = 0.34849755514442105\n",
            "Cost after 295898 iterations : Training Loss =  0.313014414213907; Validation Loss = 0.3484978029947286\n",
            "Cost after 295899 iterations : Training Loss =  0.3130145260481204; Validation Loss = 0.34849732640570763\n",
            "Cost after 295900 iterations : Training Loss =  0.3130143587431585; Validation Loss = 0.34849757425601513\n",
            "Cost after 295901 iterations : Training Loss =  0.31301452865022755; Validation Loss = 0.3484978221063224\n",
            "Cost after 295902 iterations : Training Loss =  0.31301440514345497; Validation Loss = 0.348497345517302\n",
            "Cost after 295903 iterations : Training Loss =  0.31301442938128987; Validation Loss = 0.34849759336760955\n",
            "Cost after 295904 iterations : Training Loss =  0.3130144515437517; Validation Loss = 0.3484971167785885\n",
            "Cost after 295905 iterations : Training Loss =  0.3130143301123519; Validation Loss = 0.3484973646288961\n",
            "Cost after 295906 iterations : Training Loss =  0.31301449794404834; Validation Loss = 0.3484968880398751\n",
            "Cost after 295907 iterations : Training Loss =  0.3130143306390864; Validation Loss = 0.3484971358901827\n",
            "Cost after 295908 iterations : Training Loss =  0.3130144445486722; Validation Loss = 0.3484973837404901\n",
            "Cost after 295909 iterations : Training Loss =  0.3130143770393833; Validation Loss = 0.34849690715146947\n",
            "Cost after 295910 iterations : Training Loss =  0.31301434527973415; Validation Loss = 0.34849715500177697\n",
            "Cost after 295911 iterations : Training Loss =  0.3130144234396798; Validation Loss = 0.34849667841275633\n",
            "Cost after 295912 iterations : Training Loss =  0.31301425613471834; Validation Loss = 0.3484969262630638\n",
            "Cost after 295913 iterations : Training Loss =  0.31301445971605496; Validation Loss = 0.3484971741133713\n",
            "Cost after 295914 iterations : Training Loss =  0.31301430253501483; Validation Loss = 0.34849669752435036\n",
            "Cost after 295915 iterations : Training Loss =  0.313014360447117; Validation Loss = 0.34849694537465786\n",
            "Cost after 295916 iterations : Training Loss =  0.3130143489353116; Validation Loss = 0.3484964687856371\n",
            "Cost after 295917 iterations : Training Loss =  0.31301426117817926; Validation Loss = 0.3484967166359448\n",
            "Cost after 295918 iterations : Training Loss =  0.3130143953356083; Validation Loss = 0.34849624004692387\n",
            "Cost after 295919 iterations : Training Loss =  0.31301422803064655; Validation Loss = 0.3484964878972312\n",
            "Cost after 295920 iterations : Training Loss =  0.3130143756144996; Validation Loss = 0.3484967357475388\n",
            "Cost after 295921 iterations : Training Loss =  0.31301427443094304; Validation Loss = 0.34849625915851806\n",
            "Cost after 295922 iterations : Training Loss =  0.3130142763455619; Validation Loss = 0.3484965070088255\n",
            "Cost after 295923 iterations : Training Loss =  0.3130143208312395; Validation Loss = 0.3484960304198046\n",
            "Cost after 295924 iterations : Training Loss =  0.31301417707662377; Validation Loss = 0.34849627827011226\n",
            "Cost after 295925 iterations : Training Loss =  0.3130143672315365; Validation Loss = 0.3484958016810915\n",
            "Cost after 295926 iterations : Training Loss =  0.3130141999265746; Validation Loss = 0.3484960495313992\n",
            "Cost after 295927 iterations : Training Loss =  0.31301429151294424; Validation Loss = 0.34849629738170607\n",
            "Cost after 295928 iterations : Training Loss =  0.31301424632687125; Validation Loss = 0.3484958207926857\n",
            "Cost after 295929 iterations : Training Loss =  0.31301419224400656; Validation Loss = 0.3484960686429932\n",
            "Cost after 295930 iterations : Training Loss =  0.31301429272716796; Validation Loss = 0.3484955920539724\n",
            "Cost after 295931 iterations : Training Loss =  0.3130141254222062; Validation Loss = 0.34849583990427974\n",
            "Cost after 295932 iterations : Training Loss =  0.31301430668032687; Validation Loss = 0.3484960877545876\n",
            "Cost after 295933 iterations : Training Loss =  0.3130141718225027; Validation Loss = 0.34849561116556665\n",
            "Cost after 295934 iterations : Training Loss =  0.3130142074113893; Validation Loss = 0.348495859015874\n",
            "Cost after 295935 iterations : Training Loss =  0.3130142182227995; Validation Loss = 0.34849538242685313\n",
            "Cost after 295936 iterations : Training Loss =  0.3130141081424516; Validation Loss = 0.34849563027716085\n",
            "Cost after 295937 iterations : Training Loss =  0.3130142646230962; Validation Loss = 0.34849515368814\n",
            "Cost after 295938 iterations : Training Loss =  0.3130140973181346; Validation Loss = 0.34849540153844744\n",
            "Cost after 295939 iterations : Training Loss =  0.3130142225787719; Validation Loss = 0.34849564938875516\n",
            "Cost after 295940 iterations : Training Loss =  0.31301414371843117; Validation Loss = 0.34849517279973413\n",
            "Cost after 295941 iterations : Training Loss =  0.313014123309834; Validation Loss = 0.348495420650042\n",
            "Cost after 295942 iterations : Training Loss =  0.3130141901187277; Validation Loss = 0.3484949440610209\n",
            "Cost after 295943 iterations : Training Loss =  0.31301402404089584; Validation Loss = 0.3484951919113285\n",
            "Cost after 295944 iterations : Training Loss =  0.3130142365190246; Validation Loss = 0.3484947153223078\n",
            "Cost after 295945 iterations : Training Loss =  0.31301406921406255; Validation Loss = 0.34849496317261514\n",
            "Cost after 295946 iterations : Training Loss =  0.31301413847721654; Validation Loss = 0.34849521102292286\n",
            "Cost after 295947 iterations : Training Loss =  0.3130141156143592; Validation Loss = 0.3484947344339021\n",
            "Cost after 295948 iterations : Training Loss =  0.31301403920827886; Validation Loss = 0.3484949822842097\n",
            "Cost after 295949 iterations : Training Loss =  0.31301416201465593; Validation Loss = 0.3484945056951891\n",
            "Cost after 295950 iterations : Training Loss =  0.313013994709694; Validation Loss = 0.34849475354549614\n",
            "Cost after 295951 iterations : Training Loss =  0.31301415364459934; Validation Loss = 0.34849500139580336\n",
            "Cost after 295952 iterations : Training Loss =  0.3130140411099908; Validation Loss = 0.348494524806783\n",
            "Cost after 295953 iterations : Training Loss =  0.3130140543756612; Validation Loss = 0.3484947726570901\n",
            "Cost after 295954 iterations : Training Loss =  0.3130140875102875; Validation Loss = 0.3484942960680698\n",
            "Cost after 295955 iterations : Training Loss =  0.3130139551067236; Validation Loss = 0.348494543918377\n",
            "Cost after 295956 iterations : Training Loss =  0.31301413391058414; Validation Loss = 0.3484940673293559\n",
            "Cost after 295957 iterations : Training Loss =  0.3130139666056222; Validation Loss = 0.34849431517966395\n",
            "Cost after 295958 iterations : Training Loss =  0.31301406954304406; Validation Loss = 0.34849456302997156\n",
            "Cost after 295959 iterations : Training Loss =  0.31301401300591886; Validation Loss = 0.3484940864409509\n",
            "Cost after 295960 iterations : Training Loss =  0.3130139702741061; Validation Loss = 0.34849433429125826\n",
            "Cost after 295961 iterations : Training Loss =  0.31301405940621574; Validation Loss = 0.3484938577022374\n",
            "Cost after 295962 iterations : Training Loss =  0.31301389210125413; Validation Loss = 0.3484941055525448\n",
            "Cost after 295963 iterations : Training Loss =  0.3130140847104263; Validation Loss = 0.3484943534028524\n",
            "Cost after 295964 iterations : Training Loss =  0.31301393850155057; Validation Loss = 0.3484938768138315\n",
            "Cost after 295965 iterations : Training Loss =  0.31301398544148856; Validation Loss = 0.34849412466413865\n",
            "Cost after 295966 iterations : Training Loss =  0.313013984901847; Validation Loss = 0.34849364807511835\n",
            "Cost after 295967 iterations : Training Loss =  0.3130138861725506; Validation Loss = 0.34849389592542535\n",
            "Cost after 295968 iterations : Training Loss =  0.3130140313021438; Validation Loss = 0.3484934193364049\n",
            "Cost after 295969 iterations : Training Loss =  0.31301386399718206; Validation Loss = 0.34849366718671254\n",
            "Cost after 295970 iterations : Training Loss =  0.31301400060887113; Validation Loss = 0.34849391503702015\n",
            "Cost after 295971 iterations : Training Loss =  0.3130139103974787; Validation Loss = 0.3484934384479992\n",
            "Cost after 295972 iterations : Training Loss =  0.31301390133993345; Validation Loss = 0.3484936862983065\n",
            "Cost after 295973 iterations : Training Loss =  0.3130139567977754; Validation Loss = 0.3484932097092859\n",
            "Cost after 295974 iterations : Training Loss =  0.31301380207099544; Validation Loss = 0.3484934575595932\n",
            "Cost after 295975 iterations : Training Loss =  0.3130140031980721; Validation Loss = 0.34849298097057274\n",
            "Cost after 295976 iterations : Training Loss =  0.3130138358931105; Validation Loss = 0.34849322882088\n",
            "Cost after 295977 iterations : Training Loss =  0.3130139165073159; Validation Loss = 0.34849347667118763\n",
            "Cost after 295978 iterations : Training Loss =  0.3130138822934071; Validation Loss = 0.3484930000821667\n",
            "Cost after 295979 iterations : Training Loss =  0.31301381723837807; Validation Loss = 0.3484932479324747\n",
            "Cost after 295980 iterations : Training Loss =  0.3130139286937035; Validation Loss = 0.3484927713434536\n",
            "Cost after 295981 iterations : Training Loss =  0.31301376138874176; Validation Loss = 0.3484930191937612\n",
            "Cost after 295982 iterations : Training Loss =  0.3130139316746987; Validation Loss = 0.3484932670440689\n",
            "Cost after 295983 iterations : Training Loss =  0.3130138077890384; Validation Loss = 0.34849279045504783\n",
            "Cost after 295984 iterations : Training Loss =  0.31301383240576075; Validation Loss = 0.34849303830535533\n",
            "Cost after 295985 iterations : Training Loss =  0.3130138541893353; Validation Loss = 0.3484925617163346\n",
            "Cost after 295986 iterations : Training Loss =  0.3130137331368228; Validation Loss = 0.348492809566642\n",
            "Cost after 295987 iterations : Training Loss =  0.31301390058963185; Validation Loss = 0.3484923329776212\n",
            "Cost after 295988 iterations : Training Loss =  0.3130137332846701; Validation Loss = 0.3484925808279288\n",
            "Cost after 295989 iterations : Training Loss =  0.3130138475731434; Validation Loss = 0.34849282867823617\n",
            "Cost after 295990 iterations : Training Loss =  0.3130137796849667; Validation Loss = 0.3484923520892153\n",
            "Cost after 295991 iterations : Training Loss =  0.31301374830420564; Validation Loss = 0.34849259993952314\n",
            "Cost after 295992 iterations : Training Loss =  0.31301382608526346; Validation Loss = 0.3484921233505024\n",
            "Cost after 295993 iterations : Training Loss =  0.3130136587803017; Validation Loss = 0.34849237120080984\n",
            "Cost after 295994 iterations : Training Loss =  0.3130138627405261; Validation Loss = 0.34849261905111745\n",
            "Cost after 295995 iterations : Training Loss =  0.313013705180598; Validation Loss = 0.34849214246209587\n",
            "Cost after 295996 iterations : Training Loss =  0.3130137634715882; Validation Loss = 0.34849239031240414\n",
            "Cost after 295997 iterations : Training Loss =  0.3130137515808948; Validation Loss = 0.3484919137233829\n",
            "Cost after 295998 iterations : Training Loss =  0.3130136642026504; Validation Loss = 0.34849216157369073\n",
            "Cost after 295999 iterations : Training Loss =  0.3130137979811917; Validation Loss = 0.34849168498466965\n",
            "Cost after 296000 iterations : Training Loss =  0.3130136306762298; Validation Loss = 0.34849193283497726\n",
            "Cost after 296001 iterations : Training Loss =  0.3130137786389709; Validation Loss = 0.34849218068528487\n",
            "Cost after 296002 iterations : Training Loss =  0.3130136770765265; Validation Loss = 0.34849170409626395\n",
            "Cost after 296003 iterations : Training Loss =  0.31301367937003294; Validation Loss = 0.34849195194657157\n",
            "Cost after 296004 iterations : Training Loss =  0.31301372347682327; Validation Loss = 0.3484914753575508\n",
            "Cost after 296005 iterations : Training Loss =  0.31301358010109487; Validation Loss = 0.3484917232078583\n",
            "Cost after 296006 iterations : Training Loss =  0.3130137698771196; Validation Loss = 0.34849124661883724\n",
            "Cost after 296007 iterations : Training Loss =  0.31301360257215793; Validation Loss = 0.34849149446914496\n",
            "Cost after 296008 iterations : Training Loss =  0.31301369453741557; Validation Loss = 0.3484917423194524\n",
            "Cost after 296009 iterations : Training Loss =  0.31301364897245465; Validation Loss = 0.3484912657304318\n",
            "Cost after 296010 iterations : Training Loss =  0.3130135952684776; Validation Loss = 0.3484915135807391\n",
            "Cost after 296011 iterations : Training Loss =  0.3130136953727514; Validation Loss = 0.3484910369917182\n",
            "Cost after 296012 iterations : Training Loss =  0.31301352806778965; Validation Loss = 0.3484912848420258\n",
            "Cost after 296013 iterations : Training Loss =  0.3130137097047982; Validation Loss = 0.34849153269233357\n",
            "Cost after 296014 iterations : Training Loss =  0.31301357446808614; Validation Loss = 0.3484910561033126\n",
            "Cost after 296015 iterations : Training Loss =  0.31301361043586007; Validation Loss = 0.34849130395362016\n",
            "Cost after 296016 iterations : Training Loss =  0.3130136208683828; Validation Loss = 0.3484908273645989\n",
            "Cost after 296017 iterations : Training Loss =  0.31301351116692216; Validation Loss = 0.3484910752149071\n",
            "Cost after 296018 iterations : Training Loss =  0.31301366726867946; Validation Loss = 0.34849059862588605\n",
            "Cost after 296019 iterations : Training Loss =  0.31301349996371763; Validation Loss = 0.3484908464761939\n",
            "Cost after 296020 iterations : Training Loss =  0.313013625603243; Validation Loss = 0.34849109432650144\n",
            "Cost after 296021 iterations : Training Loss =  0.3130135463640145; Validation Loss = 0.3484906177374801\n",
            "Cost after 296022 iterations : Training Loss =  0.313013526334305; Validation Loss = 0.3484908655877875\n",
            "Cost after 296023 iterations : Training Loss =  0.3130135927643112; Validation Loss = 0.34849038899876716\n",
            "Cost after 296024 iterations : Training Loss =  0.3130134270653673; Validation Loss = 0.34849063684907433\n",
            "Cost after 296025 iterations : Training Loss =  0.3130136391646079; Validation Loss = 0.34849016026005386\n",
            "Cost after 296026 iterations : Training Loss =  0.31301347185964573; Validation Loss = 0.34849040811036075\n",
            "Cost after 296027 iterations : Training Loss =  0.3130135415016877; Validation Loss = 0.3484906559606688\n",
            "Cost after 296028 iterations : Training Loss =  0.31301351825994256; Validation Loss = 0.34849017937164806\n",
            "Cost after 296029 iterations : Training Loss =  0.3130134422327497; Validation Loss = 0.3484904272219556\n",
            "Cost after 296030 iterations : Training Loss =  0.3130135646602393; Validation Loss = 0.34848995063293486\n",
            "Cost after 296031 iterations : Training Loss =  0.3130133973552777; Validation Loss = 0.3484901984832423\n",
            "Cost after 296032 iterations : Training Loss =  0.31301355666907; Validation Loss = 0.3484904463335497\n",
            "Cost after 296033 iterations : Training Loss =  0.3130134437555743; Validation Loss = 0.34848996974452884\n",
            "Cost after 296034 iterations : Training Loss =  0.31301345740013253; Validation Loss = 0.3484902175948365\n",
            "Cost after 296035 iterations : Training Loss =  0.313013490155871; Validation Loss = 0.34848974100581603\n",
            "Cost after 296036 iterations : Training Loss =  0.31301335813119463; Validation Loss = 0.3484899888561233\n",
            "Cost after 296037 iterations : Training Loss =  0.31301353655616737; Validation Loss = 0.3484895122671023\n",
            "Cost after 296038 iterations : Training Loss =  0.31301336925120576; Validation Loss = 0.3484897601174102\n",
            "Cost after 296039 iterations : Training Loss =  0.313013472567515; Validation Loss = 0.3484900079677175\n",
            "Cost after 296040 iterations : Training Loss =  0.3130134156515023; Validation Loss = 0.3484895313786967\n",
            "Cost after 296041 iterations : Training Loss =  0.313013373298577; Validation Loss = 0.3484897792290041\n",
            "Cost after 296042 iterations : Training Loss =  0.313013462051799; Validation Loss = 0.3484893026399835\n",
            "Cost after 296043 iterations : Training Loss =  0.3130132947468373; Validation Loss = 0.3484895504902912\n",
            "Cost after 296044 iterations : Training Loss =  0.3130134877348976; Validation Loss = 0.34848979834059834\n",
            "Cost after 296045 iterations : Training Loss =  0.31301334114713364; Validation Loss = 0.348489321751578\n",
            "Cost after 296046 iterations : Training Loss =  0.3130133884659598; Validation Loss = 0.3484895696018853\n",
            "Cost after 296047 iterations : Training Loss =  0.31301338754743036; Validation Loss = 0.34848909301286435\n",
            "Cost after 296048 iterations : Training Loss =  0.31301328919702187; Validation Loss = 0.348489340863172\n",
            "Cost after 296049 iterations : Training Loss =  0.313013433947727; Validation Loss = 0.3484888642741508\n",
            "Cost after 296050 iterations : Training Loss =  0.31301326664276546; Validation Loss = 0.34848911212445877\n",
            "Cost after 296051 iterations : Training Loss =  0.31301340363334235; Validation Loss = 0.3484893599747664\n",
            "Cost after 296052 iterations : Training Loss =  0.3130133130430621; Validation Loss = 0.3484888833857454\n",
            "Cost after 296053 iterations : Training Loss =  0.3130133043644045; Validation Loss = 0.348489131236053\n",
            "Cost after 296054 iterations : Training Loss =  0.3130133594433586; Validation Loss = 0.3484886546470321\n",
            "Cost after 296055 iterations : Training Loss =  0.31301320509546654; Validation Loss = 0.3484889024973397\n",
            "Cost after 296056 iterations : Training Loss =  0.3130134058436555; Validation Loss = 0.3484884259083185\n",
            "Cost after 296057 iterations : Training Loss =  0.31301323853869356; Validation Loss = 0.3484886737586264\n",
            "Cost after 296058 iterations : Training Loss =  0.31301331953178707; Validation Loss = 0.34848892160893385\n",
            "Cost after 296059 iterations : Training Loss =  0.31301328493899033; Validation Loss = 0.3484884450199131\n",
            "Cost after 296060 iterations : Training Loss =  0.3130132202628492; Validation Loss = 0.3484886928702202\n",
            "Cost after 296061 iterations : Training Loss =  0.3130133313392869; Validation Loss = 0.34848821628119964\n",
            "Cost after 296062 iterations : Training Loss =  0.31301316403432516; Validation Loss = 0.3484884641315075\n",
            "Cost after 296063 iterations : Training Loss =  0.3130133346991697; Validation Loss = 0.3484887119818147\n",
            "Cost after 296064 iterations : Training Loss =  0.3130132104346219; Validation Loss = 0.3484882353927941\n",
            "Cost after 296065 iterations : Training Loss =  0.3130132354302315; Validation Loss = 0.34848848324310167\n",
            "Cost after 296066 iterations : Training Loss =  0.31301325683491854; Validation Loss = 0.34848800665408064\n",
            "Cost after 296067 iterations : Training Loss =  0.31301313616129395; Validation Loss = 0.3484882545043877\n",
            "Cost after 296068 iterations : Training Loss =  0.313013303235215; Validation Loss = 0.3484877779153671\n",
            "Cost after 296069 iterations : Training Loss =  0.3130131359302534; Validation Loss = 0.3484880257656747\n",
            "Cost after 296070 iterations : Training Loss =  0.3130132505976147; Validation Loss = 0.3484882736159824\n",
            "Cost after 296071 iterations : Training Loss =  0.3130131823305501; Validation Loss = 0.34848779702696137\n",
            "Cost after 296072 iterations : Training Loss =  0.3130131513286764; Validation Loss = 0.3484880448772691\n",
            "Cost after 296073 iterations : Training Loss =  0.31301322873084664; Validation Loss = 0.3484875682882482\n",
            "Cost after 296074 iterations : Training Loss =  0.3130130614258851; Validation Loss = 0.34848781613855606\n",
            "Cost after 296075 iterations : Training Loss =  0.313013265764997; Validation Loss = 0.34848806398886323\n",
            "Cost after 296076 iterations : Training Loss =  0.31301310782618175; Validation Loss = 0.34848758739984265\n",
            "Cost after 296077 iterations : Training Loss =  0.3130131664960591; Validation Loss = 0.3484878352501503\n",
            "Cost after 296078 iterations : Training Loss =  0.31301315422647813; Validation Loss = 0.34848735866112895\n",
            "Cost after 296079 iterations : Training Loss =  0.31301306722712097; Validation Loss = 0.3484876065114368\n",
            "Cost after 296080 iterations : Training Loss =  0.31301320062677485; Validation Loss = 0.3484871299224159\n",
            "Cost after 296081 iterations : Training Loss =  0.31301303332181313; Validation Loss = 0.3484873777727233\n",
            "Cost after 296082 iterations : Training Loss =  0.31301318166344183; Validation Loss = 0.3484876256230309\n",
            "Cost after 296083 iterations : Training Loss =  0.31301307972210984; Validation Loss = 0.34848714903401\n",
            "Cost after 296084 iterations : Training Loss =  0.31301308239450415; Validation Loss = 0.3484873968843176\n",
            "Cost after 296085 iterations : Training Loss =  0.31301312612240656; Validation Loss = 0.3484869202952966\n",
            "Cost after 296086 iterations : Training Loss =  0.31301298312556614; Validation Loss = 0.3484871681456041\n",
            "Cost after 296087 iterations : Training Loss =  0.3130131725227029; Validation Loss = 0.3484866915565835\n",
            "Cost after 296088 iterations : Training Loss =  0.31301300521774117; Validation Loss = 0.3484869394068913\n",
            "Cost after 296089 iterations : Training Loss =  0.31301309756188667; Validation Loss = 0.3484871872571985\n",
            "Cost after 296090 iterations : Training Loss =  0.31301305161803816; Validation Loss = 0.3484867106681779\n",
            "Cost after 296091 iterations : Training Loss =  0.31301299829294865; Validation Loss = 0.3484869585184853\n",
            "Cost after 296092 iterations : Training Loss =  0.3130130980183347; Validation Loss = 0.34848648192946485\n",
            "Cost after 296093 iterations : Training Loss =  0.31301293071337294; Validation Loss = 0.348486729779772\n",
            "Cost after 296094 iterations : Training Loss =  0.31301311272926924; Validation Loss = 0.3484869776300792\n",
            "Cost after 296095 iterations : Training Loss =  0.31301297711366954; Validation Loss = 0.3484865010410589\n",
            "Cost after 296096 iterations : Training Loss =  0.31301301346033156; Validation Loss = 0.3484867488913663\n",
            "Cost after 296097 iterations : Training Loss =  0.3130130235139662; Validation Loss = 0.3484862723023454\n",
            "Cost after 296098 iterations : Training Loss =  0.31301291419139327; Validation Loss = 0.34848652015265286\n",
            "Cost after 296099 iterations : Training Loss =  0.31301306991426275; Validation Loss = 0.3484860435636321\n",
            "Cost after 296100 iterations : Training Loss =  0.313012902609301; Validation Loss = 0.34848629141393994\n",
            "Cost after 296101 iterations : Training Loss =  0.3130130286277141; Validation Loss = 0.34848653926424716\n",
            "Cost after 296102 iterations : Training Loss =  0.31301294900959753; Validation Loss = 0.3484860626752267\n",
            "Cost after 296103 iterations : Training Loss =  0.31301292935877606; Validation Loss = 0.34848631052553375\n",
            "Cost after 296104 iterations : Training Loss =  0.3130129954098941; Validation Loss = 0.34848583393651306\n",
            "Cost after 296105 iterations : Training Loss =  0.3130128300898382; Validation Loss = 0.34848608178682067\n",
            "Cost after 296106 iterations : Training Loss =  0.3130130418101909; Validation Loss = 0.3484856051978\n",
            "Cost after 296107 iterations : Training Loss =  0.3130128745052293; Validation Loss = 0.34848585304810753\n",
            "Cost after 296108 iterations : Training Loss =  0.3130129445261588; Validation Loss = 0.3484861008984153\n",
            "Cost after 296109 iterations : Training Loss =  0.3130129209055259; Validation Loss = 0.3484856243093941\n",
            "Cost after 296110 iterations : Training Loss =  0.31301284525722084; Validation Loss = 0.3484858721597017\n",
            "Cost after 296111 iterations : Training Loss =  0.3130129673058225; Validation Loss = 0.3484853955706809\n",
            "Cost after 296112 iterations : Training Loss =  0.31301280000086085; Validation Loss = 0.3484856434209884\n",
            "Cost after 296113 iterations : Training Loss =  0.3130129596935414; Validation Loss = 0.348485891271296\n",
            "Cost after 296114 iterations : Training Loss =  0.31301284640115745; Validation Loss = 0.3484854146822752\n",
            "Cost after 296115 iterations : Training Loss =  0.3130128604246035; Validation Loss = 0.34848566253258256\n",
            "Cost after 296116 iterations : Training Loss =  0.31301289280145406; Validation Loss = 0.3484851859435619\n",
            "Cost after 296117 iterations : Training Loss =  0.3130127611556655; Validation Loss = 0.34848543379386937\n",
            "Cost after 296118 iterations : Training Loss =  0.31301293920175094; Validation Loss = 0.3484849572048485\n",
            "Cost after 296119 iterations : Training Loss =  0.3130127718967891; Validation Loss = 0.348485205055156\n",
            "Cost after 296120 iterations : Training Loss =  0.31301287559198615; Validation Loss = 0.3484854529054634\n",
            "Cost after 296121 iterations : Training Loss =  0.31301281829708566; Validation Loss = 0.34848497631644293\n",
            "Cost after 296122 iterations : Training Loss =  0.3130127763230482; Validation Loss = 0.34848522416675054\n",
            "Cost after 296123 iterations : Training Loss =  0.3130128646973822; Validation Loss = 0.34848474757772946\n",
            "Cost after 296124 iterations : Training Loss =  0.3130126973924206; Validation Loss = 0.3484849954280368\n",
            "Cost after 296125 iterations : Training Loss =  0.31301289075936867; Validation Loss = 0.34848524327834457\n",
            "Cost after 296126 iterations : Training Loss =  0.3130127437927175; Validation Loss = 0.34848476668932377\n",
            "Cost after 296127 iterations : Training Loss =  0.3130127914904306; Validation Loss = 0.34848501453963127\n",
            "Cost after 296128 iterations : Training Loss =  0.3130127901930137; Validation Loss = 0.3484845379506101\n",
            "Cost after 296129 iterations : Training Loss =  0.313012692221493; Validation Loss = 0.3484847858009178\n",
            "Cost after 296130 iterations : Training Loss =  0.3130128365933105; Validation Loss = 0.34848430921189705\n",
            "Cost after 296131 iterations : Training Loss =  0.31301266928834887; Validation Loss = 0.34848455706220444\n",
            "Cost after 296132 iterations : Training Loss =  0.31301280665781345; Validation Loss = 0.34848480491251205\n",
            "Cost after 296133 iterations : Training Loss =  0.3130127156886456; Validation Loss = 0.34848432832349163\n",
            "Cost after 296134 iterations : Training Loss =  0.31301270738887566; Validation Loss = 0.3484845761737991\n",
            "Cost after 296135 iterations : Training Loss =  0.3130127620889422; Validation Loss = 0.3484840995847779\n",
            "Cost after 296136 iterations : Training Loss =  0.3130126081199377; Validation Loss = 0.3484843474350857\n",
            "Cost after 296137 iterations : Training Loss =  0.3130128084892387; Validation Loss = 0.34848387084606486\n",
            "Cost after 296138 iterations : Training Loss =  0.31301264118427696; Validation Loss = 0.34848411869637264\n",
            "Cost after 296139 iterations : Training Loss =  0.31301272255625817; Validation Loss = 0.3484843665466801\n",
            "Cost after 296140 iterations : Training Loss =  0.3130126875845737; Validation Loss = 0.34848388995765944\n",
            "Cost after 296141 iterations : Training Loss =  0.31301262328732044; Validation Loss = 0.3484841378079667\n",
            "Cost after 296142 iterations : Training Loss =  0.31301273398487034; Validation Loss = 0.34848366121894586\n",
            "Cost after 296143 iterations : Training Loss =  0.31301256667990834; Validation Loss = 0.3484839090692533\n",
            "Cost after 296144 iterations : Training Loss =  0.313012737723641; Validation Loss = 0.3484841569195611\n",
            "Cost after 296145 iterations : Training Loss =  0.3130126130802052; Validation Loss = 0.34848368033054\n",
            "Cost after 296146 iterations : Training Loss =  0.31301263845470295; Validation Loss = 0.3484839281808475\n",
            "Cost after 296147 iterations : Training Loss =  0.3130126594805018; Validation Loss = 0.348483451591827\n",
            "Cost after 296148 iterations : Training Loss =  0.31301253918576516; Validation Loss = 0.3484836994421342\n",
            "Cost after 296149 iterations : Training Loss =  0.3130127058807983; Validation Loss = 0.3484832228531135\n",
            "Cost after 296150 iterations : Training Loss =  0.31301253857583683; Validation Loss = 0.3484834707034208\n",
            "Cost after 296151 iterations : Training Loss =  0.31301265362208547; Validation Loss = 0.3484837185537282\n",
            "Cost after 296152 iterations : Training Loss =  0.3130125849761334; Validation Loss = 0.34848324196470787\n",
            "Cost after 296153 iterations : Training Loss =  0.31301255435314773; Validation Loss = 0.3484834898150153\n",
            "Cost after 296154 iterations : Training Loss =  0.31301263137643004; Validation Loss = 0.3484830132259944\n",
            "Cost after 296155 iterations : Training Loss =  0.3130124640714684; Validation Loss = 0.34848326107630184\n",
            "Cost after 296156 iterations : Training Loss =  0.31301266878946815; Validation Loss = 0.34848350892660934\n",
            "Cost after 296157 iterations : Training Loss =  0.313012510471765; Validation Loss = 0.3484830323375884\n",
            "Cost after 296158 iterations : Training Loss =  0.31301256952053025; Validation Loss = 0.34848328018789587\n",
            "Cost after 296159 iterations : Training Loss =  0.3130125568720616; Validation Loss = 0.34848280359887523\n",
            "Cost after 296160 iterations : Training Loss =  0.3130124702515923; Validation Loss = 0.34848305144918273\n",
            "Cost after 296161 iterations : Training Loss =  0.31301260327235825; Validation Loss = 0.34848257486016193\n",
            "Cost after 296162 iterations : Training Loss =  0.3130124359673964; Validation Loss = 0.34848282271046926\n",
            "Cost after 296163 iterations : Training Loss =  0.31301258468791276; Validation Loss = 0.3484830705607767\n",
            "Cost after 296164 iterations : Training Loss =  0.31301248236769313; Validation Loss = 0.348482593971756\n",
            "Cost after 296165 iterations : Training Loss =  0.3130124854189751; Validation Loss = 0.34848284182206357\n",
            "Cost after 296166 iterations : Training Loss =  0.31301252876798963; Validation Loss = 0.34848236523304293\n",
            "Cost after 296167 iterations : Training Loss =  0.31301238615003707; Validation Loss = 0.3484826130833507\n",
            "Cost after 296168 iterations : Training Loss =  0.3130125751682864; Validation Loss = 0.34848213649432985\n",
            "Cost after 296169 iterations : Training Loss =  0.3130124078633247; Validation Loss = 0.34848238434463713\n",
            "Cost after 296170 iterations : Training Loss =  0.3130125005863578; Validation Loss = 0.34848263219494485\n",
            "Cost after 296171 iterations : Training Loss =  0.3130124542636213; Validation Loss = 0.3484821556059241\n",
            "Cost after 296172 iterations : Training Loss =  0.31301240131741986; Validation Loss = 0.34848240345623177\n",
            "Cost after 296173 iterations : Training Loss =  0.31301250066391784; Validation Loss = 0.34848192686721063\n",
            "Cost after 296174 iterations : Training Loss =  0.313012333358956; Validation Loss = 0.3484821747175177\n",
            "Cost after 296175 iterations : Training Loss =  0.31301251575374067; Validation Loss = 0.3484824225678253\n",
            "Cost after 296176 iterations : Training Loss =  0.3130123797592529; Validation Loss = 0.34848194597880466\n",
            "Cost after 296177 iterations : Training Loss =  0.31301241648480227; Validation Loss = 0.3484821938291121\n",
            "Cost after 296178 iterations : Training Loss =  0.31301242615954944; Validation Loss = 0.3484817172400921\n",
            "Cost after 296179 iterations : Training Loss =  0.31301231721586453; Validation Loss = 0.3484819650903988\n",
            "Cost after 296180 iterations : Training Loss =  0.313012472559846; Validation Loss = 0.34848148850137833\n",
            "Cost after 296181 iterations : Training Loss =  0.3130123052548845; Validation Loss = 0.34848173635168594\n",
            "Cost after 296182 iterations : Training Loss =  0.313012431652185; Validation Loss = 0.34848198420199367\n",
            "Cost after 296183 iterations : Training Loss =  0.31301235165518104; Validation Loss = 0.3484815076129729\n",
            "Cost after 296184 iterations : Training Loss =  0.3130123323832471; Validation Loss = 0.34848175546328025\n",
            "Cost after 296185 iterations : Training Loss =  0.31301239805547765; Validation Loss = 0.3484812788742592\n",
            "Cost after 296186 iterations : Training Loss =  0.3130122331143093; Validation Loss = 0.34848152672456684\n",
            "Cost after 296187 iterations : Training Loss =  0.31301244445577436; Validation Loss = 0.34848105013554587\n",
            "Cost after 296188 iterations : Training Loss =  0.31301227715081287; Validation Loss = 0.3484812979858535\n",
            "Cost after 296189 iterations : Training Loss =  0.3130123475506297; Validation Loss = 0.3484815458361609\n",
            "Cost after 296190 iterations : Training Loss =  0.31301232355110936; Validation Loss = 0.3484810692471402\n",
            "Cost after 296191 iterations : Training Loss =  0.3130122482816918; Validation Loss = 0.3484813170974479\n",
            "Cost after 296192 iterations : Training Loss =  0.3130123699514058; Validation Loss = 0.34848084050842704\n",
            "Cost after 296193 iterations : Training Loss =  0.31301220264644414; Validation Loss = 0.3484810883587346\n",
            "Cost after 296194 iterations : Training Loss =  0.31301236271801214; Validation Loss = 0.34848133620904237\n",
            "Cost after 296195 iterations : Training Loss =  0.31301224904674085; Validation Loss = 0.34848085962002123\n",
            "Cost after 296196 iterations : Training Loss =  0.31301226344907473; Validation Loss = 0.3484811074703286\n",
            "Cost after 296197 iterations : Training Loss =  0.3130122954470373; Validation Loss = 0.3484806308813076\n",
            "Cost after 296198 iterations : Training Loss =  0.31301216418013655; Validation Loss = 0.3484808787316155\n",
            "Cost after 296199 iterations : Training Loss =  0.31301234184733395; Validation Loss = 0.3484804021425948\n",
            "Cost after 296200 iterations : Training Loss =  0.3130121745423724; Validation Loss = 0.3484806499929022\n",
            "Cost after 296201 iterations : Training Loss =  0.31301227861645703; Validation Loss = 0.3484808978432096\n",
            "Cost after 296202 iterations : Training Loss =  0.3130122209426691; Validation Loss = 0.3484804212541886\n",
            "Cost after 296203 iterations : Training Loss =  0.3130121793475192; Validation Loss = 0.34848066910449604\n",
            "Cost after 296204 iterations : Training Loss =  0.31301226734296556; Validation Loss = 0.34848019251547546\n",
            "Cost after 296205 iterations : Training Loss =  0.31301210003800406; Validation Loss = 0.348480440365783\n",
            "Cost after 296206 iterations : Training Loss =  0.3130122937838398; Validation Loss = 0.34848068821609074\n",
            "Cost after 296207 iterations : Training Loss =  0.3130121464383006; Validation Loss = 0.34848021162706955\n",
            "Cost after 296208 iterations : Training Loss =  0.31301219451490186; Validation Loss = 0.34848045947737716\n",
            "Cost after 296209 iterations : Training Loss =  0.31301219283859716; Validation Loss = 0.3484799828883566\n",
            "Cost after 296210 iterations : Training Loss =  0.313012095245964; Validation Loss = 0.34848023073866413\n",
            "Cost after 296211 iterations : Training Loss =  0.31301223923889354; Validation Loss = 0.3484797541496431\n",
            "Cost after 296212 iterations : Training Loss =  0.31301207193393227; Validation Loss = 0.34848000199995083\n",
            "Cost after 296213 iterations : Training Loss =  0.31301220968228416; Validation Loss = 0.34848024985025833\n",
            "Cost after 296214 iterations : Training Loss =  0.31301211833422893; Validation Loss = 0.3484797732612375\n",
            "Cost after 296215 iterations : Training Loss =  0.3130121104133467; Validation Loss = 0.3484800211115446\n",
            "Cost after 296216 iterations : Training Loss =  0.31301216473452537; Validation Loss = 0.3484795445225244\n",
            "Cost after 296217 iterations : Training Loss =  0.3130120111444087; Validation Loss = 0.34847979237283183\n",
            "Cost after 296218 iterations : Training Loss =  0.31301221113482186; Validation Loss = 0.3484793157838108\n",
            "Cost after 296219 iterations : Training Loss =  0.31301204382986003; Validation Loss = 0.34847956363411825\n",
            "Cost after 296220 iterations : Training Loss =  0.3130121255807292; Validation Loss = 0.3484798114844257\n",
            "Cost after 296221 iterations : Training Loss =  0.3130120902301568; Validation Loss = 0.34847933489540495\n",
            "Cost after 296222 iterations : Training Loss =  0.313012026311791; Validation Loss = 0.3484795827457128\n",
            "Cost after 296223 iterations : Training Loss =  0.3130121366304537; Validation Loss = 0.3484791061566919\n",
            "Cost after 296224 iterations : Training Loss =  0.31301196932549197; Validation Loss = 0.3484793540069994\n",
            "Cost after 296225 iterations : Training Loss =  0.31301214074811173; Validation Loss = 0.34847960185730686\n",
            "Cost after 296226 iterations : Training Loss =  0.31301201572578835; Validation Loss = 0.3484791252682858\n",
            "Cost after 296227 iterations : Training Loss =  0.313012041479174; Validation Loss = 0.34847937311859367\n",
            "Cost after 296228 iterations : Training Loss =  0.3130120621260851; Validation Loss = 0.3484788965295729\n",
            "Cost after 296229 iterations : Training Loss =  0.3130119422102363; Validation Loss = 0.34847914437988037\n",
            "Cost after 296230 iterations : Training Loss =  0.31301210852638195; Validation Loss = 0.3484786677908598\n",
            "Cost after 296231 iterations : Training Loss =  0.31301194122142006; Validation Loss = 0.34847891564116673\n",
            "Cost after 296232 iterations : Training Loss =  0.3130120566465567; Validation Loss = 0.3484791634914747\n",
            "Cost after 296233 iterations : Training Loss =  0.3130119876217167; Validation Loss = 0.34847868690245376\n",
            "Cost after 296234 iterations : Training Loss =  0.3130119573776187; Validation Loss = 0.34847893475276126\n",
            "Cost after 296235 iterations : Training Loss =  0.3130120340220132; Validation Loss = 0.34847845816374035\n",
            "Cost after 296236 iterations : Training Loss =  0.31301186671705167; Validation Loss = 0.34847870601404785\n",
            "Cost after 296237 iterations : Training Loss =  0.3130120718139391; Validation Loss = 0.3484789538643554\n",
            "Cost after 296238 iterations : Training Loss =  0.3130119131173481; Validation Loss = 0.3484784772753345\n",
            "Cost after 296239 iterations : Training Loss =  0.31301197254500124; Validation Loss = 0.3484787251256421\n",
            "Cost after 296240 iterations : Training Loss =  0.31301195951764493; Validation Loss = 0.34847824853662107\n",
            "Cost after 296241 iterations : Training Loss =  0.3130118732760635; Validation Loss = 0.34847849638692924\n",
            "Cost after 296242 iterations : Training Loss =  0.3130120059179413; Validation Loss = 0.34847801979790805\n",
            "Cost after 296243 iterations : Training Loss =  0.3130118386129797; Validation Loss = 0.34847826764821577\n",
            "Cost after 296244 iterations : Training Loss =  0.31301198771238403; Validation Loss = 0.34847851549852293\n",
            "Cost after 296245 iterations : Training Loss =  0.31301188501327654; Validation Loss = 0.3484780389095023\n",
            "Cost after 296246 iterations : Training Loss =  0.3130118884434463; Validation Loss = 0.3484782867598099\n",
            "Cost after 296247 iterations : Training Loss =  0.3130119314135731; Validation Loss = 0.34847781017078916\n",
            "Cost after 296248 iterations : Training Loss =  0.31301178917450834; Validation Loss = 0.3484780580210965\n",
            "Cost after 296249 iterations : Training Loss =  0.31301197781386975; Validation Loss = 0.3484775814320759\n",
            "Cost after 296250 iterations : Training Loss =  0.31301181050890814; Validation Loss = 0.3484778292823833\n",
            "Cost after 296251 iterations : Training Loss =  0.31301190361082903; Validation Loss = 0.3484780771326908\n",
            "Cost after 296252 iterations : Training Loss =  0.31301185690920463; Validation Loss = 0.34847760054367\n",
            "Cost after 296253 iterations : Training Loss =  0.31301180434189085; Validation Loss = 0.3484778483939774\n",
            "Cost after 296254 iterations : Training Loss =  0.31301190330950124; Validation Loss = 0.34847737180495686\n",
            "Cost after 296255 iterations : Training Loss =  0.31301173600453935; Validation Loss = 0.34847761965526447\n",
            "Cost after 296256 iterations : Training Loss =  0.3130119187782113; Validation Loss = 0.3484778675055719\n",
            "Cost after 296257 iterations : Training Loss =  0.31301178240483635; Validation Loss = 0.3484773909165507\n",
            "Cost after 296258 iterations : Training Loss =  0.3130118195092738; Validation Loss = 0.3484776387668584\n",
            "Cost after 296259 iterations : Training Loss =  0.31301182880513273; Validation Loss = 0.3484771621778376\n",
            "Cost after 296260 iterations : Training Loss =  0.3130117202403356; Validation Loss = 0.34847741002814514\n",
            "Cost after 296261 iterations : Training Loss =  0.31301187520542934; Validation Loss = 0.3484769334391244\n",
            "Cost after 296262 iterations : Training Loss =  0.31301170790046773; Validation Loss = 0.34847718128943184\n",
            "Cost after 296263 iterations : Training Loss =  0.31301183467665616; Validation Loss = 0.34847742913973945\n",
            "Cost after 296264 iterations : Training Loss =  0.31301175430076433; Validation Loss = 0.3484769525507187\n",
            "Cost after 296265 iterations : Training Loss =  0.31301173540771826; Validation Loss = 0.34847720040102614\n",
            "Cost after 296266 iterations : Training Loss =  0.3130118007010609; Validation Loss = 0.3484767238120054\n",
            "Cost after 296267 iterations : Training Loss =  0.3130116361387802; Validation Loss = 0.34847697166231295\n",
            "Cost after 296268 iterations : Training Loss =  0.3130118471013576; Validation Loss = 0.3484764950732918\n",
            "Cost after 296269 iterations : Training Loss =  0.31301167979639605; Validation Loss = 0.3484767429235998\n",
            "Cost after 296270 iterations : Training Loss =  0.313011750575101; Validation Loss = 0.34847699077390704\n",
            "Cost after 296271 iterations : Training Loss =  0.31301172619669265; Validation Loss = 0.3484765141848864\n",
            "Cost after 296272 iterations : Training Loss =  0.31301165130616315; Validation Loss = 0.34847676203519384\n",
            "Cost after 296273 iterations : Training Loss =  0.3130117725969895; Validation Loss = 0.34847628544617265\n",
            "Cost after 296274 iterations : Training Loss =  0.31301160529202754; Validation Loss = 0.34847653329648054\n",
            "Cost after 296275 iterations : Training Loss =  0.3130117657424833; Validation Loss = 0.34847678114678815\n",
            "Cost after 296276 iterations : Training Loss =  0.3130116516923242; Validation Loss = 0.348476304557767\n",
            "Cost after 296277 iterations : Training Loss =  0.31301166647354556; Validation Loss = 0.34847655240807485\n",
            "Cost after 296278 iterations : Training Loss =  0.31301169809262097; Validation Loss = 0.3484760758190541\n",
            "Cost after 296279 iterations : Training Loss =  0.313011567204608; Validation Loss = 0.34847632366936154\n",
            "Cost after 296280 iterations : Training Loss =  0.31301174449291735; Validation Loss = 0.3484758470803408\n",
            "Cost after 296281 iterations : Training Loss =  0.31301157718795564; Validation Loss = 0.34847609493064824\n",
            "Cost after 296282 iterations : Training Loss =  0.31301168164092824; Validation Loss = 0.3484763427809557\n",
            "Cost after 296283 iterations : Training Loss =  0.31301162358825246; Validation Loss = 0.34847586619193477\n",
            "Cost after 296284 iterations : Training Loss =  0.3130115823719904; Validation Loss = 0.34847611404224266\n",
            "Cost after 296285 iterations : Training Loss =  0.31301166998854885; Validation Loss = 0.3484756374532217\n",
            "Cost after 296286 iterations : Training Loss =  0.3130115026835875; Validation Loss = 0.3484758853035295\n",
            "Cost after 296287 iterations : Training Loss =  0.3130116968083108; Validation Loss = 0.34847613315383696\n",
            "Cost after 296288 iterations : Training Loss =  0.31301154908388384; Validation Loss = 0.34847565656481594\n",
            "Cost after 296289 iterations : Training Loss =  0.31301159753937313; Validation Loss = 0.34847590441512316\n",
            "Cost after 296290 iterations : Training Loss =  0.3130115954841805; Validation Loss = 0.34847542782610247\n",
            "Cost after 296291 iterations : Training Loss =  0.31301149827043506; Validation Loss = 0.34847567567640997\n",
            "Cost after 296292 iterations : Training Loss =  0.31301164188447705; Validation Loss = 0.34847519908738933\n",
            "Cost after 296293 iterations : Training Loss =  0.31301147457951517; Validation Loss = 0.34847544693769694\n",
            "Cost after 296294 iterations : Training Loss =  0.3130116127067555; Validation Loss = 0.34847569478800405\n",
            "Cost after 296295 iterations : Training Loss =  0.31301152097981205; Validation Loss = 0.34847521819898375\n",
            "Cost after 296296 iterations : Training Loss =  0.31301151343781775; Validation Loss = 0.3484754660492909\n",
            "Cost after 296297 iterations : Training Loss =  0.31301156738010866; Validation Loss = 0.3484749894602707\n",
            "Cost after 296298 iterations : Training Loss =  0.31301141416888006; Validation Loss = 0.3484752373105778\n",
            "Cost after 296299 iterations : Training Loss =  0.31301161378040554; Validation Loss = 0.3484747607215573\n",
            "Cost after 296300 iterations : Training Loss =  0.3130114464754436; Validation Loss = 0.3484750085718646\n",
            "Cost after 296301 iterations : Training Loss =  0.31301152860520043; Validation Loss = 0.3484752564221721\n",
            "Cost after 296302 iterations : Training Loss =  0.3130114928757401; Validation Loss = 0.3484747798331509\n",
            "Cost after 296303 iterations : Training Loss =  0.3130114293362625; Validation Loss = 0.34847502768345867\n",
            "Cost after 296304 iterations : Training Loss =  0.3130115392760368; Validation Loss = 0.3484745510944377\n",
            "Cost after 296305 iterations : Training Loss =  0.3130113719710754; Validation Loss = 0.3484747989447455\n",
            "Cost after 296306 iterations : Training Loss =  0.31301154377258283; Validation Loss = 0.3484750467950529\n",
            "Cost after 296307 iterations : Training Loss =  0.31301141837137186; Validation Loss = 0.3484745702060323\n",
            "Cost after 296308 iterations : Training Loss =  0.3130114445036451; Validation Loss = 0.34847481805633934\n",
            "Cost after 296309 iterations : Training Loss =  0.31301146477166847; Validation Loss = 0.34847434146731887\n",
            "Cost after 296310 iterations : Training Loss =  0.3130113452347074; Validation Loss = 0.34847458931762615\n",
            "Cost after 296311 iterations : Training Loss =  0.3130115111719651; Validation Loss = 0.34847411272860557\n",
            "Cost after 296312 iterations : Training Loss =  0.31301134386700347; Validation Loss = 0.3484743605789128\n",
            "Cost after 296313 iterations : Training Loss =  0.31301145967102784; Validation Loss = 0.3484746084292206\n",
            "Cost after 296314 iterations : Training Loss =  0.3130113902672998; Validation Loss = 0.34847413184020015\n",
            "Cost after 296315 iterations : Training Loss =  0.3130113604020897; Validation Loss = 0.34847437969050743\n",
            "Cost after 296316 iterations : Training Loss =  0.31301143666759684; Validation Loss = 0.34847390310148646\n",
            "Cost after 296317 iterations : Training Loss =  0.31301126936263496; Validation Loss = 0.348474150951794\n",
            "Cost after 296318 iterations : Training Loss =  0.3130114748384105; Validation Loss = 0.34847439880210174\n",
            "Cost after 296319 iterations : Training Loss =  0.3130113157629316; Validation Loss = 0.3484739222130808\n",
            "Cost after 296320 iterations : Training Loss =  0.31301137556947267; Validation Loss = 0.3484741700633883\n",
            "Cost after 296321 iterations : Training Loss =  0.3130113621632282; Validation Loss = 0.34847369347436746\n",
            "Cost after 296322 iterations : Training Loss =  0.31301127630053444; Validation Loss = 0.34847394132467485\n",
            "Cost after 296323 iterations : Training Loss =  0.3130114085635248; Validation Loss = 0.3484734647356541\n",
            "Cost after 296324 iterations : Training Loss =  0.31301124125856306; Validation Loss = 0.3484737125859617\n",
            "Cost after 296325 iterations : Training Loss =  0.313011390736855; Validation Loss = 0.3484739604362693\n",
            "Cost after 296326 iterations : Training Loss =  0.31301128765885994; Validation Loss = 0.3484734838472485\n",
            "Cost after 296327 iterations : Training Loss =  0.3130112914679171; Validation Loss = 0.3484737316975557\n",
            "Cost after 296328 iterations : Training Loss =  0.31301133405915643; Validation Loss = 0.3484732551085351\n",
            "Cost after 296329 iterations : Training Loss =  0.31301119219897944; Validation Loss = 0.34847350295884255\n",
            "Cost after 296330 iterations : Training Loss =  0.31301138045945315; Validation Loss = 0.348473026369822\n",
            "Cost after 296331 iterations : Training Loss =  0.31301121315449115; Validation Loss = 0.3484732742201295\n",
            "Cost after 296332 iterations : Training Loss =  0.31301130663529975; Validation Loss = 0.3484735220704367\n",
            "Cost after 296333 iterations : Training Loss =  0.313011259554788; Validation Loss = 0.34847304548141594\n",
            "Cost after 296334 iterations : Training Loss =  0.31301120736636184; Validation Loss = 0.34847329333172367\n",
            "Cost after 296335 iterations : Training Loss =  0.31301130595508464; Validation Loss = 0.34847281674270286\n",
            "Cost after 296336 iterations : Training Loss =  0.31301113865012303; Validation Loss = 0.3484730645930101\n",
            "Cost after 296337 iterations : Training Loss =  0.3130113218026823; Validation Loss = 0.3484733124433177\n",
            "Cost after 296338 iterations : Training Loss =  0.3130111850504192; Validation Loss = 0.34847283585429695\n",
            "Cost after 296339 iterations : Training Loss =  0.31301122253374464; Validation Loss = 0.34847308370460434\n",
            "Cost after 296340 iterations : Training Loss =  0.31301123145071624; Validation Loss = 0.3484726071155839\n",
            "Cost after 296341 iterations : Training Loss =  0.3130111232648067; Validation Loss = 0.34847285496589137\n",
            "Cost after 296342 iterations : Training Loss =  0.31301127785101285; Validation Loss = 0.34847237837687045\n",
            "Cost after 296343 iterations : Training Loss =  0.31301111054605135; Validation Loss = 0.34847262622717795\n",
            "Cost after 296344 iterations : Training Loss =  0.31301123770112715; Validation Loss = 0.3484728740774857\n",
            "Cost after 296345 iterations : Training Loss =  0.3130111569463476; Validation Loss = 0.34847239748846476\n",
            "Cost after 296346 iterations : Training Loss =  0.3130111384321894; Validation Loss = 0.34847264533877237\n",
            "Cost after 296347 iterations : Training Loss =  0.31301120334664445; Validation Loss = 0.3484721687497512\n",
            "Cost after 296348 iterations : Training Loss =  0.3130110391632514; Validation Loss = 0.34847241660005895\n",
            "Cost after 296349 iterations : Training Loss =  0.3130112497469409; Validation Loss = 0.3484719400110383\n",
            "Cost after 296350 iterations : Training Loss =  0.31301108244197917; Validation Loss = 0.34847218786134576\n",
            "Cost after 296351 iterations : Training Loss =  0.31301115359957205; Validation Loss = 0.3484724357116532\n",
            "Cost after 296352 iterations : Training Loss =  0.3130111288422758; Validation Loss = 0.34847195912263246\n",
            "Cost after 296353 iterations : Training Loss =  0.31301105433063414; Validation Loss = 0.34847220697293957\n",
            "Cost after 296354 iterations : Training Loss =  0.3130111752425726; Validation Loss = 0.3484717303839189\n",
            "Cost after 296355 iterations : Training Loss =  0.3130110079376109; Validation Loss = 0.3484719782342263\n",
            "Cost after 296356 iterations : Training Loss =  0.3130111687669546; Validation Loss = 0.3484722260845341\n",
            "Cost after 296357 iterations : Training Loss =  0.31301105433790744; Validation Loss = 0.3484717494955134\n",
            "Cost after 296358 iterations : Training Loss =  0.3130110694980165; Validation Loss = 0.34847199734582107\n",
            "Cost after 296359 iterations : Training Loss =  0.31301110073820415; Validation Loss = 0.3484715207567997\n",
            "Cost after 296360 iterations : Training Loss =  0.3130109702290788; Validation Loss = 0.34847176860710805\n",
            "Cost after 296361 iterations : Training Loss =  0.3130111471385005; Validation Loss = 0.3484712920180866\n",
            "Cost after 296362 iterations : Training Loss =  0.31301097983353904; Validation Loss = 0.3484715398683943\n",
            "Cost after 296363 iterations : Training Loss =  0.3130110846653993; Validation Loss = 0.3484717877187021\n",
            "Cost after 296364 iterations : Training Loss =  0.31301102623383575; Validation Loss = 0.3484713111296809\n",
            "Cost after 296365 iterations : Training Loss =  0.3130109853964613; Validation Loss = 0.34847155897998816\n",
            "Cost after 296366 iterations : Training Loss =  0.31301107263413236; Validation Loss = 0.34847108239096763\n",
            "Cost after 296367 iterations : Training Loss =  0.31301090532917053; Validation Loss = 0.3484713302412753\n",
            "Cost after 296368 iterations : Training Loss =  0.3130110998327821; Validation Loss = 0.3484715780915829\n",
            "Cost after 296369 iterations : Training Loss =  0.3130109517294671; Validation Loss = 0.348471101502562\n",
            "Cost after 296370 iterations : Training Loss =  0.313011000563844; Validation Loss = 0.34847134935286944\n",
            "Cost after 296371 iterations : Training Loss =  0.31301099812976374; Validation Loss = 0.3484708727638485\n",
            "Cost after 296372 iterations : Training Loss =  0.31301090129490605; Validation Loss = 0.34847112061415586\n",
            "Cost after 296373 iterations : Training Loss =  0.3130110445300608; Validation Loss = 0.34847064402513495\n",
            "Cost after 296374 iterations : Training Loss =  0.3130108772250988; Validation Loss = 0.3484708918754427\n",
            "Cost after 296375 iterations : Training Loss =  0.3130110157312268; Validation Loss = 0.3484711397257504\n",
            "Cost after 296376 iterations : Training Loss =  0.31301092362539545; Validation Loss = 0.34847066313672953\n",
            "Cost after 296377 iterations : Training Loss =  0.31301091646228885; Validation Loss = 0.34847091098703714\n",
            "Cost after 296378 iterations : Training Loss =  0.31301097002569217; Validation Loss = 0.3484704343980164\n",
            "Cost after 296379 iterations : Training Loss =  0.313010817193351; Validation Loss = 0.34847068224832384\n",
            "Cost after 296380 iterations : Training Loss =  0.3130110164259887; Validation Loss = 0.34847020565930314\n",
            "Cost after 296381 iterations : Training Loss =  0.313010849121027; Validation Loss = 0.3484704535096104\n",
            "Cost after 296382 iterations : Training Loss =  0.31301093162967136; Validation Loss = 0.3484707013599183\n",
            "Cost after 296383 iterations : Training Loss =  0.31301089552132366; Validation Loss = 0.34847022477089723\n",
            "Cost after 296384 iterations : Training Loss =  0.3130108323607334; Validation Loss = 0.3484704726212048\n",
            "Cost after 296385 iterations : Training Loss =  0.31301094192162027; Validation Loss = 0.3484699960321839\n",
            "Cost after 296386 iterations : Training Loss =  0.31301077461665844; Validation Loss = 0.34847024388249154\n",
            "Cost after 296387 iterations : Training Loss =  0.31301094679705393; Validation Loss = 0.348470491732799\n",
            "Cost after 296388 iterations : Training Loss =  0.31301082101695504; Validation Loss = 0.3484700151437783\n",
            "Cost after 296389 iterations : Training Loss =  0.3130108475281163; Validation Loss = 0.34847026299408584\n",
            "Cost after 296390 iterations : Training Loss =  0.3130108674172519; Validation Loss = 0.3484697864050651\n",
            "Cost after 296391 iterations : Training Loss =  0.31301074825917824; Validation Loss = 0.3484700342553724\n",
            "Cost after 296392 iterations : Training Loss =  0.3130109138175483; Validation Loss = 0.3484695576663516\n",
            "Cost after 296393 iterations : Training Loss =  0.31301074651258676; Validation Loss = 0.348469805516659\n",
            "Cost after 296394 iterations : Training Loss =  0.31301086269549877; Validation Loss = 0.34847005336696707\n",
            "Cost after 296395 iterations : Training Loss =  0.31301079291288325; Validation Loss = 0.34846957677794577\n",
            "Cost after 296396 iterations : Training Loss =  0.31301076342656087; Validation Loss = 0.34846982462825365\n",
            "Cost after 296397 iterations : Training Loss =  0.31301083931317986; Validation Loss = 0.34846934803923263\n",
            "Cost after 296398 iterations : Training Loss =  0.3130106720082186; Validation Loss = 0.34846959588954024\n",
            "Cost after 296399 iterations : Training Loss =  0.3130108778628814; Validation Loss = 0.3484698437398474\n",
            "Cost after 296400 iterations : Training Loss =  0.31301071840851485; Validation Loss = 0.34846936715082677\n",
            "Cost after 296401 iterations : Training Loss =  0.3130107785939437; Validation Loss = 0.3484696150011344\n",
            "Cost after 296402 iterations : Training Loss =  0.31301076480881196; Validation Loss = 0.3484691384121137\n",
            "Cost after 296403 iterations : Training Loss =  0.3130106793250057; Validation Loss = 0.34846938626242113\n",
            "Cost after 296404 iterations : Training Loss =  0.3130108112091083; Validation Loss = 0.34846890967340033\n",
            "Cost after 296405 iterations : Training Loss =  0.3130106439041465; Validation Loss = 0.34846915752370794\n",
            "Cost after 296406 iterations : Training Loss =  0.31301079376132607; Validation Loss = 0.34846940537401544\n",
            "Cost after 296407 iterations : Training Loss =  0.31301069030444295; Validation Loss = 0.34846892878499464\n",
            "Cost after 296408 iterations : Training Loss =  0.31301069449238833; Validation Loss = 0.34846917663530236\n",
            "Cost after 296409 iterations : Training Loss =  0.3130107367047399; Validation Loss = 0.34846870004628144\n",
            "Cost after 296410 iterations : Training Loss =  0.3130105952234505; Validation Loss = 0.3484689478965885\n",
            "Cost after 296411 iterations : Training Loss =  0.3130107831050363; Validation Loss = 0.3484684713075678\n",
            "Cost after 296412 iterations : Training Loss =  0.31301061580007455; Validation Loss = 0.3484687191578756\n",
            "Cost after 296413 iterations : Training Loss =  0.31301070965977085; Validation Loss = 0.3484689670081832\n",
            "Cost after 296414 iterations : Training Loss =  0.3130106622003712; Validation Loss = 0.3484684904191623\n",
            "Cost after 296415 iterations : Training Loss =  0.31301061039083317; Validation Loss = 0.3484687382694698\n",
            "Cost after 296416 iterations : Training Loss =  0.3130107086006678; Validation Loss = 0.3484682616804487\n",
            "Cost after 296417 iterations : Training Loss =  0.31301054129570605; Validation Loss = 0.3484685095307565\n",
            "Cost after 296418 iterations : Training Loss =  0.3130107248271536; Validation Loss = 0.34846875738106403\n",
            "Cost after 296419 iterations : Training Loss =  0.31301058769600304; Validation Loss = 0.34846828079204345\n",
            "Cost after 296420 iterations : Training Loss =  0.31301062555821574; Validation Loss = 0.34846852864235084\n",
            "Cost after 296421 iterations : Training Loss =  0.3130106340962993; Validation Loss = 0.3484680520533297\n",
            "Cost after 296422 iterations : Training Loss =  0.313010526289278; Validation Loss = 0.3484682999036374\n",
            "Cost after 296423 iterations : Training Loss =  0.31301068049659614; Validation Loss = 0.34846782331461656\n",
            "Cost after 296424 iterations : Training Loss =  0.3130105131916344; Validation Loss = 0.3484680711649241\n",
            "Cost after 296425 iterations : Training Loss =  0.3130106407255984; Validation Loss = 0.3484683190152315\n",
            "Cost after 296426 iterations : Training Loss =  0.3130105595919308; Validation Loss = 0.3484678424262108\n",
            "Cost after 296427 iterations : Training Loss =  0.3130105414566601; Validation Loss = 0.3484680902765183\n",
            "Cost after 296428 iterations : Training Loss =  0.31301060599222763; Validation Loss = 0.3484676136874977\n",
            "Cost after 296429 iterations : Training Loss =  0.31301044218772267; Validation Loss = 0.34846786153780507\n",
            "Cost after 296430 iterations : Training Loss =  0.3130106523925242; Validation Loss = 0.3484673849487842\n",
            "Cost after 296431 iterations : Training Loss =  0.3130104850875625; Validation Loss = 0.3484676327990917\n",
            "Cost after 296432 iterations : Training Loss =  0.31301055662404303; Validation Loss = 0.34846788064939915\n",
            "Cost after 296433 iterations : Training Loss =  0.31301053148785907; Validation Loss = 0.3484674040603783\n",
            "Cost after 296434 iterations : Training Loss =  0.3130104573551052; Validation Loss = 0.3484676519106863\n",
            "Cost after 296435 iterations : Training Loss =  0.3130105778881561; Validation Loss = 0.3484671753216652\n",
            "Cost after 296436 iterations : Training Loss =  0.3130104105831943; Validation Loss = 0.34846742317197255\n",
            "Cost after 296437 iterations : Training Loss =  0.31301057179142555; Validation Loss = 0.34846767102228\n",
            "Cost after 296438 iterations : Training Loss =  0.31301045698349067; Validation Loss = 0.34846719443325946\n",
            "Cost after 296439 iterations : Training Loss =  0.31301047252248787; Validation Loss = 0.3484674422835672\n",
            "Cost after 296440 iterations : Training Loss =  0.3130105033837877; Validation Loss = 0.3484669656945464\n",
            "Cost after 296441 iterations : Training Loss =  0.31301037325354975; Validation Loss = 0.34846721354485355\n",
            "Cost after 296442 iterations : Training Loss =  0.31301054978408416; Validation Loss = 0.3484667369558328\n",
            "Cost after 296443 iterations : Training Loss =  0.31301038247912216; Validation Loss = 0.34846698480614036\n",
            "Cost after 296444 iterations : Training Loss =  0.3130104876898705; Validation Loss = 0.34846723265644786\n",
            "Cost after 296445 iterations : Training Loss =  0.31301042887941893; Validation Loss = 0.34846675606742705\n",
            "Cost after 296446 iterations : Training Loss =  0.3130103884209329; Validation Loss = 0.34846700391773494\n",
            "Cost after 296447 iterations : Training Loss =  0.31301047527971565; Validation Loss = 0.34846652732871386\n",
            "Cost after 296448 iterations : Training Loss =  0.3130103079747539; Validation Loss = 0.3484667751790213\n",
            "Cost after 296449 iterations : Training Loss =  0.313010502857253; Validation Loss = 0.34846702302932886\n",
            "Cost after 296450 iterations : Training Loss =  0.3130103543750506; Validation Loss = 0.34846654644030806\n",
            "Cost after 296451 iterations : Training Loss =  0.31301040358831517; Validation Loss = 0.34846679429061594\n",
            "Cost after 296452 iterations : Training Loss =  0.31301040077534714; Validation Loss = 0.3484663177015947\n",
            "Cost after 296453 iterations : Training Loss =  0.31301030431937715; Validation Loss = 0.34846656555190225\n",
            "Cost after 296454 iterations : Training Loss =  0.31301044717564386; Validation Loss = 0.3484660889628816\n",
            "Cost after 296455 iterations : Training Loss =  0.3130102798706822; Validation Loss = 0.3484663368131888\n",
            "Cost after 296456 iterations : Training Loss =  0.31301041875569807; Validation Loss = 0.34846658466349645\n",
            "Cost after 296457 iterations : Training Loss =  0.3130103262709788; Validation Loss = 0.3484661080744759\n",
            "Cost after 296458 iterations : Training Loss =  0.31301031948675984; Validation Loss = 0.3484663559247829\n",
            "Cost after 296459 iterations : Training Loss =  0.3130103726712756; Validation Loss = 0.34846587933576245\n",
            "Cost after 296460 iterations : Training Loss =  0.31301022021782204; Validation Loss = 0.3484661271860699\n",
            "Cost after 296461 iterations : Training Loss =  0.313010419071572; Validation Loss = 0.3484656505970493\n",
            "Cost after 296462 iterations : Training Loss =  0.31301025176661024; Validation Loss = 0.34846589844735676\n",
            "Cost after 296463 iterations : Training Loss =  0.3130103346541424; Validation Loss = 0.3484661462976639\n",
            "Cost after 296464 iterations : Training Loss =  0.31301029816690695; Validation Loss = 0.34846566970864346\n",
            "Cost after 296465 iterations : Training Loss =  0.3130102353852045; Validation Loss = 0.3484659175589508\n",
            "Cost after 296466 iterations : Training Loss =  0.3130103445672035; Validation Loss = 0.3484654409699301\n",
            "Cost after 296467 iterations : Training Loss =  0.3130101772622416; Validation Loss = 0.3484656888202375\n",
            "Cost after 296468 iterations : Training Loss =  0.3130103498215252; Validation Loss = 0.3484659366705452\n",
            "Cost after 296469 iterations : Training Loss =  0.31301022366253844; Validation Loss = 0.34846546008152446\n",
            "Cost after 296470 iterations : Training Loss =  0.31301025055258735; Validation Loss = 0.34846570793183157\n",
            "Cost after 296471 iterations : Training Loss =  0.3130102700628353; Validation Loss = 0.34846523134281127\n",
            "Cost after 296472 iterations : Training Loss =  0.31301015128364923; Validation Loss = 0.3484654791931183\n",
            "Cost after 296473 iterations : Training Loss =  0.313010316463132; Validation Loss = 0.34846500260409813\n",
            "Cost after 296474 iterations : Training Loss =  0.31301014915817027; Validation Loss = 0.3484652504544055\n",
            "Cost after 296475 iterations : Training Loss =  0.3130102657199698; Validation Loss = 0.3484654983047127\n",
            "Cost after 296476 iterations : Training Loss =  0.3130101955584668; Validation Loss = 0.348465021715692\n",
            "Cost after 296477 iterations : Training Loss =  0.313010166451032; Validation Loss = 0.34846526956599944\n",
            "Cost after 296478 iterations : Training Loss =  0.3130102419587633; Validation Loss = 0.3484647929769787\n",
            "Cost after 296479 iterations : Training Loss =  0.31301007465380165; Validation Loss = 0.3484650408272864\n",
            "Cost after 296480 iterations : Training Loss =  0.3130102808873522; Validation Loss = 0.34846528867759363\n",
            "Cost after 296481 iterations : Training Loss =  0.3130101210540983; Validation Loss = 0.3484648120885727\n",
            "Cost after 296482 iterations : Training Loss =  0.31301018161841454; Validation Loss = 0.34846505993888044\n",
            "Cost after 296483 iterations : Training Loss =  0.31301016745439497; Validation Loss = 0.34846458334985964\n",
            "Cost after 296484 iterations : Training Loss =  0.3130100823494767; Validation Loss = 0.3484648312001675\n",
            "Cost after 296485 iterations : Training Loss =  0.3130102138546913; Validation Loss = 0.3484643546111465\n",
            "Cost after 296486 iterations : Training Loss =  0.31301004654972964; Validation Loss = 0.34846460246145383\n",
            "Cost after 296487 iterations : Training Loss =  0.31301019678579717; Validation Loss = 0.3484648503117616\n",
            "Cost after 296488 iterations : Training Loss =  0.31301009295002646; Validation Loss = 0.3484643737227408\n",
            "Cost after 296489 iterations : Training Loss =  0.3130100975168593; Validation Loss = 0.3484646215730483\n",
            "Cost after 296490 iterations : Training Loss =  0.31301013935032307; Validation Loss = 0.34846414498402767\n",
            "Cost after 296491 iterations : Training Loss =  0.3130099982479216; Validation Loss = 0.34846439283433456\n",
            "Cost after 296492 iterations : Training Loss =  0.3130101857506195; Validation Loss = 0.34846391624531436\n",
            "Cost after 296493 iterations : Training Loss =  0.31301001844565796; Validation Loss = 0.3484641640956216\n",
            "Cost after 296494 iterations : Training Loss =  0.3130101126842423; Validation Loss = 0.348464411945929\n",
            "Cost after 296495 iterations : Training Loss =  0.31301006484595456; Validation Loss = 0.3484639353569082\n",
            "Cost after 296496 iterations : Training Loss =  0.3130100134153041; Validation Loss = 0.34846418320721584\n",
            "Cost after 296497 iterations : Training Loss =  0.3130101112462512; Validation Loss = 0.348463706618195\n",
            "Cost after 296498 iterations : Training Loss =  0.3130099439412895; Validation Loss = 0.348463954468503\n",
            "Cost after 296499 iterations : Training Loss =  0.3130101278516247; Validation Loss = 0.34846420231881015\n",
            "Cost after 296500 iterations : Training Loss =  0.31300999034158616; Validation Loss = 0.34846372572978923\n",
            "Cost after 296501 iterations : Training Loss =  0.31301002858268684; Validation Loss = 0.3484639735800967\n",
            "Cost after 296502 iterations : Training Loss =  0.31301003674188266; Validation Loss = 0.34846349699107615\n",
            "Cost after 296503 iterations : Training Loss =  0.3130099293137486; Validation Loss = 0.34846374484138365\n",
            "Cost after 296504 iterations : Training Loss =  0.31301008314217954; Validation Loss = 0.34846326825236273\n",
            "Cost after 296505 iterations : Training Loss =  0.31300991583721766; Validation Loss = 0.34846351610267023\n",
            "Cost after 296506 iterations : Training Loss =  0.31301004375006936; Validation Loss = 0.3484637639529775\n",
            "Cost after 296507 iterations : Training Loss =  0.3130099622375143; Validation Loss = 0.34846328736395693\n",
            "Cost after 296508 iterations : Training Loss =  0.3130099444811317; Validation Loss = 0.3484635352142644\n",
            "Cost after 296509 iterations : Training Loss =  0.313010008637811; Validation Loss = 0.34846305862524346\n",
            "Cost after 296510 iterations : Training Loss =  0.3130098452121937; Validation Loss = 0.3484633064755511\n",
            "Cost after 296511 iterations : Training Loss =  0.3130100550381075; Validation Loss = 0.3484628298865302\n",
            "Cost after 296512 iterations : Training Loss =  0.31300988773314564; Validation Loss = 0.3484630777368376\n",
            "Cost after 296513 iterations : Training Loss =  0.31300995964851425; Validation Loss = 0.3484633255871454\n",
            "Cost after 296514 iterations : Training Loss =  0.3130099341334426; Validation Loss = 0.34846284899812474\n",
            "Cost after 296515 iterations : Training Loss =  0.31300986037957645; Validation Loss = 0.3484630968484321\n",
            "Cost after 296516 iterations : Training Loss =  0.31300998053373935; Validation Loss = 0.34846262025941144\n",
            "Cost after 296517 iterations : Training Loss =  0.3130098132287773; Validation Loss = 0.3484628681097189\n",
            "Cost after 296518 iterations : Training Loss =  0.3130099748158967; Validation Loss = 0.3484631159600262\n",
            "Cost after 296519 iterations : Training Loss =  0.31300985962907407; Validation Loss = 0.3484626393710054\n",
            "Cost after 296520 iterations : Training Loss =  0.3130098755469588; Validation Loss = 0.34846288722131313\n",
            "Cost after 296521 iterations : Training Loss =  0.3130099060293706; Validation Loss = 0.3484624106322926\n",
            "Cost after 296522 iterations : Training Loss =  0.3130097762780212; Validation Loss = 0.3484626584825999\n",
            "Cost after 296523 iterations : Training Loss =  0.3130099524296675; Validation Loss = 0.34846218189357897\n",
            "Cost after 296524 iterations : Training Loss =  0.3130097851247058; Validation Loss = 0.34846242974388647\n",
            "Cost after 296525 iterations : Training Loss =  0.3130098907143414; Validation Loss = 0.34846267759419375\n",
            "Cost after 296526 iterations : Training Loss =  0.31300983152500234; Validation Loss = 0.34846220100517317\n",
            "Cost after 296527 iterations : Training Loss =  0.3130097914454034; Validation Loss = 0.3484624488554806\n",
            "Cost after 296528 iterations : Training Loss =  0.31300987792529905; Validation Loss = 0.3484619722664601\n",
            "Cost after 296529 iterations : Training Loss =  0.31300971062033717; Validation Loss = 0.34846222011676736\n",
            "Cost after 296530 iterations : Training Loss =  0.31300990588172417; Validation Loss = 0.34846246796707503\n",
            "Cost after 296531 iterations : Training Loss =  0.31300975702063394; Validation Loss = 0.3484619913780543\n",
            "Cost after 296532 iterations : Training Loss =  0.3130098066127861; Validation Loss = 0.3484622392283617\n",
            "Cost after 296533 iterations : Training Loss =  0.3130098034209305; Validation Loss = 0.3484617626393407\n",
            "Cost after 296534 iterations : Training Loss =  0.3130097073438485; Validation Loss = 0.3484620104896483\n",
            "Cost after 296535 iterations : Training Loss =  0.31300984982122737; Validation Loss = 0.3484615339006274\n",
            "Cost after 296536 iterations : Training Loss =  0.31300968251626543; Validation Loss = 0.348461781750935\n",
            "Cost after 296537 iterations : Training Loss =  0.31300982178016873; Validation Loss = 0.3484620296012424\n",
            "Cost after 296538 iterations : Training Loss =  0.31300972891656204; Validation Loss = 0.3484615530122215\n",
            "Cost after 296539 iterations : Training Loss =  0.31300972251123105; Validation Loss = 0.34846180086252915\n",
            "Cost after 296540 iterations : Training Loss =  0.3130097753168587; Validation Loss = 0.3484613242735084\n",
            "Cost after 296541 iterations : Training Loss =  0.313009623242293; Validation Loss = 0.34846157212381584\n",
            "Cost after 296542 iterations : Training Loss =  0.3130098217171554; Validation Loss = 0.3484610955347951\n",
            "Cost after 296543 iterations : Training Loss =  0.3130096544121936; Validation Loss = 0.3484613433851027\n",
            "Cost after 296544 iterations : Training Loss =  0.3130097376786137; Validation Loss = 0.34846159123541\n",
            "Cost after 296545 iterations : Training Loss =  0.31300970081249047; Validation Loss = 0.3484611146463894\n",
            "Cost after 296546 iterations : Training Loss =  0.3130096384096757; Validation Loss = 0.3484613624966966\n",
            "Cost after 296547 iterations : Training Loss =  0.31300974721278685; Validation Loss = 0.34846088590767643\n",
            "Cost after 296548 iterations : Training Loss =  0.313009579907825; Validation Loss = 0.3484611337579838\n",
            "Cost after 296549 iterations : Training Loss =  0.3130097528459962; Validation Loss = 0.34846138160829127\n",
            "Cost after 296550 iterations : Training Loss =  0.3130096263081218; Validation Loss = 0.34846090501927035\n",
            "Cost after 296551 iterations : Training Loss =  0.3130096535770583; Validation Loss = 0.34846115286957785\n",
            "Cost after 296552 iterations : Training Loss =  0.3130096727084184; Validation Loss = 0.34846067628055705\n",
            "Cost after 296553 iterations : Training Loss =  0.31300955430812066; Validation Loss = 0.3484609241308648\n",
            "Cost after 296554 iterations : Training Loss =  0.31300971910871495; Validation Loss = 0.34846044754184347\n",
            "Cost after 296555 iterations : Training Loss =  0.3130095518037534; Validation Loss = 0.3484606953921513\n",
            "Cost after 296556 iterations : Training Loss =  0.31300966874444114; Validation Loss = 0.348460943242459\n",
            "Cost after 296557 iterations : Training Loss =  0.31300959820405005; Validation Loss = 0.3484604666534378\n",
            "Cost after 296558 iterations : Training Loss =  0.31300956947550307; Validation Loss = 0.3484607145037458\n",
            "Cost after 296559 iterations : Training Loss =  0.31300964460434666; Validation Loss = 0.3484602379147249\n",
            "Cost after 296560 iterations : Training Loss =  0.31300947729938505; Validation Loss = 0.34846048576503225\n",
            "Cost after 296561 iterations : Training Loss =  0.31300968391182343; Validation Loss = 0.3484607336153397\n",
            "Cost after 296562 iterations : Training Loss =  0.31300952369968144; Validation Loss = 0.34846025702631905\n",
            "Cost after 296563 iterations : Training Loss =  0.31300958464288575; Validation Loss = 0.3484605048766261\n",
            "Cost after 296564 iterations : Training Loss =  0.31300957009997804; Validation Loss = 0.3484600282876055\n",
            "Cost after 296565 iterations : Training Loss =  0.31300948537394774; Validation Loss = 0.34846027613791314\n",
            "Cost after 296566 iterations : Training Loss =  0.31300961650027487; Validation Loss = 0.34845979954889245\n",
            "Cost after 296567 iterations : Training Loss =  0.3130094491953132; Validation Loss = 0.3484600473992002\n",
            "Cost after 296568 iterations : Training Loss =  0.3130095998102682; Validation Loss = 0.34846029524950767\n",
            "Cost after 296569 iterations : Training Loss =  0.31300949559560975; Validation Loss = 0.34845981866048653\n",
            "Cost after 296570 iterations : Training Loss =  0.31300950054133053; Validation Loss = 0.3484600665107942\n",
            "Cost after 296571 iterations : Training Loss =  0.3130095419959066; Validation Loss = 0.34845958992177317\n",
            "Cost after 296572 iterations : Training Loss =  0.3130094012723926; Validation Loss = 0.34845983777208106\n",
            "Cost after 296573 iterations : Training Loss =  0.3130095883962029; Validation Loss = 0.34845936118306037\n",
            "Cost after 296574 iterations : Training Loss =  0.31300942109124114; Validation Loss = 0.34845960903336737\n",
            "Cost after 296575 iterations : Training Loss =  0.3130095157087131; Validation Loss = 0.3484598568836751\n",
            "Cost after 296576 iterations : Training Loss =  0.3130094674915381; Validation Loss = 0.34845938029465456\n",
            "Cost after 296577 iterations : Training Loss =  0.31300941643977526; Validation Loss = 0.3484596281449618\n",
            "Cost after 296578 iterations : Training Loss =  0.3130095138918345; Validation Loss = 0.34845915155594115\n",
            "Cost after 296579 iterations : Training Loss =  0.31300934658687285; Validation Loss = 0.3484593994062485\n",
            "Cost after 296580 iterations : Training Loss =  0.31300953087609584; Validation Loss = 0.34845964725655576\n",
            "Cost after 296581 iterations : Training Loss =  0.31300939298716957; Validation Loss = 0.3484591706675353\n",
            "Cost after 296582 iterations : Training Loss =  0.313009431607158; Validation Loss = 0.34845941851784307\n",
            "Cost after 296583 iterations : Training Loss =  0.3130094393874663; Validation Loss = 0.3484589419288222\n",
            "Cost after 296584 iterations : Training Loss =  0.31300933233822004; Validation Loss = 0.3484591897791299\n",
            "Cost after 296585 iterations : Training Loss =  0.31300948578776266; Validation Loss = 0.3484587131901087\n",
            "Cost after 296586 iterations : Training Loss =  0.313009318482801; Validation Loss = 0.3484589610404163\n",
            "Cost after 296587 iterations : Training Loss =  0.31300944677454057; Validation Loss = 0.3484592088907239\n",
            "Cost after 296588 iterations : Training Loss =  0.3130093648830977; Validation Loss = 0.3484587323017027\n",
            "Cost after 296589 iterations : Training Loss =  0.31300934750560266; Validation Loss = 0.34845898015201043\n",
            "Cost after 296590 iterations : Training Loss =  0.31300941128339466; Validation Loss = 0.3484585035629897\n",
            "Cost after 296591 iterations : Training Loss =  0.3130092482366649; Validation Loss = 0.3484587514132969\n",
            "Cost after 296592 iterations : Training Loss =  0.3130094576836908; Validation Loss = 0.34845827482427644\n",
            "Cost after 296593 iterations : Training Loss =  0.31300929037872943; Validation Loss = 0.3484585226745841\n",
            "Cost after 296594 iterations : Training Loss =  0.31300936267298524; Validation Loss = 0.3484587705248917\n",
            "Cost after 296595 iterations : Training Loss =  0.31300933677902587; Validation Loss = 0.3484582939358707\n",
            "Cost after 296596 iterations : Training Loss =  0.31300926340404744; Validation Loss = 0.34845854178617813\n",
            "Cost after 296597 iterations : Training Loss =  0.3130093831793227; Validation Loss = 0.3484580651971571\n",
            "Cost after 296598 iterations : Training Loss =  0.31300921587436054; Validation Loss = 0.34845831304746483\n",
            "Cost after 296599 iterations : Training Loss =  0.31300937784036775; Validation Loss = 0.3484585608977727\n",
            "Cost after 296600 iterations : Training Loss =  0.3130092622746576; Validation Loss = 0.3484580843087516\n",
            "Cost after 296601 iterations : Training Loss =  0.31300927857143; Validation Loss = 0.34845833215905936\n",
            "Cost after 296602 iterations : Training Loss =  0.313009308674954; Validation Loss = 0.3484578555700384\n",
            "Cost after 296603 iterations : Training Loss =  0.3130091793024923; Validation Loss = 0.34845810342034594\n",
            "Cost after 296604 iterations : Training Loss =  0.31300935507525063; Validation Loss = 0.34845762683132525\n",
            "Cost after 296605 iterations : Training Loss =  0.3130091877702891; Validation Loss = 0.34845787468163253\n",
            "Cost after 296606 iterations : Training Loss =  0.31300929373881275; Validation Loss = 0.34845812253194\n",
            "Cost after 296607 iterations : Training Loss =  0.3130092341705858; Validation Loss = 0.34845764594291906\n",
            "Cost after 296608 iterations : Training Loss =  0.31300919446987463; Validation Loss = 0.34845789379322656\n",
            "Cost after 296609 iterations : Training Loss =  0.3130092805708822; Validation Loss = 0.34845741720420564\n",
            "Cost after 296610 iterations : Training Loss =  0.31300911326592057; Validation Loss = 0.3484576650545134\n",
            "Cost after 296611 iterations : Training Loss =  0.31300930890619505; Validation Loss = 0.348457912904821\n",
            "Cost after 296612 iterations : Training Loss =  0.31300915966621723; Validation Loss = 0.34845743631580034\n",
            "Cost after 296613 iterations : Training Loss =  0.31300920963725765; Validation Loss = 0.3484576841661077\n",
            "Cost after 296614 iterations : Training Loss =  0.313009206066514; Validation Loss = 0.34845720757708676\n",
            "Cost after 296615 iterations : Training Loss =  0.3130091103683196; Validation Loss = 0.3484574554273942\n",
            "Cost after 296616 iterations : Training Loss =  0.31300925246681055; Validation Loss = 0.34845697883837345\n",
            "Cost after 296617 iterations : Training Loss =  0.3130090851618487; Validation Loss = 0.34845722668868107\n",
            "Cost after 296618 iterations : Training Loss =  0.31300922480463983; Validation Loss = 0.3484574745389885\n",
            "Cost after 296619 iterations : Training Loss =  0.3130091315621453; Validation Loss = 0.3484569979499678\n",
            "Cost after 296620 iterations : Training Loss =  0.31300912553570215; Validation Loss = 0.3484572458002754\n",
            "Cost after 296621 iterations : Training Loss =  0.3130091779624421; Validation Loss = 0.34845676921125446\n",
            "Cost after 296622 iterations : Training Loss =  0.313009026266764; Validation Loss = 0.3484570170615622\n",
            "Cost after 296623 iterations : Training Loss =  0.31300922436273854; Validation Loss = 0.3484565404725409\n",
            "Cost after 296624 iterations : Training Loss =  0.31300905705777704; Validation Loss = 0.3484567883228486\n",
            "Cost after 296625 iterations : Training Loss =  0.3130091407030848; Validation Loss = 0.3484570361731564\n",
            "Cost after 296626 iterations : Training Loss =  0.3130091034580735; Validation Loss = 0.3484565595841353\n",
            "Cost after 296627 iterations : Training Loss =  0.313009041434147; Validation Loss = 0.34845680743444296\n",
            "Cost after 296628 iterations : Training Loss =  0.3130091498583704; Validation Loss = 0.34845633084542194\n",
            "Cost after 296629 iterations : Training Loss =  0.3130089825534086; Validation Loss = 0.3484565786957296\n",
            "Cost after 296630 iterations : Training Loss =  0.3130091558704673; Validation Loss = 0.3484568265460372\n",
            "Cost after 296631 iterations : Training Loss =  0.31300902895370497; Validation Loss = 0.3484563499570166\n",
            "Cost after 296632 iterations : Training Loss =  0.3130090566015292; Validation Loss = 0.3484565978073242\n",
            "Cost after 296633 iterations : Training Loss =  0.3130090753540017; Validation Loss = 0.34845612121830327\n",
            "Cost after 296634 iterations : Training Loss =  0.31300895733259143; Validation Loss = 0.34845636906861044\n",
            "Cost after 296635 iterations : Training Loss =  0.31300912175429807; Validation Loss = 0.34845589247958997\n",
            "Cost after 296636 iterations : Training Loss =  0.3130089544493367; Validation Loss = 0.3484561403298975\n",
            "Cost after 296637 iterations : Training Loss =  0.31300907176891185; Validation Loss = 0.34845638818020525\n",
            "Cost after 296638 iterations : Training Loss =  0.31300900084963357; Validation Loss = 0.34845591159118416\n",
            "Cost after 296639 iterations : Training Loss =  0.31300897249997445; Validation Loss = 0.3484561594414919\n",
            "Cost after 296640 iterations : Training Loss =  0.31300904724992995; Validation Loss = 0.34845568285247086\n",
            "Cost after 296641 iterations : Training Loss =  0.31300887994496807; Validation Loss = 0.3484559307027783\n",
            "Cost after 296642 iterations : Training Loss =  0.31300908693629453; Validation Loss = 0.3484561785530859\n",
            "Cost after 296643 iterations : Training Loss =  0.3130089263452646; Validation Loss = 0.3484557019640651\n",
            "Cost after 296644 iterations : Training Loss =  0.3130089876673567; Validation Loss = 0.3484559498143726\n",
            "Cost after 296645 iterations : Training Loss =  0.31300897274556155; Validation Loss = 0.3484554732253518\n",
            "Cost after 296646 iterations : Training Loss =  0.31300888839841895; Validation Loss = 0.34845572107565936\n",
            "Cost after 296647 iterations : Training Loss =  0.3130090191458582; Validation Loss = 0.34845524448663884\n",
            "Cost after 296648 iterations : Training Loss =  0.3130088518408965; Validation Loss = 0.348455492336946\n",
            "Cost after 296649 iterations : Training Loss =  0.3130090028347395; Validation Loss = 0.3484557401872537\n",
            "Cost after 296650 iterations : Training Loss =  0.31300889824119327; Validation Loss = 0.3484552635982328\n",
            "Cost after 296651 iterations : Training Loss =  0.3130089035658015; Validation Loss = 0.3484555114485404\n",
            "Cost after 296652 iterations : Training Loss =  0.3130089446414897; Validation Loss = 0.3484550348595195\n",
            "Cost after 296653 iterations : Training Loss =  0.3130088042968636; Validation Loss = 0.3484552827098274\n",
            "Cost after 296654 iterations : Training Loss =  0.3130089910417865; Validation Loss = 0.3484548061208063\n",
            "Cost after 296655 iterations : Training Loss =  0.3130088237368244; Validation Loss = 0.34845505397111387\n",
            "Cost after 296656 iterations : Training Loss =  0.31300891873318437; Validation Loss = 0.3484553018214217\n",
            "Cost after 296657 iterations : Training Loss =  0.3130088701371211; Validation Loss = 0.3484548252324004\n",
            "Cost after 296658 iterations : Training Loss =  0.3130088194642462; Validation Loss = 0.3484550730827084\n",
            "Cost after 296659 iterations : Training Loss =  0.313008916537418; Validation Loss = 0.3484545964936872\n",
            "Cost after 296660 iterations : Training Loss =  0.31300874923245625; Validation Loss = 0.34845484434399493\n",
            "Cost after 296661 iterations : Training Loss =  0.3130089339005667; Validation Loss = 0.34845509219430226\n",
            "Cost after 296662 iterations : Training Loss =  0.31300879563275297; Validation Loss = 0.3484546156052816\n",
            "Cost after 296663 iterations : Training Loss =  0.31300883463162893; Validation Loss = 0.348454863455589\n",
            "Cost after 296664 iterations : Training Loss =  0.31300884203304946; Validation Loss = 0.34845438686656843\n",
            "Cost after 296665 iterations : Training Loss =  0.3130087353626909; Validation Loss = 0.3484546347168758\n",
            "Cost after 296666 iterations : Training Loss =  0.3130088884333459; Validation Loss = 0.3484541581278549\n",
            "Cost after 296667 iterations : Training Loss =  0.31300872112838446; Validation Loss = 0.34845440597816235\n",
            "Cost after 296668 iterations : Training Loss =  0.3130088497990116; Validation Loss = 0.34845465382846996\n",
            "Cost after 296669 iterations : Training Loss =  0.31300876752868095; Validation Loss = 0.3484541772394492\n",
            "Cost after 296670 iterations : Training Loss =  0.31300875053007376; Validation Loss = 0.3484544250897564\n",
            "Cost after 296671 iterations : Training Loss =  0.31300881392897767; Validation Loss = 0.34845394850073574\n",
            "Cost after 296672 iterations : Training Loss =  0.31300865126113586; Validation Loss = 0.3484541963510433\n",
            "Cost after 296673 iterations : Training Loss =  0.3130088603292741; Validation Loss = 0.34845371976202244\n",
            "Cost after 296674 iterations : Training Loss =  0.3130086930243126; Validation Loss = 0.34845396761233005\n",
            "Cost after 296675 iterations : Training Loss =  0.3130087656974563; Validation Loss = 0.3484542154626374\n",
            "Cost after 296676 iterations : Training Loss =  0.31300873942460916; Validation Loss = 0.3484537388736172\n",
            "Cost after 296677 iterations : Training Loss =  0.3130086664285182; Validation Loss = 0.3484539867239242\n",
            "Cost after 296678 iterations : Training Loss =  0.313008785824906; Validation Loss = 0.3484535101349036\n",
            "Cost after 296679 iterations : Training Loss =  0.3130086185199441; Validation Loss = 0.3484537579852108\n",
            "Cost after 296680 iterations : Training Loss =  0.31300878086483885; Validation Loss = 0.3484540058355185\n",
            "Cost after 296681 iterations : Training Loss =  0.31300866492024093; Validation Loss = 0.34845352924649786\n",
            "Cost after 296682 iterations : Training Loss =  0.3130086815959011; Validation Loss = 0.34845377709680514\n",
            "Cost after 296683 iterations : Training Loss =  0.3130087113205374; Validation Loss = 0.34845330050778456\n",
            "Cost after 296684 iterations : Training Loss =  0.313008582326963; Validation Loss = 0.3484535483580919\n",
            "Cost after 296685 iterations : Training Loss =  0.31300875772083403; Validation Loss = 0.34845307176907114\n",
            "Cost after 296686 iterations : Training Loss =  0.3130085904158725; Validation Loss = 0.34845331961937837\n",
            "Cost after 296687 iterations : Training Loss =  0.31300869676328374; Validation Loss = 0.34845356746968603\n",
            "Cost after 296688 iterations : Training Loss =  0.313008636816169; Validation Loss = 0.34845309088066484\n",
            "Cost after 296689 iterations : Training Loss =  0.3130085974943458; Validation Loss = 0.3484533387309726\n",
            "Cost after 296690 iterations : Training Loss =  0.3130086832164657; Validation Loss = 0.3484528621419519\n",
            "Cost after 296691 iterations : Training Loss =  0.31300851591150386; Validation Loss = 0.34845310999225954\n",
            "Cost after 296692 iterations : Training Loss =  0.3130087119306662; Validation Loss = 0.3484533578425672\n",
            "Cost after 296693 iterations : Training Loss =  0.31300856231180063; Validation Loss = 0.348452881253546\n",
            "Cost after 296694 iterations : Training Loss =  0.3130086126617283; Validation Loss = 0.34845312910385373\n",
            "Cost after 296695 iterations : Training Loss =  0.31300860871209696; Validation Loss = 0.34845265251483326\n",
            "Cost after 296696 iterations : Training Loss =  0.3130085133927904; Validation Loss = 0.3484529003651407\n",
            "Cost after 296697 iterations : Training Loss =  0.3130086551123939; Validation Loss = 0.3484524237761195\n",
            "Cost after 296698 iterations : Training Loss =  0.31300848780743196; Validation Loss = 0.3484526716264271\n",
            "Cost after 296699 iterations : Training Loss =  0.313008627829111; Validation Loss = 0.34845291947673473\n",
            "Cost after 296700 iterations : Training Loss =  0.3130085342077287; Validation Loss = 0.3484524428877142\n",
            "Cost after 296701 iterations : Training Loss =  0.31300852856017325; Validation Loss = 0.3484526907380213\n",
            "Cost after 296702 iterations : Training Loss =  0.3130085806080252; Validation Loss = 0.3484522141490005\n",
            "Cost after 296703 iterations : Training Loss =  0.3130084292912354; Validation Loss = 0.3484524619993081\n",
            "Cost after 296704 iterations : Training Loss =  0.31300862700832194; Validation Loss = 0.3484519854102872\n",
            "Cost after 296705 iterations : Training Loss =  0.3130084597033604; Validation Loss = 0.3484522332605948\n",
            "Cost after 296706 iterations : Training Loss =  0.31300854372755577; Validation Loss = 0.34845248111090243\n",
            "Cost after 296707 iterations : Training Loss =  0.31300850610365666; Validation Loss = 0.3484520045218818\n",
            "Cost after 296708 iterations : Training Loss =  0.3130084444586181; Validation Loss = 0.3484522523721892\n",
            "Cost after 296709 iterations : Training Loss =  0.31300855250395354; Validation Loss = 0.34845177578316827\n",
            "Cost after 296710 iterations : Training Loss =  0.3130083851989919; Validation Loss = 0.34845202363347627\n",
            "Cost after 296711 iterations : Training Loss =  0.3130085588949382; Validation Loss = 0.3484522714837837\n",
            "Cost after 296712 iterations : Training Loss =  0.3130084315992886; Validation Loss = 0.3484517948947628\n",
            "Cost after 296713 iterations : Training Loss =  0.31300845962600055; Validation Loss = 0.34845204274507013\n",
            "Cost after 296714 iterations : Training Loss =  0.3130084779995852; Validation Loss = 0.3484515661560495\n",
            "Cost after 296715 iterations : Training Loss =  0.3130083603570627; Validation Loss = 0.34845181400635666\n",
            "Cost after 296716 iterations : Training Loss =  0.3130085243998818; Validation Loss = 0.3484513374173361\n",
            "Cost after 296717 iterations : Training Loss =  0.3130083570949201; Validation Loss = 0.34845158526764347\n",
            "Cost after 296718 iterations : Training Loss =  0.3130084747933831; Validation Loss = 0.34845183311795086\n",
            "Cost after 296719 iterations : Training Loss =  0.3130084034952164; Validation Loss = 0.3484513565289302\n",
            "Cost after 296720 iterations : Training Loss =  0.31300837552444527; Validation Loss = 0.34845160437923783\n",
            "Cost after 296721 iterations : Training Loss =  0.3130084498955134; Validation Loss = 0.3484511277902169\n",
            "Cost after 296722 iterations : Training Loss =  0.3130082825905516; Validation Loss = 0.34845137564052464\n",
            "Cost after 296723 iterations : Training Loss =  0.3130084899607658; Validation Loss = 0.3484516234908321\n",
            "Cost after 296724 iterations : Training Loss =  0.3130083289908483; Validation Loss = 0.3484511469018112\n",
            "Cost after 296725 iterations : Training Loss =  0.31300839069182784; Validation Loss = 0.34845139475211867\n",
            "Cost after 296726 iterations : Training Loss =  0.3130083753911445; Validation Loss = 0.3484509181630982\n",
            "Cost after 296727 iterations : Training Loss =  0.3130082914228901; Validation Loss = 0.3484511660134054\n",
            "Cost after 296728 iterations : Training Loss =  0.3130084217914416; Validation Loss = 0.3484506894243849\n",
            "Cost after 296729 iterations : Training Loss =  0.31300825448647973; Validation Loss = 0.34845093727469206\n",
            "Cost after 296730 iterations : Training Loss =  0.3130084058592105; Validation Loss = 0.3484511851249998\n",
            "Cost after 296731 iterations : Training Loss =  0.3130083008867765; Validation Loss = 0.3484507085359786\n",
            "Cost after 296732 iterations : Training Loss =  0.3130083065902727; Validation Loss = 0.34845095638628665\n",
            "Cost after 296733 iterations : Training Loss =  0.3130083472870731; Validation Loss = 0.34845047979726546\n",
            "Cost after 296734 iterations : Training Loss =  0.31300820732133455; Validation Loss = 0.3484507276475732\n",
            "Cost after 296735 iterations : Training Loss =  0.31300839368736966; Validation Loss = 0.3484502510585521\n",
            "Cost after 296736 iterations : Training Loss =  0.3130082263824078; Validation Loss = 0.34845049890885965\n",
            "Cost after 296737 iterations : Training Loss =  0.3130083217576551; Validation Loss = 0.34845074675916765\n",
            "Cost after 296738 iterations : Training Loss =  0.3130082727827045; Validation Loss = 0.34845027017014657\n",
            "Cost after 296739 iterations : Training Loss =  0.3130082224887174; Validation Loss = 0.34845051802045374\n",
            "Cost after 296740 iterations : Training Loss =  0.3130083191830013; Validation Loss = 0.34845004143143343\n",
            "Cost after 296741 iterations : Training Loss =  0.3130081518780394; Validation Loss = 0.34845028928174104\n",
            "Cost after 296742 iterations : Training Loss =  0.3130083369250381; Validation Loss = 0.34845053713204804\n",
            "Cost after 296743 iterations : Training Loss =  0.3130081982783362; Validation Loss = 0.34845006054302746\n",
            "Cost after 296744 iterations : Training Loss =  0.31300823765610025; Validation Loss = 0.34845030839333513\n",
            "Cost after 296745 iterations : Training Loss =  0.31300824467863264; Validation Loss = 0.34844983180431427\n",
            "Cost after 296746 iterations : Training Loss =  0.3130081383871623; Validation Loss = 0.3484500796546217\n",
            "Cost after 296747 iterations : Training Loss =  0.31300829107892975; Validation Loss = 0.3484496030656007\n",
            "Cost after 296748 iterations : Training Loss =  0.31300812377396764; Validation Loss = 0.3484498509159084\n",
            "Cost after 296749 iterations : Training Loss =  0.31300825282348255; Validation Loss = 0.3484500987662163\n",
            "Cost after 296750 iterations : Training Loss =  0.3130081701742645; Validation Loss = 0.348449622177195\n",
            "Cost after 296751 iterations : Training Loss =  0.3130081535545449; Validation Loss = 0.348449870027503\n",
            "Cost after 296752 iterations : Training Loss =  0.31300821657456096; Validation Loss = 0.34844939343848197\n",
            "Cost after 296753 iterations : Training Loss =  0.3130080542856068; Validation Loss = 0.3484496412887892\n",
            "Cost after 296754 iterations : Training Loss =  0.3130082629748574; Validation Loss = 0.3484491646997687\n",
            "Cost after 296755 iterations : Training Loss =  0.3130080956698956; Validation Loss = 0.3484494125500763\n",
            "Cost after 296756 iterations : Training Loss =  0.31300816872192727; Validation Loss = 0.3484496604003837\n",
            "Cost after 296757 iterations : Training Loss =  0.31300814207019245; Validation Loss = 0.348449183811363\n",
            "Cost after 296758 iterations : Training Loss =  0.3130080694529896; Validation Loss = 0.3484494316616702\n",
            "Cost after 296759 iterations : Training Loss =  0.3130081884704892; Validation Loss = 0.34844895507264967\n",
            "Cost after 296760 iterations : Training Loss =  0.3130080211655275; Validation Loss = 0.3484492029229571\n",
            "Cost after 296761 iterations : Training Loss =  0.3130081838893102; Validation Loss = 0.3484494507732643\n",
            "Cost after 296762 iterations : Training Loss =  0.31300806756582433; Validation Loss = 0.34844897418424375\n",
            "Cost after 296763 iterations : Training Loss =  0.31300808462037233; Validation Loss = 0.34844922203455125\n",
            "Cost after 296764 iterations : Training Loss =  0.3130081139661207; Validation Loss = 0.34844874544553006\n",
            "Cost after 296765 iterations : Training Loss =  0.3130079853514341; Validation Loss = 0.3484489932958381\n",
            "Cost after 296766 iterations : Training Loss =  0.3130081603664172; Validation Loss = 0.3484485167068172\n",
            "Cost after 296767 iterations : Training Loss =  0.31300799306145577; Validation Loss = 0.34844876455712465\n",
            "Cost after 296768 iterations : Training Loss =  0.3130080997877546; Validation Loss = 0.34844901240743215\n",
            "Cost after 296769 iterations : Training Loss =  0.31300803946175215; Validation Loss = 0.3484485358184115\n",
            "Cost after 296770 iterations : Training Loss =  0.3130080005188167; Validation Loss = 0.3484487836687191\n",
            "Cost after 296771 iterations : Training Loss =  0.31300808586204915; Validation Loss = 0.34844830707969826\n",
            "Cost after 296772 iterations : Training Loss =  0.31300791855708715; Validation Loss = 0.3484485549300056\n",
            "Cost after 296773 iterations : Training Loss =  0.31300811495513736; Validation Loss = 0.34844880278031326\n",
            "Cost after 296774 iterations : Training Loss =  0.3130079649573838; Validation Loss = 0.34844832619129223\n",
            "Cost after 296775 iterations : Training Loss =  0.31300801568619924; Validation Loss = 0.3484485740415997\n",
            "Cost after 296776 iterations : Training Loss =  0.31300801135768036; Validation Loss = 0.34844809745257865\n",
            "Cost after 296777 iterations : Training Loss =  0.31300791641726167; Validation Loss = 0.34844834530288654\n",
            "Cost after 296778 iterations : Training Loss =  0.31300805775797697; Validation Loss = 0.348447868713866\n",
            "Cost after 296779 iterations : Training Loss =  0.3130078904530152; Validation Loss = 0.34844811656417335\n",
            "Cost after 296780 iterations : Training Loss =  0.313008030853582; Validation Loss = 0.34844836441448074\n",
            "Cost after 296781 iterations : Training Loss =  0.3130079368533119; Validation Loss = 0.3484478878254598\n",
            "Cost after 296782 iterations : Training Loss =  0.3130079315846442; Validation Loss = 0.3484481356757675\n",
            "Cost after 296783 iterations : Training Loss =  0.3130079832536088; Validation Loss = 0.348447659086747\n",
            "Cost after 296784 iterations : Training Loss =  0.3130078323157065; Validation Loss = 0.34844790693705435\n",
            "Cost after 296785 iterations : Training Loss =  0.3130080296539055; Validation Loss = 0.34844743034803366\n",
            "Cost after 296786 iterations : Training Loss =  0.3130078623489436; Validation Loss = 0.34844767819834116\n",
            "Cost after 296787 iterations : Training Loss =  0.313007946752027; Validation Loss = 0.34844792604864866\n",
            "Cost after 296788 iterations : Training Loss =  0.3130079087492401; Validation Loss = 0.3484474494596276\n",
            "Cost after 296789 iterations : Training Loss =  0.3130078474830889; Validation Loss = 0.3484476973099354\n",
            "Cost after 296790 iterations : Training Loss =  0.31300795514953694; Validation Loss = 0.34844722072091444\n",
            "Cost after 296791 iterations : Training Loss =  0.31300778784457506; Validation Loss = 0.34844746857122205\n",
            "Cost after 296792 iterations : Training Loss =  0.3130079619194095; Validation Loss = 0.3484477164215295\n",
            "Cost after 296793 iterations : Training Loss =  0.31300783424487194; Validation Loss = 0.34844723983250886\n",
            "Cost after 296794 iterations : Training Loss =  0.31300786265047165; Validation Loss = 0.34844748768281636\n",
            "Cost after 296795 iterations : Training Loss =  0.31300788064516816; Validation Loss = 0.34844701109379556\n",
            "Cost after 296796 iterations : Training Loss =  0.3130077633815338; Validation Loss = 0.34844725894410306\n",
            "Cost after 296797 iterations : Training Loss =  0.3130079270454651; Validation Loss = 0.34844678235508214\n",
            "Cost after 296798 iterations : Training Loss =  0.31300775974050365; Validation Loss = 0.3484470302053896\n",
            "Cost after 296799 iterations : Training Loss =  0.3130078778178541; Validation Loss = 0.348447278055697\n",
            "Cost after 296800 iterations : Training Loss =  0.3130078061407997; Validation Loss = 0.34844680146667645\n",
            "Cost after 296801 iterations : Training Loss =  0.3130077785489165; Validation Loss = 0.3484470493169839\n",
            "Cost after 296802 iterations : Training Loss =  0.3130078525410965; Validation Loss = 0.34844657272796314\n",
            "Cost after 296803 iterations : Training Loss =  0.31300768523613504; Validation Loss = 0.34844682057827037\n",
            "Cost after 296804 iterations : Training Loss =  0.31300789298523707; Validation Loss = 0.3484470684285783\n",
            "Cost after 296805 iterations : Training Loss =  0.31300773163643153; Validation Loss = 0.3484465918395574\n",
            "Cost after 296806 iterations : Training Loss =  0.313007793716299; Validation Loss = 0.3484468396898649\n",
            "Cost after 296807 iterations : Training Loss =  0.31300777803672813; Validation Loss = 0.3484463631008442\n",
            "Cost after 296808 iterations : Training Loss =  0.313007694447361; Validation Loss = 0.34844661095115165\n",
            "Cost after 296809 iterations : Training Loss =  0.3130078244370247; Validation Loss = 0.3484461343621311\n",
            "Cost after 296810 iterations : Training Loss =  0.3130076571320631; Validation Loss = 0.3484463822124383\n",
            "Cost after 296811 iterations : Training Loss =  0.31300780888368157; Validation Loss = 0.34844663006274573\n",
            "Cost after 296812 iterations : Training Loss =  0.3130077035323595; Validation Loss = 0.348446153473725\n",
            "Cost after 296813 iterations : Training Loss =  0.3130077096147439; Validation Loss = 0.34844640132403243\n",
            "Cost after 296814 iterations : Training Loss =  0.3130077499326564; Validation Loss = 0.3484459247350117\n",
            "Cost after 296815 iterations : Training Loss =  0.3130076103458059; Validation Loss = 0.3484461725853193\n",
            "Cost after 296816 iterations : Training Loss =  0.3130077963329532; Validation Loss = 0.3484456959962986\n",
            "Cost after 296817 iterations : Training Loss =  0.31300762902799145; Validation Loss = 0.3484459438466059\n",
            "Cost after 296818 iterations : Training Loss =  0.31300772478212635; Validation Loss = 0.34844619169691315\n",
            "Cost after 296819 iterations : Training Loss =  0.31300767542828806; Validation Loss = 0.3484457151078927\n",
            "Cost after 296820 iterations : Training Loss =  0.3130076255131885; Validation Loss = 0.34844596295820024\n",
            "Cost after 296821 iterations : Training Loss =  0.31300772182858455; Validation Loss = 0.3484454863691794\n",
            "Cost after 296822 iterations : Training Loss =  0.3130075545236229; Validation Loss = 0.34844573421948666\n",
            "Cost after 296823 iterations : Training Loss =  0.31300773994950903; Validation Loss = 0.34844598206979444\n",
            "Cost after 296824 iterations : Training Loss =  0.31300760092391944; Validation Loss = 0.3484455054807735\n",
            "Cost after 296825 iterations : Training Loss =  0.313007640680571; Validation Loss = 0.3484457533310809\n",
            "Cost after 296826 iterations : Training Loss =  0.31300764732421615; Validation Loss = 0.3484452767420603\n",
            "Cost after 296827 iterations : Training Loss =  0.31300754141163334; Validation Loss = 0.34844552459236766\n",
            "Cost after 296828 iterations : Training Loss =  0.31300769372451276; Validation Loss = 0.3484450480033472\n",
            "Cost after 296829 iterations : Training Loss =  0.313007526419551; Validation Loss = 0.34844529585365464\n",
            "Cost after 296830 iterations : Training Loss =  0.3130076558479535; Validation Loss = 0.34844554370396214\n",
            "Cost after 296831 iterations : Training Loss =  0.3130075728198478; Validation Loss = 0.34844506711494133\n",
            "Cost after 296832 iterations : Training Loss =  0.31300755657901586; Validation Loss = 0.3484453149652485\n",
            "Cost after 296833 iterations : Training Loss =  0.31300761922014436; Validation Loss = 0.3484448383762279\n",
            "Cost after 296834 iterations : Training Loss =  0.313007457310078; Validation Loss = 0.3484450862265356\n",
            "Cost after 296835 iterations : Training Loss =  0.3130076656204409; Validation Loss = 0.3484446096375145\n",
            "Cost after 296836 iterations : Training Loss =  0.3130074983154792; Validation Loss = 0.34844485748782233\n",
            "Cost after 296837 iterations : Training Loss =  0.31300757174639865; Validation Loss = 0.34844510533812967\n",
            "Cost after 296838 iterations : Training Loss =  0.31300754471577585; Validation Loss = 0.34844462874910903\n",
            "Cost after 296839 iterations : Training Loss =  0.3130074724774606; Validation Loss = 0.3484448765994165\n",
            "Cost after 296840 iterations : Training Loss =  0.3130075911160726; Validation Loss = 0.3484444000103957\n",
            "Cost after 296841 iterations : Training Loss =  0.31300742381111063; Validation Loss = 0.3484446478607028\n",
            "Cost after 296842 iterations : Training Loss =  0.31300758691378117; Validation Loss = 0.3484448957110102\n",
            "Cost after 296843 iterations : Training Loss =  0.31300747021140746; Validation Loss = 0.3484444191219901\n",
            "Cost after 296844 iterations : Training Loss =  0.3130074876448431; Validation Loss = 0.3484446669722977\n",
            "Cost after 296845 iterations : Training Loss =  0.3130075166117042; Validation Loss = 0.34844419038327673\n",
            "Cost after 296846 iterations : Training Loss =  0.31300738837590525; Validation Loss = 0.3484444382335839\n",
            "Cost after 296847 iterations : Training Loss =  0.31300756301200083; Validation Loss = 0.34844396164456326\n",
            "Cost after 296848 iterations : Training Loss =  0.3130073957070389; Validation Loss = 0.34844420949487087\n",
            "Cost after 296849 iterations : Training Loss =  0.31300750281222584; Validation Loss = 0.3484444573451782\n",
            "Cost after 296850 iterations : Training Loss =  0.3130074421073355; Validation Loss = 0.3484439807561573\n",
            "Cost after 296851 iterations : Training Loss =  0.31300740354328804; Validation Loss = 0.348444228606465\n",
            "Cost after 296852 iterations : Training Loss =  0.31300748850763205; Validation Loss = 0.3484437520174442\n",
            "Cost after 296853 iterations : Training Loss =  0.31300732120267055; Validation Loss = 0.3484439998677518\n",
            "Cost after 296854 iterations : Training Loss =  0.31300751797960846; Validation Loss = 0.3484442477180593\n",
            "Cost after 296855 iterations : Training Loss =  0.3130073676029672; Validation Loss = 0.34844377112903857\n",
            "Cost after 296856 iterations : Training Loss =  0.31300741871067056; Validation Loss = 0.3484440189793459\n",
            "Cost after 296857 iterations : Training Loss =  0.3130074140032638; Validation Loss = 0.348443542390325\n",
            "Cost after 296858 iterations : Training Loss =  0.3130073194417325; Validation Loss = 0.3484437902406327\n",
            "Cost after 296859 iterations : Training Loss =  0.3130074604035606; Validation Loss = 0.3484433136516117\n",
            "Cost after 296860 iterations : Training Loss =  0.3130072930985986; Validation Loss = 0.3484435615019193\n",
            "Cost after 296861 iterations : Training Loss =  0.31300743387805324; Validation Loss = 0.3484438093522271\n",
            "Cost after 296862 iterations : Training Loss =  0.31300733949889537; Validation Loss = 0.3484433327632062\n",
            "Cost after 296863 iterations : Training Loss =  0.3130073346091154; Validation Loss = 0.3484435806135137\n",
            "Cost after 296864 iterations : Training Loss =  0.31300738589919197; Validation Loss = 0.3484431040244928\n",
            "Cost after 296865 iterations : Training Loss =  0.3130072353401775; Validation Loss = 0.3484433518748006\n",
            "Cost after 296866 iterations : Training Loss =  0.3130074322994887; Validation Loss = 0.34844287528577966\n",
            "Cost after 296867 iterations : Training Loss =  0.3130072649945268; Validation Loss = 0.3484431231360873\n",
            "Cost after 296868 iterations : Training Loss =  0.3130073497764978; Validation Loss = 0.34844337098639444\n",
            "Cost after 296869 iterations : Training Loss =  0.31300731139482346; Validation Loss = 0.3484428943973737\n",
            "Cost after 296870 iterations : Training Loss =  0.31300725050756006; Validation Loss = 0.34844314224768136\n",
            "Cost after 296871 iterations : Training Loss =  0.3130073577951201; Validation Loss = 0.34844266565866056\n",
            "Cost after 296872 iterations : Training Loss =  0.31300719049015846; Validation Loss = 0.34844291350896783\n",
            "Cost after 296873 iterations : Training Loss =  0.3130073649438807; Validation Loss = 0.3484431613592757\n",
            "Cost after 296874 iterations : Training Loss =  0.31300723689045523; Validation Loss = 0.348442684770255\n",
            "Cost after 296875 iterations : Training Loss =  0.3130072656749427; Validation Loss = 0.3484429326205624\n",
            "Cost after 296876 iterations : Training Loss =  0.3130072832907517; Validation Loss = 0.3484424560315415\n",
            "Cost after 296877 iterations : Training Loss =  0.3130071664060049; Validation Loss = 0.34844270388184884\n",
            "Cost after 296878 iterations : Training Loss =  0.31300732969104844; Validation Loss = 0.3484422272928279\n",
            "Cost after 296879 iterations : Training Loss =  0.31300716238608667; Validation Loss = 0.34844247514313575\n",
            "Cost after 296880 iterations : Training Loss =  0.3130072808423253; Validation Loss = 0.34844272299344303\n",
            "Cost after 296881 iterations : Training Loss =  0.3130072087863834; Validation Loss = 0.3484422464044226\n",
            "Cost after 296882 iterations : Training Loss =  0.3130071815733874; Validation Loss = 0.34844249425472995\n",
            "Cost after 296883 iterations : Training Loss =  0.31300725518668004; Validation Loss = 0.3484420176657092\n",
            "Cost after 296884 iterations : Training Loss =  0.31300708788171816; Validation Loss = 0.3484422655160169\n",
            "Cost after 296885 iterations : Training Loss =  0.313007296009708; Validation Loss = 0.34844251336632426\n",
            "Cost after 296886 iterations : Training Loss =  0.31300713428201493; Validation Loss = 0.34844203677730334\n",
            "Cost after 296887 iterations : Training Loss =  0.31300719674077015; Validation Loss = 0.3484422846276111\n",
            "Cost after 296888 iterations : Training Loss =  0.31300718068231154; Validation Loss = 0.3484418080385901\n",
            "Cost after 296889 iterations : Training Loss =  0.3130070974718321; Validation Loss = 0.3484420558888977\n",
            "Cost after 296890 iterations : Training Loss =  0.31300722708260814; Validation Loss = 0.348441579299877\n",
            "Cost after 296891 iterations : Training Loss =  0.3130070597776463; Validation Loss = 0.34844182715018435\n",
            "Cost after 296892 iterations : Training Loss =  0.3130072119081526; Validation Loss = 0.34844207500049196\n",
            "Cost after 296893 iterations : Training Loss =  0.31300710617794303; Validation Loss = 0.3484415984114706\n",
            "Cost after 296894 iterations : Training Loss =  0.3130071126392149; Validation Loss = 0.3484418462617785\n",
            "Cost after 296895 iterations : Training Loss =  0.3130071525782398; Validation Loss = 0.3484413696727576\n",
            "Cost after 296896 iterations : Training Loss =  0.3130070133702771; Validation Loss = 0.34844161752306535\n",
            "Cost after 296897 iterations : Training Loss =  0.3130071989785365; Validation Loss = 0.34844114093404427\n",
            "Cost after 296898 iterations : Training Loss =  0.31300703167357463; Validation Loss = 0.3484413887843521\n",
            "Cost after 296899 iterations : Training Loss =  0.3130071278065977; Validation Loss = 0.34844163663465977\n",
            "Cost after 296900 iterations : Training Loss =  0.3130070780738713; Validation Loss = 0.3484411600456386\n",
            "Cost after 296901 iterations : Training Loss =  0.3130070285376595; Validation Loss = 0.3484414078959462\n",
            "Cost after 296902 iterations : Training Loss =  0.3130071244741678; Validation Loss = 0.3484409313069255\n",
            "Cost after 296903 iterations : Training Loss =  0.31300695716920623; Validation Loss = 0.34844117915723305\n",
            "Cost after 296904 iterations : Training Loss =  0.3130071429739803; Validation Loss = 0.34844142700754077\n",
            "Cost after 296905 iterations : Training Loss =  0.3130070035695028; Validation Loss = 0.34844095041851975\n",
            "Cost after 296906 iterations : Training Loss =  0.31300704370504207; Validation Loss = 0.3484411982688269\n",
            "Cost after 296907 iterations : Training Loss =  0.31300704996979956; Validation Loss = 0.3484407216798068\n",
            "Cost after 296908 iterations : Training Loss =  0.3130069444361044; Validation Loss = 0.34844096953011416\n",
            "Cost after 296909 iterations : Training Loss =  0.3130070963700961; Validation Loss = 0.34844049294109325\n",
            "Cost after 296910 iterations : Training Loss =  0.31300692906513433; Validation Loss = 0.3484407407914003\n",
            "Cost after 296911 iterations : Training Loss =  0.3130070588724249; Validation Loss = 0.3484409886417079\n",
            "Cost after 296912 iterations : Training Loss =  0.3130069754654311; Validation Loss = 0.348440512052687\n",
            "Cost after 296913 iterations : Training Loss =  0.31300695960348685; Validation Loss = 0.3484407599029946\n",
            "Cost after 296914 iterations : Training Loss =  0.3130070218657275; Validation Loss = 0.3484402833139738\n",
            "Cost after 296915 iterations : Training Loss =  0.3130068603345491; Validation Loss = 0.3484405311642816\n",
            "Cost after 296916 iterations : Training Loss =  0.3130070682660241; Validation Loss = 0.3484400545752609\n",
            "Cost after 296917 iterations : Training Loss =  0.31300690096106265; Validation Loss = 0.3484403024255683\n",
            "Cost after 296918 iterations : Training Loss =  0.31300697477086953; Validation Loss = 0.348440550275876\n",
            "Cost after 296919 iterations : Training Loss =  0.3130069473613591; Validation Loss = 0.3484400736868551\n",
            "Cost after 296920 iterations : Training Loss =  0.31300687550193157; Validation Loss = 0.3484403215371624\n",
            "Cost after 296921 iterations : Training Loss =  0.3130069937616557; Validation Loss = 0.3484398449481417\n",
            "Cost after 296922 iterations : Training Loss =  0.31300682645669403; Validation Loss = 0.3484400927984491\n",
            "Cost after 296923 iterations : Training Loss =  0.3130069899382522; Validation Loss = 0.34844034064875645\n",
            "Cost after 296924 iterations : Training Loss =  0.31300687285699075; Validation Loss = 0.34843986405973587\n",
            "Cost after 296925 iterations : Training Loss =  0.3130068906693144; Validation Loss = 0.3484401119100432\n",
            "Cost after 296926 iterations : Training Loss =  0.3130069192572875; Validation Loss = 0.3484396353210225\n",
            "Cost after 296927 iterations : Training Loss =  0.3130067914003763; Validation Loss = 0.34843988317132985\n",
            "Cost after 296928 iterations : Training Loss =  0.31300696565758396; Validation Loss = 0.3484394065823095\n",
            "Cost after 296929 iterations : Training Loss =  0.3130067983526223; Validation Loss = 0.3484396544326168\n",
            "Cost after 296930 iterations : Training Loss =  0.31300690583669705; Validation Loss = 0.34843990228292426\n",
            "Cost after 296931 iterations : Training Loss =  0.3130068447529189; Validation Loss = 0.3484394256939035\n",
            "Cost after 296932 iterations : Training Loss =  0.3130068065677589; Validation Loss = 0.3484396735442111\n",
            "Cost after 296933 iterations : Training Loss =  0.31300689115321556; Validation Loss = 0.34843919695519016\n",
            "Cost after 296934 iterations : Training Loss =  0.31300672384825395; Validation Loss = 0.34843944480549816\n",
            "Cost after 296935 iterations : Training Loss =  0.31300692100407973; Validation Loss = 0.34843969265580554\n",
            "Cost after 296936 iterations : Training Loss =  0.3130067702485504; Validation Loss = 0.3484392160667849\n",
            "Cost after 296937 iterations : Training Loss =  0.31300682173514194; Validation Loss = 0.34843946391709224\n",
            "Cost after 296938 iterations : Training Loss =  0.31300681664884716; Validation Loss = 0.3484389873280711\n",
            "Cost after 296939 iterations : Training Loss =  0.31300672246620376; Validation Loss = 0.3484392351783786\n",
            "Cost after 296940 iterations : Training Loss =  0.31300686304914394; Validation Loss = 0.34843875858935774\n",
            "Cost after 296941 iterations : Training Loss =  0.3130066957441822; Validation Loss = 0.34843900643966536\n",
            "Cost after 296942 iterations : Training Loss =  0.31300683690252423; Validation Loss = 0.3484392542899731\n",
            "Cost after 296943 iterations : Training Loss =  0.31300674214447866; Validation Loss = 0.3484387777009521\n",
            "Cost after 296944 iterations : Training Loss =  0.31300673763358655; Validation Loss = 0.3484390255512598\n",
            "Cost after 296945 iterations : Training Loss =  0.31300678854477537; Validation Loss = 0.348438548962239\n",
            "Cost after 296946 iterations : Training Loss =  0.3130066383646483; Validation Loss = 0.34843879681254647\n",
            "Cost after 296947 iterations : Training Loss =  0.31300683494507175; Validation Loss = 0.3484383202235257\n",
            "Cost after 296948 iterations : Training Loss =  0.31300666764011026; Validation Loss = 0.34843856807383333\n",
            "Cost after 296949 iterations : Training Loss =  0.3130067528009691; Validation Loss = 0.34843881592414094\n",
            "Cost after 296950 iterations : Training Loss =  0.3130067140404068; Validation Loss = 0.34843833933511986\n",
            "Cost after 296951 iterations : Training Loss =  0.3130066535320312; Validation Loss = 0.34843858718542736\n",
            "Cost after 296952 iterations : Training Loss =  0.31300676044070364; Validation Loss = 0.34843811059640667\n",
            "Cost after 296953 iterations : Training Loss =  0.3130065931357416; Validation Loss = 0.34843835844671445\n",
            "Cost after 296954 iterations : Training Loss =  0.31300676796835186; Validation Loss = 0.3484386062970215\n",
            "Cost after 296955 iterations : Training Loss =  0.3130066395360385; Validation Loss = 0.3484381297080006\n",
            "Cost after 296956 iterations : Training Loss =  0.3130066686994139; Validation Loss = 0.34843837755830825\n",
            "Cost after 296957 iterations : Training Loss =  0.31300668593633507; Validation Loss = 0.34843790096928745\n",
            "Cost after 296958 iterations : Training Loss =  0.31300656943047606; Validation Loss = 0.3484381488195952\n",
            "Cost after 296959 iterations : Training Loss =  0.31300673233663145; Validation Loss = 0.34843767223057387\n",
            "Cost after 296960 iterations : Training Loss =  0.3130065650316699; Validation Loss = 0.34843792008088187\n",
            "Cost after 296961 iterations : Training Loss =  0.3130066838667962; Validation Loss = 0.3484381679311897\n",
            "Cost after 296962 iterations : Training Loss =  0.3130066114319667; Validation Loss = 0.34843769134216884\n",
            "Cost after 296963 iterations : Training Loss =  0.31300658459785835; Validation Loss = 0.3484379391924762\n",
            "Cost after 296964 iterations : Training Loss =  0.31300665783226334; Validation Loss = 0.34843746260345543\n",
            "Cost after 296965 iterations : Training Loss =  0.31300649052730145; Validation Loss = 0.3484377104537627\n",
            "Cost after 296966 iterations : Training Loss =  0.31300669903417905; Validation Loss = 0.34843795830407004\n",
            "Cost after 296967 iterations : Training Loss =  0.3130065369275982; Validation Loss = 0.34843748171504957\n",
            "Cost after 296968 iterations : Training Loss =  0.3130065997652411; Validation Loss = 0.3484377295653569\n",
            "Cost after 296969 iterations : Training Loss =  0.31300658332789466; Validation Loss = 0.3484372529763362\n",
            "Cost after 296970 iterations : Training Loss =  0.3130065004963031; Validation Loss = 0.3484375008266435\n",
            "Cost after 296971 iterations : Training Loss =  0.3130066297281915; Validation Loss = 0.3484370242376228\n",
            "Cost after 296972 iterations : Training Loss =  0.31300646242322955; Validation Loss = 0.34843727208793057\n",
            "Cost after 296973 iterations : Training Loss =  0.31300661493262366; Validation Loss = 0.348437519938238\n",
            "Cost after 296974 iterations : Training Loss =  0.3130065088235264; Validation Loss = 0.34843704334921727\n",
            "Cost after 296975 iterations : Training Loss =  0.3130065156636858; Validation Loss = 0.34843729119952455\n",
            "Cost after 296976 iterations : Training Loss =  0.3130065552238229; Validation Loss = 0.3484368146105038\n",
            "Cost after 296977 iterations : Training Loss =  0.31300641639474774; Validation Loss = 0.3484370624608116\n",
            "Cost after 296978 iterations : Training Loss =  0.3130066016241195; Validation Loss = 0.34843658587179055\n",
            "Cost after 296979 iterations : Training Loss =  0.31300643431915787; Validation Loss = 0.34843683372209794\n",
            "Cost after 296980 iterations : Training Loss =  0.31300653083106833; Validation Loss = 0.34843708157240555\n",
            "Cost after 296981 iterations : Training Loss =  0.3130064807194547; Validation Loss = 0.34843660498338497\n",
            "Cost after 296982 iterations : Training Loss =  0.3130064315621306; Validation Loss = 0.34843685283369225\n",
            "Cost after 296983 iterations : Training Loss =  0.3130065271197511; Validation Loss = 0.34843637624467166\n",
            "Cost after 296984 iterations : Training Loss =  0.31300635981478936; Validation Loss = 0.34843662409497883\n",
            "Cost after 296985 iterations : Training Loss =  0.31300654599845146; Validation Loss = 0.34843687194528644\n",
            "Cost after 296986 iterations : Training Loss =  0.3130064062150862; Validation Loss = 0.3484363953562655\n",
            "Cost after 296987 iterations : Training Loss =  0.3130064467295132; Validation Loss = 0.34843664320657314\n",
            "Cost after 296988 iterations : Training Loss =  0.3130064526153828; Validation Loss = 0.348436166617552\n",
            "Cost after 296989 iterations : Training Loss =  0.3130063474605751; Validation Loss = 0.34843641446785983\n",
            "Cost after 296990 iterations : Training Loss =  0.3130064990156794; Validation Loss = 0.34843593787883903\n",
            "Cost after 296991 iterations : Training Loss =  0.3130063317107177; Validation Loss = 0.34843618572914653\n",
            "Cost after 296992 iterations : Training Loss =  0.3130064618968958; Validation Loss = 0.3484364335794542\n",
            "Cost after 296993 iterations : Training Loss =  0.31300637811101434; Validation Loss = 0.3484359569904335\n",
            "Cost after 296994 iterations : Training Loss =  0.31300636262795817; Validation Loss = 0.34843620484074095\n",
            "Cost after 296995 iterations : Training Loss =  0.313006424511311; Validation Loss = 0.3484357282517202\n",
            "Cost after 296996 iterations : Training Loss =  0.31300626335902026; Validation Loss = 0.34843597610202764\n",
            "Cost after 296997 iterations : Training Loss =  0.3130064709116075; Validation Loss = 0.3484354995130067\n",
            "Cost after 296998 iterations : Training Loss =  0.31300630360664583; Validation Loss = 0.3484357473633145\n",
            "Cost after 296999 iterations : Training Loss =  0.3130063777953404; Validation Loss = 0.34843599521362223\n",
            "Cost after 297000 iterations : Training Loss =  0.31300635000694244; Validation Loss = 0.3484355186246013\n",
            "Cost after 297001 iterations : Training Loss =  0.31300627852640267; Validation Loss = 0.34843576647490876\n",
            "Cost after 297002 iterations : Training Loss =  0.3130063964072391; Validation Loss = 0.34843528988588773\n",
            "Cost after 297003 iterations : Training Loss =  0.31300622910227743; Validation Loss = 0.34843553773619546\n",
            "Cost after 297004 iterations : Training Loss =  0.3130063929627232; Validation Loss = 0.3484357855865028\n",
            "Cost after 297005 iterations : Training Loss =  0.313006275502574; Validation Loss = 0.34843530899748176\n",
            "Cost after 297006 iterations : Training Loss =  0.3130062936937856; Validation Loss = 0.34843555684778943\n",
            "Cost after 297007 iterations : Training Loss =  0.31300632190287064; Validation Loss = 0.348435080258769\n",
            "Cost after 297008 iterations : Training Loss =  0.3130061944248476; Validation Loss = 0.34843532810907646\n",
            "Cost after 297009 iterations : Training Loss =  0.31300636830316747; Validation Loss = 0.34843485152005554\n",
            "Cost after 297010 iterations : Training Loss =  0.3130062009982056; Validation Loss = 0.34843509937036277\n",
            "Cost after 297011 iterations : Training Loss =  0.3130063088611681; Validation Loss = 0.3484353472206703\n",
            "Cost after 297012 iterations : Training Loss =  0.31300624739850214; Validation Loss = 0.34843487063165\n",
            "Cost after 297013 iterations : Training Loss =  0.3130062095922302; Validation Loss = 0.34843511848195746\n",
            "Cost after 297014 iterations : Training Loss =  0.3130062937987988; Validation Loss = 0.34843464189293655\n",
            "Cost after 297015 iterations : Training Loss =  0.3130061264938372; Validation Loss = 0.34843488974324394\n",
            "Cost after 297016 iterations : Training Loss =  0.3130063240285506; Validation Loss = 0.3484351375935513\n",
            "Cost after 297017 iterations : Training Loss =  0.31300617289413374; Validation Loss = 0.34843466100453085\n",
            "Cost after 297018 iterations : Training Loss =  0.3130062247596129; Validation Loss = 0.3484349088548383\n",
            "Cost after 297019 iterations : Training Loss =  0.3130062192944304; Validation Loss = 0.3484344322658172\n",
            "Cost after 297020 iterations : Training Loss =  0.31300612549067497; Validation Loss = 0.348434680116125\n",
            "Cost after 297021 iterations : Training Loss =  0.31300626569472734; Validation Loss = 0.34843420352710414\n",
            "Cost after 297022 iterations : Training Loss =  0.3130060983897651; Validation Loss = 0.3484344513774117\n",
            "Cost after 297023 iterations : Training Loss =  0.3130062399269952; Validation Loss = 0.3484346992277193\n",
            "Cost after 297024 iterations : Training Loss =  0.31300614479006195; Validation Loss = 0.3484342226386986\n",
            "Cost after 297025 iterations : Training Loss =  0.31300614065805743; Validation Loss = 0.348434470489006\n",
            "Cost after 297026 iterations : Training Loss =  0.3130061911903589; Validation Loss = 0.3484339938999847\n",
            "Cost after 297027 iterations : Training Loss =  0.31300604138911964; Validation Loss = 0.3484342417502926\n",
            "Cost after 297028 iterations : Training Loss =  0.3130062375906553; Validation Loss = 0.3484337651612718\n",
            "Cost after 297029 iterations : Training Loss =  0.31300607028569344; Validation Loss = 0.3484340130115793\n",
            "Cost after 297030 iterations : Training Loss =  0.3130061558254402; Validation Loss = 0.3484342608618867\n",
            "Cost after 297031 iterations : Training Loss =  0.3130061166859903; Validation Loss = 0.34843378427286625\n",
            "Cost after 297032 iterations : Training Loss =  0.3130060565565023; Validation Loss = 0.3484340321231734\n",
            "Cost after 297033 iterations : Training Loss =  0.3130061630862869; Validation Loss = 0.34843355553415245\n",
            "Cost after 297034 iterations : Training Loss =  0.3130059957813252; Validation Loss = 0.3484338033844604\n",
            "Cost after 297035 iterations : Training Loss =  0.3130061709928226; Validation Loss = 0.348434051234768\n",
            "Cost after 297036 iterations : Training Loss =  0.3130060421816217; Validation Loss = 0.3484335746457471\n",
            "Cost after 297037 iterations : Training Loss =  0.31300607172388506; Validation Loss = 0.3484338224960548\n",
            "Cost after 297038 iterations : Training Loss =  0.31300608858191836; Validation Loss = 0.34843334590703395\n",
            "Cost after 297039 iterations : Training Loss =  0.3130059724549469; Validation Loss = 0.3484335937573411\n",
            "Cost after 297040 iterations : Training Loss =  0.3130061349822152; Validation Loss = 0.3484331171683205\n",
            "Cost after 297041 iterations : Training Loss =  0.3130059676772535; Validation Loss = 0.3484333650186281\n",
            "Cost after 297042 iterations : Training Loss =  0.3130060868912674; Validation Loss = 0.3484336128689355\n",
            "Cost after 297043 iterations : Training Loss =  0.3130060140775499; Validation Loss = 0.3484331362799146\n",
            "Cost after 297044 iterations : Training Loss =  0.3130059876223295; Validation Loss = 0.3484333841302224\n",
            "Cost after 297045 iterations : Training Loss =  0.31300606047784657; Validation Loss = 0.34843290754120176\n",
            "Cost after 297046 iterations : Training Loss =  0.3130058931728849; Validation Loss = 0.34843315539150893\n",
            "Cost after 297047 iterations : Training Loss =  0.3130061020586505; Validation Loss = 0.3484334032418164\n",
            "Cost after 297048 iterations : Training Loss =  0.3130059395731816; Validation Loss = 0.34843292665279535\n",
            "Cost after 297049 iterations : Training Loss =  0.3130060027897122; Validation Loss = 0.34843317450310324\n",
            "Cost after 297050 iterations : Training Loss =  0.3130059859734779; Validation Loss = 0.3484326979140823\n",
            "Cost after 297051 iterations : Training Loss =  0.3130059035207745; Validation Loss = 0.3484329457643897\n",
            "Cost after 297052 iterations : Training Loss =  0.3130060323737747; Validation Loss = 0.348432469175369\n",
            "Cost after 297053 iterations : Training Loss =  0.31300586506881295; Validation Loss = 0.34843271702567663\n",
            "Cost after 297054 iterations : Training Loss =  0.31300601795709493; Validation Loss = 0.34843296487598396\n",
            "Cost after 297055 iterations : Training Loss =  0.3130059114691098; Validation Loss = 0.34843248828696355\n",
            "Cost after 297056 iterations : Training Loss =  0.31300591868815736; Validation Loss = 0.3484327361372707\n",
            "Cost after 297057 iterations : Training Loss =  0.31300595786940655; Validation Loss = 0.3484322595482502\n",
            "Cost after 297058 iterations : Training Loss =  0.3130058194192189; Validation Loss = 0.34843250739855747\n",
            "Cost after 297059 iterations : Training Loss =  0.3130060042697029; Validation Loss = 0.3484320308095367\n",
            "Cost after 297060 iterations : Training Loss =  0.3130058369647409; Validation Loss = 0.3484322786598444\n",
            "Cost after 297061 iterations : Training Loss =  0.3130059338555396; Validation Loss = 0.34843252651015166\n",
            "Cost after 297062 iterations : Training Loss =  0.31300588336503793; Validation Loss = 0.34843204992113075\n",
            "Cost after 297063 iterations : Training Loss =  0.3130058345866018; Validation Loss = 0.3484322977714382\n",
            "Cost after 297064 iterations : Training Loss =  0.31300592976533453; Validation Loss = 0.3484318211824177\n",
            "Cost after 297065 iterations : Training Loss =  0.3130057624603726; Validation Loss = 0.34843206903272517\n",
            "Cost after 297066 iterations : Training Loss =  0.31300594902292217; Validation Loss = 0.3484323168830326\n",
            "Cost after 297067 iterations : Training Loss =  0.31300580886066925; Validation Loss = 0.3484318402940114\n",
            "Cost after 297068 iterations : Training Loss =  0.31300584975398427; Validation Loss = 0.3484320881443192\n",
            "Cost after 297069 iterations : Training Loss =  0.31300585526096625; Validation Loss = 0.3484316115552982\n",
            "Cost after 297070 iterations : Training Loss =  0.3130057504850463; Validation Loss = 0.3484318594056059\n",
            "Cost after 297071 iterations : Training Loss =  0.3130059016612627; Validation Loss = 0.34843138281658514\n",
            "Cost after 297072 iterations : Training Loss =  0.3130057343563012; Validation Loss = 0.34843163066689287\n",
            "Cost after 297073 iterations : Training Loss =  0.31300586492136717; Validation Loss = 0.3484318785172002\n",
            "Cost after 297074 iterations : Training Loss =  0.3130057807565977; Validation Loss = 0.3484314019281794\n",
            "Cost after 297075 iterations : Training Loss =  0.31300576565242894; Validation Loss = 0.3484316497784872\n",
            "Cost after 297076 iterations : Training Loss =  0.31300582715689446; Validation Loss = 0.3484311731894661\n",
            "Cost after 297077 iterations : Training Loss =  0.31300566638349125; Validation Loss = 0.3484314210397737\n",
            "Cost after 297078 iterations : Training Loss =  0.31300587355719095; Validation Loss = 0.3484309444507528\n",
            "Cost after 297079 iterations : Training Loss =  0.31300570625222895; Validation Loss = 0.3484311923010604\n",
            "Cost after 297080 iterations : Training Loss =  0.31300578081981184; Validation Loss = 0.348431440151368\n",
            "Cost after 297081 iterations : Training Loss =  0.3130057526525259; Validation Loss = 0.3484309635623471\n",
            "Cost after 297082 iterations : Training Loss =  0.3130056815508738; Validation Loss = 0.3484312114126549\n",
            "Cost after 297083 iterations : Training Loss =  0.31300579905282233; Validation Loss = 0.3484307348236342\n",
            "Cost after 297084 iterations : Training Loss =  0.3130056317478606; Validation Loss = 0.3484309826739414\n",
            "Cost after 297085 iterations : Training Loss =  0.3130057959871942; Validation Loss = 0.348431230524249\n",
            "Cost after 297086 iterations : Training Loss =  0.3130056781481574; Validation Loss = 0.3484307539352281\n",
            "Cost after 297087 iterations : Training Loss =  0.3130056967182565; Validation Loss = 0.34843100178553604\n",
            "Cost after 297088 iterations : Training Loss =  0.3130057245484541; Validation Loss = 0.3484305251965151\n",
            "Cost after 297089 iterations : Training Loss =  0.31300559744931855; Validation Loss = 0.3484307730468227\n",
            "Cost after 297090 iterations : Training Loss =  0.31300577094875065; Validation Loss = 0.3484302964578015\n",
            "Cost after 297091 iterations : Training Loss =  0.313005603643789; Validation Loss = 0.3484305443081093\n",
            "Cost after 297092 iterations : Training Loss =  0.3130057118856391; Validation Loss = 0.3484307921584167\n",
            "Cost after 297093 iterations : Training Loss =  0.3130056500440856; Validation Loss = 0.34843031556939563\n",
            "Cost after 297094 iterations : Training Loss =  0.3130056126167012; Validation Loss = 0.3484305634197034\n",
            "Cost after 297095 iterations : Training Loss =  0.31300569644438214; Validation Loss = 0.3484300868306826\n",
            "Cost after 297096 iterations : Training Loss =  0.31300552913942054; Validation Loss = 0.34843033468098994\n",
            "Cost after 297097 iterations : Training Loss =  0.3130057270530217; Validation Loss = 0.34843058253129755\n",
            "Cost after 297098 iterations : Training Loss =  0.3130055755397171; Validation Loss = 0.3484301059422768\n",
            "Cost after 297099 iterations : Training Loss =  0.313005627784084; Validation Loss = 0.34843035379258425\n",
            "Cost after 297100 iterations : Training Loss =  0.31300562194001397; Validation Loss = 0.3484298772035635\n",
            "Cost after 297101 iterations : Training Loss =  0.3130055285151462; Validation Loss = 0.3484301250538708\n",
            "Cost after 297102 iterations : Training Loss =  0.3130056683403104; Validation Loss = 0.3484296484648502\n",
            "Cost after 297103 iterations : Training Loss =  0.3130055010353487; Validation Loss = 0.3484298963151575\n",
            "Cost after 297104 iterations : Training Loss =  0.3130056429514664; Validation Loss = 0.348430144165465\n",
            "Cost after 297105 iterations : Training Loss =  0.31300554743564557; Validation Loss = 0.34842966757644456\n",
            "Cost after 297106 iterations : Training Loss =  0.3130055436825286; Validation Loss = 0.34842991542675206\n",
            "Cost after 297107 iterations : Training Loss =  0.31300559383594184; Validation Loss = 0.34842943883773103\n",
            "Cost after 297108 iterations : Training Loss =  0.3130054444135905; Validation Loss = 0.3484296866880385\n",
            "Cost after 297109 iterations : Training Loss =  0.3130056402362386; Validation Loss = 0.348429210099018\n",
            "Cost after 297110 iterations : Training Loss =  0.31300547293127695; Validation Loss = 0.34842945794932545\n",
            "Cost after 297111 iterations : Training Loss =  0.3130055588499112; Validation Loss = 0.34842970579963295\n",
            "Cost after 297112 iterations : Training Loss =  0.31300551933157367; Validation Loss = 0.34842922921061187\n",
            "Cost after 297113 iterations : Training Loss =  0.3130054595809735; Validation Loss = 0.34842947706091965\n",
            "Cost after 297114 iterations : Training Loss =  0.31300556573187044; Validation Loss = 0.3484290004718987\n",
            "Cost after 297115 iterations : Training Loss =  0.31300539842690833; Validation Loss = 0.34842924832220634\n",
            "Cost after 297116 iterations : Training Loss =  0.3130055740172939; Validation Loss = 0.3484294961725138\n",
            "Cost after 297117 iterations : Training Loss =  0.313005444827205; Validation Loss = 0.34842901958349287\n",
            "Cost after 297118 iterations : Training Loss =  0.31300547474835566; Validation Loss = 0.3484292674337999\n",
            "Cost after 297119 iterations : Training Loss =  0.31300549122750154; Validation Loss = 0.34842879084477957\n",
            "Cost after 297120 iterations : Training Loss =  0.3130053754794183; Validation Loss = 0.3484290386950872\n",
            "Cost after 297121 iterations : Training Loss =  0.31300553762779837; Validation Loss = 0.3484285621060668\n",
            "Cost after 297122 iterations : Training Loss =  0.31300537032283665; Validation Loss = 0.34842880995637404\n",
            "Cost after 297123 iterations : Training Loss =  0.31300548991573857; Validation Loss = 0.34842905780668126\n",
            "Cost after 297124 iterations : Training Loss =  0.31300541672313315; Validation Loss = 0.34842858121766074\n",
            "Cost after 297125 iterations : Training Loss =  0.31300539064680066; Validation Loss = 0.34842882906796846\n",
            "Cost after 297126 iterations : Training Loss =  0.3130054631234301; Validation Loss = 0.34842835247894743\n",
            "Cost after 297127 iterations : Training Loss =  0.31300529581846825; Validation Loss = 0.348428600329255\n",
            "Cost after 297128 iterations : Training Loss =  0.31300550508312114; Validation Loss = 0.3484288481795621\n",
            "Cost after 297129 iterations : Training Loss =  0.3130053422187647; Validation Loss = 0.3484283715905414\n",
            "Cost after 297130 iterations : Training Loss =  0.31300540581418324; Validation Loss = 0.348428619440849\n",
            "Cost after 297131 iterations : Training Loss =  0.3130053886190616; Validation Loss = 0.3484281428518284\n",
            "Cost after 297132 iterations : Training Loss =  0.3130053065452458; Validation Loss = 0.3484283907021361\n",
            "Cost after 297133 iterations : Training Loss =  0.3130054350193582; Validation Loss = 0.3484279141131151\n",
            "Cost after 297134 iterations : Training Loss =  0.31300526771439624; Validation Loss = 0.3484281619634224\n",
            "Cost after 297135 iterations : Training Loss =  0.313005420981566; Validation Loss = 0.34842840981373\n",
            "Cost after 297136 iterations : Training Loss =  0.3130053141146931; Validation Loss = 0.34842793322470955\n",
            "Cost after 297137 iterations : Training Loss =  0.313005321712628; Validation Loss = 0.348428181075017\n",
            "Cost after 297138 iterations : Training Loss =  0.3130053605149896; Validation Loss = 0.3484277044859957\n",
            "Cost after 297139 iterations : Training Loss =  0.3130052224436903; Validation Loss = 0.3484279523363036\n",
            "Cost after 297140 iterations : Training Loss =  0.3130054069152864; Validation Loss = 0.3484274757472828\n",
            "Cost after 297141 iterations : Training Loss =  0.31300523961032434; Validation Loss = 0.34842772359759033\n",
            "Cost after 297142 iterations : Training Loss =  0.31300533688001075; Validation Loss = 0.3484279714478978\n",
            "Cost after 297143 iterations : Training Loss =  0.3130052860106211; Validation Loss = 0.34842749485887725\n",
            "Cost after 297144 iterations : Training Loss =  0.31300523761107285; Validation Loss = 0.3484277427091844\n",
            "Cost after 297145 iterations : Training Loss =  0.31300533241091805; Validation Loss = 0.3484272661201638\n",
            "Cost after 297146 iterations : Training Loss =  0.31300516510595616; Validation Loss = 0.3484275139704712\n",
            "Cost after 297147 iterations : Training Loss =  0.3130053520473934; Validation Loss = 0.34842776182077845\n",
            "Cost after 297148 iterations : Training Loss =  0.31300521150625277; Validation Loss = 0.34842728523175764\n",
            "Cost after 297149 iterations : Training Loss =  0.31300525277845526; Validation Loss = 0.34842753308206553\n",
            "Cost after 297150 iterations : Training Loss =  0.31300525790654926; Validation Loss = 0.34842705649304473\n",
            "Cost after 297151 iterations : Training Loss =  0.3130051535095174; Validation Loss = 0.3484273043433524\n",
            "Cost after 297152 iterations : Training Loss =  0.3130053043068461; Validation Loss = 0.3484268277543312\n",
            "Cost after 297153 iterations : Training Loss =  0.31300513700188437; Validation Loss = 0.3484270756046388\n",
            "Cost after 297154 iterations : Training Loss =  0.31300526794583805; Validation Loss = 0.34842732345494654\n",
            "Cost after 297155 iterations : Training Loss =  0.3130051834021811; Validation Loss = 0.3484268468659256\n",
            "Cost after 297156 iterations : Training Loss =  0.31300516867690004; Validation Loss = 0.34842709471623323\n",
            "Cost after 297157 iterations : Training Loss =  0.31300522980247786; Validation Loss = 0.3484266181272122\n",
            "Cost after 297158 iterations : Training Loss =  0.31300506940796224; Validation Loss = 0.34842686597751976\n",
            "Cost after 297159 iterations : Training Loss =  0.3130052762027746; Validation Loss = 0.34842638938849924\n",
            "Cost after 297160 iterations : Training Loss =  0.3130051088978125; Validation Loss = 0.3484266372388068\n",
            "Cost after 297161 iterations : Training Loss =  0.31300518384428266; Validation Loss = 0.34842688508911407\n",
            "Cost after 297162 iterations : Training Loss =  0.3130051552981094; Validation Loss = 0.34842640850009315\n",
            "Cost after 297163 iterations : Training Loss =  0.313005084575345; Validation Loss = 0.34842665635040065\n",
            "Cost after 297164 iterations : Training Loss =  0.3130052016984056; Validation Loss = 0.34842617976137985\n",
            "Cost after 297165 iterations : Training Loss =  0.31300503439344396; Validation Loss = 0.34842642761168746\n",
            "Cost after 297166 iterations : Training Loss =  0.3130051990116653; Validation Loss = 0.34842667546199524\n",
            "Cost after 297167 iterations : Training Loss =  0.3130050807937407; Validation Loss = 0.34842619887297405\n",
            "Cost after 297168 iterations : Training Loss =  0.3130050997427274; Validation Loss = 0.34842644672328166\n",
            "Cost after 297169 iterations : Training Loss =  0.3130051271940374; Validation Loss = 0.348425970134261\n",
            "Cost after 297170 iterations : Training Loss =  0.3130050004737896; Validation Loss = 0.34842621798456863\n",
            "Cost after 297171 iterations : Training Loss =  0.3130051735943339; Validation Loss = 0.3484257413955477\n",
            "Cost after 297172 iterations : Training Loss =  0.3130050062893722; Validation Loss = 0.3484259892458551\n",
            "Cost after 297173 iterations : Training Loss =  0.31300511491011007; Validation Loss = 0.34842623709616255\n",
            "Cost after 297174 iterations : Training Loss =  0.31300505268966894; Validation Loss = 0.34842576050714186\n",
            "Cost after 297175 iterations : Training Loss =  0.3130050156411724; Validation Loss = 0.3484260083574493\n",
            "Cost after 297176 iterations : Training Loss =  0.31300509908996554; Validation Loss = 0.34842553176842855\n",
            "Cost after 297177 iterations : Training Loss =  0.31300493178500377; Validation Loss = 0.3484257796187362\n",
            "Cost after 297178 iterations : Training Loss =  0.3130051300774927; Validation Loss = 0.3484260274690434\n",
            "Cost after 297179 iterations : Training Loss =  0.31300497818530054; Validation Loss = 0.3484255508800227\n",
            "Cost after 297180 iterations : Training Loss =  0.313005030808555; Validation Loss = 0.34842579873033\n",
            "Cost after 297181 iterations : Training Loss =  0.3130050245855972; Validation Loss = 0.3484253221413097\n",
            "Cost after 297182 iterations : Training Loss =  0.31300493153961717; Validation Loss = 0.34842556999161717\n",
            "Cost after 297183 iterations : Training Loss =  0.3130050709858937; Validation Loss = 0.34842509340259625\n",
            "Cost after 297184 iterations : Training Loss =  0.3130049036809321; Validation Loss = 0.34842534125290386\n",
            "Cost after 297185 iterations : Training Loss =  0.31300504597593753; Validation Loss = 0.3484255891032113\n",
            "Cost after 297186 iterations : Training Loss =  0.31300495008122875; Validation Loss = 0.3484251125141905\n",
            "Cost after 297187 iterations : Training Loss =  0.31300494670699963; Validation Loss = 0.34842536036449795\n",
            "Cost after 297188 iterations : Training Loss =  0.3130049964815254; Validation Loss = 0.3484248837754777\n",
            "Cost after 297189 iterations : Training Loss =  0.3130048474380618; Validation Loss = 0.34842513162578487\n",
            "Cost after 297190 iterations : Training Loss =  0.31300504288182185; Validation Loss = 0.34842465503676373\n",
            "Cost after 297191 iterations : Training Loss =  0.3130048755768601; Validation Loss = 0.34842490288707156\n",
            "Cost after 297192 iterations : Training Loss =  0.31300496187438237; Validation Loss = 0.348425150737379\n",
            "Cost after 297193 iterations : Training Loss =  0.3130049219771569; Validation Loss = 0.34842467414835837\n",
            "Cost after 297194 iterations : Training Loss =  0.3130048626054444; Validation Loss = 0.34842492199866554\n",
            "Cost after 297195 iterations : Training Loss =  0.3130049683774535; Validation Loss = 0.3484244454096448\n",
            "Cost after 297196 iterations : Training Loss =  0.31300480107249185; Validation Loss = 0.34842469325995234\n",
            "Cost after 297197 iterations : Training Loss =  0.3130049770417649; Validation Loss = 0.34842494111025984\n",
            "Cost after 297198 iterations : Training Loss =  0.31300484747278845; Validation Loss = 0.34842446452123926\n",
            "Cost after 297199 iterations : Training Loss =  0.31300487777282693; Validation Loss = 0.34842471237154626\n",
            "Cost after 297200 iterations : Training Loss =  0.31300489387308505; Validation Loss = 0.3484242357825258\n",
            "Cost after 297201 iterations : Training Loss =  0.3130047785038891; Validation Loss = 0.3484244836328331\n",
            "Cost after 297202 iterations : Training Loss =  0.3130049402733818; Validation Loss = 0.3484240070438125\n",
            "Cost after 297203 iterations : Training Loss =  0.31300477296842; Validation Loss = 0.3484242548941201\n",
            "Cost after 297204 iterations : Training Loss =  0.3130048929402097; Validation Loss = 0.34842450274442804\n",
            "Cost after 297205 iterations : Training Loss =  0.31300481936871655; Validation Loss = 0.3484240261554071\n",
            "Cost after 297206 iterations : Training Loss =  0.313004793671272; Validation Loss = 0.34842427400571413\n",
            "Cost after 297207 iterations : Training Loss =  0.31300486576901326; Validation Loss = 0.3484237974166933\n",
            "Cost after 297208 iterations : Training Loss =  0.31300469846405166; Validation Loss = 0.3484240452670009\n",
            "Cost after 297209 iterations : Training Loss =  0.31300490810759235; Validation Loss = 0.3484242931173086\n",
            "Cost after 297210 iterations : Training Loss =  0.3130047448643481; Validation Loss = 0.34842381652828763\n",
            "Cost after 297211 iterations : Training Loss =  0.31300480883865434; Validation Loss = 0.3484240643785951\n",
            "Cost after 297212 iterations : Training Loss =  0.31300479126464487; Validation Loss = 0.34842358778957433\n",
            "Cost after 297213 iterations : Training Loss =  0.3130047095697166; Validation Loss = 0.3484238356398823\n",
            "Cost after 297214 iterations : Training Loss =  0.3130048376649414; Validation Loss = 0.3484233590508612\n",
            "Cost after 297215 iterations : Training Loss =  0.31300467035997964; Validation Loss = 0.34842360690116875\n",
            "Cost after 297216 iterations : Training Loss =  0.3130048240060371; Validation Loss = 0.34842385475147625\n",
            "Cost after 297217 iterations : Training Loss =  0.31300471676027614; Validation Loss = 0.34842337816245517\n",
            "Cost after 297218 iterations : Training Loss =  0.31300472473709934; Validation Loss = 0.3484236260127628\n",
            "Cost after 297219 iterations : Training Loss =  0.3130047631605728; Validation Loss = 0.34842314942374203\n",
            "Cost after 297220 iterations : Training Loss =  0.3130046254681614; Validation Loss = 0.34842339727404975\n",
            "Cost after 297221 iterations : Training Loss =  0.3130048095608697; Validation Loss = 0.34842292068502884\n",
            "Cost after 297222 iterations : Training Loss =  0.31300464225590785; Validation Loss = 0.34842316853533656\n",
            "Cost after 297223 iterations : Training Loss =  0.31300473990448163; Validation Loss = 0.3484234163856437\n",
            "Cost after 297224 iterations : Training Loss =  0.3130046886562046; Validation Loss = 0.34842293979662303\n",
            "Cost after 297225 iterations : Training Loss =  0.3130046406355438; Validation Loss = 0.34842318764693064\n",
            "Cost after 297226 iterations : Training Loss =  0.31300473505650106; Validation Loss = 0.34842271105790984\n",
            "Cost after 297227 iterations : Training Loss =  0.3130045677515394; Validation Loss = 0.3484229589082169\n",
            "Cost after 297228 iterations : Training Loss =  0.3130047550718645; Validation Loss = 0.3484232067585249\n",
            "Cost after 297229 iterations : Training Loss =  0.3130046141518362; Validation Loss = 0.34842273016950404\n",
            "Cost after 297230 iterations : Training Loss =  0.3130046558029265; Validation Loss = 0.3484229780198115\n",
            "Cost after 297231 iterations : Training Loss =  0.3130046605521328; Validation Loss = 0.34842250143079057\n",
            "Cost after 297232 iterations : Training Loss =  0.31300455653398834; Validation Loss = 0.3484227492810982\n",
            "Cost after 297233 iterations : Training Loss =  0.31300470695242927; Validation Loss = 0.3484222726920771\n",
            "Cost after 297234 iterations : Training Loss =  0.31300453964746755; Validation Loss = 0.348422520542385\n",
            "Cost after 297235 iterations : Training Loss =  0.3130046709703092; Validation Loss = 0.34842276839269276\n",
            "Cost after 297236 iterations : Training Loss =  0.3130045860477643; Validation Loss = 0.34842229180367185\n",
            "Cost after 297237 iterations : Training Loss =  0.31300457170137114; Validation Loss = 0.348422539653979\n",
            "Cost after 297238 iterations : Training Loss =  0.31300463244806087; Validation Loss = 0.34842206306495854\n",
            "Cost after 297239 iterations : Training Loss =  0.3130044724324333; Validation Loss = 0.3484223109152659\n",
            "Cost after 297240 iterations : Training Loss =  0.3130046788483577; Validation Loss = 0.348421834326245\n",
            "Cost after 297241 iterations : Training Loss =  0.31300451154339576; Validation Loss = 0.3484220821765522\n",
            "Cost after 297242 iterations : Training Loss =  0.31300458686875415; Validation Loss = 0.34842233002686046\n",
            "Cost after 297243 iterations : Training Loss =  0.31300455794369236; Validation Loss = 0.34842185343783955\n",
            "Cost after 297244 iterations : Training Loss =  0.3130044875998162; Validation Loss = 0.3484221012881467\n",
            "Cost after 297245 iterations : Training Loss =  0.313004604343989; Validation Loss = 0.34842162469912624\n",
            "Cost after 297246 iterations : Training Loss =  0.3130044370390274; Validation Loss = 0.3484218725494337\n",
            "Cost after 297247 iterations : Training Loss =  0.31300460203613645; Validation Loss = 0.34842212039974113\n",
            "Cost after 297248 iterations : Training Loss =  0.31300448343932397; Validation Loss = 0.34842164381072055\n",
            "Cost after 297249 iterations : Training Loss =  0.31300450276719866; Validation Loss = 0.34842189166102816\n",
            "Cost after 297250 iterations : Training Loss =  0.31300452983962074; Validation Loss = 0.34842141507200686\n",
            "Cost after 297251 iterations : Training Loss =  0.31300440349826086; Validation Loss = 0.3484216629223145\n",
            "Cost after 297252 iterations : Training Loss =  0.3130045762399173; Validation Loss = 0.34842118633329394\n",
            "Cost after 297253 iterations : Training Loss =  0.3130044089349556; Validation Loss = 0.3484214341836012\n",
            "Cost after 297254 iterations : Training Loss =  0.3130045179345813; Validation Loss = 0.348421682033909\n",
            "Cost after 297255 iterations : Training Loss =  0.31300445533525234; Validation Loss = 0.34842120544488825\n",
            "Cost after 297256 iterations : Training Loss =  0.31300441866564344; Validation Loss = 0.34842145329519525\n",
            "Cost after 297257 iterations : Training Loss =  0.3130045017355489; Validation Loss = 0.3484209767061746\n",
            "Cost after 297258 iterations : Training Loss =  0.31300433443058717; Validation Loss = 0.348421224556482\n",
            "Cost after 297259 iterations : Training Loss =  0.3130045331019638; Validation Loss = 0.3484214724067898\n",
            "Cost after 297260 iterations : Training Loss =  0.3130043808308838; Validation Loss = 0.3484209958177688\n",
            "Cost after 297261 iterations : Training Loss =  0.31300443383302623; Validation Loss = 0.3484212436680761\n",
            "Cost after 297262 iterations : Training Loss =  0.3130044272311803; Validation Loss = 0.3484207670790555\n",
            "Cost after 297263 iterations : Training Loss =  0.313004334564088; Validation Loss = 0.3484210149293632\n",
            "Cost after 297264 iterations : Training Loss =  0.31300447363147693; Validation Loss = 0.3484205383403423\n",
            "Cost after 297265 iterations : Training Loss =  0.31300430632651544; Validation Loss = 0.3484207861906502\n",
            "Cost after 297266 iterations : Training Loss =  0.31300444900040875; Validation Loss = 0.3484210340409571\n",
            "Cost after 297267 iterations : Training Loss =  0.31300435272681204; Validation Loss = 0.3484205574519368\n",
            "Cost after 297268 iterations : Training Loss =  0.3130043497314706; Validation Loss = 0.3484208053022442\n",
            "Cost after 297269 iterations : Training Loss =  0.3130043991271083; Validation Loss = 0.3484203287132235\n",
            "Cost after 297270 iterations : Training Loss =  0.31300425046253294; Validation Loss = 0.3484205765635312\n",
            "Cost after 297271 iterations : Training Loss =  0.31300444552740525; Validation Loss = 0.34842009997450996\n",
            "Cost after 297272 iterations : Training Loss =  0.31300427822244353; Validation Loss = 0.34842034782481757\n",
            "Cost after 297273 iterations : Training Loss =  0.31300436489885336; Validation Loss = 0.3484205956751255\n",
            "Cost after 297274 iterations : Training Loss =  0.31300432462274025; Validation Loss = 0.34842011908610404\n",
            "Cost after 297275 iterations : Training Loss =  0.31300426562991535; Validation Loss = 0.34842036693641193\n",
            "Cost after 297276 iterations : Training Loss =  0.31300437102303663; Validation Loss = 0.34841989034739107\n",
            "Cost after 297277 iterations : Training Loss =  0.3130042037180753; Validation Loss = 0.34842013819769824\n",
            "Cost after 297278 iterations : Training Loss =  0.3130043800662361; Validation Loss = 0.34842038604800607\n",
            "Cost after 297279 iterations : Training Loss =  0.3130042501183719; Validation Loss = 0.3484199094589853\n",
            "Cost after 297280 iterations : Training Loss =  0.31300428079729803; Validation Loss = 0.3484201573092928\n",
            "Cost after 297281 iterations : Training Loss =  0.31300429651866846; Validation Loss = 0.34841968072027174\n",
            "Cost after 297282 iterations : Training Loss =  0.3130041815283602; Validation Loss = 0.348419928570579\n",
            "Cost after 297283 iterations : Training Loss =  0.3130043429189654; Validation Loss = 0.3484194519815588\n",
            "Cost after 297284 iterations : Training Loss =  0.31300417561400323; Validation Loss = 0.34841969983186616\n",
            "Cost after 297285 iterations : Training Loss =  0.3130042959646807; Validation Loss = 0.3484199476821735\n",
            "Cost after 297286 iterations : Training Loss =  0.3130042220142999; Validation Loss = 0.3484194710931528\n",
            "Cost after 297287 iterations : Training Loss =  0.3130041966957429; Validation Loss = 0.3484197189434603\n",
            "Cost after 297288 iterations : Training Loss =  0.31300426841459617; Validation Loss = 0.3484192423544393\n",
            "Cost after 297289 iterations : Training Loss =  0.31300410110963484; Validation Loss = 0.34841949020474716\n",
            "Cost after 297290 iterations : Training Loss =  0.3130043111320631; Validation Loss = 0.3484197380550546\n",
            "Cost after 297291 iterations : Training Loss =  0.31300414750993155; Validation Loss = 0.3484192614660337\n",
            "Cost after 297292 iterations : Training Loss =  0.3130042118631253; Validation Loss = 0.34841950931634147\n",
            "Cost after 297293 iterations : Training Loss =  0.313004193910228; Validation Loss = 0.34841903272732055\n",
            "Cost after 297294 iterations : Training Loss =  0.31300411259418737; Validation Loss = 0.34841928057762805\n",
            "Cost after 297295 iterations : Training Loss =  0.3130042403105246; Validation Loss = 0.3484188039886073\n",
            "Cost after 297296 iterations : Training Loss =  0.31300407300556304; Validation Loss = 0.34841905183891486\n",
            "Cost after 297297 iterations : Training Loss =  0.31300422703050806; Validation Loss = 0.3484192996892223\n",
            "Cost after 297298 iterations : Training Loss =  0.31300411940585976; Validation Loss = 0.3484188231002014\n",
            "Cost after 297299 iterations : Training Loss =  0.3130041277615703; Validation Loss = 0.3484190709505093\n",
            "Cost after 297300 iterations : Training Loss =  0.31300416580615625; Validation Loss = 0.3484185943614881\n",
            "Cost after 297301 iterations : Training Loss =  0.3130040284926324; Validation Loss = 0.3484188422117954\n",
            "Cost after 297302 iterations : Training Loss =  0.3130042122064529; Validation Loss = 0.3484183656227747\n",
            "Cost after 297303 iterations : Training Loss =  0.31300404490149114; Validation Loss = 0.3484186134730822\n",
            "Cost after 297304 iterations : Training Loss =  0.3130041429289528; Validation Loss = 0.3484188613233902\n",
            "Cost after 297305 iterations : Training Loss =  0.31300409130178786; Validation Loss = 0.3484183847343689\n",
            "Cost after 297306 iterations : Training Loss =  0.31300404366001494; Validation Loss = 0.3484186325846767\n",
            "Cost after 297307 iterations : Training Loss =  0.31300413770208435; Validation Loss = 0.348418155995656\n",
            "Cost after 297308 iterations : Training Loss =  0.31300397039712274; Validation Loss = 0.3484184038459631\n",
            "Cost after 297309 iterations : Training Loss =  0.31300415809633536; Validation Loss = 0.3484186516962711\n",
            "Cost after 297310 iterations : Training Loss =  0.3130040167974194; Validation Loss = 0.3484181751072501\n",
            "Cost after 297311 iterations : Training Loss =  0.31300405882739746; Validation Loss = 0.3484184229575578\n",
            "Cost after 297312 iterations : Training Loss =  0.3130040631977159; Validation Loss = 0.3484179463685366\n",
            "Cost after 297313 iterations : Training Loss =  0.31300395955845994; Validation Loss = 0.34841819421884446\n",
            "Cost after 297314 iterations : Training Loss =  0.31300410959801267; Validation Loss = 0.3484177176298236\n",
            "Cost after 297315 iterations : Training Loss =  0.3130039422930509; Validation Loss = 0.3484179654801312\n",
            "Cost after 297316 iterations : Training Loss =  0.3130040739947802; Validation Loss = 0.3484182133304381\n",
            "Cost after 297317 iterations : Training Loss =  0.3130039886933479; Validation Loss = 0.3484177367414178\n",
            "Cost after 297318 iterations : Training Loss =  0.31300397472584246; Validation Loss = 0.3484179845917254\n",
            "Cost after 297319 iterations : Training Loss =  0.3130040350936443; Validation Loss = 0.34841750800270477\n",
            "Cost after 297320 iterations : Training Loss =  0.31300387545690456; Validation Loss = 0.34841775585301193\n",
            "Cost after 297321 iterations : Training Loss =  0.31300408149394077; Validation Loss = 0.3484172792639914\n",
            "Cost after 297322 iterations : Training Loss =  0.3130039141889791; Validation Loss = 0.3484175271142987\n",
            "Cost after 297323 iterations : Training Loss =  0.31300398989322503; Validation Loss = 0.34841777496460624\n",
            "Cost after 297324 iterations : Training Loss =  0.3130039605892757; Validation Loss = 0.3484172983755855\n",
            "Cost after 297325 iterations : Training Loss =  0.3130038906242871; Validation Loss = 0.3484175462258932\n",
            "Cost after 297326 iterations : Training Loss =  0.31300400698957265; Validation Loss = 0.3484170696368722\n",
            "Cost after 297327 iterations : Training Loss =  0.3130038396846107; Validation Loss = 0.34841731748717947\n",
            "Cost after 297328 iterations : Training Loss =  0.31300400506060755; Validation Loss = 0.34841756533748697\n",
            "Cost after 297329 iterations : Training Loss =  0.3130038860849075; Validation Loss = 0.34841708874846644\n",
            "Cost after 297330 iterations : Training Loss =  0.31300390579166976; Validation Loss = 0.34841733659877444\n",
            "Cost after 297331 iterations : Training Loss =  0.313003932485204; Validation Loss = 0.34841686000975325\n",
            "Cost after 297332 iterations : Training Loss =  0.31300380652273196; Validation Loss = 0.34841710786006064\n",
            "Cost after 297333 iterations : Training Loss =  0.3130039788855009; Validation Loss = 0.34841663127103983\n",
            "Cost after 297334 iterations : Training Loss =  0.31300381158053886; Validation Loss = 0.34841687912134744\n",
            "Cost after 297335 iterations : Training Loss =  0.3130039209590522; Validation Loss = 0.3484171269716549\n",
            "Cost after 297336 iterations : Training Loss =  0.31300385798083563; Validation Loss = 0.3484166503826343\n",
            "Cost after 297337 iterations : Training Loss =  0.31300382169011465; Validation Loss = 0.34841689823294175\n",
            "Cost after 297338 iterations : Training Loss =  0.31300390438113235; Validation Loss = 0.3484164216439207\n",
            "Cost after 297339 iterations : Training Loss =  0.3130037370761705; Validation Loss = 0.3484166694942284\n",
            "Cost after 297340 iterations : Training Loss =  0.3130039361264351; Validation Loss = 0.348416917344536\n",
            "Cost after 297341 iterations : Training Loss =  0.3130037834764672; Validation Loss = 0.348416440755515\n",
            "Cost after 297342 iterations : Training Loss =  0.3130038368574973; Validation Loss = 0.3484166886058225\n",
            "Cost after 297343 iterations : Training Loss =  0.31300382987676384; Validation Loss = 0.3484162120168017\n",
            "Cost after 297344 iterations : Training Loss =  0.3130037375885593; Validation Loss = 0.34841645986710945\n",
            "Cost after 297345 iterations : Training Loss =  0.3130038762770604; Validation Loss = 0.3484159832780887\n",
            "Cost after 297346 iterations : Training Loss =  0.31300370897209884; Validation Loss = 0.34841623112839615\n",
            "Cost after 297347 iterations : Training Loss =  0.3130038520248797; Validation Loss = 0.3484164789787035\n",
            "Cost after 297348 iterations : Training Loss =  0.3130037553723953; Validation Loss = 0.3484160023896827\n",
            "Cost after 297349 iterations : Training Loss =  0.31300375275594206; Validation Loss = 0.3484162502399902\n",
            "Cost after 297350 iterations : Training Loss =  0.3130038017726923; Validation Loss = 0.3484157736509694\n",
            "Cost after 297351 iterations : Training Loss =  0.31300365348700393; Validation Loss = 0.34841602150127726\n",
            "Cost after 297352 iterations : Training Loss =  0.3130038481729887; Validation Loss = 0.3484155449122562\n",
            "Cost after 297353 iterations : Training Loss =  0.3130036808680269; Validation Loss = 0.34841579276256385\n",
            "Cost after 297354 iterations : Training Loss =  0.3130037679233247; Validation Loss = 0.34841604061287085\n",
            "Cost after 297355 iterations : Training Loss =  0.3130037272683234; Validation Loss = 0.34841556402385054\n",
            "Cost after 297356 iterations : Training Loss =  0.3130036686543865; Validation Loss = 0.3484158118741577\n",
            "Cost after 297357 iterations : Training Loss =  0.31300377366862026; Validation Loss = 0.3484153352851373\n",
            "Cost after 297358 iterations : Training Loss =  0.3130036063636585; Validation Loss = 0.34841558313544474\n",
            "Cost after 297359 iterations : Training Loss =  0.31300378309070703; Validation Loss = 0.34841583098575213\n",
            "Cost after 297360 iterations : Training Loss =  0.31300365276395503; Validation Loss = 0.34841535439673094\n",
            "Cost after 297361 iterations : Training Loss =  0.31300368382176924; Validation Loss = 0.348415602247039\n",
            "Cost after 297362 iterations : Training Loss =  0.3130036991642514; Validation Loss = 0.3484151256580179\n",
            "Cost after 297363 iterations : Training Loss =  0.31300358455283134; Validation Loss = 0.3484153735083257\n",
            "Cost after 297364 iterations : Training Loss =  0.31300374556454813; Validation Loss = 0.3484148969193045\n",
            "Cost after 297365 iterations : Training Loss =  0.3130035782595865; Validation Loss = 0.34841514476961183\n",
            "Cost after 297366 iterations : Training Loss =  0.3130036989891518; Validation Loss = 0.3484153926199196\n",
            "Cost after 297367 iterations : Training Loss =  0.31300362465988313; Validation Loss = 0.348414916030899\n",
            "Cost after 297368 iterations : Training Loss =  0.3130035997202138; Validation Loss = 0.34841516388120625\n",
            "Cost after 297369 iterations : Training Loss =  0.3130036710601802; Validation Loss = 0.3484146872921858\n",
            "Cost after 297370 iterations : Training Loss =  0.31300350375521824; Validation Loss = 0.34841493514249267\n",
            "Cost after 297371 iterations : Training Loss =  0.31300371415653433; Validation Loss = 0.3484151829928004\n",
            "Cost after 297372 iterations : Training Loss =  0.313003550155515; Validation Loss = 0.3484147064037799\n",
            "Cost after 297373 iterations : Training Loss =  0.31300361488759665; Validation Loss = 0.34841495425408725\n",
            "Cost after 297374 iterations : Training Loss =  0.3130035965558115; Validation Loss = 0.3484144776650668\n",
            "Cost after 297375 iterations : Training Loss =  0.3130035156186587; Validation Loss = 0.3484147255153743\n",
            "Cost after 297376 iterations : Training Loss =  0.31300364295610805; Validation Loss = 0.34841424892635336\n",
            "Cost after 297377 iterations : Training Loss =  0.3130034756511461; Validation Loss = 0.3484144967766609\n",
            "Cost after 297378 iterations : Training Loss =  0.31300363005497905; Validation Loss = 0.34841474462696836\n",
            "Cost after 297379 iterations : Training Loss =  0.31300352205144294; Validation Loss = 0.3484142680379476\n",
            "Cost after 297380 iterations : Training Loss =  0.3130035307860413; Validation Loss = 0.3484145158882548\n",
            "Cost after 297381 iterations : Training Loss =  0.3130035684517398; Validation Loss = 0.34841403929923415\n",
            "Cost after 297382 iterations : Training Loss =  0.31300343151710336; Validation Loss = 0.34841428714954176\n",
            "Cost after 297383 iterations : Training Loss =  0.3130036148520361; Validation Loss = 0.34841381056052073\n",
            "Cost after 297384 iterations : Training Loss =  0.3130034475470746; Validation Loss = 0.3484140584108286\n",
            "Cost after 297385 iterations : Training Loss =  0.313003545953424; Validation Loss = 0.348414306261136\n",
            "Cost after 297386 iterations : Training Loss =  0.3130034939473713; Validation Loss = 0.34841382967211515\n",
            "Cost after 297387 iterations : Training Loss =  0.31300344668448593; Validation Loss = 0.3484140775224229\n",
            "Cost after 297388 iterations : Training Loss =  0.31300354034766786; Validation Loss = 0.34841360093340185\n",
            "Cost after 297389 iterations : Training Loss =  0.31300337304270615; Validation Loss = 0.3484138487837097\n",
            "Cost after 297390 iterations : Training Loss =  0.3130035611208065; Validation Loss = 0.3484140966340169\n",
            "Cost after 297391 iterations : Training Loss =  0.31300341944300264; Validation Loss = 0.3484136200449963\n",
            "Cost after 297392 iterations : Training Loss =  0.31300346185186867; Validation Loss = 0.34841386789530376\n",
            "Cost after 297393 iterations : Training Loss =  0.31300346584329924; Validation Loss = 0.3484133913062832\n",
            "Cost after 297394 iterations : Training Loss =  0.3130033625829307; Validation Loss = 0.3484136391565904\n",
            "Cost after 297395 iterations : Training Loss =  0.3130035122435959; Validation Loss = 0.3484131625675697\n",
            "Cost after 297396 iterations : Training Loss =  0.3130033449386344; Validation Loss = 0.34841341041787716\n",
            "Cost after 297397 iterations : Training Loss =  0.3130034770192514; Validation Loss = 0.3484136582681846\n",
            "Cost after 297398 iterations : Training Loss =  0.3130033913389312; Validation Loss = 0.34841318167916396\n",
            "Cost after 297399 iterations : Training Loss =  0.31300337775031345; Validation Loss = 0.348413429529471\n",
            "Cost after 297400 iterations : Training Loss =  0.3130034377392274; Validation Loss = 0.3484129529404508\n",
            "Cost after 297401 iterations : Training Loss =  0.31300327848137544; Validation Loss = 0.34841320079075827\n",
            "Cost after 297402 iterations : Training Loss =  0.31300348413952406; Validation Loss = 0.3484127242017376\n",
            "Cost after 297403 iterations : Training Loss =  0.31300331683456223; Validation Loss = 0.3484129720520451\n",
            "Cost after 297404 iterations : Training Loss =  0.31300339291769574; Validation Loss = 0.3484132199023523\n",
            "Cost after 297405 iterations : Training Loss =  0.31300336323485917; Validation Loss = 0.34841274331333194\n",
            "Cost after 297406 iterations : Training Loss =  0.31300329364875823; Validation Loss = 0.34841299116363916\n",
            "Cost after 297407 iterations : Training Loss =  0.3130034096351558; Validation Loss = 0.34841251457461814\n",
            "Cost after 297408 iterations : Training Loss =  0.3130032423301937; Validation Loss = 0.34841276242492597\n",
            "Cost after 297409 iterations : Training Loss =  0.3130034080850788; Validation Loss = 0.34841301027523314\n",
            "Cost after 297410 iterations : Training Loss =  0.3130032887304906; Validation Loss = 0.3484125336862124\n",
            "Cost after 297411 iterations : Training Loss =  0.31300330881614086; Validation Loss = 0.34841278153652006\n",
            "Cost after 297412 iterations : Training Loss =  0.3130033351307872; Validation Loss = 0.3484123049474994\n",
            "Cost after 297413 iterations : Training Loss =  0.3130032095472031; Validation Loss = 0.348412552797807\n",
            "Cost after 297414 iterations : Training Loss =  0.313003381531084; Validation Loss = 0.3484120762087858\n",
            "Cost after 297415 iterations : Training Loss =  0.3130032142261224; Validation Loss = 0.3484123240590932\n",
            "Cost after 297416 iterations : Training Loss =  0.3130033239835234; Validation Loss = 0.3484125719094011\n",
            "Cost after 297417 iterations : Training Loss =  0.3130032606264189; Validation Loss = 0.3484120953203801\n",
            "Cost after 297418 iterations : Training Loss =  0.31300322471458564; Validation Loss = 0.348412343170688\n",
            "Cost after 297419 iterations : Training Loss =  0.31300330702671547; Validation Loss = 0.34841186658166706\n",
            "Cost after 297420 iterations : Training Loss =  0.3130031397217537; Validation Loss = 0.3484121144319746\n",
            "Cost after 297421 iterations : Training Loss =  0.3130033391509061; Validation Loss = 0.3484123622822819\n",
            "Cost after 297422 iterations : Training Loss =  0.3130031861220504; Validation Loss = 0.3484118856932612\n",
            "Cost after 297423 iterations : Training Loss =  0.3130032398819683; Validation Loss = 0.3484121335435688\n",
            "Cost after 297424 iterations : Training Loss =  0.31300323252234713; Validation Loss = 0.3484116569545476\n",
            "Cost after 297425 iterations : Training Loss =  0.3130031406130302; Validation Loss = 0.3484119048048553\n",
            "Cost after 297426 iterations : Training Loss =  0.3130032789226439; Validation Loss = 0.3484114282158346\n",
            "Cost after 297427 iterations : Training Loss =  0.31300311161768174; Validation Loss = 0.3484116760661421\n",
            "Cost after 297428 iterations : Training Loss =  0.31300325504935095; Validation Loss = 0.3484119239164498\n",
            "Cost after 297429 iterations : Training Loss =  0.31300315801797873; Validation Loss = 0.34841144732742885\n",
            "Cost after 297430 iterations : Training Loss =  0.3130031557804129; Validation Loss = 0.3484116951777363\n",
            "Cost after 297431 iterations : Training Loss =  0.3130032044182754; Validation Loss = 0.34841121858871577\n",
            "Cost after 297432 iterations : Training Loss =  0.31300305651147503; Validation Loss = 0.34841146643902304\n",
            "Cost after 297433 iterations : Training Loss =  0.3130032508185719; Validation Loss = 0.3484109898500022\n",
            "Cost after 297434 iterations : Training Loss =  0.31300308351361017; Validation Loss = 0.34841123770030974\n",
            "Cost after 297435 iterations : Training Loss =  0.31300317094779556; Validation Loss = 0.34841148555061696\n",
            "Cost after 297436 iterations : Training Loss =  0.31300312991390683; Validation Loss = 0.3484110089615963\n",
            "Cost after 297437 iterations : Training Loss =  0.3130030716788574; Validation Loss = 0.34841125681190394\n",
            "Cost after 297438 iterations : Training Loss =  0.31300317631420355; Validation Loss = 0.3484107802228831\n",
            "Cost after 297439 iterations : Training Loss =  0.31300300900924166; Validation Loss = 0.3484110280731905\n",
            "Cost after 297440 iterations : Training Loss =  0.3130031861151782; Validation Loss = 0.34841127592349824\n",
            "Cost after 297441 iterations : Training Loss =  0.3130030554095384; Validation Loss = 0.34841079933447705\n",
            "Cost after 297442 iterations : Training Loss =  0.31300308684624023; Validation Loss = 0.34841104718478505\n",
            "Cost after 297443 iterations : Training Loss =  0.3130031018098349; Validation Loss = 0.34841057059576414\n",
            "Cost after 297444 iterations : Training Loss =  0.31300298757730255; Validation Loss = 0.34841081844607147\n",
            "Cost after 297445 iterations : Training Loss =  0.31300314821013153; Validation Loss = 0.3484103418570507\n",
            "Cost after 297446 iterations : Training Loss =  0.31300298090517004; Validation Loss = 0.34841058970735833\n",
            "Cost after 297447 iterations : Training Loss =  0.31300310201362286; Validation Loss = 0.3484108375576661\n",
            "Cost after 297448 iterations : Training Loss =  0.31300302730546653; Validation Loss = 0.34841036096864486\n",
            "Cost after 297449 iterations : Training Loss =  0.31300300274468507; Validation Loss = 0.34841060881895247\n",
            "Cost after 297450 iterations : Training Loss =  0.31300307370576297; Validation Loss = 0.3484101322299317\n",
            "Cost after 297451 iterations : Training Loss =  0.3130029064008014; Validation Loss = 0.34841038008023917\n",
            "Cost after 297452 iterations : Training Loss =  0.31300311718100543; Validation Loss = 0.34841062793054717\n",
            "Cost after 297453 iterations : Training Loss =  0.3130029528010981; Validation Loss = 0.34841015134152586\n",
            "Cost after 297454 iterations : Training Loss =  0.3130030179120677; Validation Loss = 0.3484103991918333\n",
            "Cost after 297455 iterations : Training Loss =  0.3130029992013948; Validation Loss = 0.34840992260281256\n",
            "Cost after 297456 iterations : Training Loss =  0.3130029186431298; Validation Loss = 0.34841017045311995\n",
            "Cost after 297457 iterations : Training Loss =  0.31300304560169123; Validation Loss = 0.3484096938640995\n",
            "Cost after 297458 iterations : Training Loss =  0.3130028782967295; Validation Loss = 0.34840994171440715\n",
            "Cost after 297459 iterations : Training Loss =  0.3130030330794503; Validation Loss = 0.34841018956471415\n",
            "Cost after 297460 iterations : Training Loss =  0.3130029246970263; Validation Loss = 0.34840971297569356\n",
            "Cost after 297461 iterations : Training Loss =  0.31300293381051225; Validation Loss = 0.34840996082600106\n",
            "Cost after 297462 iterations : Training Loss =  0.31300297109732284; Validation Loss = 0.34840948423698037\n",
            "Cost after 297463 iterations : Training Loss =  0.31300283454157446; Validation Loss = 0.3484097320872878\n",
            "Cost after 297464 iterations : Training Loss =  0.31300301749761966; Validation Loss = 0.34840925549826723\n",
            "Cost after 297465 iterations : Training Loss =  0.31300285019265794; Validation Loss = 0.3484095033485746\n",
            "Cost after 297466 iterations : Training Loss =  0.3130029489778952; Validation Loss = 0.348409751198882\n",
            "Cost after 297467 iterations : Training Loss =  0.31300289659295466; Validation Loss = 0.3484092746098614\n",
            "Cost after 297468 iterations : Training Loss =  0.313002849708957; Validation Loss = 0.3484095224601688\n",
            "Cost after 297469 iterations : Training Loss =  0.3130029429932513; Validation Loss = 0.3484090458711478\n",
            "Cost after 297470 iterations : Training Loss =  0.31300277568828966; Validation Loss = 0.3484092937214554\n",
            "Cost after 297471 iterations : Training Loss =  0.3130029641452776; Validation Loss = 0.348409541571763\n",
            "Cost after 297472 iterations : Training Loss =  0.31300282208858626; Validation Loss = 0.3484090649827424\n",
            "Cost after 297473 iterations : Training Loss =  0.3130028648763396; Validation Loss = 0.34840931283305\n",
            "Cost after 297474 iterations : Training Loss =  0.3130028684888829; Validation Loss = 0.34840883624402913\n",
            "Cost after 297475 iterations : Training Loss =  0.31300276560740165; Validation Loss = 0.34840908409433674\n",
            "Cost after 297476 iterations : Training Loss =  0.31300291488917936; Validation Loss = 0.34840860750531577\n",
            "Cost after 297477 iterations : Training Loss =  0.3130027475842177; Validation Loss = 0.3484088553556234\n",
            "Cost after 297478 iterations : Training Loss =  0.31300288004372256; Validation Loss = 0.3484091032059308\n",
            "Cost after 297479 iterations : Training Loss =  0.31300279398451425; Validation Loss = 0.34840862661690963\n",
            "Cost after 297480 iterations : Training Loss =  0.3130027807747846; Validation Loss = 0.3484088744672171\n",
            "Cost after 297481 iterations : Training Loss =  0.3130028403848111; Validation Loss = 0.3484083978781966\n",
            "Cost after 297482 iterations : Training Loss =  0.3130026815058466; Validation Loss = 0.3484086457285038\n",
            "Cost after 297483 iterations : Training Loss =  0.3130028867851077; Validation Loss = 0.3484081691394837\n",
            "Cost after 297484 iterations : Training Loss =  0.31300271948014563; Validation Loss = 0.34840841698979097\n",
            "Cost after 297485 iterations : Training Loss =  0.31300279594216707; Validation Loss = 0.3484086648400985\n",
            "Cost after 297486 iterations : Training Loss =  0.3130027658804426; Validation Loss = 0.3484081882510778\n",
            "Cost after 297487 iterations : Training Loss =  0.3130026966732294; Validation Loss = 0.34840843610138506\n",
            "Cost after 297488 iterations : Training Loss =  0.31300281228073906; Validation Loss = 0.34840795951236436\n",
            "Cost after 297489 iterations : Training Loss =  0.3130026449757776; Validation Loss = 0.3484082073626719\n",
            "Cost after 297490 iterations : Training Loss =  0.3130028111095501; Validation Loss = 0.34840845521297925\n",
            "Cost after 297491 iterations : Training Loss =  0.313002691376074; Validation Loss = 0.3484079786239589\n",
            "Cost after 297492 iterations : Training Loss =  0.3130027118406118; Validation Loss = 0.3484082264742657\n",
            "Cost after 297493 iterations : Training Loss =  0.3130027377763704; Validation Loss = 0.34840774988524525\n",
            "Cost after 297494 iterations : Training Loss =  0.313002612571674; Validation Loss = 0.3484079977355529\n",
            "Cost after 297495 iterations : Training Loss =  0.3130027841766673; Validation Loss = 0.348407521146532\n",
            "Cost after 297496 iterations : Training Loss =  0.3130026168717055; Validation Loss = 0.3484077689968392\n",
            "Cost after 297497 iterations : Training Loss =  0.3130027270079947; Validation Loss = 0.34840801684714723\n",
            "Cost after 297498 iterations : Training Loss =  0.31300266327200216; Validation Loss = 0.34840754025812587\n",
            "Cost after 297499 iterations : Training Loss =  0.3130026277390563; Validation Loss = 0.34840778810843404\n",
            "Cost after 297500 iterations : Training Loss =  0.3130027096722989; Validation Loss = 0.34840731151941284\n",
            "Cost after 297501 iterations : Training Loss =  0.3130025423673374; Validation Loss = 0.3484075593697206\n",
            "Cost after 297502 iterations : Training Loss =  0.3130027421753771; Validation Loss = 0.348407807220028\n",
            "Cost after 297503 iterations : Training Loss =  0.3130025887676337; Validation Loss = 0.348407330631007\n",
            "Cost after 297504 iterations : Training Loss =  0.31300264290643925; Validation Loss = 0.3484075784813149\n",
            "Cost after 297505 iterations : Training Loss =  0.31300263516793025; Validation Loss = 0.348407101892294\n",
            "Cost after 297506 iterations : Training Loss =  0.31300254363750146; Validation Loss = 0.34840734974260146\n",
            "Cost after 297507 iterations : Training Loss =  0.31300268156822725; Validation Loss = 0.3484068731535807\n",
            "Cost after 297508 iterations : Training Loss =  0.3130025142632652; Validation Loss = 0.348407121003888\n",
            "Cost after 297509 iterations : Training Loss =  0.3130026580738218; Validation Loss = 0.3484073688541956\n",
            "Cost after 297510 iterations : Training Loss =  0.31300256066356197; Validation Loss = 0.34840689226517507\n",
            "Cost after 297511 iterations : Training Loss =  0.3130025588048839; Validation Loss = 0.3484071401154825\n",
            "Cost after 297512 iterations : Training Loss =  0.3130026070638585; Validation Loss = 0.34840666352646127\n",
            "Cost after 297513 iterations : Training Loss =  0.31300245953594613; Validation Loss = 0.3484069113767689\n",
            "Cost after 297514 iterations : Training Loss =  0.31300265346415534; Validation Loss = 0.3484064347877481\n",
            "Cost after 297515 iterations : Training Loss =  0.31300248615919346; Validation Loss = 0.3484066826380557\n",
            "Cost after 297516 iterations : Training Loss =  0.3130025739722668; Validation Loss = 0.3484069304883633\n",
            "Cost after 297517 iterations : Training Loss =  0.31300253255949007; Validation Loss = 0.34840645389934255\n",
            "Cost after 297518 iterations : Training Loss =  0.3130024747033287; Validation Loss = 0.3484067017496497\n",
            "Cost after 297519 iterations : Training Loss =  0.31300257895978684; Validation Loss = 0.34840622516062913\n",
            "Cost after 297520 iterations : Training Loss =  0.3130024116548252; Validation Loss = 0.348406473010937\n",
            "Cost after 297521 iterations : Training Loss =  0.3130025891396493; Validation Loss = 0.3484067208612446\n",
            "Cost after 297522 iterations : Training Loss =  0.3130024580551215; Validation Loss = 0.34840624427222366\n",
            "Cost after 297523 iterations : Training Loss =  0.3130024898707114; Validation Loss = 0.34840649212253083\n",
            "Cost after 297524 iterations : Training Loss =  0.3130025044554184; Validation Loss = 0.3484060155335099\n",
            "Cost after 297525 iterations : Training Loss =  0.3130023906017734; Validation Loss = 0.3484062633838181\n",
            "Cost after 297526 iterations : Training Loss =  0.3130025508557151; Validation Loss = 0.3484057867947966\n",
            "Cost after 297527 iterations : Training Loss =  0.3130023835507532; Validation Loss = 0.3484060346451044\n",
            "Cost after 297528 iterations : Training Loss =  0.3130025050380937; Validation Loss = 0.3484062824954118\n",
            "Cost after 297529 iterations : Training Loss =  0.3130024299510498; Validation Loss = 0.3484058059063909\n",
            "Cost after 297530 iterations : Training Loss =  0.3130024057691561; Validation Loss = 0.34840605375669853\n",
            "Cost after 297531 iterations : Training Loss =  0.31300247635134654; Validation Loss = 0.3484055771676779\n",
            "Cost after 297532 iterations : Training Loss =  0.3130023090463847; Validation Loss = 0.34840582501798495\n",
            "Cost after 297533 iterations : Training Loss =  0.3130025202054766; Validation Loss = 0.34840607286829256\n",
            "Cost after 297534 iterations : Training Loss =  0.31300235544668137; Validation Loss = 0.3484055962792719\n",
            "Cost after 297535 iterations : Training Loss =  0.3130024209365389; Validation Loss = 0.34840584412957953\n",
            "Cost after 297536 iterations : Training Loss =  0.313002401846978; Validation Loss = 0.3484053675405589\n",
            "Cost after 297537 iterations : Training Loss =  0.31300232166760095; Validation Loss = 0.3484056153908665\n",
            "Cost after 297538 iterations : Training Loss =  0.3130024482472748; Validation Loss = 0.34840513880184537\n",
            "Cost after 297539 iterations : Training Loss =  0.3130022809423129; Validation Loss = 0.34840538665215304\n",
            "Cost after 297540 iterations : Training Loss =  0.31300243610392137; Validation Loss = 0.34840563450246065\n",
            "Cost after 297541 iterations : Training Loss =  0.31300232734260974; Validation Loss = 0.3484051579134399\n",
            "Cost after 297542 iterations : Training Loss =  0.31300233683498335; Validation Loss = 0.34840540576374734\n",
            "Cost after 297543 iterations : Training Loss =  0.31300237374290646; Validation Loss = 0.3484049291747263\n",
            "Cost after 297544 iterations : Training Loss =  0.3130022375660456; Validation Loss = 0.34840517702503365\n",
            "Cost after 297545 iterations : Training Loss =  0.3130024201432028; Validation Loss = 0.348404700436013\n",
            "Cost after 297546 iterations : Training Loss =  0.3130022528382411; Validation Loss = 0.34840494828632096\n",
            "Cost after 297547 iterations : Training Loss =  0.31300235200236604; Validation Loss = 0.3484051961366283\n",
            "Cost after 297548 iterations : Training Loss =  0.31300229923853784; Validation Loss = 0.3484047195476076\n",
            "Cost after 297549 iterations : Training Loss =  0.31300225273342824; Validation Loss = 0.34840496739791493\n",
            "Cost after 297550 iterations : Training Loss =  0.31300234563883467; Validation Loss = 0.348404490808894\n",
            "Cost after 297551 iterations : Training Loss =  0.31300217833387256; Validation Loss = 0.348404738659202\n",
            "Cost after 297552 iterations : Training Loss =  0.3130023671697487; Validation Loss = 0.3484049865095092\n",
            "Cost after 297553 iterations : Training Loss =  0.31300222473416933; Validation Loss = 0.34840450992048844\n",
            "Cost after 297554 iterations : Training Loss =  0.31300226790081087; Validation Loss = 0.34840475777079605\n",
            "Cost after 297555 iterations : Training Loss =  0.31300227113446605; Validation Loss = 0.3484042811817749\n",
            "Cost after 297556 iterations : Training Loss =  0.31300216863187297; Validation Loss = 0.3484045290320827\n",
            "Cost after 297557 iterations : Training Loss =  0.3130023175347626; Validation Loss = 0.34840405244306183\n",
            "Cost after 297558 iterations : Training Loss =  0.3130021502298008; Validation Loss = 0.3484043002933693\n",
            "Cost after 297559 iterations : Training Loss =  0.3130022830681934; Validation Loss = 0.34840454814367705\n",
            "Cost after 297560 iterations : Training Loss =  0.31300219663009754; Validation Loss = 0.34840407155465647\n",
            "Cost after 297561 iterations : Training Loss =  0.3130021837992555; Validation Loss = 0.3484043194049633\n",
            "Cost after 297562 iterations : Training Loss =  0.31300224303039437; Validation Loss = 0.34840384281594283\n",
            "Cost after 297563 iterations : Training Loss =  0.31300208453031775; Validation Loss = 0.34840409066625\n",
            "Cost after 297564 iterations : Training Loss =  0.31300228943069097; Validation Loss = 0.3484036140772296\n",
            "Cost after 297565 iterations : Training Loss =  0.3130021221257291; Validation Loss = 0.34840386192753686\n",
            "Cost after 297566 iterations : Training Loss =  0.3130021989666384; Validation Loss = 0.3484041097778446\n",
            "Cost after 297567 iterations : Training Loss =  0.31300216852602575; Validation Loss = 0.34840363318882395\n",
            "Cost after 297568 iterations : Training Loss =  0.3130020996977004; Validation Loss = 0.3484038810391313\n",
            "Cost after 297569 iterations : Training Loss =  0.31300221492632224; Validation Loss = 0.3484034044501108\n",
            "Cost after 297570 iterations : Training Loss =  0.3130020476213607; Validation Loss = 0.348403652300418\n",
            "Cost after 297571 iterations : Training Loss =  0.31300221413402085; Validation Loss = 0.3484039001507256\n",
            "Cost after 297572 iterations : Training Loss =  0.31300209402165735; Validation Loss = 0.34840342356170456\n",
            "Cost after 297573 iterations : Training Loss =  0.31300211486508306; Validation Loss = 0.3484036714120123\n",
            "Cost after 297574 iterations : Training Loss =  0.31300214042195396; Validation Loss = 0.3484031948229914\n",
            "Cost after 297575 iterations : Training Loss =  0.313002015596145; Validation Loss = 0.3484034426732988\n",
            "Cost after 297576 iterations : Training Loss =  0.3130021868222505; Validation Loss = 0.34840296608427807\n",
            "Cost after 297577 iterations : Training Loss =  0.31300201951728895; Validation Loss = 0.3484032139345855\n",
            "Cost after 297578 iterations : Training Loss =  0.31300213003246574; Validation Loss = 0.348403461784893\n",
            "Cost after 297579 iterations : Training Loss =  0.3130020659175856; Validation Loss = 0.3484029851958725\n",
            "Cost after 297580 iterations : Training Loss =  0.31300203076352767; Validation Loss = 0.34840323304618\n",
            "Cost after 297581 iterations : Training Loss =  0.3130021123178821; Validation Loss = 0.34840275645715907\n",
            "Cost after 297582 iterations : Training Loss =  0.31300194501292034; Validation Loss = 0.34840300430746657\n",
            "Cost after 297583 iterations : Training Loss =  0.31300214519984826; Validation Loss = 0.3484032521577738\n",
            "Cost after 297584 iterations : Training Loss =  0.31300199141321705; Validation Loss = 0.3484027755687531\n",
            "Cost after 297585 iterations : Training Loss =  0.31300204593091024; Validation Loss = 0.3484030234190611\n",
            "Cost after 297586 iterations : Training Loss =  0.31300203781351377; Validation Loss = 0.3484025468300399\n",
            "Cost after 297587 iterations : Training Loss =  0.3130019466619726; Validation Loss = 0.3484027946803475\n",
            "Cost after 297588 iterations : Training Loss =  0.3130020842138105; Validation Loss = 0.3484023180913265\n",
            "Cost after 297589 iterations : Training Loss =  0.31300191690884865; Validation Loss = 0.34840256594163393\n",
            "Cost after 297590 iterations : Training Loss =  0.31300206109829326; Validation Loss = 0.3484028137919418\n",
            "Cost after 297591 iterations : Training Loss =  0.3130019633091451; Validation Loss = 0.34840233720292124\n",
            "Cost after 297592 iterations : Training Loss =  0.313001961829355; Validation Loss = 0.3484025850532283\n",
            "Cost after 297593 iterations : Training Loss =  0.3130020097094418; Validation Loss = 0.3484021084642076\n",
            "Cost after 297594 iterations : Training Loss =  0.3130018625604174; Validation Loss = 0.34840235631451505\n",
            "Cost after 297595 iterations : Training Loss =  0.31300205610973864; Validation Loss = 0.34840187972549413\n",
            "Cost after 297596 iterations : Training Loss =  0.31300188880477686; Validation Loss = 0.348402127575802\n",
            "Cost after 297597 iterations : Training Loss =  0.31300197699673765; Validation Loss = 0.34840237542610963\n",
            "Cost after 297598 iterations : Training Loss =  0.31300193520507325; Validation Loss = 0.3484018988370887\n",
            "Cost after 297599 iterations : Training Loss =  0.3130018777277997; Validation Loss = 0.34840214668739605\n",
            "Cost after 297600 iterations : Training Loss =  0.31300198160537007; Validation Loss = 0.3484016700983754\n",
            "Cost after 297601 iterations : Training Loss =  0.3130018143004083; Validation Loss = 0.34840191794868297\n",
            "Cost after 297602 iterations : Training Loss =  0.31300199216412017; Validation Loss = 0.34840216579899064\n",
            "Cost after 297603 iterations : Training Loss =  0.3130018607007052; Validation Loss = 0.3484016892099696\n",
            "Cost after 297604 iterations : Training Loss =  0.3130018928951821; Validation Loss = 0.3484019370602769\n",
            "Cost after 297605 iterations : Training Loss =  0.31300190710100156; Validation Loss = 0.3484014604712562\n",
            "Cost after 297606 iterations : Training Loss =  0.31300179362624464; Validation Loss = 0.3484017083215636\n",
            "Cost after 297607 iterations : Training Loss =  0.313001953501298; Validation Loss = 0.34840123173254284\n",
            "Cost after 297608 iterations : Training Loss =  0.3130017861963364; Validation Loss = 0.3484014795828503\n",
            "Cost after 297609 iterations : Training Loss =  0.31300190806256484; Validation Loss = 0.348401727433158\n",
            "Cost after 297610 iterations : Training Loss =  0.31300183259663333; Validation Loss = 0.3484012508441372\n",
            "Cost after 297611 iterations : Training Loss =  0.31300180879362716; Validation Loss = 0.34840149869444503\n",
            "Cost after 297612 iterations : Training Loss =  0.31300187899693; Validation Loss = 0.348401022105424\n",
            "Cost after 297613 iterations : Training Loss =  0.3130017116919683; Validation Loss = 0.34840126995573173\n",
            "Cost after 297614 iterations : Training Loss =  0.3130019232299478; Validation Loss = 0.34840151780603884\n",
            "Cost after 297615 iterations : Training Loss =  0.3130017580922649; Validation Loss = 0.34840104121701815\n",
            "Cost after 297616 iterations : Training Loss =  0.3130018239610098; Validation Loss = 0.34840128906732576\n",
            "Cost after 297617 iterations : Training Loss =  0.3130018044925615; Validation Loss = 0.34840081247830484\n",
            "Cost after 297618 iterations : Training Loss =  0.31300172469207194; Validation Loss = 0.3484010603286123\n",
            "Cost after 297619 iterations : Training Loss =  0.31300185089285815; Validation Loss = 0.34840058373959176\n",
            "Cost after 297620 iterations : Training Loss =  0.31300168358789665; Validation Loss = 0.34840083158989926\n",
            "Cost after 297621 iterations : Training Loss =  0.3130018391283924; Validation Loss = 0.34840107944020643\n",
            "Cost after 297622 iterations : Training Loss =  0.313001729988193; Validation Loss = 0.34840060285118585\n",
            "Cost after 297623 iterations : Training Loss =  0.3130017398594547; Validation Loss = 0.34840085070149346\n",
            "Cost after 297624 iterations : Training Loss =  0.31300177638848986; Validation Loss = 0.3484003741124729\n",
            "Cost after 297625 iterations : Training Loss =  0.31300164059051677; Validation Loss = 0.34840062196278027\n",
            "Cost after 297626 iterations : Training Loss =  0.31300182278878624; Validation Loss = 0.34840014537375935\n",
            "Cost after 297627 iterations : Training Loss =  0.3130016554838246; Validation Loss = 0.3484003932240672\n",
            "Cost after 297628 iterations : Training Loss =  0.3130017550268373; Validation Loss = 0.3484006410743741\n",
            "Cost after 297629 iterations : Training Loss =  0.313001701884121; Validation Loss = 0.3484001644853534\n",
            "Cost after 297630 iterations : Training Loss =  0.3130016557578994; Validation Loss = 0.3484004123356611\n",
            "Cost after 297631 iterations : Training Loss =  0.31300174828441807; Validation Loss = 0.3483999357466405\n",
            "Cost after 297632 iterations : Training Loss =  0.3130015809794559; Validation Loss = 0.3484001835969475\n",
            "Cost after 297633 iterations : Training Loss =  0.3130017701942198; Validation Loss = 0.34840043144725513\n",
            "Cost after 297634 iterations : Training Loss =  0.3130016273797528; Validation Loss = 0.3483999548582344\n",
            "Cost after 297635 iterations : Training Loss =  0.3130016709252821; Validation Loss = 0.348400202708542\n",
            "Cost after 297636 iterations : Training Loss =  0.3130016737800492; Validation Loss = 0.34839972611952086\n",
            "Cost after 297637 iterations : Training Loss =  0.3130015716563441; Validation Loss = 0.34839997396982886\n",
            "Cost after 297638 iterations : Training Loss =  0.31300172018034594; Validation Loss = 0.3483994973808078\n",
            "Cost after 297639 iterations : Training Loss =  0.31300155287538434; Validation Loss = 0.3483997452311154\n",
            "Cost after 297640 iterations : Training Loss =  0.3130016860926644; Validation Loss = 0.34839999308142294\n",
            "Cost after 297641 iterations : Training Loss =  0.31300159927568094; Validation Loss = 0.34839951649240186\n",
            "Cost after 297642 iterations : Training Loss =  0.3130015868237267; Validation Loss = 0.34839976434270975\n",
            "Cost after 297643 iterations : Training Loss =  0.3130016456759776; Validation Loss = 0.3483992877536886\n",
            "Cost after 297644 iterations : Training Loss =  0.31300148755478885; Validation Loss = 0.3483995356039962\n",
            "Cost after 297645 iterations : Training Loss =  0.3130016920762742; Validation Loss = 0.3483990590149756\n",
            "Cost after 297646 iterations : Training Loss =  0.31300152477131266; Validation Loss = 0.34839930686528287\n",
            "Cost after 297647 iterations : Training Loss =  0.3130016019911093; Validation Loss = 0.34839955471559064\n",
            "Cost after 297648 iterations : Training Loss =  0.3130015711716092; Validation Loss = 0.3483990781265699\n",
            "Cost after 297649 iterations : Training Loss =  0.3130015027221713; Validation Loss = 0.3483993259768772\n",
            "Cost after 297650 iterations : Training Loss =  0.31300161757190564; Validation Loss = 0.3483988493878564\n",
            "Cost after 297651 iterations : Training Loss =  0.31300145026694387; Validation Loss = 0.34839909723816403\n",
            "Cost after 297652 iterations : Training Loss =  0.31300161715849206; Validation Loss = 0.3483993450884718\n",
            "Cost after 297653 iterations : Training Loss =  0.3130014966672406; Validation Loss = 0.348398868499451\n",
            "Cost after 297654 iterations : Training Loss =  0.313001517889554; Validation Loss = 0.3483991163497584\n",
            "Cost after 297655 iterations : Training Loss =  0.31300154306753736; Validation Loss = 0.34839863976073754\n",
            "Cost after 297656 iterations : Training Loss =  0.3130014186206161; Validation Loss = 0.34839888761104504\n",
            "Cost after 297657 iterations : Training Loss =  0.3130015894678341; Validation Loss = 0.3483984110220241\n",
            "Cost after 297658 iterations : Training Loss =  0.3130014221628722; Validation Loss = 0.3483986588723316\n",
            "Cost after 297659 iterations : Training Loss =  0.3130015330569368; Validation Loss = 0.34839890672263935\n",
            "Cost after 297660 iterations : Training Loss =  0.313001468563169; Validation Loss = 0.3483984301336183\n",
            "Cost after 297661 iterations : Training Loss =  0.31300143378799883; Validation Loss = 0.34839867798392604\n",
            "Cost after 297662 iterations : Training Loss =  0.31300151496346545; Validation Loss = 0.3483982013949051\n",
            "Cost after 297663 iterations : Training Loss =  0.31300134765850374; Validation Loss = 0.3483984492452125\n",
            "Cost after 297664 iterations : Training Loss =  0.313001548224319; Validation Loss = 0.3483986970955201\n",
            "Cost after 297665 iterations : Training Loss =  0.3130013940588005; Validation Loss = 0.34839822050649916\n",
            "Cost after 297666 iterations : Training Loss =  0.31300144895538135; Validation Loss = 0.34839846835680677\n",
            "Cost after 297667 iterations : Training Loss =  0.31300144045909717; Validation Loss = 0.34839799176778585\n",
            "Cost after 297668 iterations : Training Loss =  0.3130013496864436; Validation Loss = 0.3483982396180936\n",
            "Cost after 297669 iterations : Training Loss =  0.3130014868593935; Validation Loss = 0.3483977630290726\n",
            "Cost after 297670 iterations : Training Loss =  0.3130013195544317; Validation Loss = 0.34839801087938044\n",
            "Cost after 297671 iterations : Training Loss =  0.3130014641227639; Validation Loss = 0.3483982587296873\n",
            "Cost after 297672 iterations : Training Loss =  0.3130013659547289; Validation Loss = 0.3483977821406671\n",
            "Cost after 297673 iterations : Training Loss =  0.3130013648538262; Validation Loss = 0.3483980299909745\n",
            "Cost after 297674 iterations : Training Loss =  0.3130014123550255; Validation Loss = 0.34839755340195383\n",
            "Cost after 297675 iterations : Training Loss =  0.3130012655848882; Validation Loss = 0.34839780125226144\n",
            "Cost after 297676 iterations : Training Loss =  0.31300145875532187; Validation Loss = 0.34839732466324064\n",
            "Cost after 297677 iterations : Training Loss =  0.3130012914503602; Validation Loss = 0.34839757251354764\n",
            "Cost after 297678 iterations : Training Loss =  0.3130013800212088; Validation Loss = 0.34839782036385536\n",
            "Cost after 297679 iterations : Training Loss =  0.3130013378506568; Validation Loss = 0.34839734377483456\n",
            "Cost after 297680 iterations : Training Loss =  0.31300128075227074; Validation Loss = 0.3483975916251421\n",
            "Cost after 297681 iterations : Training Loss =  0.3130013842509535; Validation Loss = 0.3483971150361214\n",
            "Cost after 297682 iterations : Training Loss =  0.31300121694599187; Validation Loss = 0.34839736288642914\n",
            "Cost after 297683 iterations : Training Loss =  0.3130013951885916; Validation Loss = 0.3483976107367364\n",
            "Cost after 297684 iterations : Training Loss =  0.31300126334628847; Validation Loss = 0.34839713414771584\n",
            "Cost after 297685 iterations : Training Loss =  0.3130012959196535; Validation Loss = 0.3483973819980231\n",
            "Cost after 297686 iterations : Training Loss =  0.31300130974658497; Validation Loss = 0.3483969054090022\n",
            "Cost after 297687 iterations : Training Loss =  0.3130011966507157; Validation Loss = 0.3483971532593097\n",
            "Cost after 297688 iterations : Training Loss =  0.3130013561468817; Validation Loss = 0.3483966766702889\n",
            "Cost after 297689 iterations : Training Loss =  0.3130011888419197; Validation Loss = 0.3483969245205968\n",
            "Cost after 297690 iterations : Training Loss =  0.3130013110870361; Validation Loss = 0.3483971723709044\n",
            "Cost after 297691 iterations : Training Loss =  0.3130012352422164; Validation Loss = 0.3483966957818833\n",
            "Cost after 297692 iterations : Training Loss =  0.3130012118180983; Validation Loss = 0.3483969436321907\n",
            "Cost after 297693 iterations : Training Loss =  0.3130012816425132; Validation Loss = 0.34839646704317007\n",
            "Cost after 297694 iterations : Training Loss =  0.3130011143375518; Validation Loss = 0.3483967148934777\n",
            "Cost after 297695 iterations : Training Loss =  0.31300132625441884; Validation Loss = 0.3483969627437851\n",
            "Cost after 297696 iterations : Training Loss =  0.31300116073784817; Validation Loss = 0.3483964861547644\n",
            "Cost after 297697 iterations : Training Loss =  0.3130012269854808; Validation Loss = 0.34839673400507165\n",
            "Cost after 297698 iterations : Training Loss =  0.3130012071381448; Validation Loss = 0.3483962574160516\n",
            "Cost after 297699 iterations : Training Loss =  0.313001127716543; Validation Loss = 0.34839650526635885\n",
            "Cost after 297700 iterations : Training Loss =  0.3130012535384415; Validation Loss = 0.3483960286773378\n",
            "Cost after 297701 iterations : Training Loss =  0.3130010862334795; Validation Loss = 0.3483962765276453\n",
            "Cost after 297702 iterations : Training Loss =  0.3130012421528634; Validation Loss = 0.3483965243779528\n",
            "Cost after 297703 iterations : Training Loss =  0.3130011326337763; Validation Loss = 0.34839604778893163\n",
            "Cost after 297704 iterations : Training Loss =  0.3130011428839257; Validation Loss = 0.3483962956392395\n",
            "Cost after 297705 iterations : Training Loss =  0.3130011790340727; Validation Loss = 0.34839581905021866\n",
            "Cost after 297706 iterations : Training Loss =  0.3130010436149877; Validation Loss = 0.3483960669005262\n",
            "Cost after 297707 iterations : Training Loss =  0.3130012254343696; Validation Loss = 0.3483955903115054\n",
            "Cost after 297708 iterations : Training Loss =  0.31300105812940776; Validation Loss = 0.3483958381618127\n",
            "Cost after 297709 iterations : Training Loss =  0.3130011580513082; Validation Loss = 0.3483960860121204\n",
            "Cost after 297710 iterations : Training Loss =  0.3130011045297044; Validation Loss = 0.3483956094230999\n",
            "Cost after 297711 iterations : Training Loss =  0.3130010587823702; Validation Loss = 0.3483958572734069\n",
            "Cost after 297712 iterations : Training Loss =  0.3130011509300011; Validation Loss = 0.3483953806843864\n",
            "Cost after 297713 iterations : Training Loss =  0.3130009836250395; Validation Loss = 0.3483956285346942\n",
            "Cost after 297714 iterations : Training Loss =  0.3130011732186911; Validation Loss = 0.3483958763850014\n",
            "Cost after 297715 iterations : Training Loss =  0.31300103002533614; Validation Loss = 0.3483953997959806\n",
            "Cost after 297716 iterations : Training Loss =  0.313001073949753; Validation Loss = 0.3483956476462878\n",
            "Cost after 297717 iterations : Training Loss =  0.31300107642563274; Validation Loss = 0.34839517105726736\n",
            "Cost after 297718 iterations : Training Loss =  0.31300097468081506; Validation Loss = 0.3483954189075748\n",
            "Cost after 297719 iterations : Training Loss =  0.3130011228259294; Validation Loss = 0.348394942318554\n",
            "Cost after 297720 iterations : Training Loss =  0.3130009555209674; Validation Loss = 0.34839519016886145\n",
            "Cost after 297721 iterations : Training Loss =  0.3130010891171355; Validation Loss = 0.3483954380191689\n",
            "Cost after 297722 iterations : Training Loss =  0.31300100192126434; Validation Loss = 0.3483949614301482\n",
            "Cost after 297723 iterations : Training Loss =  0.3130009898481978; Validation Loss = 0.3483952092804556\n",
            "Cost after 297724 iterations : Training Loss =  0.31300104832156095; Validation Loss = 0.34839473269143484\n",
            "Cost after 297725 iterations : Training Loss =  0.3130008905792597; Validation Loss = 0.3483949805417423\n",
            "Cost after 297726 iterations : Training Loss =  0.3130010947218574; Validation Loss = 0.3483945039527216\n",
            "Cost after 297727 iterations : Training Loss =  0.31300092741689584; Validation Loss = 0.34839475180302937\n",
            "Cost after 297728 iterations : Training Loss =  0.3130010050155804; Validation Loss = 0.34839499965333703\n",
            "Cost after 297729 iterations : Training Loss =  0.3130009738171925; Validation Loss = 0.34839452306431595\n",
            "Cost after 297730 iterations : Training Loss =  0.31300090574664247; Validation Loss = 0.3483947709146233\n",
            "Cost after 297731 iterations : Training Loss =  0.313001020217489; Validation Loss = 0.34839429432560265\n",
            "Cost after 297732 iterations : Training Loss =  0.313000852912527; Validation Loss = 0.34839454217591015\n",
            "Cost after 297733 iterations : Training Loss =  0.313001020182963; Validation Loss = 0.3483947900262176\n",
            "Cost after 297734 iterations : Training Loss =  0.31300089931282393; Validation Loss = 0.3483943134371967\n",
            "Cost after 297735 iterations : Training Loss =  0.313000920914025; Validation Loss = 0.3483945612875041\n",
            "Cost after 297736 iterations : Training Loss =  0.3130009457131206; Validation Loss = 0.3483940846984837\n",
            "Cost after 297737 iterations : Training Loss =  0.31300082164508747; Validation Loss = 0.3483943325487907\n",
            "Cost after 297738 iterations : Training Loss =  0.31300099211341703; Validation Loss = 0.34839385595977035\n",
            "Cost after 297739 iterations : Training Loss =  0.3130008248084555; Validation Loss = 0.3483941038100777\n",
            "Cost after 297740 iterations : Training Loss =  0.3130009360814078; Validation Loss = 0.3483943516603853\n",
            "Cost after 297741 iterations : Training Loss =  0.31300087120875225; Validation Loss = 0.3483938750713642\n",
            "Cost after 297742 iterations : Training Loss =  0.3130008368124698; Validation Loss = 0.3483941229216721\n",
            "Cost after 297743 iterations : Training Loss =  0.3130009176090489; Validation Loss = 0.34839364633265135\n",
            "Cost after 297744 iterations : Training Loss =  0.3130007503040871; Validation Loss = 0.3483938941829588\n",
            "Cost after 297745 iterations : Training Loss =  0.3130009512487901; Validation Loss = 0.3483941420332662\n",
            "Cost after 297746 iterations : Training Loss =  0.31300079670438363; Validation Loss = 0.3483936654442454\n",
            "Cost after 297747 iterations : Training Loss =  0.3130008519798524; Validation Loss = 0.3483939132945524\n",
            "Cost after 297748 iterations : Training Loss =  0.31300084310468035; Validation Loss = 0.3483934367055319\n",
            "Cost after 297749 iterations : Training Loss =  0.31300075271091465; Validation Loss = 0.3483936845558398\n",
            "Cost after 297750 iterations : Training Loss =  0.3130008895049771; Validation Loss = 0.3483932079668188\n",
            "Cost after 297751 iterations : Training Loss =  0.3130007222000154; Validation Loss = 0.34839345581712666\n",
            "Cost after 297752 iterations : Training Loss =  0.3130008671472352; Validation Loss = 0.34839370366743394\n",
            "Cost after 297753 iterations : Training Loss =  0.31300076860031195; Validation Loss = 0.3483932270784131\n",
            "Cost after 297754 iterations : Training Loss =  0.31300076787829706; Validation Loss = 0.34839347492872047\n",
            "Cost after 297755 iterations : Training Loss =  0.31300081500060856; Validation Loss = 0.3483929983396999\n",
            "Cost after 297756 iterations : Training Loss =  0.3130006686093594; Validation Loss = 0.34839324619000733\n",
            "Cost after 297757 iterations : Training Loss =  0.3130008614009053; Validation Loss = 0.34839276960098675\n",
            "Cost after 297758 iterations : Training Loss =  0.3130006940959434; Validation Loss = 0.3483930174512943\n",
            "Cost after 297759 iterations : Training Loss =  0.3130007830456799; Validation Loss = 0.34839326530160153\n",
            "Cost after 297760 iterations : Training Loss =  0.3130007404962403; Validation Loss = 0.3483927887125809\n",
            "Cost after 297761 iterations : Training Loss =  0.31300068377674223; Validation Loss = 0.3483930365628885\n",
            "Cost after 297762 iterations : Training Loss =  0.3130007868965366; Validation Loss = 0.34839255997386737\n",
            "Cost after 297763 iterations : Training Loss =  0.31300061959157516; Validation Loss = 0.348392807824175\n",
            "Cost after 297764 iterations : Training Loss =  0.31300079821306254; Validation Loss = 0.3483930556744828\n",
            "Cost after 297765 iterations : Training Loss =  0.3130006659918716; Validation Loss = 0.34839257908546173\n",
            "Cost after 297766 iterations : Training Loss =  0.3130006989441245; Validation Loss = 0.3483928269357692\n",
            "Cost after 297767 iterations : Training Loss =  0.3130007123921685; Validation Loss = 0.3483923503467486\n",
            "Cost after 297768 iterations : Training Loss =  0.3130005996751868; Validation Loss = 0.34839259819705576\n",
            "Cost after 297769 iterations : Training Loss =  0.3130007587924647; Validation Loss = 0.3483921216080354\n",
            "Cost after 297770 iterations : Training Loss =  0.31300059148750314; Validation Loss = 0.3483923694583429\n",
            "Cost after 297771 iterations : Training Loss =  0.3130007141115075; Validation Loss = 0.3483926173086499\n",
            "Cost after 297772 iterations : Training Loss =  0.3130006378877998; Validation Loss = 0.3483921407196295\n",
            "Cost after 297773 iterations : Training Loss =  0.31300061484256925; Validation Loss = 0.34839238856993715\n",
            "Cost after 297774 iterations : Training Loss =  0.31300068428809663; Validation Loss = 0.3483919119809161\n",
            "Cost after 297775 iterations : Training Loss =  0.3130005169831348; Validation Loss = 0.34839215983122385\n",
            "Cost after 297776 iterations : Training Loss =  0.3130007292788901; Validation Loss = 0.34839240768153107\n",
            "Cost after 297777 iterations : Training Loss =  0.3130005633834316; Validation Loss = 0.3483919310925104\n",
            "Cost after 297778 iterations : Training Loss =  0.31300063000995204; Validation Loss = 0.34839217894281804\n",
            "Cost after 297779 iterations : Training Loss =  0.31300060978372796; Validation Loss = 0.34839170235379685\n",
            "Cost after 297780 iterations : Training Loss =  0.3130005307410142; Validation Loss = 0.3483919502041043\n",
            "Cost after 297781 iterations : Training Loss =  0.31300065618402473; Validation Loss = 0.34839147361508394\n",
            "Cost after 297782 iterations : Training Loss =  0.3130004888790631; Validation Loss = 0.34839172146539116\n",
            "Cost after 297783 iterations : Training Loss =  0.3130006451773347; Validation Loss = 0.3483919693156987\n",
            "Cost after 297784 iterations : Training Loss =  0.3130005352793598; Validation Loss = 0.34839149272667785\n",
            "Cost after 297785 iterations : Training Loss =  0.3130005459083969; Validation Loss = 0.3483917405769856\n",
            "Cost after 297786 iterations : Training Loss =  0.313000581679656; Validation Loss = 0.34839126398796444\n",
            "Cost after 297787 iterations : Training Loss =  0.31300044663945886; Validation Loss = 0.3483915118382721\n",
            "Cost after 297788 iterations : Training Loss =  0.3130006280799528; Validation Loss = 0.34839103524925136\n",
            "Cost after 297789 iterations : Training Loss =  0.3130004607749911; Validation Loss = 0.34839128309955913\n",
            "Cost after 297790 iterations : Training Loss =  0.3130005610757791; Validation Loss = 0.3483915309498667\n",
            "Cost after 297791 iterations : Training Loss =  0.31300050717528793; Validation Loss = 0.34839105436084566\n",
            "Cost after 297792 iterations : Training Loss =  0.31300046180684155; Validation Loss = 0.3483913022111534\n",
            "Cost after 297793 iterations : Training Loss =  0.3130005535755847; Validation Loss = 0.34839082562213236\n",
            "Cost after 297794 iterations : Training Loss =  0.31300038627062277; Validation Loss = 0.34839107347243975\n",
            "Cost after 297795 iterations : Training Loss =  0.3130005762431621; Validation Loss = 0.3483913213227478\n",
            "Cost after 297796 iterations : Training Loss =  0.3130004326709193; Validation Loss = 0.3483908447337265\n",
            "Cost after 297797 iterations : Training Loss =  0.3130004769742242; Validation Loss = 0.348391092584034\n",
            "Cost after 297798 iterations : Training Loss =  0.31300047907121625; Validation Loss = 0.3483906159950132\n",
            "Cost after 297799 iterations : Training Loss =  0.31300037770528616; Validation Loss = 0.3483908638453207\n",
            "Cost after 297800 iterations : Training Loss =  0.3130005254715125; Validation Loss = 0.34839038725630006\n",
            "Cost after 297801 iterations : Training Loss =  0.3130003581665507; Validation Loss = 0.3483906351066075\n",
            "Cost after 297802 iterations : Training Loss =  0.3130004921416067; Validation Loss = 0.3483908829569153\n",
            "Cost after 297803 iterations : Training Loss =  0.3130004045668474; Validation Loss = 0.3483904063678945\n",
            "Cost after 297804 iterations : Training Loss =  0.31300039287266906; Validation Loss = 0.3483906542182018\n",
            "Cost after 297805 iterations : Training Loss =  0.3130004509671443; Validation Loss = 0.34839017762918106\n",
            "Cost after 297806 iterations : Training Loss =  0.31300029360373116; Validation Loss = 0.3483904254794885\n",
            "Cost after 297807 iterations : Training Loss =  0.3130004973674407; Validation Loss = 0.34838994889046776\n",
            "Cost after 297808 iterations : Training Loss =  0.31300033006247907; Validation Loss = 0.3483901967407754\n",
            "Cost after 297809 iterations : Training Loss =  0.3130004080400517; Validation Loss = 0.3483904445910828\n",
            "Cost after 297810 iterations : Training Loss =  0.31300037646277573; Validation Loss = 0.34838996800206173\n",
            "Cost after 297811 iterations : Training Loss =  0.31300030877111373; Validation Loss = 0.34839021585236934\n",
            "Cost after 297812 iterations : Training Loss =  0.3130004228630724; Validation Loss = 0.3483897392633486\n",
            "Cost after 297813 iterations : Training Loss =  0.3130002555581108; Validation Loss = 0.34838998711365604\n",
            "Cost after 297814 iterations : Training Loss =  0.31300042320743404; Validation Loss = 0.34839023496396354\n",
            "Cost after 297815 iterations : Training Loss =  0.3130003019584073; Validation Loss = 0.3483897583749429\n",
            "Cost after 297816 iterations : Training Loss =  0.31300032393849636; Validation Loss = 0.34839000622525035\n",
            "Cost after 297817 iterations : Training Loss =  0.3130003483587037; Validation Loss = 0.3483895296362293\n",
            "Cost after 297818 iterations : Training Loss =  0.3130002246695585; Validation Loss = 0.34838977748653693\n",
            "Cost after 297819 iterations : Training Loss =  0.3130003947590006; Validation Loss = 0.3483893008975163\n",
            "Cost after 297820 iterations : Training Loss =  0.31300022745403894; Validation Loss = 0.34838954874782363\n",
            "Cost after 297821 iterations : Training Loss =  0.3130003391058788; Validation Loss = 0.34838979659813146\n",
            "Cost after 297822 iterations : Training Loss =  0.3130002738543354; Validation Loss = 0.34838932000911044\n",
            "Cost after 297823 iterations : Training Loss =  0.3130002398369411; Validation Loss = 0.34838956785941816\n",
            "Cost after 297824 iterations : Training Loss =  0.3130003202546322; Validation Loss = 0.3483890912703973\n",
            "Cost after 297825 iterations : Training Loss =  0.3130001529496704; Validation Loss = 0.34838933912070447\n",
            "Cost after 297826 iterations : Training Loss =  0.31300035427326167; Validation Loss = 0.34838958697101197\n",
            "Cost after 297827 iterations : Training Loss =  0.3130001993499669; Validation Loss = 0.3483891103819917\n",
            "Cost after 297828 iterations : Training Loss =  0.3130002550043236; Validation Loss = 0.34838935823229905\n",
            "Cost after 297829 iterations : Training Loss =  0.3130002457502636; Validation Loss = 0.34838888164327825\n",
            "Cost after 297830 iterations : Training Loss =  0.31300015573538587; Validation Loss = 0.34838912949358575\n",
            "Cost after 297831 iterations : Training Loss =  0.3130002921505604; Validation Loss = 0.3483886529045651\n",
            "Cost after 297832 iterations : Training Loss =  0.31300012484559836; Validation Loss = 0.34838890075487255\n",
            "Cost after 297833 iterations : Training Loss =  0.31300027017170645; Validation Loss = 0.34838914860518005\n",
            "Cost after 297834 iterations : Training Loss =  0.313000171245895; Validation Loss = 0.34838867201615914\n",
            "Cost after 297835 iterations : Training Loss =  0.3130001709027683; Validation Loss = 0.34838891986646703\n",
            "Cost after 297836 iterations : Training Loss =  0.3130002176461919; Validation Loss = 0.34838844327744584\n",
            "Cost after 297837 iterations : Training Loss =  0.31300007163383037; Validation Loss = 0.3483886911277533\n",
            "Cost after 297838 iterations : Training Loss =  0.31300026404648856; Validation Loss = 0.34838821453873264\n",
            "Cost after 297839 iterations : Training Loss =  0.31300009674152685; Validation Loss = 0.34838846238904025\n",
            "Cost after 297840 iterations : Training Loss =  0.31300018607015107; Validation Loss = 0.3483887102393477\n",
            "Cost after 297841 iterations : Training Loss =  0.3130001431418233; Validation Loss = 0.3483882336503267\n",
            "Cost after 297842 iterations : Training Loss =  0.31300008680121283; Validation Loss = 0.3483884815006344\n",
            "Cost after 297843 iterations : Training Loss =  0.3130001895421201; Validation Loss = 0.3483880049116131\n",
            "Cost after 297844 iterations : Training Loss =  0.3130000222371584; Validation Loss = 0.34838825276192104\n",
            "Cost after 297845 iterations : Training Loss =  0.3130002012375336; Validation Loss = 0.3483885006122286\n",
            "Cost after 297846 iterations : Training Loss =  0.3130000686374549; Validation Loss = 0.34838802402320795\n",
            "Cost after 297847 iterations : Training Loss =  0.3130001019685961; Validation Loss = 0.3483882718735154\n",
            "Cost after 297848 iterations : Training Loss =  0.31300011503775155; Validation Loss = 0.3483877952844944\n",
            "Cost after 297849 iterations : Training Loss =  0.31300000269965766; Validation Loss = 0.34838804313480226\n",
            "Cost after 297850 iterations : Training Loss =  0.3130001614380483; Validation Loss = 0.34838756654578107\n",
            "Cost after 297851 iterations : Training Loss =  0.31299999413308643; Validation Loss = 0.34838781439608896\n",
            "Cost after 297852 iterations : Training Loss =  0.31300011713597836; Validation Loss = 0.3483880622463963\n",
            "Cost after 297853 iterations : Training Loss =  0.31300004053338326; Validation Loss = 0.3483875856573757\n",
            "Cost after 297854 iterations : Training Loss =  0.31300001786704046; Validation Loss = 0.348387833507683\n",
            "Cost after 297855 iterations : Training Loss =  0.3130000869336799; Validation Loss = 0.34838735691866235\n",
            "Cost after 297856 iterations : Training Loss =  0.3129999196287183; Validation Loss = 0.3483876047689697\n",
            "Cost after 297857 iterations : Training Loss =  0.3130001323033608; Validation Loss = 0.3483878526192771\n",
            "Cost after 297858 iterations : Training Loss =  0.31299996602901453; Validation Loss = 0.34838737603025643\n",
            "Cost after 297859 iterations : Training Loss =  0.31300003303442303; Validation Loss = 0.34838762388056366\n",
            "Cost after 297860 iterations : Training Loss =  0.3130000124293115; Validation Loss = 0.3483871472915432\n",
            "Cost after 297861 iterations : Training Loss =  0.3129999337654851; Validation Loss = 0.3483873951418505\n",
            "Cost after 297862 iterations : Training Loss =  0.31300005882960785; Validation Loss = 0.3483869185528298\n",
            "Cost after 297863 iterations : Training Loss =  0.3129998915246463; Validation Loss = 0.3483871664031375\n",
            "Cost after 297864 iterations : Training Loss =  0.3130000482018058; Validation Loss = 0.34838741425344494\n",
            "Cost after 297865 iterations : Training Loss =  0.312999937924943; Validation Loss = 0.3483869376644242\n",
            "Cost after 297866 iterations : Training Loss =  0.31299994893286787; Validation Loss = 0.3483871855147319\n",
            "Cost after 297867 iterations : Training Loss =  0.3129999843252396; Validation Loss = 0.3483867089257109\n",
            "Cost after 297868 iterations : Training Loss =  0.31299984966392985; Validation Loss = 0.3483869567760185\n",
            "Cost after 297869 iterations : Training Loss =  0.3130000307255364; Validation Loss = 0.34838648018699775\n",
            "Cost after 297870 iterations : Training Loss =  0.3129998634205747; Validation Loss = 0.34838672803730464\n",
            "Cost after 297871 iterations : Training Loss =  0.31299996410025055; Validation Loss = 0.34838697588761236\n",
            "Cost after 297872 iterations : Training Loss =  0.3129999098208713; Validation Loss = 0.348386499298592\n",
            "Cost after 297873 iterations : Training Loss =  0.3129998648313126; Validation Loss = 0.34838674714889933\n",
            "Cost after 297874 iterations : Training Loss =  0.31299995622116783; Validation Loss = 0.3483862705598786\n",
            "Cost after 297875 iterations : Training Loss =  0.3129997889162059; Validation Loss = 0.3483865184101861\n",
            "Cost after 297876 iterations : Training Loss =  0.31299997926763273; Validation Loss = 0.3483867662604935\n",
            "Cost after 297877 iterations : Training Loss =  0.31299983531650283; Validation Loss = 0.3483862896714729\n",
            "Cost after 297878 iterations : Training Loss =  0.3129998799986952; Validation Loss = 0.34838653752178017\n",
            "Cost after 297879 iterations : Training Loss =  0.3129998817167992; Validation Loss = 0.34838606093275915\n",
            "Cost after 297880 iterations : Training Loss =  0.31299978072975715; Validation Loss = 0.3483863087830667\n",
            "Cost after 297881 iterations : Training Loss =  0.3129999281170959; Validation Loss = 0.3483858321940457\n",
            "Cost after 297882 iterations : Training Loss =  0.3129997608121342; Validation Loss = 0.34838608004435345\n",
            "Cost after 297883 iterations : Training Loss =  0.3129998951660779; Validation Loss = 0.3483863278946612\n",
            "Cost after 297884 iterations : Training Loss =  0.312999807212431; Validation Loss = 0.34838585130564026\n",
            "Cost after 297885 iterations : Training Loss =  0.31299979589713994; Validation Loss = 0.34838609915594787\n",
            "Cost after 297886 iterations : Training Loss =  0.3129998536127275; Validation Loss = 0.3483856225669271\n",
            "Cost after 297887 iterations : Training Loss =  0.31299969662820226; Validation Loss = 0.34838587041723457\n",
            "Cost after 297888 iterations : Training Loss =  0.31299990001302413; Validation Loss = 0.3483853938282134\n",
            "Cost after 297889 iterations : Training Loss =  0.3129997327080622; Validation Loss = 0.3483856416785212\n",
            "Cost after 297890 iterations : Training Loss =  0.3129998110645226; Validation Loss = 0.3483858895288289\n",
            "Cost after 297891 iterations : Training Loss =  0.3129997791083593; Validation Loss = 0.34838541293980824\n",
            "Cost after 297892 iterations : Training Loss =  0.3129997117955845; Validation Loss = 0.3483856607901154\n",
            "Cost after 297893 iterations : Training Loss =  0.3129998255086558; Validation Loss = 0.3483851842010946\n",
            "Cost after 297894 iterations : Training Loss =  0.31299965820369396; Validation Loss = 0.34838543205140204\n",
            "Cost after 297895 iterations : Training Loss =  0.31299982623190536; Validation Loss = 0.3483856799017099\n",
            "Cost after 297896 iterations : Training Loss =  0.3129997046039907; Validation Loss = 0.34838520331268924\n",
            "Cost after 297897 iterations : Training Loss =  0.31299972696296735; Validation Loss = 0.3483854511629963\n",
            "Cost after 297898 iterations : Training Loss =  0.3129997510042872; Validation Loss = 0.3483849745739758\n",
            "Cost after 297899 iterations : Training Loss =  0.3129996276940294; Validation Loss = 0.34838522242428327\n",
            "Cost after 297900 iterations : Training Loss =  0.3129997974045838; Validation Loss = 0.34838474583526235\n",
            "Cost after 297901 iterations : Training Loss =  0.3129996300996218; Validation Loss = 0.34838499368556997\n",
            "Cost after 297902 iterations : Training Loss =  0.31299974213035003; Validation Loss = 0.34838524153587785\n",
            "Cost after 297903 iterations : Training Loss =  0.31299967649991867; Validation Loss = 0.3483847649468565\n",
            "Cost after 297904 iterations : Training Loss =  0.31299964286141235; Validation Loss = 0.3483850127971641\n",
            "Cost after 297905 iterations : Training Loss =  0.3129997229002154; Validation Loss = 0.3483845362081432\n",
            "Cost after 297906 iterations : Training Loss =  0.3129995555952537; Validation Loss = 0.34838478405845136\n",
            "Cost after 297907 iterations : Training Loss =  0.3129997572977325; Validation Loss = 0.34838503190875847\n",
            "Cost after 297908 iterations : Training Loss =  0.31299960199555055; Validation Loss = 0.3483845553197375\n",
            "Cost after 297909 iterations : Training Loss =  0.31299965802879487; Validation Loss = 0.34838480317004494\n",
            "Cost after 297910 iterations : Training Loss =  0.312999648395847; Validation Loss = 0.3483843265810245\n",
            "Cost after 297911 iterations : Training Loss =  0.3129995587598567; Validation Loss = 0.348384574431332\n",
            "Cost after 297912 iterations : Training Loss =  0.3129996947961438; Validation Loss = 0.34838409784231134\n",
            "Cost after 297913 iterations : Training Loss =  0.31299952749118176; Validation Loss = 0.34838434569261834\n",
            "Cost after 297914 iterations : Training Loss =  0.312999673196177; Validation Loss = 0.3483845935429261\n",
            "Cost after 297915 iterations : Training Loss =  0.3129995738914786; Validation Loss = 0.3483841169539052\n",
            "Cost after 297916 iterations : Training Loss =  0.3129995739272395; Validation Loss = 0.34838436480421275\n",
            "Cost after 297917 iterations : Training Loss =  0.31299962029177536; Validation Loss = 0.3483838882151922\n",
            "Cost after 297918 iterations : Training Loss =  0.31299947465830164; Validation Loss = 0.34838413606549995\n",
            "Cost after 297919 iterations : Training Loss =  0.31299966669207185; Validation Loss = 0.34838365947647887\n",
            "Cost after 297920 iterations : Training Loss =  0.3129994993871101; Validation Loss = 0.34838390732678604\n",
            "Cost after 297921 iterations : Training Loss =  0.3129995890946222; Validation Loss = 0.3483841551770941\n",
            "Cost after 297922 iterations : Training Loss =  0.3129995457874068; Validation Loss = 0.348383678588073\n",
            "Cost after 297923 iterations : Training Loss =  0.3129994898256841; Validation Loss = 0.3483839264383805\n",
            "Cost after 297924 iterations : Training Loss =  0.31299959218770346; Validation Loss = 0.3483834498493596\n",
            "Cost after 297925 iterations : Training Loss =  0.3129994248827417; Validation Loss = 0.34838369769966726\n",
            "Cost after 297926 iterations : Training Loss =  0.31299960426200474; Validation Loss = 0.3483839455499746\n",
            "Cost after 297927 iterations : Training Loss =  0.3129994712830383; Validation Loss = 0.3483834689609539\n",
            "Cost after 297928 iterations : Training Loss =  0.31299950499306717; Validation Loss = 0.3483837168112616\n",
            "Cost after 297929 iterations : Training Loss =  0.3129995176833352; Validation Loss = 0.3483832402222409\n",
            "Cost after 297930 iterations : Training Loss =  0.3129994057241291; Validation Loss = 0.3483834880725481\n",
            "Cost after 297931 iterations : Training Loss =  0.3129995640836318; Validation Loss = 0.3483830114835273\n",
            "Cost after 297932 iterations : Training Loss =  0.31299939677867017; Validation Loss = 0.348383259333835\n",
            "Cost after 297933 iterations : Training Loss =  0.31299952016044935; Validation Loss = 0.3483835071841422\n",
            "Cost after 297934 iterations : Training Loss =  0.3129994431789665; Validation Loss = 0.34838303059512177\n",
            "Cost after 297935 iterations : Training Loss =  0.3129994208915114; Validation Loss = 0.3483832784454289\n",
            "Cost after 297936 iterations : Training Loss =  0.3129994895792632; Validation Loss = 0.3483828018564086\n",
            "Cost after 297937 iterations : Training Loss =  0.3129993222743013; Validation Loss = 0.34838304970671585\n",
            "Cost after 297938 iterations : Training Loss =  0.31299953532783215; Validation Loss = 0.34838329755702346\n",
            "Cost after 297939 iterations : Training Loss =  0.31299936867459804; Validation Loss = 0.34838282096800255\n",
            "Cost after 297940 iterations : Training Loss =  0.31299943605889424; Validation Loss = 0.34838306881831044\n",
            "Cost after 297941 iterations : Training Loss =  0.31299941507489476; Validation Loss = 0.3483825922292894\n",
            "Cost after 297942 iterations : Training Loss =  0.31299933678995656; Validation Loss = 0.34838284007959686\n",
            "Cost after 297943 iterations : Training Loss =  0.31299946147519125; Validation Loss = 0.3483823634905759\n",
            "Cost after 297944 iterations : Training Loss =  0.3129992941702298; Validation Loss = 0.34838261134088355\n",
            "Cost after 297945 iterations : Training Loss =  0.31299945122627676; Validation Loss = 0.34838285919119116\n",
            "Cost after 297946 iterations : Training Loss =  0.31299934057052614; Validation Loss = 0.34838238260217025\n",
            "Cost after 297947 iterations : Training Loss =  0.3129993519573387; Validation Loss = 0.3483826304524774\n",
            "Cost after 297948 iterations : Training Loss =  0.31299938697082275; Validation Loss = 0.3483821538634572\n",
            "Cost after 297949 iterations : Training Loss =  0.3129992526884009; Validation Loss = 0.3483824017137643\n",
            "Cost after 297950 iterations : Training Loss =  0.3129994333711195; Validation Loss = 0.34838192512474364\n",
            "Cost after 297951 iterations : Training Loss =  0.3129992660661578; Validation Loss = 0.3483821729750514\n",
            "Cost after 297952 iterations : Training Loss =  0.3129993671247213; Validation Loss = 0.34838242082535886\n",
            "Cost after 297953 iterations : Training Loss =  0.3129993124664546; Validation Loss = 0.34838194423633767\n",
            "Cost after 297954 iterations : Training Loss =  0.3129992678557837; Validation Loss = 0.3483821920866454\n",
            "Cost after 297955 iterations : Training Loss =  0.31299935886675134; Validation Loss = 0.3483817154976245\n",
            "Cost after 297956 iterations : Training Loss =  0.3129991915617893; Validation Loss = 0.34838196334793237\n",
            "Cost after 297957 iterations : Training Loss =  0.3129993822921041; Validation Loss = 0.3483822111982396\n",
            "Cost after 297958 iterations : Training Loss =  0.312999237962086; Validation Loss = 0.3483817346092187\n",
            "Cost after 297959 iterations : Training Loss =  0.3129992830231663; Validation Loss = 0.3483819824595263\n",
            "Cost after 297960 iterations : Training Loss =  0.3129992843623825; Validation Loss = 0.34838150587050565\n",
            "Cost after 297961 iterations : Training Loss =  0.31299918375422825; Validation Loss = 0.34838175372081304\n",
            "Cost after 297962 iterations : Training Loss =  0.3129993307626791; Validation Loss = 0.3483812771317926\n",
            "Cost after 297963 iterations : Training Loss =  0.3129991634577176; Validation Loss = 0.34838152498209995\n",
            "Cost after 297964 iterations : Training Loss =  0.31299929819054895; Validation Loss = 0.3483817728324071\n",
            "Cost after 297965 iterations : Training Loss =  0.31299920985801405; Validation Loss = 0.3483812962433864\n",
            "Cost after 297966 iterations : Training Loss =  0.3129991989216111; Validation Loss = 0.3483815440936941\n",
            "Cost after 297967 iterations : Training Loss =  0.312999256258311; Validation Loss = 0.34838106750467335\n",
            "Cost after 297968 iterations : Training Loss =  0.312999099652673; Validation Loss = 0.3483813153549808\n",
            "Cost after 297969 iterations : Training Loss =  0.31299930265860765; Validation Loss = 0.34838083876596004\n",
            "Cost after 297970 iterations : Training Loss =  0.3129991353536459; Validation Loss = 0.34838108661626727\n",
            "Cost after 297971 iterations : Training Loss =  0.3129992140889936; Validation Loss = 0.3483813344665749\n",
            "Cost after 297972 iterations : Training Loss =  0.3129991817539424; Validation Loss = 0.3483808578775539\n",
            "Cost after 297973 iterations : Training Loss =  0.3129991148200555; Validation Loss = 0.3483811057278615\n",
            "Cost after 297974 iterations : Training Loss =  0.3129992281542389; Validation Loss = 0.3483806291388407\n",
            "Cost after 297975 iterations : Training Loss =  0.3129990608492774; Validation Loss = 0.3483808769891481\n",
            "Cost after 297976 iterations : Training Loss =  0.31299922925637624; Validation Loss = 0.3483811248394558\n",
            "Cost after 297977 iterations : Training Loss =  0.3129991072495741; Validation Loss = 0.348380648250435\n",
            "Cost after 297978 iterations : Training Loss =  0.31299912998743856; Validation Loss = 0.3483808961007425\n",
            "Cost after 297979 iterations : Training Loss =  0.3129991536498705; Validation Loss = 0.3483804195117221\n",
            "Cost after 297980 iterations : Training Loss =  0.3129990307185002; Validation Loss = 0.34838066736202955\n",
            "Cost after 297981 iterations : Training Loss =  0.3129992000501672; Validation Loss = 0.3483801907730083\n",
            "Cost after 297982 iterations : Training Loss =  0.3129990327452053; Validation Loss = 0.3483804386233159\n",
            "Cost after 297983 iterations : Training Loss =  0.312999145154821; Validation Loss = 0.34838068647362347\n",
            "Cost after 297984 iterations : Training Loss =  0.3129990791455022; Validation Loss = 0.34838020988460244\n",
            "Cost after 297985 iterations : Training Loss =  0.3129990458858832; Validation Loss = 0.34838045773491033\n",
            "Cost after 297986 iterations : Training Loss =  0.31299912554579884; Validation Loss = 0.3483799811458894\n",
            "Cost after 297987 iterations : Training Loss =  0.3129989582408373; Validation Loss = 0.34838022899619703\n",
            "Cost after 297988 iterations : Training Loss =  0.3129991603222037; Validation Loss = 0.34838047684650447\n",
            "Cost after 297989 iterations : Training Loss =  0.31299900464113356; Validation Loss = 0.34838000025748367\n",
            "Cost after 297990 iterations : Training Loss =  0.3129990610532657; Validation Loss = 0.34838024810779117\n",
            "Cost after 297991 iterations : Training Loss =  0.31299905104143044; Validation Loss = 0.3483797715187704\n",
            "Cost after 297992 iterations : Training Loss =  0.3129989617843276; Validation Loss = 0.34838001936907786\n",
            "Cost after 297993 iterations : Training Loss =  0.3129990974417268; Validation Loss = 0.3483795427800573\n",
            "Cost after 297994 iterations : Training Loss =  0.31299893013676494; Validation Loss = 0.34837979063036484\n",
            "Cost after 297995 iterations : Training Loss =  0.3129990762206482; Validation Loss = 0.34838003848067234\n",
            "Cost after 297996 iterations : Training Loss =  0.31299897653706177; Validation Loss = 0.34837956189165126\n",
            "Cost after 297997 iterations : Training Loss =  0.31299897695171053; Validation Loss = 0.3483798097419589\n",
            "Cost after 297998 iterations : Training Loss =  0.3129990229373587; Validation Loss = 0.34837933315293795\n",
            "Cost after 297999 iterations : Training Loss =  0.3129988776827727; Validation Loss = 0.34837958100324556\n",
            "Cost after 298000 iterations : Training Loss =  0.3129990693376554; Validation Loss = 0.3483791044142251\n",
            "Cost after 298001 iterations : Training Loss =  0.31299890203269365; Validation Loss = 0.34837935226453254\n",
            "Cost after 298002 iterations : Training Loss =  0.3129989921190932; Validation Loss = 0.34837960011483987\n",
            "Cost after 298003 iterations : Training Loss =  0.31299894843299014; Validation Loss = 0.34837912352581896\n",
            "Cost after 298004 iterations : Training Loss =  0.3129988928501554; Validation Loss = 0.34837937137612685\n",
            "Cost after 298005 iterations : Training Loss =  0.3129989948332867; Validation Loss = 0.34837889478710565\n",
            "Cost after 298006 iterations : Training Loss =  0.312998827528325; Validation Loss = 0.3483791426374132\n",
            "Cost after 298007 iterations : Training Loss =  0.3129990072864757; Validation Loss = 0.34837939048772076\n",
            "Cost after 298008 iterations : Training Loss =  0.3129988739286219; Validation Loss = 0.34837891389870007\n",
            "Cost after 298009 iterations : Training Loss =  0.31299890801753777; Validation Loss = 0.3483791617490071\n",
            "Cost after 298010 iterations : Training Loss =  0.31299892032891835; Validation Loss = 0.3483786851599865\n",
            "Cost after 298011 iterations : Training Loss =  0.31299880874860014; Validation Loss = 0.34837893301029416\n",
            "Cost after 298012 iterations : Training Loss =  0.31299896672921484; Validation Loss = 0.34837845642127346\n",
            "Cost after 298013 iterations : Training Loss =  0.3129987994242534; Validation Loss = 0.3483787042715808\n",
            "Cost after 298014 iterations : Training Loss =  0.31299892318492034; Validation Loss = 0.3483789521218886\n",
            "Cost after 298015 iterations : Training Loss =  0.31299884582454984; Validation Loss = 0.34837847553286766\n",
            "Cost after 298016 iterations : Training Loss =  0.3129988239159826; Validation Loss = 0.3483787233831749\n",
            "Cost after 298017 iterations : Training Loss =  0.3129988922248463; Validation Loss = 0.3483782467941542\n",
            "Cost after 298018 iterations : Training Loss =  0.3129987249198849; Validation Loss = 0.34837849464446163\n",
            "Cost after 298019 iterations : Training Loss =  0.3129989383523031; Validation Loss = 0.3483787424947694\n",
            "Cost after 298020 iterations : Training Loss =  0.31299877132018145; Validation Loss = 0.3483782659057485\n",
            "Cost after 298021 iterations : Training Loss =  0.31299883908336545; Validation Loss = 0.3483785137560561\n",
            "Cost after 298022 iterations : Training Loss =  0.31299881772047805; Validation Loss = 0.348378037167035\n",
            "Cost after 298023 iterations : Training Loss =  0.3129987398144274; Validation Loss = 0.3483782850173428\n",
            "Cost after 298024 iterations : Training Loss =  0.31299886412077466; Validation Loss = 0.34837780842832194\n",
            "Cost after 298025 iterations : Training Loss =  0.31299869681581305; Validation Loss = 0.34837805627862944\n",
            "Cost after 298026 iterations : Training Loss =  0.3129988542507481; Validation Loss = 0.3483783041289374\n",
            "Cost after 298027 iterations : Training Loss =  0.31299874321610943; Validation Loss = 0.34837782753991603\n",
            "Cost after 298028 iterations : Training Loss =  0.3129987549818101; Validation Loss = 0.34837807539022386\n",
            "Cost after 298029 iterations : Training Loss =  0.31299878961640626; Validation Loss = 0.3483775988012029\n",
            "Cost after 298030 iterations : Training Loss =  0.31299865571287194; Validation Loss = 0.34837784665151034\n",
            "Cost after 298031 iterations : Training Loss =  0.3129988360167027; Validation Loss = 0.34837737006248987\n",
            "Cost after 298032 iterations : Training Loss =  0.3129986687117409; Validation Loss = 0.34837761791279703\n",
            "Cost after 298033 iterations : Training Loss =  0.31299877014919264; Validation Loss = 0.3483778657631046\n",
            "Cost after 298034 iterations : Training Loss =  0.3129987151120378; Validation Loss = 0.3483773891740839\n",
            "Cost after 298035 iterations : Training Loss =  0.3129986708802547; Validation Loss = 0.34837763702439134\n",
            "Cost after 298036 iterations : Training Loss =  0.3129987615123343; Validation Loss = 0.3483771604353704\n",
            "Cost after 298037 iterations : Training Loss =  0.3129985942073728; Validation Loss = 0.3483774082856783\n",
            "Cost after 298038 iterations : Training Loss =  0.31299878531657527; Validation Loss = 0.34837765613598565\n",
            "Cost after 298039 iterations : Training Loss =  0.31299864060766913; Validation Loss = 0.3483771795469653\n",
            "Cost after 298040 iterations : Training Loss =  0.31299868604763736; Validation Loss = 0.3483774273972726\n",
            "Cost after 298041 iterations : Training Loss =  0.3129986870079661; Validation Loss = 0.34837695080825143\n",
            "Cost after 298042 iterations : Training Loss =  0.31299858677869913; Validation Loss = 0.3483771986585591\n",
            "Cost after 298043 iterations : Training Loss =  0.31299873340826273; Validation Loss = 0.34837672206953807\n",
            "Cost after 298044 iterations : Training Loss =  0.31299856610330096; Validation Loss = 0.3483769699198462\n",
            "Cost after 298045 iterations : Training Loss =  0.31299870121502; Validation Loss = 0.34837721777015346\n",
            "Cost after 298046 iterations : Training Loss =  0.3129986125035973; Validation Loss = 0.34837674118113254\n",
            "Cost after 298047 iterations : Training Loss =  0.3129986019460821; Validation Loss = 0.34837698903144015\n",
            "Cost after 298048 iterations : Training Loss =  0.3129986589038942; Validation Loss = 0.3483765124424194\n",
            "Cost after 298049 iterations : Training Loss =  0.31299850267714396; Validation Loss = 0.34837676029272685\n",
            "Cost after 298050 iterations : Training Loss =  0.31299870530419066; Validation Loss = 0.3483762837037061\n",
            "Cost after 298051 iterations : Training Loss =  0.312998537999229; Validation Loss = 0.34837653155401366\n",
            "Cost after 298052 iterations : Training Loss =  0.3129986171134647; Validation Loss = 0.34837677940432055\n",
            "Cost after 298053 iterations : Training Loss =  0.3129985843995257; Validation Loss = 0.34837630281530024\n",
            "Cost after 298054 iterations : Training Loss =  0.3129985178445268; Validation Loss = 0.34837655066560774\n",
            "Cost after 298055 iterations : Training Loss =  0.31299863079982226; Validation Loss = 0.34837607407658666\n",
            "Cost after 298056 iterations : Training Loss =  0.3129984634948609; Validation Loss = 0.3483763219268943\n",
            "Cost after 298057 iterations : Training Loss =  0.3129986322808475; Validation Loss = 0.34837656977720227\n",
            "Cost after 298058 iterations : Training Loss =  0.3129985098951573; Validation Loss = 0.34837609318818125\n",
            "Cost after 298059 iterations : Training Loss =  0.3129985330119094; Validation Loss = 0.34837634103848886\n",
            "Cost after 298060 iterations : Training Loss =  0.3129985562954539; Validation Loss = 0.348375864449468\n",
            "Cost after 298061 iterations : Training Loss =  0.31299843374297154; Validation Loss = 0.34837611229977533\n",
            "Cost after 298062 iterations : Training Loss =  0.31299860269575047; Validation Loss = 0.3483756357107548\n",
            "Cost after 298063 iterations : Training Loss =  0.31299843539078903; Validation Loss = 0.3483758835610621\n",
            "Cost after 298064 iterations : Training Loss =  0.3129985481792921; Validation Loss = 0.3483761314113697\n",
            "Cost after 298065 iterations : Training Loss =  0.31299848179108564; Validation Loss = 0.34837565482234895\n",
            "Cost after 298066 iterations : Training Loss =  0.312998448910354; Validation Loss = 0.3483759026726563\n",
            "Cost after 298067 iterations : Training Loss =  0.31299852819138196; Validation Loss = 0.34837542608363564\n",
            "Cost after 298068 iterations : Training Loss =  0.3129983608864204; Validation Loss = 0.348375673933943\n",
            "Cost after 298069 iterations : Training Loss =  0.3129985633466747; Validation Loss = 0.3483759217842505\n",
            "Cost after 298070 iterations : Training Loss =  0.31299840728671713; Validation Loss = 0.3483754451952297\n",
            "Cost after 298071 iterations : Training Loss =  0.31299846407773696; Validation Loss = 0.3483756930455372\n",
            "Cost after 298072 iterations : Training Loss =  0.3129984536870137; Validation Loss = 0.34837521645651665\n",
            "Cost after 298073 iterations : Training Loss =  0.312998364808799; Validation Loss = 0.3483754643068241\n",
            "Cost after 298074 iterations : Training Loss =  0.3129985000873102; Validation Loss = 0.3483749877178034\n",
            "Cost after 298075 iterations : Training Loss =  0.3129983327823486; Validation Loss = 0.34837523556811095\n",
            "Cost after 298076 iterations : Training Loss =  0.31299847924511953; Validation Loss = 0.34837548341841823\n",
            "Cost after 298077 iterations : Training Loss =  0.3129983791826453; Validation Loss = 0.3483750068293973\n",
            "Cost after 298078 iterations : Training Loss =  0.3129983799761817; Validation Loss = 0.34837525467970515\n",
            "Cost after 298079 iterations : Training Loss =  0.3129984255829417; Validation Loss = 0.3483747780906842\n",
            "Cost after 298080 iterations : Training Loss =  0.31299828070724384; Validation Loss = 0.3483750259409916\n",
            "Cost after 298081 iterations : Training Loss =  0.3129984719832385; Validation Loss = 0.3483745493519709\n",
            "Cost after 298082 iterations : Training Loss =  0.3129983046782769; Validation Loss = 0.3483747972022785\n",
            "Cost after 298083 iterations : Training Loss =  0.3129983951435643; Validation Loss = 0.3483750450525858\n",
            "Cost after 298084 iterations : Training Loss =  0.31299835107857343; Validation Loss = 0.3483745684635652\n",
            "Cost after 298085 iterations : Training Loss =  0.3129982958746262; Validation Loss = 0.3483748163138725\n",
            "Cost after 298086 iterations : Training Loss =  0.31299839747887015; Validation Loss = 0.3483743397248517\n",
            "Cost after 298087 iterations : Training Loss =  0.3129982301739084; Validation Loss = 0.3483745875751593\n",
            "Cost after 298088 iterations : Training Loss =  0.31299841031094683; Validation Loss = 0.348374835425467\n",
            "Cost after 298089 iterations : Training Loss =  0.31299827657420504; Validation Loss = 0.3483743588364462\n",
            "Cost after 298090 iterations : Training Loss =  0.31299831104200904; Validation Loss = 0.34837460668675363\n",
            "Cost after 298091 iterations : Training Loss =  0.3129983229745016; Validation Loss = 0.34837413009773266\n",
            "Cost after 298092 iterations : Training Loss =  0.3129982117730711; Validation Loss = 0.34837437794804027\n",
            "Cost after 298093 iterations : Training Loss =  0.3129983693747983; Validation Loss = 0.3483739013590194\n",
            "Cost after 298094 iterations : Training Loss =  0.31299820206983664; Validation Loss = 0.348374149209327\n",
            "Cost after 298095 iterations : Training Loss =  0.31299832620939133; Validation Loss = 0.34837439705963474\n",
            "Cost after 298096 iterations : Training Loss =  0.31299824847013313; Validation Loss = 0.34837392047061394\n",
            "Cost after 298097 iterations : Training Loss =  0.3129982269404537; Validation Loss = 0.3483741683209213\n",
            "Cost after 298098 iterations : Training Loss =  0.3129982948704299; Validation Loss = 0.3483736917319007\n",
            "Cost after 298099 iterations : Training Loss =  0.3129981276715161; Validation Loss = 0.34837393958220814\n",
            "Cost after 298100 iterations : Training Loss =  0.3129983412707263; Validation Loss = 0.3483734629931873\n",
            "Cost after 298101 iterations : Training Loss =  0.3129981739657647; Validation Loss = 0.34837371084349483\n",
            "Cost after 298102 iterations : Training Loss =  0.3129982421078363; Validation Loss = 0.34837395869380205\n",
            "Cost after 298103 iterations : Training Loss =  0.31299822036606145; Validation Loss = 0.3483734821047814\n",
            "Cost after 298104 iterations : Training Loss =  0.3129981428388983; Validation Loss = 0.34837372995508886\n",
            "Cost after 298105 iterations : Training Loss =  0.31299826676635817; Validation Loss = 0.34837325336606817\n",
            "Cost after 298106 iterations : Training Loss =  0.3129980994613963; Validation Loss = 0.3483735012163755\n",
            "Cost after 298107 iterations : Training Loss =  0.3129982572752188; Validation Loss = 0.348373749066683\n",
            "Cost after 298108 iterations : Training Loss =  0.3129981458616927; Validation Loss = 0.34837327247766253\n",
            "Cost after 298109 iterations : Training Loss =  0.3129981580062812; Validation Loss = 0.34837352032797003\n",
            "Cost after 298110 iterations : Training Loss =  0.31299819226198966; Validation Loss = 0.3483730437389494\n",
            "Cost after 298111 iterations : Training Loss =  0.3129980587373432; Validation Loss = 0.34837329158925656\n",
            "Cost after 298112 iterations : Training Loss =  0.3129982386622861; Validation Loss = 0.34837281500023565\n",
            "Cost after 298113 iterations : Training Loss =  0.3129980713573243; Validation Loss = 0.34837306285054326\n",
            "Cost after 298114 iterations : Training Loss =  0.31299817317366346; Validation Loss = 0.34837331070085065\n",
            "Cost after 298115 iterations : Training Loss =  0.31299811775762093; Validation Loss = 0.34837283411183\n",
            "Cost after 298116 iterations : Training Loss =  0.3129980739047259; Validation Loss = 0.3483730819621374\n",
            "Cost after 298117 iterations : Training Loss =  0.31299816415791776; Validation Loss = 0.3483726053731165\n",
            "Cost after 298118 iterations : Training Loss =  0.3129979968529562; Validation Loss = 0.3483728532234238\n",
            "Cost after 298119 iterations : Training Loss =  0.31299818834104665; Validation Loss = 0.3483731010737316\n",
            "Cost after 298120 iterations : Training Loss =  0.31299804325325276; Validation Loss = 0.34837262448471124\n",
            "Cost after 298121 iterations : Training Loss =  0.3129980890721085; Validation Loss = 0.3483728723350184\n",
            "Cost after 298122 iterations : Training Loss =  0.3129980896535495; Validation Loss = 0.34837239574599765\n",
            "Cost after 298123 iterations : Training Loss =  0.31299798980317056; Validation Loss = 0.34837264359630504\n",
            "Cost after 298124 iterations : Training Loss =  0.3129981360538461; Validation Loss = 0.34837216700728457\n",
            "Cost after 298125 iterations : Training Loss =  0.31299796874888425; Validation Loss = 0.34837241485759196\n",
            "Cost after 298126 iterations : Training Loss =  0.3129981042394913; Validation Loss = 0.3483726627078995\n",
            "Cost after 298127 iterations : Training Loss =  0.3129980151491808; Validation Loss = 0.3483721861188788\n",
            "Cost after 298128 iterations : Training Loss =  0.31299800497055313; Validation Loss = 0.3483724339691861\n",
            "Cost after 298129 iterations : Training Loss =  0.3129980615494774; Validation Loss = 0.34837195738016546\n",
            "Cost after 298130 iterations : Training Loss =  0.3129979057016152; Validation Loss = 0.3483722052304728\n",
            "Cost after 298131 iterations : Training Loss =  0.31299810794977395; Validation Loss = 0.3483717286414519\n",
            "Cost after 298132 iterations : Training Loss =  0.31299794064481246; Validation Loss = 0.34837197649175944\n",
            "Cost after 298133 iterations : Training Loss =  0.31299802013793593; Validation Loss = 0.3483722243420673\n",
            "Cost after 298134 iterations : Training Loss =  0.31299798704510906; Validation Loss = 0.3483717477530462\n",
            "Cost after 298135 iterations : Training Loss =  0.3129979208689979; Validation Loss = 0.34837199560335363\n",
            "Cost after 298136 iterations : Training Loss =  0.3129980334454057; Validation Loss = 0.34837151901433333\n",
            "Cost after 298137 iterations : Training Loss =  0.31299786614044406; Validation Loss = 0.3483717668646405\n",
            "Cost after 298138 iterations : Training Loss =  0.31299803530531844; Validation Loss = 0.3483720147149481\n",
            "Cost after 298139 iterations : Training Loss =  0.3129979125407406; Validation Loss = 0.348371538125927\n",
            "Cost after 298140 iterations : Training Loss =  0.3129979360363808; Validation Loss = 0.3483717859762349\n",
            "Cost after 298141 iterations : Training Loss =  0.3129979589410372; Validation Loss = 0.3483713093872141\n",
            "Cost after 298142 iterations : Training Loss =  0.31299783676744264; Validation Loss = 0.34837155723752167\n",
            "Cost after 298143 iterations : Training Loss =  0.3129980053413337; Validation Loss = 0.3483710806485007\n",
            "Cost after 298144 iterations : Training Loss =  0.3129978380363721; Validation Loss = 0.3483713284988082\n",
            "Cost after 298145 iterations : Training Loss =  0.31299795120376334; Validation Loss = 0.34837157634911564\n",
            "Cost after 298146 iterations : Training Loss =  0.31299788443666876; Validation Loss = 0.34837109976009517\n",
            "Cost after 298147 iterations : Training Loss =  0.3129978519348254; Validation Loss = 0.34837134761040234\n",
            "Cost after 298148 iterations : Training Loss =  0.3129979308369657; Validation Loss = 0.3483708710213817\n",
            "Cost after 298149 iterations : Training Loss =  0.3129977635320038; Validation Loss = 0.34837111887168903\n",
            "Cost after 298150 iterations : Training Loss =  0.31299796637114585; Validation Loss = 0.3483713667219967\n",
            "Cost after 298151 iterations : Training Loss =  0.31299780993230014; Validation Loss = 0.348370890132976\n",
            "Cost after 298152 iterations : Training Loss =  0.31299786710220795; Validation Loss = 0.34837113798328345\n",
            "Cost after 298153 iterations : Training Loss =  0.3129978563325969; Validation Loss = 0.348370661394263\n",
            "Cost after 298154 iterations : Training Loss =  0.3129977678332702; Validation Loss = 0.34837090924456987\n",
            "Cost after 298155 iterations : Training Loss =  0.31299790273289363; Validation Loss = 0.3483704326555494\n",
            "Cost after 298156 iterations : Training Loss =  0.31299773542793197; Validation Loss = 0.34837068050585657\n",
            "Cost after 298157 iterations : Training Loss =  0.3129978822695906; Validation Loss = 0.34837092835616434\n",
            "Cost after 298158 iterations : Training Loss =  0.3129977818282285; Validation Loss = 0.34837045176714343\n",
            "Cost after 298159 iterations : Training Loss =  0.3129977830006528; Validation Loss = 0.34837069961745093\n",
            "Cost after 298160 iterations : Training Loss =  0.31299782822852523; Validation Loss = 0.3483702230284302\n",
            "Cost after 298161 iterations : Training Loss =  0.3129976837317149; Validation Loss = 0.34837047087873785\n",
            "Cost after 298162 iterations : Training Loss =  0.3129978746288218; Validation Loss = 0.34836999428971666\n",
            "Cost after 298163 iterations : Training Loss =  0.3129977073238602; Validation Loss = 0.34837024214002427\n",
            "Cost after 298164 iterations : Training Loss =  0.312997798168035; Validation Loss = 0.34837048999033193\n",
            "Cost after 298165 iterations : Training Loss =  0.3129977537241569; Validation Loss = 0.34837001340131113\n",
            "Cost after 298166 iterations : Training Loss =  0.3129976988990974; Validation Loss = 0.348370261251619\n",
            "Cost after 298167 iterations : Training Loss =  0.3129978001244534; Validation Loss = 0.34836978466259794\n",
            "Cost after 298168 iterations : Training Loss =  0.3129976328194918; Validation Loss = 0.3483700325129055\n",
            "Cost after 298169 iterations : Training Loss =  0.31299781333541793; Validation Loss = 0.3483702803632131\n",
            "Cost after 298170 iterations : Training Loss =  0.31299767921978844; Validation Loss = 0.3483698037741924\n",
            "Cost after 298171 iterations : Training Loss =  0.31299771406648; Validation Loss = 0.34837005162449985\n",
            "Cost after 298172 iterations : Training Loss =  0.31299772562008493; Validation Loss = 0.34836957503547894\n",
            "Cost after 298173 iterations : Training Loss =  0.312997614797542; Validation Loss = 0.3483698228857864\n",
            "Cost after 298174 iterations : Training Loss =  0.3129977720203816; Validation Loss = 0.34836934629676564\n",
            "Cost after 298175 iterations : Training Loss =  0.31299760471541965; Validation Loss = 0.34836959414707297\n",
            "Cost after 298176 iterations : Training Loss =  0.31299772923386276; Validation Loss = 0.3483698419973807\n",
            "Cost after 298177 iterations : Training Loss =  0.31299765111571637; Validation Loss = 0.34836936540835955\n",
            "Cost after 298178 iterations : Training Loss =  0.31299762996492475; Validation Loss = 0.34836961325866744\n",
            "Cost after 298179 iterations : Training Loss =  0.3129976975160131; Validation Loss = 0.34836913666964636\n",
            "Cost after 298180 iterations : Training Loss =  0.3129975306959872; Validation Loss = 0.348369384519954\n",
            "Cost after 298181 iterations : Training Loss =  0.31299774391630975; Validation Loss = 0.34836890793093334\n",
            "Cost after 298182 iterations : Training Loss =  0.3129975766113482; Validation Loss = 0.34836915578124067\n",
            "Cost after 298183 iterations : Training Loss =  0.31299764513230727; Validation Loss = 0.3483694036315484\n",
            "Cost after 298184 iterations : Training Loss =  0.3129976230116445; Validation Loss = 0.3483689270425275\n",
            "Cost after 298185 iterations : Training Loss =  0.3129975458633697; Validation Loss = 0.3483691748928348\n",
            "Cost after 298186 iterations : Training Loss =  0.31299766941194124; Validation Loss = 0.3483686983038141\n",
            "Cost after 298187 iterations : Training Loss =  0.3129975021069795; Validation Loss = 0.34836894615412184\n",
            "Cost after 298188 iterations : Training Loss =  0.31299766029969006; Validation Loss = 0.34836919400442923\n",
            "Cost after 298189 iterations : Training Loss =  0.3129975485072761; Validation Loss = 0.3483687174154083\n",
            "Cost after 298190 iterations : Training Loss =  0.3129975610307519; Validation Loss = 0.3483689652657162\n",
            "Cost after 298191 iterations : Training Loss =  0.31299759490757284; Validation Loss = 0.3483684886766951\n",
            "Cost after 298192 iterations : Training Loss =  0.3129974617618143; Validation Loss = 0.34836873652700306\n",
            "Cost after 298193 iterations : Training Loss =  0.3129976413078696; Validation Loss = 0.3483682599379816\n",
            "Cost after 298194 iterations : Training Loss =  0.31299747400290784; Validation Loss = 0.3483685077882892\n",
            "Cost after 298195 iterations : Training Loss =  0.3129975761981348; Validation Loss = 0.3483687556385967\n",
            "Cost after 298196 iterations : Training Loss =  0.3129975204032041; Validation Loss = 0.34836827904957607\n",
            "Cost after 298197 iterations : Training Loss =  0.31299747692919694; Validation Loss = 0.3483685268998839\n",
            "Cost after 298198 iterations : Training Loss =  0.312997566803501; Validation Loss = 0.348368050310863\n",
            "Cost after 298199 iterations : Training Loss =  0.3129973994985393; Validation Loss = 0.3483682981611703\n",
            "Cost after 298200 iterations : Training Loss =  0.31299759136551736; Validation Loss = 0.34836854601147765\n",
            "Cost after 298201 iterations : Training Loss =  0.3129974458988361; Validation Loss = 0.34836806942245685\n",
            "Cost after 298202 iterations : Training Loss =  0.3129974920965797; Validation Loss = 0.34836831727276435\n",
            "Cost after 298203 iterations : Training Loss =  0.3129974922991327; Validation Loss = 0.3483678406837439\n",
            "Cost after 298204 iterations : Training Loss =  0.31299739282764166; Validation Loss = 0.3483680885340513\n",
            "Cost after 298205 iterations : Training Loss =  0.31299753869942915; Validation Loss = 0.34836761194503046\n",
            "Cost after 298206 iterations : Training Loss =  0.31299737139446765; Validation Loss = 0.34836785979533774\n",
            "Cost after 298207 iterations : Training Loss =  0.3129975072639621; Validation Loss = 0.34836810764564563\n",
            "Cost after 298208 iterations : Training Loss =  0.31299741779476414; Validation Loss = 0.34836763105662455\n",
            "Cost after 298209 iterations : Training Loss =  0.31299740799502446; Validation Loss = 0.34836787890693216\n",
            "Cost after 298210 iterations : Training Loss =  0.3129974641950608; Validation Loss = 0.34836740231791125\n",
            "Cost after 298211 iterations : Training Loss =  0.3129973087260863; Validation Loss = 0.348367650168219\n",
            "Cost after 298212 iterations : Training Loss =  0.3129975105953572; Validation Loss = 0.3483671735791984\n",
            "Cost after 298213 iterations : Training Loss =  0.31299734329039597; Validation Loss = 0.3483674214295058\n",
            "Cost after 298214 iterations : Training Loss =  0.31299742316240703; Validation Loss = 0.3483676692798132\n",
            "Cost after 298215 iterations : Training Loss =  0.31299738969069235; Validation Loss = 0.3483671926907927\n",
            "Cost after 298216 iterations : Training Loss =  0.3129973238934692; Validation Loss = 0.3483674405411\n",
            "Cost after 298217 iterations : Training Loss =  0.31299743609098896; Validation Loss = 0.3483669639520791\n",
            "Cost after 298218 iterations : Training Loss =  0.3129972687860274; Validation Loss = 0.34836721180238683\n",
            "Cost after 298219 iterations : Training Loss =  0.31299743832978943; Validation Loss = 0.3483674596526943\n",
            "Cost after 298220 iterations : Training Loss =  0.3129973151863239; Validation Loss = 0.3483669830636734\n",
            "Cost after 298221 iterations : Training Loss =  0.31299733906085186; Validation Loss = 0.34836723091398086\n",
            "Cost after 298222 iterations : Training Loss =  0.31299736158662056; Validation Loss = 0.34836675432496017\n",
            "Cost after 298223 iterations : Training Loss =  0.3129972397919137; Validation Loss = 0.3483670021752675\n",
            "Cost after 298224 iterations : Training Loss =  0.31299740798691705; Validation Loss = 0.3483665255862469\n",
            "Cost after 298225 iterations : Training Loss =  0.3129972406819557; Validation Loss = 0.34836677343655426\n",
            "Cost after 298226 iterations : Training Loss =  0.3129973542282342; Validation Loss = 0.3483670212868617\n",
            "Cost after 298227 iterations : Training Loss =  0.31299728708225205; Validation Loss = 0.348366544697841\n",
            "Cost after 298228 iterations : Training Loss =  0.3129972549592965; Validation Loss = 0.34836679254814884\n",
            "Cost after 298229 iterations : Training Loss =  0.3129973334825488; Validation Loss = 0.34836631595912776\n",
            "Cost after 298230 iterations : Training Loss =  0.3129971661775869; Validation Loss = 0.34836656380943537\n",
            "Cost after 298231 iterations : Training Loss =  0.31299736939561706; Validation Loss = 0.3483668116597425\n",
            "Cost after 298232 iterations : Training Loss =  0.3129972125778835; Validation Loss = 0.348366335070722\n",
            "Cost after 298233 iterations : Training Loss =  0.312997270126679; Validation Loss = 0.34836658292102957\n",
            "Cost after 298234 iterations : Training Loss =  0.31299725897818037; Validation Loss = 0.34836610633200865\n",
            "Cost after 298235 iterations : Training Loss =  0.31299717085774115; Validation Loss = 0.3483663541823164\n",
            "Cost after 298236 iterations : Training Loss =  0.31299730537847703; Validation Loss = 0.34836587759329546\n",
            "Cost after 298237 iterations : Training Loss =  0.31299713807351554; Validation Loss = 0.34836612544360324\n",
            "Cost after 298238 iterations : Training Loss =  0.31299728529406173; Validation Loss = 0.3483663732939107\n",
            "Cost after 298239 iterations : Training Loss =  0.3129971844738118; Validation Loss = 0.3483658967048895\n",
            "Cost after 298240 iterations : Training Loss =  0.31299718602512383; Validation Loss = 0.34836614455519715\n",
            "Cost after 298241 iterations : Training Loss =  0.31299723087410847; Validation Loss = 0.34836566796617613\n",
            "Cost after 298242 iterations : Training Loss =  0.3129970867561858; Validation Loss = 0.3483659158164842\n",
            "Cost after 298243 iterations : Training Loss =  0.31299727727440513; Validation Loss = 0.3483654392274629\n",
            "Cost after 298244 iterations : Training Loss =  0.31299710996944347; Validation Loss = 0.3483656870777705\n",
            "Cost after 298245 iterations : Training Loss =  0.3129972011925063; Validation Loss = 0.348365934928078\n",
            "Cost after 298246 iterations : Training Loss =  0.31299715636974024; Validation Loss = 0.3483654583390572\n",
            "Cost after 298247 iterations : Training Loss =  0.31299710192356855; Validation Loss = 0.34836570618936463\n",
            "Cost after 298248 iterations : Training Loss =  0.3129972027700368; Validation Loss = 0.3483652296003439\n",
            "Cost after 298249 iterations : Training Loss =  0.31299703546507507; Validation Loss = 0.34836547745065166\n",
            "Cost after 298250 iterations : Training Loss =  0.3129972163598892; Validation Loss = 0.348365725300959\n",
            "Cost after 298251 iterations : Training Loss =  0.3129970818653715; Validation Loss = 0.3483652487119382\n",
            "Cost after 298252 iterations : Training Loss =  0.31299711709095135; Validation Loss = 0.34836549656224575\n",
            "Cost after 298253 iterations : Training Loss =  0.3129971282656683; Validation Loss = 0.3483650199732249\n",
            "Cost after 298254 iterations : Training Loss =  0.31299701782201345; Validation Loss = 0.34836526782353244\n",
            "Cost after 298255 iterations : Training Loss =  0.312997174665965; Validation Loss = 0.3483647912345117\n",
            "Cost after 298256 iterations : Training Loss =  0.31299700736100333; Validation Loss = 0.34836503908481914\n",
            "Cost after 298257 iterations : Training Loss =  0.31299713225833364; Validation Loss = 0.34836528693512675\n",
            "Cost after 298258 iterations : Training Loss =  0.31299705376129977; Validation Loss = 0.3483648103461058\n",
            "Cost after 298259 iterations : Training Loss =  0.3129970329893961; Validation Loss = 0.3483650581964131\n",
            "Cost after 298260 iterations : Training Loss =  0.31299710016159665; Validation Loss = 0.34836458160739253\n",
            "Cost after 298261 iterations : Training Loss =  0.31299693372045784; Validation Loss = 0.34836482945770003\n",
            "Cost after 298262 iterations : Training Loss =  0.31299714656189304; Validation Loss = 0.3483643528686794\n",
            "Cost after 298263 iterations : Training Loss =  0.3129969792569314; Validation Loss = 0.34836460071898706\n",
            "Cost after 298264 iterations : Training Loss =  0.3129970481567785; Validation Loss = 0.34836484856929434\n",
            "Cost after 298265 iterations : Training Loss =  0.3129970256572281; Validation Loss = 0.34836437198027326\n",
            "Cost after 298266 iterations : Training Loss =  0.3129969488878405; Validation Loss = 0.34836461983058115\n",
            "Cost after 298267 iterations : Training Loss =  0.31299707205752464; Validation Loss = 0.34836414324156\n",
            "Cost after 298268 iterations : Training Loss =  0.3129969047525631; Validation Loss = 0.34836439109186784\n",
            "Cost after 298269 iterations : Training Loss =  0.31299706332416133; Validation Loss = 0.3483646389421752\n",
            "Cost after 298270 iterations : Training Loss =  0.3129969511528597; Validation Loss = 0.34836416235315426\n",
            "Cost after 298271 iterations : Training Loss =  0.3129969640552234; Validation Loss = 0.34836441020346187\n",
            "Cost after 298272 iterations : Training Loss =  0.312996997553156; Validation Loss = 0.3483639336144414\n",
            "Cost after 298273 iterations : Training Loss =  0.3129968647862855; Validation Loss = 0.34836418146474885\n",
            "Cost after 298274 iterations : Training Loss =  0.31299704395345274; Validation Loss = 0.3483637048757282\n",
            "Cost after 298275 iterations : Training Loss =  0.31299687664849113; Validation Loss = 0.3483639527260357\n",
            "Cost after 298276 iterations : Training Loss =  0.312996979222606; Validation Loss = 0.348364200576343\n",
            "Cost after 298277 iterations : Training Loss =  0.31299692304878773; Validation Loss = 0.3483637239873222\n",
            "Cost after 298278 iterations : Training Loss =  0.312996879953668; Validation Loss = 0.34836397183762985\n",
            "Cost after 298279 iterations : Training Loss =  0.3129969694490845; Validation Loss = 0.34836349524860893\n",
            "Cost after 298280 iterations : Training Loss =  0.31299680214412284; Validation Loss = 0.34836374309891655\n",
            "Cost after 298281 iterations : Training Loss =  0.31299699438998874; Validation Loss = 0.34836399094922366\n",
            "Cost after 298282 iterations : Training Loss =  0.31299684854441934; Validation Loss = 0.3483635143602031\n",
            "Cost after 298283 iterations : Training Loss =  0.3129968951210508; Validation Loss = 0.34836376221051085\n",
            "Cost after 298284 iterations : Training Loss =  0.31299689494471566; Validation Loss = 0.34836328562149005\n",
            "Cost after 298285 iterations : Training Loss =  0.3129967958521128; Validation Loss = 0.34836353347179716\n",
            "Cost after 298286 iterations : Training Loss =  0.3129969413450126; Validation Loss = 0.3483630568827766\n",
            "Cost after 298287 iterations : Training Loss =  0.3129967740400507; Validation Loss = 0.34836330473308424\n",
            "Cost after 298288 iterations : Training Loss =  0.3129969102884332; Validation Loss = 0.3483635525833917\n",
            "Cost after 298289 iterations : Training Loss =  0.31299682044034743; Validation Loss = 0.34836307599437094\n",
            "Cost after 298290 iterations : Training Loss =  0.31299681101949534; Validation Loss = 0.34836332384467855\n",
            "Cost after 298291 iterations : Training Loss =  0.3129968668406442; Validation Loss = 0.34836284725565764\n",
            "Cost after 298292 iterations : Training Loss =  0.3129967117505576; Validation Loss = 0.3483630951059649\n",
            "Cost after 298293 iterations : Training Loss =  0.31299691324094076; Validation Loss = 0.34836261851694433\n",
            "Cost after 298294 iterations : Training Loss =  0.31299674593597904; Validation Loss = 0.3483628663672521\n",
            "Cost after 298295 iterations : Training Loss =  0.3129968261868779; Validation Loss = 0.34836311421755933\n",
            "Cost after 298296 iterations : Training Loss =  0.3129967923362759; Validation Loss = 0.34836263762853875\n",
            "Cost after 298297 iterations : Training Loss =  0.3129967269179402; Validation Loss = 0.3483628854788459\n",
            "Cost after 298298 iterations : Training Loss =  0.31299683873657236; Validation Loss = 0.34836240888982545\n",
            "Cost after 298299 iterations : Training Loss =  0.3129966714316107; Validation Loss = 0.34836265674013317\n",
            "Cost after 298300 iterations : Training Loss =  0.3129968413542605; Validation Loss = 0.34836290459044067\n",
            "Cost after 298301 iterations : Training Loss =  0.312996717831907; Validation Loss = 0.3483624280014196\n",
            "Cost after 298302 iterations : Training Loss =  0.3129967420853225; Validation Loss = 0.3483626758517271\n",
            "Cost after 298303 iterations : Training Loss =  0.31299676423220385; Validation Loss = 0.3483621992627061\n",
            "Cost after 298304 iterations : Training Loss =  0.312996642816385; Validation Loss = 0.3483624471130138\n",
            "Cost after 298305 iterations : Training Loss =  0.3129968106325005; Validation Loss = 0.34836197052399287\n",
            "Cost after 298306 iterations : Training Loss =  0.31299664332753885; Validation Loss = 0.34836221837430076\n",
            "Cost after 298307 iterations : Training Loss =  0.31299675725270526; Validation Loss = 0.3483624662246079\n",
            "Cost after 298308 iterations : Training Loss =  0.31299668972783545; Validation Loss = 0.34836198963558723\n",
            "Cost after 298309 iterations : Training Loss =  0.3129966579837675; Validation Loss = 0.34836223748589473\n",
            "Cost after 298310 iterations : Training Loss =  0.31299673612813217; Validation Loss = 0.3483617608968739\n",
            "Cost after 298311 iterations : Training Loss =  0.31299656882317034; Validation Loss = 0.34836200874718115\n",
            "Cost after 298312 iterations : Training Loss =  0.312996772420088; Validation Loss = 0.3483622565974888\n",
            "Cost after 298313 iterations : Training Loss =  0.3129966152234672; Validation Loss = 0.34836178000846796\n",
            "Cost after 298314 iterations : Training Loss =  0.31299667315115026; Validation Loss = 0.3483620278587756\n",
            "Cost after 298315 iterations : Training Loss =  0.3129966616237636; Validation Loss = 0.3483615512697548\n",
            "Cost after 298316 iterations : Training Loss =  0.31299657388221247; Validation Loss = 0.3483617991200626\n",
            "Cost after 298317 iterations : Training Loss =  0.3129967080240604; Validation Loss = 0.34836132253104185\n",
            "Cost after 298318 iterations : Training Loss =  0.31299654071909844; Validation Loss = 0.34836157038134885\n",
            "Cost after 298319 iterations : Training Loss =  0.31299668831853295; Validation Loss = 0.34836181823165663\n",
            "Cost after 298320 iterations : Training Loss =  0.31299658711939515; Validation Loss = 0.3483613416426358\n",
            "Cost after 298321 iterations : Training Loss =  0.31299658904959493; Validation Loss = 0.3483615894929434\n",
            "Cost after 298322 iterations : Training Loss =  0.3129966335196917; Validation Loss = 0.34836111290392247\n",
            "Cost after 298323 iterations : Training Loss =  0.31299648978065703; Validation Loss = 0.3483613607542297\n",
            "Cost after 298324 iterations : Training Loss =  0.3129966799199885; Validation Loss = 0.3483608841652092\n",
            "Cost after 298325 iterations : Training Loss =  0.3129965126150268; Validation Loss = 0.34836113201551655\n",
            "Cost after 298326 iterations : Training Loss =  0.3129966042169775; Validation Loss = 0.34836137986582416\n",
            "Cost after 298327 iterations : Training Loss =  0.31299655901532336; Validation Loss = 0.3483609032768034\n",
            "Cost after 298328 iterations : Training Loss =  0.31299650494803977; Validation Loss = 0.3483611511271107\n",
            "Cost after 298329 iterations : Training Loss =  0.3129966054156202; Validation Loss = 0.3483606745380901\n",
            "Cost after 298330 iterations : Training Loss =  0.3129964381106583; Validation Loss = 0.34836092238839733\n",
            "Cost after 298331 iterations : Training Loss =  0.31299661938436; Validation Loss = 0.3483611702387052\n",
            "Cost after 298332 iterations : Training Loss =  0.31299648451095496; Validation Loss = 0.34836069364968425\n",
            "Cost after 298333 iterations : Training Loss =  0.31299652011542217; Validation Loss = 0.34836094149999197\n",
            "Cost after 298334 iterations : Training Loss =  0.3129965309112515; Validation Loss = 0.34836046491097106\n",
            "Cost after 298335 iterations : Training Loss =  0.31299642084648427; Validation Loss = 0.34836071276127856\n",
            "Cost after 298336 iterations : Training Loss =  0.3129965773115481; Validation Loss = 0.34836023617225764\n",
            "Cost after 298337 iterations : Training Loss =  0.3129964100065862; Validation Loss = 0.34836048402256536\n",
            "Cost after 298338 iterations : Training Loss =  0.31299653528280474; Validation Loss = 0.34836073187287225\n",
            "Cost after 298339 iterations : Training Loss =  0.3129964564068832; Validation Loss = 0.34836025528385184\n",
            "Cost after 298340 iterations : Training Loss =  0.31299643601386706; Validation Loss = 0.34836050313415945\n",
            "Cost after 298341 iterations : Training Loss =  0.3129965028071798; Validation Loss = 0.34836002654513865\n",
            "Cost after 298342 iterations : Training Loss =  0.3129963367449291; Validation Loss = 0.34836027439544626\n",
            "Cost after 298343 iterations : Training Loss =  0.31299654920747644; Validation Loss = 0.34835979780642573\n",
            "Cost after 298344 iterations : Training Loss =  0.31299638190251466; Validation Loss = 0.34836004565673306\n",
            "Cost after 298345 iterations : Training Loss =  0.3129964511812497; Validation Loss = 0.34836029350704073\n",
            "Cost after 298346 iterations : Training Loss =  0.31299642830281144; Validation Loss = 0.34835981691801976\n",
            "Cost after 298347 iterations : Training Loss =  0.31299635191231157; Validation Loss = 0.3483600647683272\n",
            "Cost after 298348 iterations : Training Loss =  0.3129964747031079; Validation Loss = 0.3483595881793064\n",
            "Cost after 298349 iterations : Training Loss =  0.31299630739814627; Validation Loss = 0.348359836029614\n",
            "Cost after 298350 iterations : Training Loss =  0.31299646634863226; Validation Loss = 0.3483600838799215\n",
            "Cost after 298351 iterations : Training Loss =  0.3129963537984434; Validation Loss = 0.34835960729090076\n",
            "Cost after 298352 iterations : Training Loss =  0.3129963670796942; Validation Loss = 0.3483598551412081\n",
            "Cost after 298353 iterations : Training Loss =  0.3129964001987397; Validation Loss = 0.3483593785521873\n",
            "Cost after 298354 iterations : Training Loss =  0.3129962678107565; Validation Loss = 0.3483596264024948\n",
            "Cost after 298355 iterations : Training Loss =  0.3129964465990361; Validation Loss = 0.3483591498134743\n",
            "Cost after 298356 iterations : Training Loss =  0.31299627929407436; Validation Loss = 0.3483593976637816\n",
            "Cost after 298357 iterations : Training Loss =  0.312996382247077; Validation Loss = 0.348359645514089\n",
            "Cost after 298358 iterations : Training Loss =  0.3129963256943711; Validation Loss = 0.3483591689250681\n",
            "Cost after 298359 iterations : Training Loss =  0.31299628297813914; Validation Loss = 0.3483594167753758\n",
            "Cost after 298360 iterations : Training Loss =  0.31299637209466774; Validation Loss = 0.34835894018635527\n",
            "Cost after 298361 iterations : Training Loss =  0.3129962047897059; Validation Loss = 0.3483591880366626\n",
            "Cost after 298362 iterations : Training Loss =  0.31299639741445956; Validation Loss = 0.34835943588696994\n",
            "Cost after 298363 iterations : Training Loss =  0.3129962511900027; Validation Loss = 0.3483589592979492\n",
            "Cost after 298364 iterations : Training Loss =  0.3129962981455217; Validation Loss = 0.34835920714825674\n",
            "Cost after 298365 iterations : Training Loss =  0.3129962975902992; Validation Loss = 0.3483587305592363\n",
            "Cost after 298366 iterations : Training Loss =  0.312996198876584; Validation Loss = 0.34835897840954383\n",
            "Cost after 298367 iterations : Training Loss =  0.31299634399059567; Validation Loss = 0.3483585018205227\n",
            "Cost after 298368 iterations : Training Loss =  0.31299617668563423; Validation Loss = 0.3483587496708304\n",
            "Cost after 298369 iterations : Training Loss =  0.3129963133129045; Validation Loss = 0.3483589975211379\n",
            "Cost after 298370 iterations : Training Loss =  0.3129962230859309; Validation Loss = 0.348358520932117\n",
            "Cost after 298371 iterations : Training Loss =  0.3129962140439664; Validation Loss = 0.34835876878242444\n",
            "Cost after 298372 iterations : Training Loss =  0.31299626948622766; Validation Loss = 0.34835829219340353\n",
            "Cost after 298373 iterations : Training Loss =  0.3129961147750284; Validation Loss = 0.34835854004371103\n",
            "Cost after 298374 iterations : Training Loss =  0.31299631588652405; Validation Loss = 0.3483580634546902\n",
            "Cost after 298375 iterations : Training Loss =  0.31299614858156244; Validation Loss = 0.3483583113049979\n",
            "Cost after 298376 iterations : Training Loss =  0.3129962292113493; Validation Loss = 0.3483585591553054\n",
            "Cost after 298377 iterations : Training Loss =  0.3129961949818588; Validation Loss = 0.348358082566285\n",
            "Cost after 298378 iterations : Training Loss =  0.3129961299424113; Validation Loss = 0.34835833041659214\n",
            "Cost after 298379 iterations : Training Loss =  0.31299624138215576; Validation Loss = 0.3483578538275715\n",
            "Cost after 298380 iterations : Training Loss =  0.312996074077194; Validation Loss = 0.34835810167787884\n",
            "Cost after 298381 iterations : Training Loss =  0.3129962443787317; Validation Loss = 0.3483583495281865\n",
            "Cost after 298382 iterations : Training Loss =  0.31299612047749087; Validation Loss = 0.34835787293916554\n",
            "Cost after 298383 iterations : Training Loss =  0.31299614510979373; Validation Loss = 0.3483581207894727\n",
            "Cost after 298384 iterations : Training Loss =  0.31299616687778736; Validation Loss = 0.34835764420045195\n",
            "Cost after 298385 iterations : Training Loss =  0.3129960458408559; Validation Loss = 0.3483578920507594\n",
            "Cost after 298386 iterations : Training Loss =  0.3129962132780838; Validation Loss = 0.3483574154617382\n",
            "Cost after 298387 iterations : Training Loss =  0.31299604597312214; Validation Loss = 0.34835766331204626\n",
            "Cost after 298388 iterations : Training Loss =  0.3129961602771765; Validation Loss = 0.348357911162354\n",
            "Cost after 298389 iterations : Training Loss =  0.31299609237341863; Validation Loss = 0.34835743457333307\n",
            "Cost after 298390 iterations : Training Loss =  0.31299606100823857; Validation Loss = 0.3483576824236407\n",
            "Cost after 298391 iterations : Training Loss =  0.31299613877371546; Validation Loss = 0.34835720583461993\n",
            "Cost after 298392 iterations : Training Loss =  0.3129959714687536; Validation Loss = 0.34835745368492727\n",
            "Cost after 298393 iterations : Training Loss =  0.3129961754445592; Validation Loss = 0.3483577015352348\n",
            "Cost after 298394 iterations : Training Loss =  0.31299601786905035; Validation Loss = 0.348357224946214\n",
            "Cost after 298395 iterations : Training Loss =  0.31299607617562136; Validation Loss = 0.34835747279652185\n",
            "Cost after 298396 iterations : Training Loss =  0.31299606426934695; Validation Loss = 0.34835699620750094\n",
            "Cost after 298397 iterations : Training Loss =  0.3129959769066833; Validation Loss = 0.3483572440578081\n",
            "Cost after 298398 iterations : Training Loss =  0.31299611066964356; Validation Loss = 0.34835676746878746\n",
            "Cost after 298399 iterations : Training Loss =  0.3129959433646818; Validation Loss = 0.3483570153190948\n",
            "Cost after 298400 iterations : Training Loss =  0.3129960913430037; Validation Loss = 0.3483572631694024\n",
            "Cost after 298401 iterations : Training Loss =  0.3129959897649785; Validation Loss = 0.3483567865803815\n",
            "Cost after 298402 iterations : Training Loss =  0.31299599207406625; Validation Loss = 0.34835703443068916\n",
            "Cost after 298403 iterations : Training Loss =  0.3129960361652752; Validation Loss = 0.34835655784166825\n",
            "Cost after 298404 iterations : Training Loss =  0.3129958928051279; Validation Loss = 0.3483568056919762\n",
            "Cost after 298405 iterations : Training Loss =  0.31299608256557176; Validation Loss = 0.348356329102955\n",
            "Cost after 298406 iterations : Training Loss =  0.3129959152606102; Validation Loss = 0.34835657695326305\n",
            "Cost after 298407 iterations : Training Loss =  0.3129960072414486; Validation Loss = 0.3483568248035701\n",
            "Cost after 298408 iterations : Training Loss =  0.3129959616609066; Validation Loss = 0.3483563482145493\n",
            "Cost after 298409 iterations : Training Loss =  0.31299590797251053; Validation Loss = 0.3483565960648569\n",
            "Cost after 298410 iterations : Training Loss =  0.3129960080612035; Validation Loss = 0.3483561194758363\n",
            "Cost after 298411 iterations : Training Loss =  0.3129958407562416; Validation Loss = 0.34835636732614367\n",
            "Cost after 298412 iterations : Training Loss =  0.31299602240883106; Validation Loss = 0.34835661517645117\n",
            "Cost after 298413 iterations : Training Loss =  0.31299588715653853; Validation Loss = 0.3483561385874303\n",
            "Cost after 298414 iterations : Training Loss =  0.31299592313989333; Validation Loss = 0.34835638643773775\n",
            "Cost after 298415 iterations : Training Loss =  0.3129959335568348; Validation Loss = 0.34835590984871717\n",
            "Cost after 298416 iterations : Training Loss =  0.3129958238709555; Validation Loss = 0.34835615769902484\n",
            "Cost after 298417 iterations : Training Loss =  0.31299597995713174; Validation Loss = 0.3483556811100037\n",
            "Cost after 298418 iterations : Training Loss =  0.31299581265216964; Validation Loss = 0.34835592896031115\n",
            "Cost after 298419 iterations : Training Loss =  0.312995938307276; Validation Loss = 0.34835617681061865\n",
            "Cost after 298420 iterations : Training Loss =  0.31299585905246635; Validation Loss = 0.348355700221598\n",
            "Cost after 298421 iterations : Training Loss =  0.3129958390383381; Validation Loss = 0.3483559480719057\n",
            "Cost after 298422 iterations : Training Loss =  0.3129959054527631; Validation Loss = 0.34835547148288526\n",
            "Cost after 298423 iterations : Training Loss =  0.3129957397694; Validation Loss = 0.3483557193331923\n",
            "Cost after 298424 iterations : Training Loss =  0.3129959518530599; Validation Loss = 0.34835524274417123\n",
            "Cost after 298425 iterations : Training Loss =  0.31299578454809807; Validation Loss = 0.348355490594479\n",
            "Cost after 298426 iterations : Training Loss =  0.31299585420572057; Validation Loss = 0.348355738444787\n",
            "Cost after 298427 iterations : Training Loss =  0.31299583094839484; Validation Loss = 0.3483552618557658\n",
            "Cost after 298428 iterations : Training Loss =  0.3129957549367829; Validation Loss = 0.3483555097060736\n",
            "Cost after 298429 iterations : Training Loss =  0.31299587734869105; Validation Loss = 0.34835503311705235\n",
            "Cost after 298430 iterations : Training Loss =  0.3129957100437298; Validation Loss = 0.3483552809673598\n",
            "Cost after 298431 iterations : Training Loss =  0.3129958693731032; Validation Loss = 0.34835552881766757\n",
            "Cost after 298432 iterations : Training Loss =  0.3129957564440262; Validation Loss = 0.3483550522286468\n",
            "Cost after 298433 iterations : Training Loss =  0.3129957701041655; Validation Loss = 0.34835530007895427\n",
            "Cost after 298434 iterations : Training Loss =  0.31299580284432293; Validation Loss = 0.3483548234899337\n",
            "Cost after 298435 iterations : Training Loss =  0.31299567083522756; Validation Loss = 0.34835507134024085\n",
            "Cost after 298436 iterations : Training Loss =  0.3129958492446196; Validation Loss = 0.3483545947512202\n",
            "Cost after 298437 iterations : Training Loss =  0.3129956819396578; Validation Loss = 0.34835484260152755\n",
            "Cost after 298438 iterations : Training Loss =  0.31299578527154814; Validation Loss = 0.34835509045183566\n",
            "Cost after 298439 iterations : Training Loss =  0.3129957283399544; Validation Loss = 0.34835461386281447\n",
            "Cost after 298440 iterations : Training Loss =  0.31299568600260996; Validation Loss = 0.34835486171312185\n",
            "Cost after 298441 iterations : Training Loss =  0.31299577474025114; Validation Loss = 0.3483543851241012\n",
            "Cost after 298442 iterations : Training Loss =  0.3129956074352894; Validation Loss = 0.3483546329744091\n",
            "Cost after 298443 iterations : Training Loss =  0.31299580043893077; Validation Loss = 0.3483548808247163\n",
            "Cost after 298444 iterations : Training Loss =  0.3129956538355861; Validation Loss = 0.3483544042356956\n",
            "Cost after 298445 iterations : Training Loss =  0.31299570116999287; Validation Loss = 0.34835465208600286\n",
            "Cost after 298446 iterations : Training Loss =  0.3129957002358825; Validation Loss = 0.34835417549698205\n",
            "Cost after 298447 iterations : Training Loss =  0.31299560190105497; Validation Loss = 0.3483544233472897\n",
            "Cost after 298448 iterations : Training Loss =  0.31299574663617924; Validation Loss = 0.34835394675826886\n",
            "Cost after 298449 iterations : Training Loss =  0.3129955793312175; Validation Loss = 0.34835419460857653\n",
            "Cost after 298450 iterations : Training Loss =  0.3129957163373755; Validation Loss = 0.3483544424588838\n",
            "Cost after 298451 iterations : Training Loss =  0.3129956257315146; Validation Loss = 0.34835396586986306\n",
            "Cost after 298452 iterations : Training Loss =  0.3129956170684377; Validation Loss = 0.3483542137201705\n",
            "Cost after 298453 iterations : Training Loss =  0.3129956721318107; Validation Loss = 0.34835373713114975\n",
            "Cost after 298454 iterations : Training Loss =  0.3129955177994996; Validation Loss = 0.3483539849814571\n",
            "Cost after 298455 iterations : Training Loss =  0.31299571853210717; Validation Loss = 0.348353508392436\n",
            "Cost after 298456 iterations : Training Loss =  0.31299555122714545; Validation Loss = 0.34835375624274384\n",
            "Cost after 298457 iterations : Training Loss =  0.3129956322358201; Validation Loss = 0.3483540040930515\n",
            "Cost after 298458 iterations : Training Loss =  0.31299559762744245; Validation Loss = 0.3483535275040305\n",
            "Cost after 298459 iterations : Training Loss =  0.31299553296688226; Validation Loss = 0.3483537753543381\n",
            "Cost after 298460 iterations : Training Loss =  0.312995644027739; Validation Loss = 0.348353298765317\n",
            "Cost after 298461 iterations : Training Loss =  0.31299547672277733; Validation Loss = 0.34835354661562484\n",
            "Cost after 298462 iterations : Training Loss =  0.312995647403203; Validation Loss = 0.348353794465932\n",
            "Cost after 298463 iterations : Training Loss =  0.31299552312307394; Validation Loss = 0.3483533178769116\n",
            "Cost after 298464 iterations : Training Loss =  0.31299554813426494; Validation Loss = 0.34835356572721937\n",
            "Cost after 298465 iterations : Training Loss =  0.31299556952337065; Validation Loss = 0.34835308913819824\n",
            "Cost after 298466 iterations : Training Loss =  0.31299544886532693; Validation Loss = 0.3483533369885057\n",
            "Cost after 298467 iterations : Training Loss =  0.3129956159236671; Validation Loss = 0.34835286039948515\n",
            "Cost after 298468 iterations : Training Loss =  0.31299544861870543; Validation Loss = 0.3483531082497925\n",
            "Cost after 298469 iterations : Training Loss =  0.31299556330164746; Validation Loss = 0.3483533561001002\n",
            "Cost after 298470 iterations : Training Loss =  0.3129954950190021; Validation Loss = 0.34835287951107946\n",
            "Cost after 298471 iterations : Training Loss =  0.3129954640327095; Validation Loss = 0.34835312736138635\n",
            "Cost after 298472 iterations : Training Loss =  0.3129955414192988; Validation Loss = 0.34835265077236616\n",
            "Cost after 298473 iterations : Training Loss =  0.31299537411433714; Validation Loss = 0.3483528986226733\n",
            "Cost after 298474 iterations : Training Loss =  0.3129955784690301; Validation Loss = 0.34835314647298105\n",
            "Cost after 298475 iterations : Training Loss =  0.3129954205146336; Validation Loss = 0.3483526698839601\n",
            "Cost after 298476 iterations : Training Loss =  0.3129954792000924; Validation Loss = 0.3483529177342679\n",
            "Cost after 298477 iterations : Training Loss =  0.3129954669149303; Validation Loss = 0.348352441145247\n",
            "Cost after 298478 iterations : Training Loss =  0.3129953799311544; Validation Loss = 0.3483526889955543\n",
            "Cost after 298479 iterations : Training Loss =  0.3129955133152271; Validation Loss = 0.34835221240653363\n",
            "Cost after 298480 iterations : Training Loss =  0.31299534601026496; Validation Loss = 0.34835246025684113\n",
            "Cost after 298481 iterations : Training Loss =  0.3129954943674748; Validation Loss = 0.34835270810714863\n",
            "Cost after 298482 iterations : Training Loss =  0.31299539241056196; Validation Loss = 0.3483522315181277\n",
            "Cost after 298483 iterations : Training Loss =  0.312995395098537; Validation Loss = 0.34835247936843544\n",
            "Cost after 298484 iterations : Training Loss =  0.3129954388108583; Validation Loss = 0.3483520027794148\n",
            "Cost after 298485 iterations : Training Loss =  0.312995295829599; Validation Loss = 0.34835225062972214\n",
            "Cost after 298486 iterations : Training Loss =  0.3129954852111551; Validation Loss = 0.34835177404070167\n",
            "Cost after 298487 iterations : Training Loss =  0.3129953179061936; Validation Loss = 0.348352021891009\n",
            "Cost after 298488 iterations : Training Loss =  0.31299541026591976; Validation Loss = 0.34835226974131644\n",
            "Cost after 298489 iterations : Training Loss =  0.3129953643064899; Validation Loss = 0.3483517931522957\n",
            "Cost after 298490 iterations : Training Loss =  0.3129953109969819; Validation Loss = 0.34835204100260314\n",
            "Cost after 298491 iterations : Training Loss =  0.31299541070678677; Validation Loss = 0.3483515644135821\n",
            "Cost after 298492 iterations : Training Loss =  0.3129952434018249; Validation Loss = 0.34835181226388984\n",
            "Cost after 298493 iterations : Training Loss =  0.31299542543330233; Validation Loss = 0.3483520601141972\n",
            "Cost after 298494 iterations : Training Loss =  0.31299528980212166; Validation Loss = 0.34835158352517637\n",
            "Cost after 298495 iterations : Training Loss =  0.3129953261643643; Validation Loss = 0.348351831375484\n",
            "Cost after 298496 iterations : Training Loss =  0.3129953362024184; Validation Loss = 0.34835135478646306\n",
            "Cost after 298497 iterations : Training Loss =  0.3129952268954266; Validation Loss = 0.34835160263677073\n",
            "Cost after 298498 iterations : Training Loss =  0.312995382602715; Validation Loss = 0.3483511260477502\n",
            "Cost after 298499 iterations : Training Loss =  0.31299521529775304; Validation Loss = 0.34835137389805737\n",
            "Cost after 298500 iterations : Training Loss =  0.312995341331747; Validation Loss = 0.34835162174836454\n",
            "Cost after 298501 iterations : Training Loss =  0.3129952616980498; Validation Loss = 0.3483511451593443\n",
            "Cost after 298502 iterations : Training Loss =  0.3129952420628092; Validation Loss = 0.34835139300965173\n",
            "Cost after 298503 iterations : Training Loss =  0.3129953080983464; Validation Loss = 0.34835091642063076\n",
            "Cost after 298504 iterations : Training Loss =  0.31299514279387114; Validation Loss = 0.34835116427093854\n",
            "Cost after 298505 iterations : Training Loss =  0.3129953544986431; Validation Loss = 0.34835068768191774\n",
            "Cost after 298506 iterations : Training Loss =  0.31299518719368147; Validation Loss = 0.34835093553222535\n",
            "Cost after 298507 iterations : Training Loss =  0.31299525723019167; Validation Loss = 0.3483511833825328\n",
            "Cost after 298508 iterations : Training Loss =  0.312995233593978; Validation Loss = 0.3483507067935122\n",
            "Cost after 298509 iterations : Training Loss =  0.3129951579612539; Validation Loss = 0.3483509546438195\n",
            "Cost after 298510 iterations : Training Loss =  0.3129952799942746; Validation Loss = 0.3483504780547985\n",
            "Cost after 298511 iterations : Training Loss =  0.31299511268931296; Validation Loss = 0.34835072590510613\n",
            "Cost after 298512 iterations : Training Loss =  0.3129952723975743; Validation Loss = 0.3483509737554137\n",
            "Cost after 298513 iterations : Training Loss =  0.31299515908960956; Validation Loss = 0.34835049716639305\n",
            "Cost after 298514 iterations : Training Loss =  0.31299517312863656; Validation Loss = 0.34835074501670066\n",
            "Cost after 298515 iterations : Training Loss =  0.3129952054899063; Validation Loss = 0.3483502684276793\n",
            "Cost after 298516 iterations : Training Loss =  0.31299507385969866; Validation Loss = 0.3483505162779869\n",
            "Cost after 298517 iterations : Training Loss =  0.3129952518902028; Validation Loss = 0.3483500396889663\n",
            "Cost after 298518 iterations : Training Loss =  0.31299508458524117; Validation Loss = 0.34835028753927405\n",
            "Cost after 298519 iterations : Training Loss =  0.31299518829601913; Validation Loss = 0.3483505353895812\n",
            "Cost after 298520 iterations : Training Loss =  0.31299513098553783; Validation Loss = 0.3483500588005602\n",
            "Cost after 298521 iterations : Training Loss =  0.3129950890270812; Validation Loss = 0.3483503066508679\n",
            "Cost after 298522 iterations : Training Loss =  0.3129951773858344; Validation Loss = 0.34834983006184744\n",
            "Cost after 298523 iterations : Training Loss =  0.31299501008087277; Validation Loss = 0.3483500779121548\n",
            "Cost after 298524 iterations : Training Loss =  0.312995203463402; Validation Loss = 0.3483503257624625\n",
            "Cost after 298525 iterations : Training Loss =  0.31299505648116926; Validation Loss = 0.3483498491734416\n",
            "Cost after 298526 iterations : Training Loss =  0.312995104194464; Validation Loss = 0.34835009702374903\n",
            "Cost after 298527 iterations : Training Loss =  0.3129951028814657; Validation Loss = 0.34834962043472784\n",
            "Cost after 298528 iterations : Training Loss =  0.31299500492552607; Validation Loss = 0.3483498682850359\n",
            "Cost after 298529 iterations : Training Loss =  0.3129951492817626; Validation Loss = 0.34834939169601503\n",
            "Cost after 298530 iterations : Training Loss =  0.31299498197680076; Validation Loss = 0.3483496395463224\n",
            "Cost after 298531 iterations : Training Loss =  0.3129951193618465; Validation Loss = 0.34834988739663003\n",
            "Cost after 298532 iterations : Training Loss =  0.3129950283770975; Validation Loss = 0.348349410807609\n",
            "Cost after 298533 iterations : Training Loss =  0.31299502009290886; Validation Loss = 0.34834965865791645\n",
            "Cost after 298534 iterations : Training Loss =  0.31299507477739397; Validation Loss = 0.3483491820688957\n",
            "Cost after 298535 iterations : Training Loss =  0.3129949208239708; Validation Loss = 0.3483494299192033\n",
            "Cost after 298536 iterations : Training Loss =  0.31299512117769046; Validation Loss = 0.3483489533301825\n",
            "Cost after 298537 iterations : Training Loss =  0.31299495387272896; Validation Loss = 0.34834920118048973\n",
            "Cost after 298538 iterations : Training Loss =  0.31299503526029115; Validation Loss = 0.3483494490307974\n",
            "Cost after 298539 iterations : Training Loss =  0.31299500027302574; Validation Loss = 0.34834897244177654\n",
            "Cost after 298540 iterations : Training Loss =  0.3129949359913534; Validation Loss = 0.3483492202920844\n",
            "Cost after 298541 iterations : Training Loss =  0.3129950466733227; Validation Loss = 0.3483487437030631\n",
            "Cost after 298542 iterations : Training Loss =  0.3129948793683604; Validation Loss = 0.348348991553371\n",
            "Cost after 298543 iterations : Training Loss =  0.31299505042767395; Validation Loss = 0.34834923940367873\n",
            "Cost after 298544 iterations : Training Loss =  0.3129949257686575; Validation Loss = 0.348348762814658\n",
            "Cost after 298545 iterations : Training Loss =  0.3129949511587361; Validation Loss = 0.348349010664965\n",
            "Cost after 298546 iterations : Training Loss =  0.3129949721689538; Validation Loss = 0.3483485340759443\n",
            "Cost after 298547 iterations : Training Loss =  0.31299485188979814; Validation Loss = 0.3483487819262519\n",
            "Cost after 298548 iterations : Training Loss =  0.3129950185692507; Validation Loss = 0.3483483053372312\n",
            "Cost after 298549 iterations : Training Loss =  0.312994851264289; Validation Loss = 0.34834855318753855\n",
            "Cost after 298550 iterations : Training Loss =  0.3129949663261188; Validation Loss = 0.34834880103784627\n",
            "Cost after 298551 iterations : Training Loss =  0.3129948976645854; Validation Loss = 0.34834832444882535\n",
            "Cost after 298552 iterations : Training Loss =  0.31299486705718055; Validation Loss = 0.34834857229913296\n",
            "Cost after 298553 iterations : Training Loss =  0.31299494406488204; Validation Loss = 0.34834809571011205\n",
            "Cost after 298554 iterations : Training Loss =  0.3129947767599202; Validation Loss = 0.3483483435604194\n",
            "Cost after 298555 iterations : Training Loss =  0.3129949814935013; Validation Loss = 0.34834859141072727\n",
            "Cost after 298556 iterations : Training Loss =  0.3129948231602171; Validation Loss = 0.3483481148217061\n",
            "Cost after 298557 iterations : Training Loss =  0.3129948822245634; Validation Loss = 0.34834836267201424\n",
            "Cost after 298558 iterations : Training Loss =  0.3129948695605137; Validation Loss = 0.34834788608299305\n",
            "Cost after 298559 iterations : Training Loss =  0.31299478295562566; Validation Loss = 0.34834813393330094\n",
            "Cost after 298560 iterations : Training Loss =  0.31299491596081036; Validation Loss = 0.3483476573442799\n",
            "Cost after 298561 iterations : Training Loss =  0.31299474865584853; Validation Loss = 0.3483479051945871\n",
            "Cost after 298562 iterations : Training Loss =  0.3129948973919459; Validation Loss = 0.3483481530448945\n",
            "Cost after 298563 iterations : Training Loss =  0.3129947950561453; Validation Loss = 0.3483476764558739\n",
            "Cost after 298564 iterations : Training Loss =  0.3129947981230081; Validation Loss = 0.3483479243061812\n",
            "Cost after 298565 iterations : Training Loss =  0.31299484145644163; Validation Loss = 0.3483474477171607\n",
            "Cost after 298566 iterations : Training Loss =  0.3129946988540703; Validation Loss = 0.34834769556746836\n",
            "Cost after 298567 iterations : Training Loss =  0.3129948878567385; Validation Loss = 0.3483472189784476\n",
            "Cost after 298568 iterations : Training Loss =  0.3129947205517769; Validation Loss = 0.34834746682875506\n",
            "Cost after 298569 iterations : Training Loss =  0.31299481329039064; Validation Loss = 0.3483477146790625\n",
            "Cost after 298570 iterations : Training Loss =  0.3129947669520734; Validation Loss = 0.34834723809004187\n",
            "Cost after 298571 iterations : Training Loss =  0.31299471402145296; Validation Loss = 0.3483474859403492\n",
            "Cost after 298572 iterations : Training Loss =  0.3129948133523699; Validation Loss = 0.34834700935132845\n",
            "Cost after 298573 iterations : Training Loss =  0.31299464604740823; Validation Loss = 0.3483472572016358\n",
            "Cost after 298574 iterations : Training Loss =  0.3129948284577733; Validation Loss = 0.34834750505194334\n",
            "Cost after 298575 iterations : Training Loss =  0.31299469244770506; Validation Loss = 0.3483470284629226\n",
            "Cost after 298576 iterations : Training Loss =  0.3129947291888355; Validation Loss = 0.3483472763132299\n",
            "Cost after 298577 iterations : Training Loss =  0.3129947388480015; Validation Loss = 0.3483467997242092\n",
            "Cost after 298578 iterations : Training Loss =  0.31299462991989757; Validation Loss = 0.3483470475745169\n",
            "Cost after 298579 iterations : Training Loss =  0.31299478524829805; Validation Loss = 0.348346570985496\n",
            "Cost after 298580 iterations : Training Loss =  0.31299461794333644; Validation Loss = 0.34834681883580376\n",
            "Cost after 298581 iterations : Training Loss =  0.31299474435621827; Validation Loss = 0.3483470666861113\n",
            "Cost after 298582 iterations : Training Loss =  0.3129946643436332; Validation Loss = 0.34834659009709046\n",
            "Cost after 298583 iterations : Training Loss =  0.3129946450872803; Validation Loss = 0.3483468379473977\n",
            "Cost after 298584 iterations : Training Loss =  0.31299471074392976; Validation Loss = 0.348346361358377\n",
            "Cost after 298585 iterations : Training Loss =  0.31299454581834235; Validation Loss = 0.34834660920868443\n",
            "Cost after 298586 iterations : Training Loss =  0.3129947571442262; Validation Loss = 0.34834613261966396\n",
            "Cost after 298587 iterations : Training Loss =  0.3129945898392647; Validation Loss = 0.348346380469971\n",
            "Cost after 298588 iterations : Training Loss =  0.31299466025466266; Validation Loss = 0.34834662832027874\n",
            "Cost after 298589 iterations : Training Loss =  0.31299463623956114; Validation Loss = 0.34834615173125827\n",
            "Cost after 298590 iterations : Training Loss =  0.31299456098572487; Validation Loss = 0.3483463995815653\n",
            "Cost after 298591 iterations : Training Loss =  0.312994682639858; Validation Loss = 0.3483459229925444\n",
            "Cost after 298592 iterations : Training Loss =  0.3129945153348961; Validation Loss = 0.3483461708428521\n",
            "Cost after 298593 iterations : Training Loss =  0.3129946754220456; Validation Loss = 0.3483464186931596\n",
            "Cost after 298594 iterations : Training Loss =  0.3129945617351929; Validation Loss = 0.34834594210413883\n",
            "Cost after 298595 iterations : Training Loss =  0.31299457615310744; Validation Loss = 0.34834618995444644\n",
            "Cost after 298596 iterations : Training Loss =  0.3129946081354894; Validation Loss = 0.34834571336542575\n",
            "Cost after 298597 iterations : Training Loss =  0.31299447688416987; Validation Loss = 0.3483459612157334\n",
            "Cost after 298598 iterations : Training Loss =  0.3129946545357864; Validation Loss = 0.3483454846267121\n",
            "Cost after 298599 iterations : Training Loss =  0.31299448723082424; Validation Loss = 0.34834573247701983\n",
            "Cost after 298600 iterations : Training Loss =  0.31299459132049007; Validation Loss = 0.3483459803273273\n",
            "Cost after 298601 iterations : Training Loss =  0.31299453363112095; Validation Loss = 0.3483455037383067\n",
            "Cost after 298602 iterations : Training Loss =  0.31299449205155233; Validation Loss = 0.3483457515886144\n",
            "Cost after 298603 iterations : Training Loss =  0.3129945800314176; Validation Loss = 0.3483452749995934\n",
            "Cost after 298604 iterations : Training Loss =  0.3129944127264561; Validation Loss = 0.34834552284990083\n",
            "Cost after 298605 iterations : Training Loss =  0.31299460648787264; Validation Loss = 0.34834577070020856\n",
            "Cost after 298606 iterations : Training Loss =  0.31299445912675256; Validation Loss = 0.34834529411118753\n",
            "Cost after 298607 iterations : Training Loss =  0.312994507218935; Validation Loss = 0.34834554196149486\n",
            "Cost after 298608 iterations : Training Loss =  0.3129945055270493; Validation Loss = 0.34834506537247395\n",
            "Cost after 298609 iterations : Training Loss =  0.3129944079499971; Validation Loss = 0.34834531322278156\n",
            "Cost after 298610 iterations : Training Loss =  0.312994551927346; Validation Loss = 0.34834483663376087\n",
            "Cost after 298611 iterations : Training Loss =  0.31299438462238427; Validation Loss = 0.34834508448406865\n",
            "Cost after 298612 iterations : Training Loss =  0.31299452238631753; Validation Loss = 0.348345332334376\n",
            "Cost after 298613 iterations : Training Loss =  0.3129944310226807; Validation Loss = 0.34834485574535534\n",
            "Cost after 298614 iterations : Training Loss =  0.3129944231173797; Validation Loss = 0.3483451035956628\n",
            "Cost after 298615 iterations : Training Loss =  0.31299447742297737; Validation Loss = 0.34834462700664204\n",
            "Cost after 298616 iterations : Training Loss =  0.3129943238484419; Validation Loss = 0.34834487485694965\n",
            "Cost after 298617 iterations : Training Loss =  0.31299452382327386; Validation Loss = 0.3483443982679286\n",
            "Cost after 298618 iterations : Training Loss =  0.3129943565183122; Validation Loss = 0.3483446461182362\n",
            "Cost after 298619 iterations : Training Loss =  0.3129944382847623; Validation Loss = 0.34834489396854357\n",
            "Cost after 298620 iterations : Training Loss =  0.3129944029186092; Validation Loss = 0.34834441737952276\n",
            "Cost after 298621 iterations : Training Loss =  0.31299433901582424; Validation Loss = 0.3483446652298304\n",
            "Cost after 298622 iterations : Training Loss =  0.3129944493189057; Validation Loss = 0.34834418864080974\n",
            "Cost after 298623 iterations : Training Loss =  0.31299428201394397; Validation Loss = 0.34834443649111707\n",
            "Cost after 298624 iterations : Training Loss =  0.31299445345214494; Validation Loss = 0.34834468434142435\n",
            "Cost after 298625 iterations : Training Loss =  0.3129943284142406; Validation Loss = 0.3483442077524039\n",
            "Cost after 298626 iterations : Training Loss =  0.31299435418320704; Validation Loss = 0.3483444556027112\n",
            "Cost after 298627 iterations : Training Loss =  0.3129943748145373; Validation Loss = 0.3483439790136906\n",
            "Cost after 298628 iterations : Training Loss =  0.3129942549142692; Validation Loss = 0.3483442268639974\n",
            "Cost after 298629 iterations : Training Loss =  0.3129944212148339; Validation Loss = 0.34834375027497716\n",
            "Cost after 298630 iterations : Training Loss =  0.3129942539098722; Validation Loss = 0.34834399812528477\n",
            "Cost after 298631 iterations : Training Loss =  0.31299436935058966; Validation Loss = 0.3483442459755923\n",
            "Cost after 298632 iterations : Training Loss =  0.31299430031016867; Validation Loss = 0.34834376938657124\n",
            "Cost after 298633 iterations : Training Loss =  0.3129942700816516; Validation Loss = 0.34834401723687886\n",
            "Cost after 298634 iterations : Training Loss =  0.31299434671046533; Validation Loss = 0.34834354064785816\n",
            "Cost after 298635 iterations : Training Loss =  0.3129941794055035; Validation Loss = 0.3483437884981656\n",
            "Cost after 298636 iterations : Training Loss =  0.31299438451797224; Validation Loss = 0.34834403634847333\n",
            "Cost after 298637 iterations : Training Loss =  0.31299422580580033; Validation Loss = 0.34834355975945236\n",
            "Cost after 298638 iterations : Training Loss =  0.3129942852490346; Validation Loss = 0.3483438076097599\n",
            "Cost after 298639 iterations : Training Loss =  0.31299427220609705; Validation Loss = 0.348343331020739\n",
            "Cost after 298640 iterations : Training Loss =  0.31299418598009665; Validation Loss = 0.3483435788710467\n",
            "Cost after 298641 iterations : Training Loss =  0.31299431860639376; Validation Loss = 0.3483431022820259\n",
            "Cost after 298642 iterations : Training Loss =  0.3129941513014318; Validation Loss = 0.3483433501323333\n",
            "Cost after 298643 iterations : Training Loss =  0.3129943004164171; Validation Loss = 0.34834359798264103\n",
            "Cost after 298644 iterations : Training Loss =  0.3129941977017286; Validation Loss = 0.3483431213936197\n",
            "Cost after 298645 iterations : Training Loss =  0.3129942011474791; Validation Loss = 0.34834336924392745\n",
            "Cost after 298646 iterations : Training Loss =  0.312994244102025; Validation Loss = 0.3483428926549069\n",
            "Cost after 298647 iterations : Training Loss =  0.3129941018785414; Validation Loss = 0.3483431405052144\n",
            "Cost after 298648 iterations : Training Loss =  0.3129942905023218; Validation Loss = 0.3483426639161934\n",
            "Cost after 298649 iterations : Training Loss =  0.3129941231973599; Validation Loss = 0.3483429117665013\n",
            "Cost after 298650 iterations : Training Loss =  0.31299421631486185; Validation Loss = 0.34834315961680856\n",
            "Cost after 298651 iterations : Training Loss =  0.3129941695976566; Validation Loss = 0.3483426830277878\n",
            "Cost after 298652 iterations : Training Loss =  0.312994117045924; Validation Loss = 0.34834293087809526\n",
            "Cost after 298653 iterations : Training Loss =  0.31299421599795346; Validation Loss = 0.3483424542890744\n",
            "Cost after 298654 iterations : Training Loss =  0.31299404869299174; Validation Loss = 0.34834270213938184\n",
            "Cost after 298655 iterations : Training Loss =  0.3129942314822444; Validation Loss = 0.34834294998968923\n",
            "Cost after 298656 iterations : Training Loss =  0.3129940950932883; Validation Loss = 0.34834247340066865\n",
            "Cost after 298657 iterations : Training Loss =  0.3129941322133069; Validation Loss = 0.34834272125097626\n",
            "Cost after 298658 iterations : Training Loss =  0.3129941414935848; Validation Loss = 0.34834224466195535\n",
            "Cost after 298659 iterations : Training Loss =  0.31299403294436884; Validation Loss = 0.34834249251226285\n",
            "Cost after 298660 iterations : Training Loss =  0.3129941878938816; Validation Loss = 0.34834201592324215\n",
            "Cost after 298661 iterations : Training Loss =  0.31299402058891973; Validation Loss = 0.34834226377354965\n",
            "Cost after 298662 iterations : Training Loss =  0.31299414738068915; Validation Loss = 0.34834251162385715\n",
            "Cost after 298663 iterations : Training Loss =  0.31299406698921645; Validation Loss = 0.34834203503483663\n",
            "Cost after 298664 iterations : Training Loss =  0.3129940481117511; Validation Loss = 0.348342282885144\n",
            "Cost after 298665 iterations : Training Loss =  0.31299411338951333; Validation Loss = 0.3483418062961232\n",
            "Cost after 298666 iterations : Training Loss =  0.31299394884281356; Validation Loss = 0.34834205414643066\n",
            "Cost after 298667 iterations : Training Loss =  0.3129941597898099; Validation Loss = 0.34834157755740963\n",
            "Cost after 298668 iterations : Training Loss =  0.3129939924848481; Validation Loss = 0.3483418254077171\n",
            "Cost after 298669 iterations : Training Loss =  0.31299406327913376; Validation Loss = 0.3483420732580248\n",
            "Cost after 298670 iterations : Training Loss =  0.3129940388851443; Validation Loss = 0.3483415966690038\n",
            "Cost after 298671 iterations : Training Loss =  0.31299396401019597; Validation Loss = 0.3483418445193114\n",
            "Cost after 298672 iterations : Training Loss =  0.3129940852854413; Validation Loss = 0.34834136793029075\n",
            "Cost after 298673 iterations : Training Loss =  0.3129939179804795; Validation Loss = 0.3483416157805983\n",
            "Cost after 298674 iterations : Training Loss =  0.31299407844651655; Validation Loss = 0.3483418636309061\n",
            "Cost after 298675 iterations : Training Loss =  0.31299396438077615; Validation Loss = 0.34834138704188533\n",
            "Cost after 298676 iterations : Training Loss =  0.31299397917757854; Validation Loss = 0.3483416348921925\n",
            "Cost after 298677 iterations : Training Loss =  0.31299401078107264; Validation Loss = 0.34834115830317186\n",
            "Cost after 298678 iterations : Training Loss =  0.31299387990864086; Validation Loss = 0.3483414061534791\n",
            "Cost after 298679 iterations : Training Loss =  0.3129940571813696; Validation Loss = 0.34834092956445817\n",
            "Cost after 298680 iterations : Training Loss =  0.3129938898764079; Validation Loss = 0.34834117741476606\n",
            "Cost after 298681 iterations : Training Loss =  0.3129939943449612; Validation Loss = 0.3483414252650735\n",
            "Cost after 298682 iterations : Training Loss =  0.31299393627670435; Validation Loss = 0.348340948676053\n",
            "Cost after 298683 iterations : Training Loss =  0.31299389507602343; Validation Loss = 0.3483411965263602\n",
            "Cost after 298684 iterations : Training Loss =  0.31299398267700107; Validation Loss = 0.34834071993733945\n",
            "Cost after 298685 iterations : Training Loss =  0.3129938153720392; Validation Loss = 0.3483409677876469\n",
            "Cost after 298686 iterations : Training Loss =  0.31299400951234413; Validation Loss = 0.34834121563795384\n",
            "Cost after 298687 iterations : Training Loss =  0.31299386177233607; Validation Loss = 0.3483407390489336\n",
            "Cost after 298688 iterations : Training Loss =  0.31299391024340606; Validation Loss = 0.34834098689924103\n",
            "Cost after 298689 iterations : Training Loss =  0.3129939081726326; Validation Loss = 0.34834051031021984\n",
            "Cost after 298690 iterations : Training Loss =  0.3129938109744682; Validation Loss = 0.3483407581605279\n",
            "Cost after 298691 iterations : Training Loss =  0.31299395457292933; Validation Loss = 0.34834028157150676\n",
            "Cost after 298692 iterations : Training Loss =  0.31299378726796756; Validation Loss = 0.3483405294218147\n",
            "Cost after 298693 iterations : Training Loss =  0.3129939254107885; Validation Loss = 0.3483407772721222\n",
            "Cost after 298694 iterations : Training Loss =  0.31299383366826405; Validation Loss = 0.3483403006831013\n",
            "Cost after 298695 iterations : Training Loss =  0.31299382614185084; Validation Loss = 0.34834054853340873\n",
            "Cost after 298696 iterations : Training Loss =  0.31299388006856077; Validation Loss = 0.348340071944388\n",
            "Cost after 298697 iterations : Training Loss =  0.3129937268729129; Validation Loss = 0.348340319794696\n",
            "Cost after 298698 iterations : Training Loss =  0.31299392646885743; Validation Loss = 0.3483398432056748\n",
            "Cost after 298699 iterations : Training Loss =  0.3129937591638956; Validation Loss = 0.3483400910559823\n",
            "Cost after 298700 iterations : Training Loss =  0.31299384130923336; Validation Loss = 0.3483403389062897\n",
            "Cost after 298701 iterations : Training Loss =  0.3129938055641924; Validation Loss = 0.3483398623172691\n",
            "Cost after 298702 iterations : Training Loss =  0.3129937420402957; Validation Loss = 0.34834011016757627\n",
            "Cost after 298703 iterations : Training Loss =  0.312993851964489; Validation Loss = 0.3483396335785558\n",
            "Cost after 298704 iterations : Training Loss =  0.3129936846595273; Validation Loss = 0.34833988142886296\n",
            "Cost after 298705 iterations : Training Loss =  0.31299385647661593; Validation Loss = 0.3483401292791706\n",
            "Cost after 298706 iterations : Training Loss =  0.3129937310598238; Validation Loss = 0.3483396526901498\n",
            "Cost after 298707 iterations : Training Loss =  0.3129937572076782; Validation Loss = 0.34833990054045744\n",
            "Cost after 298708 iterations : Training Loss =  0.31299377746012064; Validation Loss = 0.3483394239514368\n",
            "Cost after 298709 iterations : Training Loss =  0.31299365793874023; Validation Loss = 0.3483396718017441\n",
            "Cost after 298710 iterations : Training Loss =  0.3129938238604172; Validation Loss = 0.3483391952127235\n",
            "Cost after 298711 iterations : Training Loss =  0.3129936565554554; Validation Loss = 0.34833944306303066\n",
            "Cost after 298712 iterations : Training Loss =  0.3129937723750608; Validation Loss = 0.34833969091333855\n",
            "Cost after 298713 iterations : Training Loss =  0.312993702955752; Validation Loss = 0.34833921432431764\n",
            "Cost after 298714 iterations : Training Loss =  0.3129936731061229; Validation Loss = 0.3483394621746249\n",
            "Cost after 298715 iterations : Training Loss =  0.3129937493560488; Validation Loss = 0.34833898558560433\n",
            "Cost after 298716 iterations : Training Loss =  0.31299358205108696; Validation Loss = 0.34833923343591155\n",
            "Cost after 298717 iterations : Training Loss =  0.3129937875424434; Validation Loss = 0.3483394812862193\n",
            "Cost after 298718 iterations : Training Loss =  0.31299362845138357; Validation Loss = 0.34833900469719875\n",
            "Cost after 298719 iterations : Training Loss =  0.3129936882735055; Validation Loss = 0.34833925254750614\n",
            "Cost after 298720 iterations : Training Loss =  0.3129936748516803; Validation Loss = 0.34833877595848534\n",
            "Cost after 298721 iterations : Training Loss =  0.3129935890045677; Validation Loss = 0.34833902380879295\n",
            "Cost after 298722 iterations : Training Loss =  0.3129937212519768; Validation Loss = 0.34833854721977203\n",
            "Cost after 298723 iterations : Training Loss =  0.3129935539470155; Validation Loss = 0.3483387950700792\n",
            "Cost after 298724 iterations : Training Loss =  0.3129937034408883; Validation Loss = 0.3483390429203867\n",
            "Cost after 298725 iterations : Training Loss =  0.31299360034731183; Validation Loss = 0.34833856633136606\n",
            "Cost after 298726 iterations : Training Loss =  0.3129936041719502; Validation Loss = 0.3483388141816735\n",
            "Cost after 298727 iterations : Training Loss =  0.31299364674760843; Validation Loss = 0.348338337592653\n",
            "Cost after 298728 iterations : Training Loss =  0.31299350490301264; Validation Loss = 0.34833858544296037\n",
            "Cost after 298729 iterations : Training Loss =  0.3129936931479052; Validation Loss = 0.3483381088539396\n",
            "Cost after 298730 iterations : Training Loss =  0.31299352584294327; Validation Loss = 0.34833835670424707\n",
            "Cost after 298731 iterations : Training Loss =  0.31299361933933306; Validation Loss = 0.3483386045545548\n",
            "Cost after 298732 iterations : Training Loss =  0.31299357224323987; Validation Loss = 0.34833812796553376\n",
            "Cost after 298733 iterations : Training Loss =  0.31299352007039505; Validation Loss = 0.3483383758158412\n",
            "Cost after 298734 iterations : Training Loss =  0.3129936186435367; Validation Loss = 0.34833789922682046\n",
            "Cost after 298735 iterations : Training Loss =  0.3129934513385749; Validation Loss = 0.3483381470771279\n",
            "Cost after 298736 iterations : Training Loss =  0.3129936345067157; Validation Loss = 0.34833839492743557\n",
            "Cost after 298737 iterations : Training Loss =  0.3129934977388715; Validation Loss = 0.3483379183384149\n",
            "Cost after 298738 iterations : Training Loss =  0.3129935352377776; Validation Loss = 0.3483381661887222\n",
            "Cost after 298739 iterations : Training Loss =  0.3129935441391682; Validation Loss = 0.3483376895997013\n",
            "Cost after 298740 iterations : Training Loss =  0.3129934359688398; Validation Loss = 0.34833793745000874\n",
            "Cost after 298741 iterations : Training Loss =  0.3129935905394651; Validation Loss = 0.3483374608609879\n",
            "Cost after 298742 iterations : Training Loss =  0.31299342323450313; Validation Loss = 0.3483377087112959\n",
            "Cost after 298743 iterations : Training Loss =  0.3129935504051606; Validation Loss = 0.34833795656160305\n",
            "Cost after 298744 iterations : Training Loss =  0.3129934696347998; Validation Loss = 0.3483374799725826\n",
            "Cost after 298745 iterations : Training Loss =  0.3129934511362224; Validation Loss = 0.34833772782288996\n",
            "Cost after 298746 iterations : Training Loss =  0.3129935160350965; Validation Loss = 0.348337251233869\n",
            "Cost after 298747 iterations : Training Loss =  0.31299335186728444; Validation Loss = 0.3483374990841769\n",
            "Cost after 298748 iterations : Training Loss =  0.3129935624353931; Validation Loss = 0.3483370224951558\n",
            "Cost after 298749 iterations : Training Loss =  0.3129933951304314; Validation Loss = 0.3483372703454632\n",
            "Cost after 298750 iterations : Training Loss =  0.31299346630360503; Validation Loss = 0.3483375181957712\n",
            "Cost after 298751 iterations : Training Loss =  0.3129934415307279; Validation Loss = 0.34833704160674983\n",
            "Cost after 298752 iterations : Training Loss =  0.3129933670346671; Validation Loss = 0.3483372894570577\n",
            "Cost after 298753 iterations : Training Loss =  0.31299348793102444; Validation Loss = 0.3483368128680368\n",
            "Cost after 298754 iterations : Training Loss =  0.31299332062606317; Validation Loss = 0.34833706071834436\n",
            "Cost after 298755 iterations : Training Loss =  0.3129934814709876; Validation Loss = 0.3483373085686516\n",
            "Cost after 298756 iterations : Training Loss =  0.3129933670263596; Validation Loss = 0.3483368319796311\n",
            "Cost after 298757 iterations : Training Loss =  0.3129933822020498; Validation Loss = 0.34833707982993856\n",
            "Cost after 298758 iterations : Training Loss =  0.31299341342665604; Validation Loss = 0.3483366032409178\n",
            "Cost after 298759 iterations : Training Loss =  0.31299328293311174; Validation Loss = 0.34833685109122514\n",
            "Cost after 298760 iterations : Training Loss =  0.3129934598269529; Validation Loss = 0.3483363745022045\n",
            "Cost after 298761 iterations : Training Loss =  0.31299329252199115; Validation Loss = 0.34833662235251167\n",
            "Cost after 298762 iterations : Training Loss =  0.3129933973694323; Validation Loss = 0.34833687020281945\n",
            "Cost after 298763 iterations : Training Loss =  0.31299333892228787; Validation Loss = 0.3483363936137985\n",
            "Cost after 298764 iterations : Training Loss =  0.31299329810049453; Validation Loss = 0.3483366414641062\n",
            "Cost after 298765 iterations : Training Loss =  0.3129933853225845; Validation Loss = 0.34833616487508534\n",
            "Cost after 298766 iterations : Training Loss =  0.31299321801762264; Validation Loss = 0.34833641272539256\n",
            "Cost after 298767 iterations : Training Loss =  0.3129934125368149; Validation Loss = 0.3483366605757001\n",
            "Cost after 298768 iterations : Training Loss =  0.31299326441791914; Validation Loss = 0.3483361839866798\n",
            "Cost after 298769 iterations : Training Loss =  0.31299331326787694; Validation Loss = 0.3483364318369868\n",
            "Cost after 298770 iterations : Training Loss =  0.31299331081821585; Validation Loss = 0.3483359552479665\n",
            "Cost after 298771 iterations : Training Loss =  0.3129932139989395; Validation Loss = 0.3483362030982737\n",
            "Cost after 298772 iterations : Training Loss =  0.3129933572185125; Validation Loss = 0.3483357265092528\n",
            "Cost after 298773 iterations : Training Loss =  0.31299318991355074; Validation Loss = 0.34833597435956043\n",
            "Cost after 298774 iterations : Training Loss =  0.3129933284352599; Validation Loss = 0.348336222209868\n",
            "Cost after 298775 iterations : Training Loss =  0.3129932363138476; Validation Loss = 0.3483357456208475\n",
            "Cost after 298776 iterations : Training Loss =  0.31299322916632194; Validation Loss = 0.3483359934711551\n",
            "Cost after 298777 iterations : Training Loss =  0.3129932827141442; Validation Loss = 0.3483355168821338\n",
            "Cost after 298778 iterations : Training Loss =  0.3129931298973838; Validation Loss = 0.3483357647324415\n",
            "Cost after 298779 iterations : Training Loss =  0.3129933291144408; Validation Loss = 0.34833528814342063\n",
            "Cost after 298780 iterations : Training Loss =  0.31299316180947906; Validation Loss = 0.34833553599372835\n",
            "Cost after 298781 iterations : Training Loss =  0.31299324433370435; Validation Loss = 0.3483357838440362\n",
            "Cost after 298782 iterations : Training Loss =  0.3129932082097755; Validation Loss = 0.34833530725501505\n",
            "Cost after 298783 iterations : Training Loss =  0.3129931450647667; Validation Loss = 0.3483355551053224\n",
            "Cost after 298784 iterations : Training Loss =  0.31299325461007227; Validation Loss = 0.3483350785163019\n",
            "Cost after 298785 iterations : Training Loss =  0.3129930873051107; Validation Loss = 0.34833532636660913\n",
            "Cost after 298786 iterations : Training Loss =  0.3129932595010871; Validation Loss = 0.34833557421691697\n",
            "Cost after 298787 iterations : Training Loss =  0.31299313370540716; Validation Loss = 0.3483350976278959\n",
            "Cost after 298788 iterations : Training Loss =  0.312993160232149; Validation Loss = 0.3483353454782035\n",
            "Cost after 298789 iterations : Training Loss =  0.312993180105704; Validation Loss = 0.34833486888918247\n",
            "Cost after 298790 iterations : Training Loss =  0.31299306096321117; Validation Loss = 0.34833511673949036\n",
            "Cost after 298791 iterations : Training Loss =  0.31299322650600075; Validation Loss = 0.34833464015046955\n",
            "Cost after 298792 iterations : Training Loss =  0.31299305920103876; Validation Loss = 0.34833488800077705\n",
            "Cost after 298793 iterations : Training Loss =  0.3129931753995319; Validation Loss = 0.3483351358510848\n",
            "Cost after 298794 iterations : Training Loss =  0.3129931056013356; Validation Loss = 0.34833465926206375\n",
            "Cost after 298795 iterations : Training Loss =  0.31299307613059396; Validation Loss = 0.3483349071123712\n",
            "Cost after 298796 iterations : Training Loss =  0.31299315200163214; Validation Loss = 0.34833443052335017\n",
            "Cost after 298797 iterations : Training Loss =  0.3129929846966703; Validation Loss = 0.3483346783736576\n",
            "Cost after 298798 iterations : Training Loss =  0.31299319056691455; Validation Loss = 0.3483349262239652\n",
            "Cost after 298799 iterations : Training Loss =  0.31299303109696675; Validation Loss = 0.34833444963494453\n",
            "Cost after 298800 iterations : Training Loss =  0.3129930912979767; Validation Loss = 0.3483346974852521\n",
            "Cost after 298801 iterations : Training Loss =  0.3129930774972637; Validation Loss = 0.3483342208962313\n",
            "Cost after 298802 iterations : Training Loss =  0.3129929920290388; Validation Loss = 0.3483344687465389\n",
            "Cost after 298803 iterations : Training Loss =  0.3129931238975602; Validation Loss = 0.34833399215751826\n",
            "Cost after 298804 iterations : Training Loss =  0.31299295659259857; Validation Loss = 0.348334240007826\n",
            "Cost after 298805 iterations : Training Loss =  0.3129931064653592; Validation Loss = 0.3483344878581332\n",
            "Cost after 298806 iterations : Training Loss =  0.3129930029928953; Validation Loss = 0.3483340112691121\n",
            "Cost after 298807 iterations : Training Loss =  0.3129930071964215; Validation Loss = 0.34833425911941973\n",
            "Cost after 298808 iterations : Training Loss =  0.3129930493931916; Validation Loss = 0.34833378253039876\n",
            "Cost after 298809 iterations : Training Loss =  0.31299290792748335; Validation Loss = 0.34833403038070665\n",
            "Cost after 298810 iterations : Training Loss =  0.3129930957934884; Validation Loss = 0.3483335537916858\n",
            "Cost after 298811 iterations : Training Loss =  0.31299292848852683; Validation Loss = 0.3483338016419931\n",
            "Cost after 298812 iterations : Training Loss =  0.31299302236380383; Validation Loss = 0.34833404949230073\n",
            "Cost after 298813 iterations : Training Loss =  0.3129929748888233; Validation Loss = 0.3483335729032799\n",
            "Cost after 298814 iterations : Training Loss =  0.3129929230948662; Validation Loss = 0.34833382075358743\n",
            "Cost after 298815 iterations : Training Loss =  0.3129930212891201; Validation Loss = 0.3483333441645667\n",
            "Cost after 298816 iterations : Training Loss =  0.3129928539841583; Validation Loss = 0.3483335920148741\n",
            "Cost after 298817 iterations : Training Loss =  0.3129930375311866; Validation Loss = 0.3483338398651818\n",
            "Cost after 298818 iterations : Training Loss =  0.3129929003844551; Validation Loss = 0.34833336327616077\n",
            "Cost after 298819 iterations : Training Loss =  0.31299293826224883; Validation Loss = 0.34833361112646843\n",
            "Cost after 298820 iterations : Training Loss =  0.3129929467847517; Validation Loss = 0.3483331345374478\n",
            "Cost after 298821 iterations : Training Loss =  0.3129928389933109; Validation Loss = 0.34833338238775496\n",
            "Cost after 298822 iterations : Training Loss =  0.3129929931850482; Validation Loss = 0.3483329057987345\n",
            "Cost after 298823 iterations : Training Loss =  0.31299282588008653; Validation Loss = 0.34833315364904194\n",
            "Cost after 298824 iterations : Training Loss =  0.3129929534296312; Validation Loss = 0.348333401499349\n",
            "Cost after 298825 iterations : Training Loss =  0.3129928722803831; Validation Loss = 0.34833292491032863\n",
            "Cost after 298826 iterations : Training Loss =  0.3129928541606936; Validation Loss = 0.34833317276063613\n",
            "Cost after 298827 iterations : Training Loss =  0.31299291868067974; Validation Loss = 0.3483326961716153\n",
            "Cost after 298828 iterations : Training Loss =  0.3129927548917555; Validation Loss = 0.34833294402192283\n",
            "Cost after 298829 iterations : Training Loss =  0.3129929650809764; Validation Loss = 0.3483324674329019\n",
            "Cost after 298830 iterations : Training Loss =  0.3129927977760146; Validation Loss = 0.3483327152832097\n",
            "Cost after 298831 iterations : Training Loss =  0.31299286932807613; Validation Loss = 0.34833296313351697\n",
            "Cost after 298832 iterations : Training Loss =  0.31299284417631146; Validation Loss = 0.34833248654449606\n",
            "Cost after 298833 iterations : Training Loss =  0.3129927700591379; Validation Loss = 0.34833273439480367\n",
            "Cost after 298834 iterations : Training Loss =  0.3129928905766079; Validation Loss = 0.34833225780578303\n",
            "Cost after 298835 iterations : Training Loss =  0.3129927232716463; Validation Loss = 0.34833250565609014\n",
            "Cost after 298836 iterations : Training Loss =  0.31299288449545887; Validation Loss = 0.34833275350639786\n",
            "Cost after 298837 iterations : Training Loss =  0.312992769671943; Validation Loss = 0.34833227691737706\n",
            "Cost after 298838 iterations : Training Loss =  0.312992785226521; Validation Loss = 0.34833252476768467\n",
            "Cost after 298839 iterations : Training Loss =  0.31299281607223955; Validation Loss = 0.34833204817866387\n",
            "Cost after 298840 iterations : Training Loss =  0.31299268595758295; Validation Loss = 0.3483322960289712\n",
            "Cost after 298841 iterations : Training Loss =  0.3129928624725361; Validation Loss = 0.34833181943995073\n",
            "Cost after 298842 iterations : Training Loss =  0.3129926951675744; Validation Loss = 0.3483320672902576\n",
            "Cost after 298843 iterations : Training Loss =  0.31299280039390337; Validation Loss = 0.34833231514056534\n",
            "Cost after 298844 iterations : Training Loss =  0.31299274156787105; Validation Loss = 0.3483318385515448\n",
            "Cost after 298845 iterations : Training Loss =  0.31299270112496563; Validation Loss = 0.34833208640185254\n",
            "Cost after 298846 iterations : Training Loss =  0.3129927879681676; Validation Loss = 0.34833160981283173\n",
            "Cost after 298847 iterations : Training Loss =  0.31299262066320593; Validation Loss = 0.3483318576631392\n",
            "Cost after 298848 iterations : Training Loss =  0.3129928155612861; Validation Loss = 0.3483321055134466\n",
            "Cost after 298849 iterations : Training Loss =  0.31299266706350265; Validation Loss = 0.3483316289244258\n",
            "Cost after 298850 iterations : Training Loss =  0.312992716292348; Validation Loss = 0.3483318767747333\n",
            "Cost after 298851 iterations : Training Loss =  0.3129927134637993; Validation Loss = 0.34833140018571274\n",
            "Cost after 298852 iterations : Training Loss =  0.3129926170234104; Validation Loss = 0.34833164803602\n",
            "Cost after 298853 iterations : Training Loss =  0.31299275986409575; Validation Loss = 0.34833117144699927\n",
            "Cost after 298854 iterations : Training Loss =  0.3129925925591341; Validation Loss = 0.3483314192973069\n",
            "Cost after 298855 iterations : Training Loss =  0.3129927314597309; Validation Loss = 0.3483316671476145\n",
            "Cost after 298856 iterations : Training Loss =  0.31299263895943097; Validation Loss = 0.3483311905585936\n",
            "Cost after 298857 iterations : Training Loss =  0.31299263219079293; Validation Loss = 0.348331438408901\n",
            "Cost after 298858 iterations : Training Loss =  0.31299268535972746; Validation Loss = 0.34833096181987994\n",
            "Cost after 298859 iterations : Training Loss =  0.312992532921855; Validation Loss = 0.3483312096701879\n",
            "Cost after 298860 iterations : Training Loss =  0.31299273176002407; Validation Loss = 0.3483307330811667\n",
            "Cost after 298861 iterations : Training Loss =  0.3129925644550624; Validation Loss = 0.3483309809314746\n",
            "Cost after 298862 iterations : Training Loss =  0.31299264735817545; Validation Loss = 0.3483312287817818\n",
            "Cost after 298863 iterations : Training Loss =  0.31299261085535923; Validation Loss = 0.34833075219276116\n",
            "Cost after 298864 iterations : Training Loss =  0.3129925480892376; Validation Loss = 0.3483310000430687\n",
            "Cost after 298865 iterations : Training Loss =  0.3129926572556556; Validation Loss = 0.34833052345404797\n",
            "Cost after 298866 iterations : Training Loss =  0.3129924899506939; Validation Loss = 0.34833077130435536\n",
            "Cost after 298867 iterations : Training Loss =  0.31299266252555835; Validation Loss = 0.34833101915466297\n",
            "Cost after 298868 iterations : Training Loss =  0.3129925363509907; Validation Loss = 0.34833054256564194\n",
            "Cost after 298869 iterations : Training Loss =  0.3129925632566203; Validation Loss = 0.34833079041594955\n",
            "Cost after 298870 iterations : Training Loss =  0.3129925827512872; Validation Loss = 0.34833031382692853\n",
            "Cost after 298871 iterations : Training Loss =  0.3129924639876824; Validation Loss = 0.3483305616772362\n",
            "Cost after 298872 iterations : Training Loss =  0.31299262915158393; Validation Loss = 0.34833008508821545\n",
            "Cost after 298873 iterations : Training Loss =  0.31299246184662194; Validation Loss = 0.3483303329385231\n",
            "Cost after 298874 iterations : Training Loss =  0.31299257842400297; Validation Loss = 0.3483305807888307\n",
            "Cost after 298875 iterations : Training Loss =  0.312992508246919; Validation Loss = 0.3483301041998101\n",
            "Cost after 298876 iterations : Training Loss =  0.3129924791550653; Validation Loss = 0.34833035205011714\n",
            "Cost after 298877 iterations : Training Loss =  0.3129925546472153; Validation Loss = 0.34832987546109634\n",
            "Cost after 298878 iterations : Training Loss =  0.3129923873422537; Validation Loss = 0.34833012331140395\n",
            "Cost after 298879 iterations : Training Loss =  0.31299259359138565; Validation Loss = 0.3483303711617114\n",
            "Cost after 298880 iterations : Training Loss =  0.3129924337425501; Validation Loss = 0.3483298945726904\n",
            "Cost after 298881 iterations : Training Loss =  0.3129924943224478; Validation Loss = 0.3483301424229982\n",
            "Cost after 298882 iterations : Training Loss =  0.3129924801428471; Validation Loss = 0.3483296658339775\n",
            "Cost after 298883 iterations : Training Loss =  0.31299239505350984; Validation Loss = 0.34832991368428495\n",
            "Cost after 298884 iterations : Training Loss =  0.3129925265431438; Validation Loss = 0.34832943709526404\n",
            "Cost after 298885 iterations : Training Loss =  0.312992359238182; Validation Loss = 0.34832968494557165\n",
            "Cost after 298886 iterations : Training Loss =  0.3129925094898302; Validation Loss = 0.34832993279587926\n",
            "Cost after 298887 iterations : Training Loss =  0.3129924056384785; Validation Loss = 0.34832945620685835\n",
            "Cost after 298888 iterations : Training Loss =  0.3129924102208926; Validation Loss = 0.3483297040571658\n",
            "Cost after 298889 iterations : Training Loss =  0.3129924520387751; Validation Loss = 0.3483292274681452\n",
            "Cost after 298890 iterations : Training Loss =  0.31299231095195446; Validation Loss = 0.3483294753184527\n",
            "Cost after 298891 iterations : Training Loss =  0.31299249843907173; Validation Loss = 0.34832899872943174\n",
            "Cost after 298892 iterations : Training Loss =  0.31299233113411; Validation Loss = 0.34832924657973935\n",
            "Cost after 298893 iterations : Training Loss =  0.312992425388275; Validation Loss = 0.3483294944300468\n",
            "Cost after 298894 iterations : Training Loss =  0.31299237753440684; Validation Loss = 0.3483290178410259\n",
            "Cost after 298895 iterations : Training Loss =  0.31299232611933725; Validation Loss = 0.34832926569133366\n",
            "Cost after 298896 iterations : Training Loss =  0.3129924239347031; Validation Loss = 0.34832878910231246\n",
            "Cost after 298897 iterations : Training Loss =  0.3129922566297417; Validation Loss = 0.3483290369526202\n",
            "Cost after 298898 iterations : Training Loss =  0.31299244055565767; Validation Loss = 0.34832928480292813\n",
            "Cost after 298899 iterations : Training Loss =  0.31299230303003833; Validation Loss = 0.34832880821390655\n",
            "Cost after 298900 iterations : Training Loss =  0.31299234128672; Validation Loss = 0.3483290560642145\n",
            "Cost after 298901 iterations : Training Loss =  0.312992349430335; Validation Loss = 0.3483285794751936\n",
            "Cost after 298902 iterations : Training Loss =  0.3129922420177821; Validation Loss = 0.3483288273255012\n",
            "Cost after 298903 iterations : Training Loss =  0.3129923958306313; Validation Loss = 0.34832835073648055\n",
            "Cost after 298904 iterations : Training Loss =  0.31299222852566977; Validation Loss = 0.34832859858678805\n",
            "Cost after 298905 iterations : Training Loss =  0.3129923564541025; Validation Loss = 0.3483288464370955\n",
            "Cost after 298906 iterations : Training Loss =  0.31299227492596626; Validation Loss = 0.34832836984807486\n",
            "Cost after 298907 iterations : Training Loss =  0.3129922571851646; Validation Loss = 0.34832861769838197\n",
            "Cost after 298908 iterations : Training Loss =  0.31299232132626303; Validation Loss = 0.34832814110936133\n",
            "Cost after 298909 iterations : Training Loss =  0.31299215791622664; Validation Loss = 0.34832838895966917\n",
            "Cost after 298910 iterations : Training Loss =  0.31299236772655964; Validation Loss = 0.34832791237064814\n",
            "Cost after 298911 iterations : Training Loss =  0.31299220042159787; Validation Loss = 0.3483281602209557\n",
            "Cost after 298912 iterations : Training Loss =  0.31299227235254706; Validation Loss = 0.34832840807126303\n",
            "Cost after 298913 iterations : Training Loss =  0.3129922468218945; Validation Loss = 0.3483279314822423\n",
            "Cost after 298914 iterations : Training Loss =  0.31299217308360916; Validation Loss = 0.3483281793325497\n",
            "Cost after 298915 iterations : Training Loss =  0.3129922932221913; Validation Loss = 0.3483277027435288\n",
            "Cost after 298916 iterations : Training Loss =  0.3129921259172295; Validation Loss = 0.3483279505938365\n",
            "Cost after 298917 iterations : Training Loss =  0.3129922875199298; Validation Loss = 0.3483281984441441\n",
            "Cost after 298918 iterations : Training Loss =  0.3129921723175263; Validation Loss = 0.3483277218551231\n",
            "Cost after 298919 iterations : Training Loss =  0.31299218825099173; Validation Loss = 0.348327969705431\n",
            "Cost after 298920 iterations : Training Loss =  0.31299221871782285; Validation Loss = 0.34832749311641\n",
            "Cost after 298921 iterations : Training Loss =  0.31299208898205416; Validation Loss = 0.3483277409667174\n",
            "Cost after 298922 iterations : Training Loss =  0.31299226511811934; Validation Loss = 0.3483272643776967\n",
            "Cost after 298923 iterations : Training Loss =  0.3129920978131577; Validation Loss = 0.34832751222800395\n",
            "Cost after 298924 iterations : Training Loss =  0.31299220341837436; Validation Loss = 0.34832776007831184\n",
            "Cost after 298925 iterations : Training Loss =  0.3129921442134544; Validation Loss = 0.34832728348929076\n",
            "Cost after 298926 iterations : Training Loss =  0.31299210414943673; Validation Loss = 0.34832753133959815\n",
            "Cost after 298927 iterations : Training Loss =  0.3129921906137507; Validation Loss = 0.3483270547505778\n",
            "Cost after 298928 iterations : Training Loss =  0.31299202330878906; Validation Loss = 0.34832730260088524\n",
            "Cost after 298929 iterations : Training Loss =  0.3129922185857569; Validation Loss = 0.34832755045119274\n",
            "Cost after 298930 iterations : Training Loss =  0.3129920697090859; Validation Loss = 0.3483270738621716\n",
            "Cost after 298931 iterations : Training Loss =  0.3129921193168193; Validation Loss = 0.3483273217124791\n",
            "Cost after 298932 iterations : Training Loss =  0.3129921161093825; Validation Loss = 0.3483268451234588\n",
            "Cost after 298933 iterations : Training Loss =  0.3129920200478815; Validation Loss = 0.34832709297376624\n",
            "Cost after 298934 iterations : Training Loss =  0.31299216250967915; Validation Loss = 0.3483266163847453\n",
            "Cost after 298935 iterations : Training Loss =  0.31299199520471754; Validation Loss = 0.348326864235053\n",
            "Cost after 298936 iterations : Training Loss =  0.312992134484202; Validation Loss = 0.3483271120853604\n",
            "Cost after 298937 iterations : Training Loss =  0.3129920416050143; Validation Loss = 0.3483266354963395\n",
            "Cost after 298938 iterations : Training Loss =  0.3129920352152641; Validation Loss = 0.3483268833466468\n",
            "Cost after 298939 iterations : Training Loss =  0.3129920880053108; Validation Loss = 0.34832640675762633\n",
            "Cost after 298940 iterations : Training Loss =  0.3129919359463262; Validation Loss = 0.3483266546079335\n",
            "Cost after 298941 iterations : Training Loss =  0.3129921344056074; Validation Loss = 0.3483261780189123\n",
            "Cost after 298942 iterations : Training Loss =  0.31299196710064575; Validation Loss = 0.34832642586922063\n",
            "Cost after 298943 iterations : Training Loss =  0.31299205038264655; Validation Loss = 0.3483266737195283\n",
            "Cost after 298944 iterations : Training Loss =  0.3129920135009424; Validation Loss = 0.3483261971305071\n",
            "Cost after 298945 iterations : Training Loss =  0.3129919511137087; Validation Loss = 0.3483264449808148\n",
            "Cost after 298946 iterations : Training Loss =  0.3129920599012391; Validation Loss = 0.34832596839179375\n",
            "Cost after 298947 iterations : Training Loss =  0.312991892596277; Validation Loss = 0.34832621624210136\n",
            "Cost after 298948 iterations : Training Loss =  0.31299206555002945; Validation Loss = 0.34832646409240897\n",
            "Cost after 298949 iterations : Training Loss =  0.31299193899657396; Validation Loss = 0.34832598750338806\n",
            "Cost after 298950 iterations : Training Loss =  0.3129919662810914; Validation Loss = 0.3483262353536955\n",
            "Cost after 298951 iterations : Training Loss =  0.3129919853968706; Validation Loss = 0.34832575876467475\n",
            "Cost after 298952 iterations : Training Loss =  0.31299186701215365; Validation Loss = 0.3483260066149827\n",
            "Cost after 298953 iterations : Training Loss =  0.31299203179716706; Validation Loss = 0.34832553002596145\n",
            "Cost after 298954 iterations : Training Loss =  0.31299186449220545; Validation Loss = 0.34832577787626917\n",
            "Cost after 298955 iterations : Training Loss =  0.312991981448474; Validation Loss = 0.3483260257265766\n",
            "Cost after 298956 iterations : Training Loss =  0.312991910892502; Validation Loss = 0.34832554913755576\n",
            "Cost after 298957 iterations : Training Loss =  0.31299188217953616; Validation Loss = 0.3483257969878633\n",
            "Cost after 298958 iterations : Training Loss =  0.3129919572927989; Validation Loss = 0.34832532039884284\n",
            "Cost after 298959 iterations : Training Loss =  0.3129917899878369; Validation Loss = 0.3483255682491499\n",
            "Cost after 298960 iterations : Training Loss =  0.31299199661585664; Validation Loss = 0.34832581609945734\n",
            "Cost after 298961 iterations : Training Loss =  0.31299183638813355; Validation Loss = 0.3483253395104368\n",
            "Cost after 298962 iterations : Training Loss =  0.31299189734691873; Validation Loss = 0.3483255873607444\n",
            "Cost after 298963 iterations : Training Loss =  0.3129918827884303; Validation Loss = 0.34832511077172373\n",
            "Cost after 298964 iterations : Training Loss =  0.312991798077981; Validation Loss = 0.34832535862203073\n",
            "Cost after 298965 iterations : Training Loss =  0.31299192918872687; Validation Loss = 0.34832488203301043\n",
            "Cost after 298966 iterations : Training Loss =  0.3129917618837651; Validation Loss = 0.34832512988331793\n",
            "Cost after 298967 iterations : Training Loss =  0.31299191251430153; Validation Loss = 0.3483253777336252\n",
            "Cost after 298968 iterations : Training Loss =  0.31299180828406187; Validation Loss = 0.3483249011446043\n",
            "Cost after 298969 iterations : Training Loss =  0.3129918132453634; Validation Loss = 0.34832514899491174\n",
            "Cost after 298970 iterations : Training Loss =  0.3129918546843586; Validation Loss = 0.3483246724058913\n",
            "Cost after 298971 iterations : Training Loss =  0.31299171397642583; Validation Loss = 0.34832492025619877\n",
            "Cost after 298972 iterations : Training Loss =  0.3129919010846551; Validation Loss = 0.3483244436671781\n",
            "Cost after 298973 iterations : Training Loss =  0.3129917337796934; Validation Loss = 0.3483246915174853\n",
            "Cost after 298974 iterations : Training Loss =  0.3129918284127462; Validation Loss = 0.34832493936779285\n",
            "Cost after 298975 iterations : Training Loss =  0.31299178017999013; Validation Loss = 0.3483244627787718\n",
            "Cost after 298976 iterations : Training Loss =  0.31299172914380824; Validation Loss = 0.34832471062907944\n",
            "Cost after 298977 iterations : Training Loss =  0.31299182658028657; Validation Loss = 0.34832423404005897\n",
            "Cost after 298978 iterations : Training Loss =  0.3129916592753249; Validation Loss = 0.3483244818903664\n",
            "Cost after 298979 iterations : Training Loss =  0.3129918435801288; Validation Loss = 0.3483247297406738\n",
            "Cost after 298980 iterations : Training Loss =  0.31299170567562157; Validation Loss = 0.34832425315165316\n",
            "Cost after 298981 iterations : Training Loss =  0.31299174431119087; Validation Loss = 0.34832450100196083\n",
            "Cost after 298982 iterations : Training Loss =  0.3129917520759185; Validation Loss = 0.3483240244129398\n",
            "Cost after 298983 iterations : Training Loss =  0.31299164504225313; Validation Loss = 0.3483242722632474\n",
            "Cost after 298984 iterations : Training Loss =  0.3129917984762148; Validation Loss = 0.3483237956742262\n",
            "Cost after 298985 iterations : Training Loss =  0.3129916311712531; Validation Loss = 0.34832404352453367\n",
            "Cost after 298986 iterations : Training Loss =  0.3129917594785735; Validation Loss = 0.34832429137484183\n",
            "Cost after 298987 iterations : Training Loss =  0.3129916775715496; Validation Loss = 0.34832381478582064\n",
            "Cost after 298988 iterations : Training Loss =  0.31299166020963576; Validation Loss = 0.3483240626361282\n",
            "Cost after 298989 iterations : Training Loss =  0.31299172397184655; Validation Loss = 0.3483235860471078\n",
            "Cost after 298990 iterations : Training Loss =  0.31299156094069785; Validation Loss = 0.3483238338974148\n",
            "Cost after 298991 iterations : Training Loss =  0.31299177037214315; Validation Loss = 0.34832335730839403\n",
            "Cost after 298992 iterations : Training Loss =  0.31299160306718127; Validation Loss = 0.3483236051587018\n",
            "Cost after 298993 iterations : Training Loss =  0.31299167537701833; Validation Loss = 0.34832385300900925\n",
            "Cost after 298994 iterations : Training Loss =  0.31299164946747776; Validation Loss = 0.3483233764199885\n",
            "Cost after 298995 iterations : Training Loss =  0.31299157610808037; Validation Loss = 0.3483236242702958\n",
            "Cost after 298996 iterations : Training Loss =  0.3129916958677746; Validation Loss = 0.34832314768127504\n",
            "Cost after 298997 iterations : Training Loss =  0.312991528562813; Validation Loss = 0.3483233955315827\n",
            "Cost after 298998 iterations : Training Loss =  0.31299169054440096; Validation Loss = 0.34832364338189004\n",
            "Cost after 298999 iterations : Training Loss =  0.31299157496310964; Validation Loss = 0.3483231667928696\n",
            "Cost after 299000 iterations : Training Loss =  0.31299159127546305; Validation Loss = 0.3483234146431768\n",
            "Cost after 299001 iterations : Training Loss =  0.3129916213634061; Validation Loss = 0.34832293805415604\n",
            "Cost after 299002 iterations : Training Loss =  0.31299149200652504; Validation Loss = 0.3483231859044635\n",
            "Cost after 299003 iterations : Training Loss =  0.3129916677637028; Validation Loss = 0.34832270931544274\n",
            "Cost after 299004 iterations : Training Loss =  0.31299150045874125; Validation Loss = 0.34832295716575035\n",
            "Cost after 299005 iterations : Training Loss =  0.3129916064428452; Validation Loss = 0.3483232050160578\n",
            "Cost after 299006 iterations : Training Loss =  0.31299154685903774; Validation Loss = 0.34832272842703704\n",
            "Cost after 299007 iterations : Training Loss =  0.312991507173908; Validation Loss = 0.34832297627734443\n",
            "Cost after 299008 iterations : Training Loss =  0.3129915932593344; Validation Loss = 0.3483224996883236\n",
            "Cost after 299009 iterations : Training Loss =  0.3129914259543727; Validation Loss = 0.34832274753863046\n",
            "Cost after 299010 iterations : Training Loss =  0.31299162161022837; Validation Loss = 0.34832299538893835\n",
            "Cost after 299011 iterations : Training Loss =  0.3129914723546692; Validation Loss = 0.3483225187999177\n",
            "Cost after 299012 iterations : Training Loss =  0.31299152234129013; Validation Loss = 0.3483227666502255\n",
            "Cost after 299013 iterations : Training Loss =  0.31299151875496595; Validation Loss = 0.3483222900612046\n",
            "Cost after 299014 iterations : Training Loss =  0.31299142307235256; Validation Loss = 0.3483225379115122\n",
            "Cost after 299015 iterations : Training Loss =  0.3129915651552626; Validation Loss = 0.3483220613224916\n",
            "Cost after 299016 iterations : Training Loss =  0.3129913978503007; Validation Loss = 0.34832230917279866\n",
            "Cost after 299017 iterations : Training Loss =  0.3129915375086731; Validation Loss = 0.3483225570231065\n",
            "Cost after 299018 iterations : Training Loss =  0.31299144425059733; Validation Loss = 0.3483220804340857\n",
            "Cost after 299019 iterations : Training Loss =  0.3129914382397352; Validation Loss = 0.348322328284393\n",
            "Cost after 299020 iterations : Training Loss =  0.31299149065089416; Validation Loss = 0.3483218516953721\n",
            "Cost after 299021 iterations : Training Loss =  0.3129913389707974; Validation Loss = 0.34832209954567944\n",
            "Cost after 299022 iterations : Training Loss =  0.312991537051191; Validation Loss = 0.3483216229566588\n",
            "Cost after 299023 iterations : Training Loss =  0.31299136974622876; Validation Loss = 0.3483218708069666\n",
            "Cost after 299024 iterations : Training Loss =  0.3129914534071178; Validation Loss = 0.34832211865727386\n",
            "Cost after 299025 iterations : Training Loss =  0.3129914161465257; Validation Loss = 0.34832164206825333\n",
            "Cost after 299026 iterations : Training Loss =  0.31299135413817974; Validation Loss = 0.348321889918561\n",
            "Cost after 299027 iterations : Training Loss =  0.31299146254682214; Validation Loss = 0.34832141332954\n",
            "Cost after 299028 iterations : Training Loss =  0.3129912952418605; Validation Loss = 0.3483216611798477\n",
            "Cost after 299029 iterations : Training Loss =  0.31299146857450066; Validation Loss = 0.34832190903015475\n",
            "Cost after 299030 iterations : Training Loss =  0.3129913416421574; Validation Loss = 0.3483214324411343\n",
            "Cost after 299031 iterations : Training Loss =  0.31299136930556254; Validation Loss = 0.3483216802914417\n",
            "Cost after 299032 iterations : Training Loss =  0.312991388042454; Validation Loss = 0.34832120370242053\n",
            "Cost after 299033 iterations : Training Loss =  0.3129912700366247; Validation Loss = 0.3483214515527282\n",
            "Cost after 299034 iterations : Training Loss =  0.3129914344427505; Validation Loss = 0.34832097496370745\n",
            "Cost after 299035 iterations : Training Loss =  0.31299126713778874; Validation Loss = 0.34832122281401495\n",
            "Cost after 299036 iterations : Training Loss =  0.31299138447294517; Validation Loss = 0.348321470664323\n",
            "Cost after 299037 iterations : Training Loss =  0.31299131353808557; Validation Loss = 0.3483209940753018\n",
            "Cost after 299038 iterations : Training Loss =  0.3129912852040071; Validation Loss = 0.3483212419256094\n",
            "Cost after 299039 iterations : Training Loss =  0.31299135993838184; Validation Loss = 0.3483207653365885\n",
            "Cost after 299040 iterations : Training Loss =  0.3129911926334201; Validation Loss = 0.34832101318689623\n",
            "Cost after 299041 iterations : Training Loss =  0.31299139964032785; Validation Loss = 0.3483212610372037\n",
            "Cost after 299042 iterations : Training Loss =  0.3129912390337169; Validation Loss = 0.348320784448183\n",
            "Cost after 299043 iterations : Training Loss =  0.31299130037139006; Validation Loss = 0.3483210322984907\n",
            "Cost after 299044 iterations : Training Loss =  0.31299128543401344; Validation Loss = 0.3483205557094698\n",
            "Cost after 299045 iterations : Training Loss =  0.31299120110245193; Validation Loss = 0.34832080355977724\n",
            "Cost after 299046 iterations : Training Loss =  0.3129913318343098; Validation Loss = 0.3483203269707566\n",
            "Cost after 299047 iterations : Training Loss =  0.3129911645293485; Validation Loss = 0.3483205748210635\n",
            "Cost after 299048 iterations : Training Loss =  0.3129913155387727; Validation Loss = 0.34832082267137127\n",
            "Cost after 299049 iterations : Training Loss =  0.31299121092964505; Validation Loss = 0.34832034608235063\n",
            "Cost after 299050 iterations : Training Loss =  0.3129912162698347; Validation Loss = 0.3483205939326581\n",
            "Cost after 299051 iterations : Training Loss =  0.31299125732994165; Validation Loss = 0.34832011734363755\n",
            "Cost after 299052 iterations : Training Loss =  0.3129911170008968; Validation Loss = 0.348320365193945\n",
            "Cost after 299053 iterations : Training Loss =  0.3129913037302384; Validation Loss = 0.348319888604924\n",
            "Cost after 299054 iterations : Training Loss =  0.31299113642527665; Validation Loss = 0.34832013645523147\n",
            "Cost after 299055 iterations : Training Loss =  0.31299123143721724; Validation Loss = 0.3483203843055388\n",
            "Cost after 299056 iterations : Training Loss =  0.31299118282557337; Validation Loss = 0.34831990771651816\n",
            "Cost after 299057 iterations : Training Loss =  0.31299113216827906; Validation Loss = 0.34832015556682605\n",
            "Cost after 299058 iterations : Training Loss =  0.31299122922586997; Validation Loss = 0.34831967897780486\n",
            "Cost after 299059 iterations : Training Loss =  0.31299106192090814; Validation Loss = 0.34831992682811236\n",
            "Cost after 299060 iterations : Training Loss =  0.31299124660459965; Validation Loss = 0.34832017467841986\n",
            "Cost after 299061 iterations : Training Loss =  0.3129911083212049; Validation Loss = 0.34831969808939894\n",
            "Cost after 299062 iterations : Training Loss =  0.3129911473356621; Validation Loss = 0.3483199459397068\n",
            "Cost after 299063 iterations : Training Loss =  0.3129911547215016; Validation Loss = 0.34831946935068575\n",
            "Cost after 299064 iterations : Training Loss =  0.31299104806672406; Validation Loss = 0.3483197172009933\n",
            "Cost after 299065 iterations : Training Loss =  0.31299120112179846; Validation Loss = 0.3483192406119727\n",
            "Cost after 299066 iterations : Training Loss =  0.3129910338168365; Validation Loss = 0.34831948846228017\n",
            "Cost after 299067 iterations : Training Loss =  0.3129911625030444; Validation Loss = 0.3483197363125875\n",
            "Cost after 299068 iterations : Training Loss =  0.31299108021713323; Validation Loss = 0.3483192597235667\n",
            "Cost after 299069 iterations : Training Loss =  0.31299106323410675; Validation Loss = 0.3483195075738744\n",
            "Cost after 299070 iterations : Training Loss =  0.31299112661742967; Validation Loss = 0.34831903098485356\n",
            "Cost after 299071 iterations : Training Loss =  0.3129909639651689; Validation Loss = 0.3483192788351612\n",
            "Cost after 299072 iterations : Training Loss =  0.3129911730177264; Validation Loss = 0.34831880224614026\n",
            "Cost after 299073 iterations : Training Loss =  0.3129910057127646; Validation Loss = 0.3483190500964477\n",
            "Cost after 299074 iterations : Training Loss =  0.31299107840148926; Validation Loss = 0.3483192979467553\n",
            "Cost after 299075 iterations : Training Loss =  0.31299105211306144; Validation Loss = 0.34831882135773456\n",
            "Cost after 299076 iterations : Training Loss =  0.3129909791325513; Validation Loss = 0.34831906920804173\n",
            "Cost after 299077 iterations : Training Loss =  0.3129910985133579; Validation Loss = 0.3483185926190211\n",
            "Cost after 299078 iterations : Training Loss =  0.3129909312083963; Validation Loss = 0.3483188404693284\n",
            "Cost after 299079 iterations : Training Loss =  0.3129910935688719; Validation Loss = 0.34831908831963626\n",
            "Cost after 299080 iterations : Training Loss =  0.3129909776086929; Validation Loss = 0.3483186117306154\n",
            "Cost after 299081 iterations : Training Loss =  0.3129909942999342; Validation Loss = 0.348318859580923\n",
            "Cost after 299082 iterations : Training Loss =  0.31299102400898965; Validation Loss = 0.3483183829919021\n",
            "Cost after 299083 iterations : Training Loss =  0.3129908950309963; Validation Loss = 0.34831863084220976\n",
            "Cost after 299084 iterations : Training Loss =  0.3129910704092862; Validation Loss = 0.3483181542531887\n",
            "Cost after 299085 iterations : Training Loss =  0.3129909031043245; Validation Loss = 0.34831840210349657\n",
            "Cost after 299086 iterations : Training Loss =  0.31299100946731684; Validation Loss = 0.348318649953804\n",
            "Cost after 299087 iterations : Training Loss =  0.312990949504621; Validation Loss = 0.34831817336478316\n",
            "Cost after 299088 iterations : Training Loss =  0.3129909101983789; Validation Loss = 0.34831842121509066\n",
            "Cost after 299089 iterations : Training Loss =  0.3129909959049177; Validation Loss = 0.34831794462606996\n",
            "Cost after 299090 iterations : Training Loss =  0.3129908285999559; Validation Loss = 0.34831819247637724\n",
            "Cost after 299091 iterations : Training Loss =  0.3129910246346994; Validation Loss = 0.34831844032668485\n",
            "Cost after 299092 iterations : Training Loss =  0.31299087500025263; Validation Loss = 0.34831796373766366\n",
            "Cost after 299093 iterations : Training Loss =  0.3129909253657616; Validation Loss = 0.34831821158797144\n",
            "Cost after 299094 iterations : Training Loss =  0.3129909214005489; Validation Loss = 0.34831773499895025\n",
            "Cost after 299095 iterations : Training Loss =  0.3129908260968235; Validation Loss = 0.3483179828492585\n",
            "Cost after 299096 iterations : Training Loss =  0.3129909678008458; Validation Loss = 0.3483175062602373\n",
            "Cost after 299097 iterations : Training Loss =  0.3129908004958842; Validation Loss = 0.348317754110545\n",
            "Cost after 299098 iterations : Training Loss =  0.31299094053314414; Validation Loss = 0.34831800196085266\n",
            "Cost after 299099 iterations : Training Loss =  0.3129908468961808; Validation Loss = 0.3483175253718318\n",
            "Cost after 299100 iterations : Training Loss =  0.31299084126420623; Validation Loss = 0.3483177732221391\n",
            "Cost after 299101 iterations : Training Loss =  0.3129908932964774; Validation Loss = 0.3483172966331182\n",
            "Cost after 299102 iterations : Training Loss =  0.3129907419952683; Validation Loss = 0.34831754448342594\n",
            "Cost after 299103 iterations : Training Loss =  0.3129909396967741; Validation Loss = 0.3483170678944055\n",
            "Cost after 299104 iterations : Training Loss =  0.31299077239181244; Validation Loss = 0.3483173157447122\n",
            "Cost after 299105 iterations : Training Loss =  0.3129908564315889; Validation Loss = 0.34831756359501975\n",
            "Cost after 299106 iterations : Training Loss =  0.31299081879210894; Validation Loss = 0.3483170870059994\n",
            "Cost after 299107 iterations : Training Loss =  0.3129907571626509; Validation Loss = 0.34831733485630667\n",
            "Cost after 299108 iterations : Training Loss =  0.3129908651924059; Validation Loss = 0.34831685826728614\n",
            "Cost after 299109 iterations : Training Loss =  0.31299069788744377; Validation Loss = 0.34831710611759364\n",
            "Cost after 299110 iterations : Training Loss =  0.31299087159897165; Validation Loss = 0.34831735396790126\n",
            "Cost after 299111 iterations : Training Loss =  0.31299074428774043; Validation Loss = 0.34831687737888006\n",
            "Cost after 299112 iterations : Training Loss =  0.31299077233003364; Validation Loss = 0.3483171252291878\n",
            "Cost after 299113 iterations : Training Loss =  0.3129907906880372; Validation Loss = 0.34831664864016676\n",
            "Cost after 299114 iterations : Training Loss =  0.31299067306109574; Validation Loss = 0.34831689649047454\n",
            "Cost after 299115 iterations : Training Loss =  0.3129908370883338; Validation Loss = 0.3483164199014535\n",
            "Cost after 299116 iterations : Training Loss =  0.31299066978337187; Validation Loss = 0.34831666775176146\n",
            "Cost after 299117 iterations : Training Loss =  0.31299078749741627; Validation Loss = 0.3483169156020686\n",
            "Cost after 299118 iterations : Training Loss =  0.31299071618366897; Validation Loss = 0.3483164390130479\n",
            "Cost after 299119 iterations : Training Loss =  0.31299068822847825; Validation Loss = 0.34831668686335593\n",
            "Cost after 299120 iterations : Training Loss =  0.31299076258396535; Validation Loss = 0.34831621027433457\n",
            "Cost after 299121 iterations : Training Loss =  0.31299059527900364; Validation Loss = 0.3483164581246423\n",
            "Cost after 299122 iterations : Training Loss =  0.3129908026647988; Validation Loss = 0.3483167059749496\n",
            "Cost after 299123 iterations : Training Loss =  0.3129906416793004; Validation Loss = 0.3483162293859289\n",
            "Cost after 299124 iterations : Training Loss =  0.31299070339586116; Validation Loss = 0.34831647723623654\n",
            "Cost after 299125 iterations : Training Loss =  0.3129906880795969; Validation Loss = 0.348316000647216\n",
            "Cost after 299126 iterations : Training Loss =  0.31299060412692303; Validation Loss = 0.3483162484975232\n",
            "Cost after 299127 iterations : Training Loss =  0.31299073447989345; Validation Loss = 0.3483157719085024\n",
            "Cost after 299128 iterations : Training Loss =  0.3129905671749319; Validation Loss = 0.3483160197588099\n",
            "Cost after 299129 iterations : Training Loss =  0.31299071856324356; Validation Loss = 0.3483162676091175\n",
            "Cost after 299130 iterations : Training Loss =  0.31299061357522867; Validation Loss = 0.34831579102009685\n",
            "Cost after 299131 iterations : Training Loss =  0.31299061929430566; Validation Loss = 0.348316038870404\n",
            "Cost after 299132 iterations : Training Loss =  0.3129906599755249; Validation Loss = 0.34831556228138316\n",
            "Cost after 299133 iterations : Training Loss =  0.31299052002536776; Validation Loss = 0.3483158101316908\n",
            "Cost after 299134 iterations : Training Loss =  0.3129907063758219; Validation Loss = 0.3483153335426701\n",
            "Cost after 299135 iterations : Training Loss =  0.3129905390708603; Validation Loss = 0.3483155813929777\n",
            "Cost after 299136 iterations : Training Loss =  0.3129906344616881; Validation Loss = 0.34831582924328486\n",
            "Cost after 299137 iterations : Training Loss =  0.3129905854711567; Validation Loss = 0.3483153526542643\n",
            "Cost after 299138 iterations : Training Loss =  0.31299053519275016; Validation Loss = 0.3483156005045713\n",
            "Cost after 299139 iterations : Training Loss =  0.3129906318714531; Validation Loss = 0.3483151239155511\n",
            "Cost after 299140 iterations : Training Loss =  0.3129904645664916; Validation Loss = 0.3483153717658587\n",
            "Cost after 299141 iterations : Training Loss =  0.3129906496290712; Validation Loss = 0.3483156196161661\n",
            "Cost after 299142 iterations : Training Loss =  0.3129905109667881; Validation Loss = 0.34831514302714534\n",
            "Cost after 299143 iterations : Training Loss =  0.31299055036013324; Validation Loss = 0.34831539087745284\n",
            "Cost after 299144 iterations : Training Loss =  0.3129905573670849; Validation Loss = 0.3483149142884318\n",
            "Cost after 299145 iterations : Training Loss =  0.3129904510911953; Validation Loss = 0.3483151621387398\n",
            "Cost after 299146 iterations : Training Loss =  0.3129906037673815; Validation Loss = 0.3483146855497186\n",
            "Cost after 299147 iterations : Training Loss =  0.3129904364624197; Validation Loss = 0.3483149334000262\n",
            "Cost after 299148 iterations : Training Loss =  0.31299056552751575; Validation Loss = 0.34831518125033356\n",
            "Cost after 299149 iterations : Training Loss =  0.31299048286271636; Validation Loss = 0.34831470466131254\n",
            "Cost after 299150 iterations : Training Loss =  0.3129904662585776; Validation Loss = 0.3483149525116202\n",
            "Cost after 299151 iterations : Training Loss =  0.31299052926301313; Validation Loss = 0.34831447592259984\n",
            "Cost after 299152 iterations : Training Loss =  0.3129903669896401; Validation Loss = 0.34831472377290723\n",
            "Cost after 299153 iterations : Training Loss =  0.3129905756633098; Validation Loss = 0.3483142471838862\n",
            "Cost after 299154 iterations : Training Loss =  0.31299040835834824; Validation Loss = 0.3483144950341938\n",
            "Cost after 299155 iterations : Training Loss =  0.3129904814259604; Validation Loss = 0.3483147428845011\n",
            "Cost after 299156 iterations : Training Loss =  0.3129904547586446; Validation Loss = 0.3483142662954805\n",
            "Cost after 299157 iterations : Training Loss =  0.3129903821570225; Validation Loss = 0.34831451414578823\n",
            "Cost after 299158 iterations : Training Loss =  0.31299050115894117; Validation Loss = 0.3483140375567675\n",
            "Cost after 299159 iterations : Training Loss =  0.3129903338539794; Validation Loss = 0.3483142854070749\n",
            "Cost after 299160 iterations : Training Loss =  0.31299049659334294; Validation Loss = 0.3483145332573824\n",
            "Cost after 299161 iterations : Training Loss =  0.31299038025427633; Validation Loss = 0.34831405666836146\n",
            "Cost after 299162 iterations : Training Loss =  0.31299039732440503; Validation Loss = 0.34831430451866924\n",
            "Cost after 299163 iterations : Training Loss =  0.3129904266545727; Validation Loss = 0.34831382792964816\n",
            "Cost after 299164 iterations : Training Loss =  0.3129902980554674; Validation Loss = 0.34831407577995566\n",
            "Cost after 299165 iterations : Training Loss =  0.31299047305486943; Validation Loss = 0.3483135991909346\n",
            "Cost after 299166 iterations : Training Loss =  0.3129903057499077; Validation Loss = 0.3483138470412422\n",
            "Cost after 299167 iterations : Training Loss =  0.3129904124917879; Validation Loss = 0.34831409489154963\n",
            "Cost after 299168 iterations : Training Loss =  0.3129903521502041; Validation Loss = 0.3483136183025292\n",
            "Cost after 299169 iterations : Training Loss =  0.31299031322285004; Validation Loss = 0.3483138661528367\n",
            "Cost after 299170 iterations : Training Loss =  0.3129903985505011; Validation Loss = 0.34831338956381613\n",
            "Cost after 299171 iterations : Training Loss =  0.31299023124553943; Validation Loss = 0.3483136374141233\n",
            "Cost after 299172 iterations : Training Loss =  0.31299042765917023; Validation Loss = 0.3483138852644308\n",
            "Cost after 299173 iterations : Training Loss =  0.312990277645836; Validation Loss = 0.34831340867541033\n",
            "Cost after 299174 iterations : Training Loss =  0.3129903283902324; Validation Loss = 0.3483136565257176\n",
            "Cost after 299175 iterations : Training Loss =  0.31299032404613253; Validation Loss = 0.3483131799366966\n",
            "Cost after 299176 iterations : Training Loss =  0.31299022912129465; Validation Loss = 0.348313427787004\n",
            "Cost after 299177 iterations : Training Loss =  0.31299037044642924; Validation Loss = 0.34831295119798333\n",
            "Cost after 299178 iterations : Training Loss =  0.3129902031414674; Validation Loss = 0.3483131990482912\n",
            "Cost after 299179 iterations : Training Loss =  0.31299034355761524; Validation Loss = 0.3483134468985986\n",
            "Cost after 299180 iterations : Training Loss =  0.31299024954176435; Validation Loss = 0.34831297030957786\n",
            "Cost after 299181 iterations : Training Loss =  0.31299024428867733; Validation Loss = 0.3483132181598855\n",
            "Cost after 299182 iterations : Training Loss =  0.3129902959420606; Validation Loss = 0.3483127415708643\n",
            "Cost after 299183 iterations : Training Loss =  0.3129901450197392; Validation Loss = 0.3483129894211717\n",
            "Cost after 299184 iterations : Training Loss =  0.3129903423423575; Validation Loss = 0.34831251283215137\n",
            "Cost after 299185 iterations : Training Loss =  0.3129901750373958; Validation Loss = 0.348312760682459\n",
            "Cost after 299186 iterations : Training Loss =  0.3129902594560602; Validation Loss = 0.3483130085327661\n",
            "Cost after 299187 iterations : Training Loss =  0.3129902214376922; Validation Loss = 0.3483125319437454\n",
            "Cost after 299188 iterations : Training Loss =  0.312990160187122; Validation Loss = 0.34831277979405273\n",
            "Cost after 299189 iterations : Training Loss =  0.31299026783798906; Validation Loss = 0.34831230320503237\n",
            "Cost after 299190 iterations : Training Loss =  0.3129901005330273; Validation Loss = 0.34831255105534015\n",
            "Cost after 299191 iterations : Training Loss =  0.31299027462344264; Validation Loss = 0.3483127989056474\n",
            "Cost after 299192 iterations : Training Loss =  0.31299014693332394; Validation Loss = 0.3483123223166264\n",
            "Cost after 299193 iterations : Training Loss =  0.3129901753545045; Validation Loss = 0.348312570166934\n",
            "Cost after 299194 iterations : Training Loss =  0.3129901933336206; Validation Loss = 0.34831209357791293\n",
            "Cost after 299195 iterations : Training Loss =  0.31299007608556656; Validation Loss = 0.348312341428221\n",
            "Cost after 299196 iterations : Training Loss =  0.3129902397339173; Validation Loss = 0.34831186483919974\n",
            "Cost after 299197 iterations : Training Loss =  0.3129900724289553; Validation Loss = 0.3483121126895075\n",
            "Cost after 299198 iterations : Training Loss =  0.3129901905218873; Validation Loss = 0.348312360539815\n",
            "Cost after 299199 iterations : Training Loss =  0.3129901188292521; Validation Loss = 0.3483118839507944\n",
            "Cost after 299200 iterations : Training Loss =  0.3129900912529494; Validation Loss = 0.3483121318011016\n",
            "Cost after 299201 iterations : Training Loss =  0.3129901652295486; Validation Loss = 0.34831165521208124\n",
            "Cost after 299202 iterations : Training Loss =  0.3129899979245868; Validation Loss = 0.34831190306238824\n",
            "Cost after 299203 iterations : Training Loss =  0.31299020568926983; Validation Loss = 0.34831215091269585\n",
            "Cost after 299204 iterations : Training Loss =  0.3129900443248835; Validation Loss = 0.3483116743236756\n",
            "Cost after 299205 iterations : Training Loss =  0.3129901064203321; Validation Loss = 0.3483119221739826\n",
            "Cost after 299206 iterations : Training Loss =  0.3129900907251803; Validation Loss = 0.3483114455849619\n",
            "Cost after 299207 iterations : Training Loss =  0.31299000715139413; Validation Loss = 0.34831169343526924\n",
            "Cost after 299208 iterations : Training Loss =  0.3129901371254767; Validation Loss = 0.3483112168462486\n",
            "Cost after 299209 iterations : Training Loss =  0.3129899698205153; Validation Loss = 0.34831146469655566\n",
            "Cost after 299210 iterations : Training Loss =  0.3129901215877148; Validation Loss = 0.34831171254686355\n",
            "Cost after 299211 iterations : Training Loss =  0.31299001622081174; Validation Loss = 0.3483112359578428\n",
            "Cost after 299212 iterations : Training Loss =  0.3129900223187769; Validation Loss = 0.34831148380815025\n",
            "Cost after 299213 iterations : Training Loss =  0.3129900626211085; Validation Loss = 0.34831100721912955\n",
            "Cost after 299214 iterations : Training Loss =  0.31298992304983897; Validation Loss = 0.3483112550694367\n",
            "Cost after 299215 iterations : Training Loss =  0.3129901090214052; Validation Loss = 0.3483107784804163\n",
            "Cost after 299216 iterations : Training Loss =  0.31298994171644334; Validation Loss = 0.34831102633072364\n",
            "Cost after 299217 iterations : Training Loss =  0.31299003748615933; Validation Loss = 0.34831127418103136\n",
            "Cost after 299218 iterations : Training Loss =  0.3129899881167398; Validation Loss = 0.34831079759201045\n",
            "Cost after 299219 iterations : Training Loss =  0.3129899382172216; Validation Loss = 0.34831104544231795\n",
            "Cost after 299220 iterations : Training Loss =  0.31299003451703633; Validation Loss = 0.34831056885329686\n",
            "Cost after 299221 iterations : Training Loss =  0.31298986721207495; Validation Loss = 0.34831081670360475\n",
            "Cost after 299222 iterations : Training Loss =  0.312990052653542; Validation Loss = 0.3483110645539123\n",
            "Cost after 299223 iterations : Training Loss =  0.31298991361237144; Validation Loss = 0.3483105879648914\n",
            "Cost after 299224 iterations : Training Loss =  0.31298995338460434; Validation Loss = 0.3483108358151986\n",
            "Cost after 299225 iterations : Training Loss =  0.31298996001266816; Validation Loss = 0.34831035922617815\n",
            "Cost after 299226 iterations : Training Loss =  0.31298985411566654; Validation Loss = 0.3483106070764853\n",
            "Cost after 299227 iterations : Training Loss =  0.3129900064129651; Validation Loss = 0.34831013048746445\n",
            "Cost after 299228 iterations : Training Loss =  0.3129898391080031; Validation Loss = 0.34831037833777245\n",
            "Cost after 299229 iterations : Training Loss =  0.3129899685519869; Validation Loss = 0.3483106261880796\n",
            "Cost after 299230 iterations : Training Loss =  0.3129898855082997; Validation Loss = 0.34831014959905887\n",
            "Cost after 299231 iterations : Training Loss =  0.3129898692830486; Validation Loss = 0.34831039744936637\n",
            "Cost after 299232 iterations : Training Loss =  0.3129899319085964; Validation Loss = 0.3483099208603458\n",
            "Cost after 299233 iterations : Training Loss =  0.3129897700141109; Validation Loss = 0.3483101687106533\n",
            "Cost after 299234 iterations : Training Loss =  0.31298997830889297; Validation Loss = 0.34830969212163226\n",
            "Cost after 299235 iterations : Training Loss =  0.31298981100393114; Validation Loss = 0.3483099399719399\n",
            "Cost after 299236 iterations : Training Loss =  0.31298988445043147; Validation Loss = 0.3483101878222476\n",
            "Cost after 299237 iterations : Training Loss =  0.31298985740422774; Validation Loss = 0.34830971123322657\n",
            "Cost after 299238 iterations : Training Loss =  0.31298978518149356; Validation Loss = 0.3483099590835342\n",
            "Cost after 299239 iterations : Training Loss =  0.31298990380452457; Validation Loss = 0.3483094824945134\n",
            "Cost after 299240 iterations : Training Loss =  0.31298973649956296; Validation Loss = 0.34830973034482093\n",
            "Cost after 299241 iterations : Training Loss =  0.3129898996178141; Validation Loss = 0.34830997819512854\n",
            "Cost after 299242 iterations : Training Loss =  0.3129897828998594; Validation Loss = 0.3483095016061074\n",
            "Cost after 299243 iterations : Training Loss =  0.31298980034887647; Validation Loss = 0.34830974945641485\n",
            "Cost after 299244 iterations : Training Loss =  0.31298982930015606; Validation Loss = 0.34830927286739427\n",
            "Cost after 299245 iterations : Training Loss =  0.31298970107993845; Validation Loss = 0.34830952071770155\n",
            "Cost after 299246 iterations : Training Loss =  0.3129898757004527; Validation Loss = 0.34830904412868086\n",
            "Cost after 299247 iterations : Training Loss =  0.31298970839549095; Validation Loss = 0.3483092919789884\n",
            "Cost after 299248 iterations : Training Loss =  0.312989815516259; Validation Loss = 0.3483095398292963\n",
            "Cost after 299249 iterations : Training Loss =  0.3129897547957879; Validation Loss = 0.34830906324027544\n",
            "Cost after 299250 iterations : Training Loss =  0.312989716247321; Validation Loss = 0.3483093110905832\n",
            "Cost after 299251 iterations : Training Loss =  0.31298980119608427; Validation Loss = 0.3483088345015619\n",
            "Cost after 299252 iterations : Training Loss =  0.31298963389112255; Validation Loss = 0.3483090823518694\n",
            "Cost after 299253 iterations : Training Loss =  0.31298983068364133; Validation Loss = 0.3483093302021767\n",
            "Cost after 299254 iterations : Training Loss =  0.31298968029141916; Validation Loss = 0.3483088536131561\n",
            "Cost after 299255 iterations : Training Loss =  0.3129897314147037; Validation Loss = 0.34830910146346383\n",
            "Cost after 299256 iterations : Training Loss =  0.31298972669171593; Validation Loss = 0.3483086248744431\n",
            "Cost after 299257 iterations : Training Loss =  0.312989632145766; Validation Loss = 0.3483088727247504\n",
            "Cost after 299258 iterations : Training Loss =  0.3129897730920126; Validation Loss = 0.3483083961357297\n",
            "Cost after 299259 iterations : Training Loss =  0.31298960578705076; Validation Loss = 0.3483086439860374\n",
            "Cost after 299260 iterations : Training Loss =  0.3129897465820863; Validation Loss = 0.34830889183634467\n",
            "Cost after 299261 iterations : Training Loss =  0.31298965218734753; Validation Loss = 0.34830841524732364\n",
            "Cost after 299262 iterations : Training Loss =  0.3129896473131483; Validation Loss = 0.34830866309763125\n",
            "Cost after 299263 iterations : Training Loss =  0.31298969858764397; Validation Loss = 0.34830818650861056\n",
            "Cost after 299264 iterations : Training Loss =  0.3129895480442107; Validation Loss = 0.3483084343589182\n",
            "Cost after 299265 iterations : Training Loss =  0.3129897449879408; Validation Loss = 0.3483079577698973\n",
            "Cost after 299266 iterations : Training Loss =  0.3129895776829789; Validation Loss = 0.34830820562020465\n",
            "Cost after 299267 iterations : Training Loss =  0.312989662480531; Validation Loss = 0.34830845347051237\n",
            "Cost after 299268 iterations : Training Loss =  0.31298962408327563; Validation Loss = 0.3483079768814914\n",
            "Cost after 299269 iterations : Training Loss =  0.31298956321159316; Validation Loss = 0.34830822473179923\n",
            "Cost after 299270 iterations : Training Loss =  0.31298967048357235; Validation Loss = 0.34830774814277815\n",
            "Cost after 299271 iterations : Training Loss =  0.3129895031786105; Validation Loss = 0.34830799599308565\n",
            "Cost after 299272 iterations : Training Loss =  0.3129896776479135; Validation Loss = 0.3483082438433933\n",
            "Cost after 299273 iterations : Training Loss =  0.31298954957890707; Validation Loss = 0.3483077672543726\n",
            "Cost after 299274 iterations : Training Loss =  0.31298957837897573; Validation Loss = 0.34830801510468024\n",
            "Cost after 299275 iterations : Training Loss =  0.312989595979204; Validation Loss = 0.3483075385156593\n",
            "Cost after 299276 iterations : Training Loss =  0.3129894791100379; Validation Loss = 0.3483077863659667\n",
            "Cost after 299277 iterations : Training Loss =  0.31298964237950033; Validation Loss = 0.34830730977694563\n",
            "Cost after 299278 iterations : Training Loss =  0.31298947507453884; Validation Loss = 0.3483075576272539\n",
            "Cost after 299279 iterations : Training Loss =  0.3129895935463585; Validation Loss = 0.34830780547756107\n",
            "Cost after 299280 iterations : Training Loss =  0.3129895214748354; Validation Loss = 0.34830732888854016\n",
            "Cost after 299281 iterations : Training Loss =  0.31298949427742045; Validation Loss = 0.3483075767388475\n",
            "Cost after 299282 iterations : Training Loss =  0.31298956787513194; Validation Loss = 0.34830710014982685\n",
            "Cost after 299283 iterations : Training Loss =  0.3129894005701701; Validation Loss = 0.3483073480001343\n",
            "Cost after 299284 iterations : Training Loss =  0.312989608713741; Validation Loss = 0.3483075958504419\n",
            "Cost after 299285 iterations : Training Loss =  0.3129894469704669; Validation Loss = 0.34830711926142083\n",
            "Cost after 299286 iterations : Training Loss =  0.31298950944480325; Validation Loss = 0.3483073671117288\n",
            "Cost after 299287 iterations : Training Loss =  0.31298949337076365; Validation Loss = 0.3483068905227077\n",
            "Cost after 299288 iterations : Training Loss =  0.3129894101758652; Validation Loss = 0.3483071383730153\n",
            "Cost after 299289 iterations : Training Loss =  0.3129895397710602; Validation Loss = 0.348306661783994\n",
            "Cost after 299290 iterations : Training Loss =  0.3129893724660985; Validation Loss = 0.3483069096343019\n",
            "Cost after 299291 iterations : Training Loss =  0.31298952461218604; Validation Loss = 0.34830715748460933\n",
            "Cost after 299292 iterations : Training Loss =  0.31298941886639514; Validation Loss = 0.34830668089558886\n",
            "Cost after 299293 iterations : Training Loss =  0.31298942534324786; Validation Loss = 0.3483069287458966\n",
            "Cost after 299294 iterations : Training Loss =  0.3129894652666916; Validation Loss = 0.3483064521568756\n",
            "Cost after 299295 iterations : Training Loss =  0.31298932607431035; Validation Loss = 0.34830670000718306\n",
            "Cost after 299296 iterations : Training Loss =  0.31298951166698835; Validation Loss = 0.34830622341816236\n",
            "Cost after 299297 iterations : Training Loss =  0.31298934436202647; Validation Loss = 0.34830647126847014\n",
            "Cost after 299298 iterations : Training Loss =  0.31298944051063055; Validation Loss = 0.3483067191187774\n",
            "Cost after 299299 iterations : Training Loss =  0.312989390762323; Validation Loss = 0.34830624252975634\n",
            "Cost after 299300 iterations : Training Loss =  0.31298934124169253; Validation Loss = 0.348306490380064\n",
            "Cost after 299301 iterations : Training Loss =  0.3129894371626199; Validation Loss = 0.34830601379104337\n",
            "Cost after 299302 iterations : Training Loss =  0.31298926985765824; Validation Loss = 0.34830626164135053\n",
            "Cost after 299303 iterations : Training Loss =  0.31298945567801306; Validation Loss = 0.34830650949165815\n",
            "Cost after 299304 iterations : Training Loss =  0.3129893162579547; Validation Loss = 0.3483060329026374\n",
            "Cost after 299305 iterations : Training Loss =  0.3129893564090755; Validation Loss = 0.34830628075294484\n",
            "Cost after 299306 iterations : Training Loss =  0.3129893626582516; Validation Loss = 0.3483058041639241\n",
            "Cost after 299307 iterations : Training Loss =  0.31298925714013726; Validation Loss = 0.3483060520142314\n",
            "Cost after 299308 iterations : Training Loss =  0.31298940905854833; Validation Loss = 0.3483055754252108\n",
            "Cost after 299309 iterations : Training Loss =  0.31298924175358644; Validation Loss = 0.3483058232755181\n",
            "Cost after 299310 iterations : Training Loss =  0.3129893715764579; Validation Loss = 0.348306071125826\n",
            "Cost after 299311 iterations : Training Loss =  0.3129892881538831; Validation Loss = 0.3483055945368051\n",
            "Cost after 299312 iterations : Training Loss =  0.31298927230752; Validation Loss = 0.34830584238711254\n",
            "Cost after 299313 iterations : Training Loss =  0.31298933455417977; Validation Loss = 0.34830536579809185\n",
            "Cost after 299314 iterations : Training Loss =  0.3129891730385822; Validation Loss = 0.3483056136483994\n",
            "Cost after 299315 iterations : Training Loss =  0.3129893809544763; Validation Loss = 0.3483051370593783\n",
            "Cost after 299316 iterations : Training Loss =  0.3129892136495146; Validation Loss = 0.34830538490968593\n",
            "Cost after 299317 iterations : Training Loss =  0.31298928747490273; Validation Loss = 0.34830563275999316\n",
            "Cost after 299318 iterations : Training Loss =  0.31298926004981154; Validation Loss = 0.3483051561709728\n",
            "Cost after 299319 iterations : Training Loss =  0.3129891882059646; Validation Loss = 0.34830540402128024\n",
            "Cost after 299320 iterations : Training Loss =  0.312989306450108; Validation Loss = 0.3483049274322596\n",
            "Cost after 299321 iterations : Training Loss =  0.3129891391451462; Validation Loss = 0.34830517528256705\n",
            "Cost after 299322 iterations : Training Loss =  0.3129893026422852; Validation Loss = 0.3483054231328746\n",
            "Cost after 299323 iterations : Training Loss =  0.3129891855454428; Validation Loss = 0.34830494654385374\n",
            "Cost after 299324 iterations : Training Loss =  0.3129892033733475; Validation Loss = 0.34830519439416135\n",
            "Cost after 299325 iterations : Training Loss =  0.3129892319457397; Validation Loss = 0.34830471780514033\n",
            "Cost after 299326 iterations : Training Loss =  0.31298910410440955; Validation Loss = 0.3483049656554478\n",
            "Cost after 299327 iterations : Training Loss =  0.31298927834603607; Validation Loss = 0.3483044890664271\n",
            "Cost after 299328 iterations : Training Loss =  0.3129891110410744; Validation Loss = 0.34830473691673436\n",
            "Cost after 299329 iterations : Training Loss =  0.3129892185407299; Validation Loss = 0.3483049847670421\n",
            "Cost after 299330 iterations : Training Loss =  0.3129891574413711; Validation Loss = 0.3483045081780212\n",
            "Cost after 299331 iterations : Training Loss =  0.31298911927179196; Validation Loss = 0.34830475602832883\n",
            "Cost after 299332 iterations : Training Loss =  0.31298920384166784; Validation Loss = 0.34830427943930814\n",
            "Cost after 299333 iterations : Training Loss =  0.312989036536706; Validation Loss = 0.3483045272896159\n",
            "Cost after 299334 iterations : Training Loss =  0.31298923370811244; Validation Loss = 0.3483047751399229\n",
            "Cost after 299335 iterations : Training Loss =  0.31298908293700267; Validation Loss = 0.34830429855090217\n",
            "Cost after 299336 iterations : Training Loss =  0.3129891344391748; Validation Loss = 0.34830454640120956\n",
            "Cost after 299337 iterations : Training Loss =  0.3129891293372992; Validation Loss = 0.34830406981218914\n",
            "Cost after 299338 iterations : Training Loss =  0.3129890351702369; Validation Loss = 0.3483043176624966\n",
            "Cost after 299339 iterations : Training Loss =  0.31298917573759577; Validation Loss = 0.34830384107347584\n",
            "Cost after 299340 iterations : Training Loss =  0.3129890084326341; Validation Loss = 0.3483040889237831\n",
            "Cost after 299341 iterations : Training Loss =  0.3129891496065574; Validation Loss = 0.3483043367740906\n",
            "Cost after 299342 iterations : Training Loss =  0.3129890548329308; Validation Loss = 0.34830386018507015\n",
            "Cost after 299343 iterations : Training Loss =  0.31298905033761953; Validation Loss = 0.3483041080353776\n",
            "Cost after 299344 iterations : Training Loss =  0.3129891012332274; Validation Loss = 0.34830363144635695\n",
            "Cost after 299345 iterations : Training Loss =  0.3129889510686816; Validation Loss = 0.348303879296664\n",
            "Cost after 299346 iterations : Training Loss =  0.31298914763352437; Validation Loss = 0.34830340270764326\n",
            "Cost after 299347 iterations : Training Loss =  0.3129889803285623; Validation Loss = 0.3483036505579509\n",
            "Cost after 299348 iterations : Training Loss =  0.31298906550500216; Validation Loss = 0.3483038984082586\n",
            "Cost after 299349 iterations : Training Loss =  0.312989026728859; Validation Loss = 0.3483034218192376\n",
            "Cost after 299350 iterations : Training Loss =  0.3129889662360642; Validation Loss = 0.348303669669545\n",
            "Cost after 299351 iterations : Training Loss =  0.3129890731291554; Validation Loss = 0.34830319308052426\n",
            "Cost after 299352 iterations : Training Loss =  0.3129889058241936; Validation Loss = 0.3483034409308318\n",
            "Cost after 299353 iterations : Training Loss =  0.31298908067238457; Validation Loss = 0.34830368878113943\n",
            "Cost after 299354 iterations : Training Loss =  0.31298895222449047; Validation Loss = 0.3483032121921187\n",
            "Cost after 299355 iterations : Training Loss =  0.3129889814034468; Validation Loss = 0.348303460042426\n",
            "Cost after 299356 iterations : Training Loss =  0.31298899862478696; Validation Loss = 0.3483029834534051\n",
            "Cost after 299357 iterations : Training Loss =  0.31298888213450904; Validation Loss = 0.3483032313037128\n",
            "Cost after 299358 iterations : Training Loss =  0.3129890450250836; Validation Loss = 0.3483027547146923\n",
            "Cost after 299359 iterations : Training Loss =  0.31298887772012207; Validation Loss = 0.3483030025649995\n",
            "Cost after 299360 iterations : Training Loss =  0.3129889965708297; Validation Loss = 0.3483032504153067\n",
            "Cost after 299361 iterations : Training Loss =  0.3129889241204187; Validation Loss = 0.3483027738262864\n",
            "Cost after 299362 iterations : Training Loss =  0.31298889730189156; Validation Loss = 0.3483030216765934\n",
            "Cost after 299363 iterations : Training Loss =  0.31298897052071556; Validation Loss = 0.34830254508757313\n",
            "Cost after 299364 iterations : Training Loss =  0.3129888032157537; Validation Loss = 0.3483027929378804\n",
            "Cost after 299365 iterations : Training Loss =  0.31298901173821203; Validation Loss = 0.34830304078818813\n",
            "Cost after 299366 iterations : Training Loss =  0.3129888496160501; Validation Loss = 0.3483025641991671\n",
            "Cost after 299367 iterations : Training Loss =  0.3129889124692742; Validation Loss = 0.3483028120494748\n",
            "Cost after 299368 iterations : Training Loss =  0.31298889601634683; Validation Loss = 0.3483023354604539\n",
            "Cost after 299369 iterations : Training Loss =  0.3129888132003361; Validation Loss = 0.3483025833107617\n",
            "Cost after 299370 iterations : Training Loss =  0.31298894241664366; Validation Loss = 0.34830210672174033\n",
            "Cost after 299371 iterations : Training Loss =  0.31298877511168177; Validation Loss = 0.3483023545720482\n",
            "Cost after 299372 iterations : Training Loss =  0.31298892763665664; Validation Loss = 0.34830260242235567\n",
            "Cost after 299373 iterations : Training Loss =  0.3129888215119784; Validation Loss = 0.34830212583333464\n",
            "Cost after 299374 iterations : Training Loss =  0.3129888283677191; Validation Loss = 0.34830237368364225\n",
            "Cost after 299375 iterations : Training Loss =  0.3129888679122751; Validation Loss = 0.3483018970946218\n",
            "Cost after 299376 iterations : Training Loss =  0.3129887290987811; Validation Loss = 0.34830214494492895\n",
            "Cost after 299377 iterations : Training Loss =  0.3129889143125717; Validation Loss = 0.3483016683559085\n",
            "Cost after 299378 iterations : Training Loss =  0.31298874700761015; Validation Loss = 0.34830191620621603\n",
            "Cost after 299379 iterations : Training Loss =  0.3129888435351015; Validation Loss = 0.3483021640565231\n",
            "Cost after 299380 iterations : Training Loss =  0.3129887934079067; Validation Loss = 0.3483016874675026\n",
            "Cost after 299381 iterations : Training Loss =  0.31298874426616374; Validation Loss = 0.34830193531781\n",
            "Cost after 299382 iterations : Training Loss =  0.3129888398082032; Validation Loss = 0.3483014587287893\n",
            "Cost after 299383 iterations : Training Loss =  0.3129886725032413; Validation Loss = 0.3483017065790969\n",
            "Cost after 299384 iterations : Training Loss =  0.3129888587024842; Validation Loss = 0.3483019544294041\n",
            "Cost after 299385 iterations : Training Loss =  0.3129887189035381; Validation Loss = 0.3483014778403834\n",
            "Cost after 299386 iterations : Training Loss =  0.3129887594335462; Validation Loss = 0.3483017256906906\n",
            "Cost after 299387 iterations : Training Loss =  0.31298876530383496; Validation Loss = 0.3483012491016703\n",
            "Cost after 299388 iterations : Training Loss =  0.3129886601646084; Validation Loss = 0.34830149695197793\n",
            "Cost after 299389 iterations : Training Loss =  0.3129888117041317; Validation Loss = 0.3483010203629571\n",
            "Cost after 299390 iterations : Training Loss =  0.3129886443991698; Validation Loss = 0.3483012682132643\n",
            "Cost after 299391 iterations : Training Loss =  0.312988774600929; Validation Loss = 0.3483015160635719\n",
            "Cost after 299392 iterations : Training Loss =  0.31298869079946634; Validation Loss = 0.34830103947455143\n",
            "Cost after 299393 iterations : Training Loss =  0.3129886753319913; Validation Loss = 0.3483012873248589\n",
            "Cost after 299394 iterations : Training Loss =  0.31298873719976306; Validation Loss = 0.3483008107358379\n",
            "Cost after 299395 iterations : Training Loss =  0.31298857606305336; Validation Loss = 0.34830105858614546\n",
            "Cost after 299396 iterations : Training Loss =  0.31298878360005966; Validation Loss = 0.34830058199712455\n",
            "Cost after 299397 iterations : Training Loss =  0.3129886162950979; Validation Loss = 0.348300829847432\n",
            "Cost after 299398 iterations : Training Loss =  0.31298869049937367; Validation Loss = 0.3483010776977395\n",
            "Cost after 299399 iterations : Training Loss =  0.31298866269539455; Validation Loss = 0.34830060110871847\n",
            "Cost after 299400 iterations : Training Loss =  0.31298859123043565; Validation Loss = 0.3483008489590262\n",
            "Cost after 299401 iterations : Training Loss =  0.3129887090956912; Validation Loss = 0.3483003723700054\n",
            "Cost after 299402 iterations : Training Loss =  0.31298854179072977; Validation Loss = 0.348300620220313\n",
            "Cost after 299403 iterations : Training Loss =  0.31298870566675613; Validation Loss = 0.34830086807062066\n",
            "Cost after 299404 iterations : Training Loss =  0.31298858819102615; Validation Loss = 0.34830039148159986\n",
            "Cost after 299405 iterations : Training Loss =  0.3129886063978184; Validation Loss = 0.3483006393319074\n",
            "Cost after 299406 iterations : Training Loss =  0.31298863459132287; Validation Loss = 0.34830016274288655\n",
            "Cost after 299407 iterations : Training Loss =  0.31298850712888054; Validation Loss = 0.3483004105931941\n",
            "Cost after 299408 iterations : Training Loss =  0.31298868099161953; Validation Loss = 0.3482999340041731\n",
            "Cost after 299409 iterations : Training Loss =  0.31298851368665764; Validation Loss = 0.34830018185448053\n",
            "Cost after 299410 iterations : Training Loss =  0.31298862156520085; Validation Loss = 0.3483004297047883\n",
            "Cost after 299411 iterations : Training Loss =  0.3129885600869543; Validation Loss = 0.3482999531157675\n",
            "Cost after 299412 iterations : Training Loss =  0.3129885222962633; Validation Loss = 0.3483002009660753\n",
            "Cost after 299413 iterations : Training Loss =  0.312988606487251; Validation Loss = 0.3482997243770542\n",
            "Cost after 299414 iterations : Training Loss =  0.3129884391822894; Validation Loss = 0.3482999722273618\n",
            "Cost after 299415 iterations : Training Loss =  0.31298863673258376; Validation Loss = 0.3483002200776693\n",
            "Cost after 299416 iterations : Training Loss =  0.31298848558258596; Validation Loss = 0.34829974348864845\n",
            "Cost after 299417 iterations : Training Loss =  0.31298853746364574; Validation Loss = 0.34829999133895584\n",
            "Cost after 299418 iterations : Training Loss =  0.31298853198288257; Validation Loss = 0.3482995147499351\n",
            "Cost after 299419 iterations : Training Loss =  0.3129884381947079; Validation Loss = 0.34829976260024237\n",
            "Cost after 299420 iterations : Training Loss =  0.31298857838317906; Validation Loss = 0.3482992860112219\n",
            "Cost after 299421 iterations : Training Loss =  0.31298841107821745; Validation Loss = 0.34829953386152945\n",
            "Cost after 299422 iterations : Training Loss =  0.31298855263102837; Validation Loss = 0.34829978171183695\n",
            "Cost after 299423 iterations : Training Loss =  0.31298845747851417; Validation Loss = 0.3482993051228161\n",
            "Cost after 299424 iterations : Training Loss =  0.3129884533620907; Validation Loss = 0.34829955297312337\n",
            "Cost after 299425 iterations : Training Loss =  0.31298850387881066; Validation Loss = 0.3482990763841028\n",
            "Cost after 299426 iterations : Training Loss =  0.3129883540931527; Validation Loss = 0.3482993242344105\n",
            "Cost after 299427 iterations : Training Loss =  0.31298855027910705; Validation Loss = 0.3482988476453896\n",
            "Cost after 299428 iterations : Training Loss =  0.31298838297414544; Validation Loss = 0.34829909549569693\n",
            "Cost after 299429 iterations : Training Loss =  0.31298846852947315; Validation Loss = 0.34829934334600454\n",
            "Cost after 299430 iterations : Training Loss =  0.31298842937444227; Validation Loss = 0.34829886675698385\n",
            "Cost after 299431 iterations : Training Loss =  0.31298836926053514; Validation Loss = 0.3482991146072915\n",
            "Cost after 299432 iterations : Training Loss =  0.312988475774739; Validation Loss = 0.34829863801827043\n",
            "Cost after 299433 iterations : Training Loss =  0.31298830846977704; Validation Loss = 0.34829888586857793\n",
            "Cost after 299434 iterations : Training Loss =  0.312988483696856; Validation Loss = 0.3482991337188854\n",
            "Cost after 299435 iterations : Training Loss =  0.3129883548700737; Validation Loss = 0.34829865712986463\n",
            "Cost after 299436 iterations : Training Loss =  0.312988384427918; Validation Loss = 0.3482989049801718\n",
            "Cost after 299437 iterations : Training Loss =  0.3129884012703702; Validation Loss = 0.34829842839115116\n",
            "Cost after 299438 iterations : Training Loss =  0.3129882851589802; Validation Loss = 0.3482986762414586\n",
            "Cost after 299439 iterations : Training Loss =  0.31298844767066725; Validation Loss = 0.3482981996524381\n",
            "Cost after 299440 iterations : Training Loss =  0.3129882803657053; Validation Loss = 0.34829844750274563\n",
            "Cost after 299441 iterations : Training Loss =  0.31298839959530056; Validation Loss = 0.34829869535305336\n",
            "Cost after 299442 iterations : Training Loss =  0.3129883267660019; Validation Loss = 0.34829821876403244\n",
            "Cost after 299443 iterations : Training Loss =  0.3129883003263626; Validation Loss = 0.3482984666143399\n",
            "Cost after 299444 iterations : Training Loss =  0.3129883731662985; Validation Loss = 0.3482979900253189\n",
            "Cost after 299445 iterations : Training Loss =  0.31298820586133685; Validation Loss = 0.34829823787562647\n",
            "Cost after 299446 iterations : Training Loss =  0.31298841476268313; Validation Loss = 0.34829848572593447\n",
            "Cost after 299447 iterations : Training Loss =  0.3129882522616336; Validation Loss = 0.3482980091369133\n",
            "Cost after 299448 iterations : Training Loss =  0.3129883154937453; Validation Loss = 0.348298256987221\n",
            "Cost after 299449 iterations : Training Loss =  0.3129882986619301; Validation Loss = 0.3482977803982\n",
            "Cost after 299450 iterations : Training Loss =  0.31298821622480755; Validation Loss = 0.3482980282485076\n",
            "Cost after 299451 iterations : Training Loss =  0.31298834506222684; Validation Loss = 0.34829755165948684\n",
            "Cost after 299452 iterations : Training Loss =  0.3129881777572653; Validation Loss = 0.3482977995097943\n",
            "Cost after 299453 iterations : Training Loss =  0.3129883306611281; Validation Loss = 0.34829804736010156\n",
            "Cost after 299454 iterations : Training Loss =  0.3129882241575617; Validation Loss = 0.34829757077108087\n",
            "Cost after 299455 iterations : Training Loss =  0.31298823139218995; Validation Loss = 0.3482978186213883\n",
            "Cost after 299456 iterations : Training Loss =  0.3129882705578584; Validation Loss = 0.3482973420323677\n",
            "Cost after 299457 iterations : Training Loss =  0.31298813212325227; Validation Loss = 0.3482975898826755\n",
            "Cost after 299458 iterations : Training Loss =  0.3129883169581551; Validation Loss = 0.34829711329365454\n",
            "Cost after 299459 iterations : Training Loss =  0.3129881496531932; Validation Loss = 0.34829736114396154\n",
            "Cost after 299460 iterations : Training Loss =  0.3129882465595725; Validation Loss = 0.3482976089942696\n",
            "Cost after 299461 iterations : Training Loss =  0.3129881960534899; Validation Loss = 0.3482971324052484\n",
            "Cost after 299462 iterations : Training Loss =  0.3129881472906347; Validation Loss = 0.3482973802555561\n",
            "Cost after 299463 iterations : Training Loss =  0.3129882424537867; Validation Loss = 0.3482969036665351\n",
            "Cost after 299464 iterations : Training Loss =  0.31298807514882504; Validation Loss = 0.3482971515168428\n",
            "Cost after 299465 iterations : Training Loss =  0.31298826172695515; Validation Loss = 0.3482973993671503\n",
            "Cost after 299466 iterations : Training Loss =  0.3129881215491215; Validation Loss = 0.34829692277812974\n",
            "Cost after 299467 iterations : Training Loss =  0.3129881624580174; Validation Loss = 0.3482971706284371\n",
            "Cost after 299468 iterations : Training Loss =  0.31298816794941814; Validation Loss = 0.3482966940394161\n",
            "Cost after 299469 iterations : Training Loss =  0.3129880631890797; Validation Loss = 0.34829694188972354\n",
            "Cost after 299470 iterations : Training Loss =  0.31298821434971463; Validation Loss = 0.3482964653007031\n",
            "Cost after 299471 iterations : Training Loss =  0.312988047044753; Validation Loss = 0.34829671315101096\n",
            "Cost after 299472 iterations : Training Loss =  0.31298817762540004; Validation Loss = 0.3482969610013184\n",
            "Cost after 299473 iterations : Training Loss =  0.3129880934450499; Validation Loss = 0.3482964844122972\n",
            "Cost after 299474 iterations : Training Loss =  0.31298807835646236; Validation Loss = 0.3482967322626048\n",
            "Cost after 299475 iterations : Training Loss =  0.31298813984534646; Validation Loss = 0.3482962556735839\n",
            "Cost after 299476 iterations : Training Loss =  0.3129879790875243; Validation Loss = 0.3482965035238917\n",
            "Cost after 299477 iterations : Training Loss =  0.3129881862456429; Validation Loss = 0.34829602693487105\n",
            "Cost after 299478 iterations : Training Loss =  0.3129880189406813; Validation Loss = 0.3482962747851778\n",
            "Cost after 299479 iterations : Training Loss =  0.3129880935238447; Validation Loss = 0.34829652263548566\n",
            "Cost after 299480 iterations : Training Loss =  0.3129880653409778; Validation Loss = 0.3482960460464647\n",
            "Cost after 299481 iterations : Training Loss =  0.31298799425490703; Validation Loss = 0.3482962938967721\n",
            "Cost after 299482 iterations : Training Loss =  0.3129881117412745; Validation Loss = 0.34829581730775144\n",
            "Cost after 299483 iterations : Training Loss =  0.31298794443631284; Validation Loss = 0.34829606515805905\n",
            "Cost after 299484 iterations : Training Loss =  0.31298810869122745; Validation Loss = 0.3482963130083669\n",
            "Cost after 299485 iterations : Training Loss =  0.31298799083660944; Validation Loss = 0.34829583641934575\n",
            "Cost after 299486 iterations : Training Loss =  0.3129880094222893; Validation Loss = 0.3482960842696535\n",
            "Cost after 299487 iterations : Training Loss =  0.3129880372369061; Validation Loss = 0.34829560768063245\n",
            "Cost after 299488 iterations : Training Loss =  0.3129879101533516; Validation Loss = 0.34829585553094\n",
            "Cost after 299489 iterations : Training Loss =  0.3129880836372027; Validation Loss = 0.34829537894191903\n",
            "Cost after 299490 iterations : Training Loss =  0.31298791633224105; Validation Loss = 0.34829562679222675\n",
            "Cost after 299491 iterations : Training Loss =  0.312988024589672; Validation Loss = 0.3482958746425342\n",
            "Cost after 299492 iterations : Training Loss =  0.31298796273253787; Validation Loss = 0.34829539805351317\n",
            "Cost after 299493 iterations : Training Loss =  0.3129879253207344; Validation Loss = 0.3482956459038209\n",
            "Cost after 299494 iterations : Training Loss =  0.3129880091328343; Validation Loss = 0.3482951693148003\n",
            "Cost after 299495 iterations : Training Loss =  0.31298784182787265; Validation Loss = 0.34829541716510776\n",
            "Cost after 299496 iterations : Training Loss =  0.31298803975705486; Validation Loss = 0.3482956650154155\n",
            "Cost after 299497 iterations : Training Loss =  0.3129878882281691; Validation Loss = 0.3482951884263943\n",
            "Cost after 299498 iterations : Training Loss =  0.31298794048811707; Validation Loss = 0.34829543627670206\n",
            "Cost after 299499 iterations : Training Loss =  0.3129879346284657; Validation Loss = 0.34829495968768115\n",
            "Cost after 299500 iterations : Training Loss =  0.31298784121917883; Validation Loss = 0.34829520753798887\n",
            "Cost after 299501 iterations : Training Loss =  0.3129879810287624; Validation Loss = 0.348294730948968\n",
            "Cost after 299502 iterations : Training Loss =  0.3129878137238007; Validation Loss = 0.3482949787992755\n",
            "Cost after 299503 iterations : Training Loss =  0.31298795565549936; Validation Loss = 0.34829522664958285\n",
            "Cost after 299504 iterations : Training Loss =  0.31298786012409757; Validation Loss = 0.34829475006056215\n",
            "Cost after 299505 iterations : Training Loss =  0.3129878563865614; Validation Loss = 0.34829499791086976\n",
            "Cost after 299506 iterations : Training Loss =  0.31298790652439384; Validation Loss = 0.34829452132184896\n",
            "Cost after 299507 iterations : Training Loss =  0.31298775711762367; Validation Loss = 0.34829476917215646\n",
            "Cost after 299508 iterations : Training Loss =  0.3129879529246907; Validation Loss = 0.348294292583136\n",
            "Cost after 299509 iterations : Training Loss =  0.3129877856197287; Validation Loss = 0.34829454043344327\n",
            "Cost after 299510 iterations : Training Loss =  0.3129878715539442; Validation Loss = 0.34829478828375043\n",
            "Cost after 299511 iterations : Training Loss =  0.3129878320200258; Validation Loss = 0.34829431169472974\n",
            "Cost after 299512 iterations : Training Loss =  0.31298777228500635; Validation Loss = 0.3482945595450376\n",
            "Cost after 299513 iterations : Training Loss =  0.31298787842032233; Validation Loss = 0.34829408295601627\n",
            "Cost after 299514 iterations : Training Loss =  0.31298771111536056; Validation Loss = 0.3482943308063237\n",
            "Cost after 299515 iterations : Training Loss =  0.3129878867213268; Validation Loss = 0.34829457865663105\n",
            "Cost after 299516 iterations : Training Loss =  0.312987757515657; Validation Loss = 0.34829410206761113\n",
            "Cost after 299517 iterations : Training Loss =  0.31298778745238887; Validation Loss = 0.34829434991791813\n",
            "Cost after 299518 iterations : Training Loss =  0.31298780391595377; Validation Loss = 0.34829387332889766\n",
            "Cost after 299519 iterations : Training Loss =  0.31298768818345124; Validation Loss = 0.34829412117920483\n",
            "Cost after 299520 iterations : Training Loss =  0.3129878503162503; Validation Loss = 0.34829364459018414\n",
            "Cost after 299521 iterations : Training Loss =  0.31298768301128876; Validation Loss = 0.3482938924404921\n",
            "Cost after 299522 iterations : Training Loss =  0.3129878026197717; Validation Loss = 0.3482941402907994\n",
            "Cost after 299523 iterations : Training Loss =  0.31298772941158565; Validation Loss = 0.3482936637017785\n",
            "Cost after 299524 iterations : Training Loss =  0.31298770335083365; Validation Loss = 0.3482939115520861\n",
            "Cost after 299525 iterations : Training Loss =  0.312987775811882; Validation Loss = 0.3482934349630649\n",
            "Cost after 299526 iterations : Training Loss =  0.3129876085069202; Validation Loss = 0.3482936828133727\n",
            "Cost after 299527 iterations : Training Loss =  0.3129878177871544; Validation Loss = 0.34829393066368\n",
            "Cost after 299528 iterations : Training Loss =  0.3129876549072172; Validation Loss = 0.34829345407465967\n",
            "Cost after 299529 iterations : Training Loss =  0.31298771851821633; Validation Loss = 0.3482937019249669\n",
            "Cost after 299530 iterations : Training Loss =  0.3129877013075133; Validation Loss = 0.3482932253359459\n",
            "Cost after 299531 iterations : Training Loss =  0.3129876192492785; Validation Loss = 0.34829347318625353\n",
            "Cost after 299532 iterations : Training Loss =  0.31298774770780996; Validation Loss = 0.3482929965972325\n",
            "Cost after 299533 iterations : Training Loss =  0.31298758040284835; Validation Loss = 0.34829324444754023\n",
            "Cost after 299534 iterations : Training Loss =  0.31298773368559923; Validation Loss = 0.34829349229784784\n",
            "Cost after 299535 iterations : Training Loss =  0.3129876268031451; Validation Loss = 0.3482930157088272\n",
            "Cost after 299536 iterations : Training Loss =  0.31298763441666116; Validation Loss = 0.34829326355913454\n",
            "Cost after 299537 iterations : Training Loss =  0.3129876732034417; Validation Loss = 0.34829278697011373\n",
            "Cost after 299538 iterations : Training Loss =  0.3129875351477235; Validation Loss = 0.3482930348204211\n",
            "Cost after 299539 iterations : Training Loss =  0.31298771960373845; Validation Loss = 0.3482925582314006\n",
            "Cost after 299540 iterations : Training Loss =  0.31298755229877656; Validation Loss = 0.34829280608170776\n",
            "Cost after 299541 iterations : Training Loss =  0.31298764958404385; Validation Loss = 0.34829305393201554\n",
            "Cost after 299542 iterations : Training Loss =  0.3129875986990732; Validation Loss = 0.3482925773429946\n",
            "Cost after 299543 iterations : Training Loss =  0.31298755031510594; Validation Loss = 0.3482928251933025\n",
            "Cost after 299544 iterations : Training Loss =  0.31298764509936994; Validation Loss = 0.3482923486042813\n",
            "Cost after 299545 iterations : Training Loss =  0.3129874777944081; Validation Loss = 0.34829259645458893\n",
            "Cost after 299546 iterations : Training Loss =  0.3129876647514264; Validation Loss = 0.3482928443048965\n",
            "Cost after 299547 iterations : Training Loss =  0.3129875241947049; Validation Loss = 0.3482923677158754\n",
            "Cost after 299548 iterations : Training Loss =  0.3129875654824884; Validation Loss = 0.348292615566183\n",
            "Cost after 299549 iterations : Training Loss =  0.31298757059500143; Validation Loss = 0.34829213897716244\n",
            "Cost after 299550 iterations : Training Loss =  0.3129874662135508; Validation Loss = 0.3482923868274699\n",
            "Cost after 299551 iterations : Training Loss =  0.31298761699529826; Validation Loss = 0.3482919102384493\n",
            "Cost after 299552 iterations : Training Loss =  0.31298744969033615; Validation Loss = 0.3482921580887566\n",
            "Cost after 299553 iterations : Training Loss =  0.31298758064987126; Validation Loss = 0.3482924059390641\n",
            "Cost after 299554 iterations : Training Loss =  0.3129874960906332; Validation Loss = 0.3482919293500437\n",
            "Cost after 299555 iterations : Training Loss =  0.31298748138093324; Validation Loss = 0.3482921772003509\n",
            "Cost after 299556 iterations : Training Loss =  0.31298754249092975; Validation Loss = 0.3482917006113301\n",
            "Cost after 299557 iterations : Training Loss =  0.3129873821119954; Validation Loss = 0.34829194846163725\n",
            "Cost after 299558 iterations : Training Loss =  0.3129875888912261; Validation Loss = 0.34829147187261666\n",
            "Cost after 299559 iterations : Training Loss =  0.3129874215862646; Validation Loss = 0.3482917197229243\n",
            "Cost after 299560 iterations : Training Loss =  0.31298749654831576; Validation Loss = 0.3482919675732319\n",
            "Cost after 299561 iterations : Training Loss =  0.31298746798656124; Validation Loss = 0.34829149098421086\n",
            "Cost after 299562 iterations : Training Loss =  0.312987397279378; Validation Loss = 0.34829173883451875\n",
            "Cost after 299563 iterations : Training Loss =  0.3129875143868577; Validation Loss = 0.34829126224549756\n",
            "Cost after 299564 iterations : Training Loss =  0.3129873470818962; Validation Loss = 0.348291510095805\n",
            "Cost after 299565 iterations : Training Loss =  0.3129875117156985; Validation Loss = 0.3482917579461129\n",
            "Cost after 299566 iterations : Training Loss =  0.31298739348219295; Validation Loss = 0.34829128135709225\n",
            "Cost after 299567 iterations : Training Loss =  0.31298741244676065; Validation Loss = 0.3482915292073994\n",
            "Cost after 299568 iterations : Training Loss =  0.31298743988248967; Validation Loss = 0.3482910526183791\n",
            "Cost after 299569 iterations : Training Loss =  0.3129873131778227; Validation Loss = 0.3482913004686863\n",
            "Cost after 299570 iterations : Training Loss =  0.31298748628278616; Validation Loss = 0.34829082387966553\n",
            "Cost after 299571 iterations : Training Loss =  0.31298731897782456; Validation Loss = 0.34829107172997315\n",
            "Cost after 299572 iterations : Training Loss =  0.31298742761414317; Validation Loss = 0.3482913195802806\n",
            "Cost after 299573 iterations : Training Loss =  0.31298736537812105; Validation Loss = 0.34829084299125995\n",
            "Cost after 299574 iterations : Training Loss =  0.3129873283452053; Validation Loss = 0.34829109084156706\n",
            "Cost after 299575 iterations : Training Loss =  0.31298741177841755; Validation Loss = 0.34829061425254654\n",
            "Cost after 299576 iterations : Training Loss =  0.3129872444734558; Validation Loss = 0.3482908621028538\n",
            "Cost after 299577 iterations : Training Loss =  0.31298744278152557; Validation Loss = 0.348291109953161\n",
            "Cost after 299578 iterations : Training Loss =  0.3129872908737525; Validation Loss = 0.3482906333641405\n",
            "Cost after 299579 iterations : Training Loss =  0.3129873435125878; Validation Loss = 0.348290881214448\n",
            "Cost after 299580 iterations : Training Loss =  0.3129873372740492; Validation Loss = 0.3482904046254275\n",
            "Cost after 299581 iterations : Training Loss =  0.3129872442436498; Validation Loss = 0.348290652475735\n",
            "Cost after 299582 iterations : Training Loss =  0.3129873836743456; Validation Loss = 0.34829017588671407\n",
            "Cost after 299583 iterations : Training Loss =  0.31298721636938404; Validation Loss = 0.34829042373702146\n",
            "Cost after 299584 iterations : Training Loss =  0.3129873586799705; Validation Loss = 0.34829067158732907\n",
            "Cost after 299585 iterations : Training Loss =  0.3129872627696807; Validation Loss = 0.3482901949983082\n",
            "Cost after 299586 iterations : Training Loss =  0.31298725941103284; Validation Loss = 0.3482904428486154\n",
            "Cost after 299587 iterations : Training Loss =  0.31298730916997747; Validation Loss = 0.3482899662595949\n",
            "Cost after 299588 iterations : Training Loss =  0.3129871601420949; Validation Loss = 0.3482902141099025\n",
            "Cost after 299589 iterations : Training Loss =  0.31298735557027385; Validation Loss = 0.34828973752088177\n",
            "Cost after 299590 iterations : Training Loss =  0.3129871882653123; Validation Loss = 0.3482899853711892\n",
            "Cost after 299591 iterations : Training Loss =  0.31298727457841524; Validation Loss = 0.34829023322149666\n",
            "Cost after 299592 iterations : Training Loss =  0.31298723466560885; Validation Loss = 0.34828975663247586\n",
            "Cost after 299593 iterations : Training Loss =  0.31298717530947734; Validation Loss = 0.3482900044827834\n",
            "Cost after 299594 iterations : Training Loss =  0.3129872810659054; Validation Loss = 0.34828952789376244\n",
            "Cost after 299595 iterations : Training Loss =  0.3129871137609439; Validation Loss = 0.34828977574407033\n",
            "Cost after 299596 iterations : Training Loss =  0.31298728974579804; Validation Loss = 0.34829002359437794\n",
            "Cost after 299597 iterations : Training Loss =  0.3129871601612407; Validation Loss = 0.3482895470053568\n",
            "Cost after 299598 iterations : Training Loss =  0.31298719047686; Validation Loss = 0.3482897948556642\n",
            "Cost after 299599 iterations : Training Loss =  0.3129872065615371; Validation Loss = 0.348289318266644\n",
            "Cost after 299600 iterations : Training Loss =  0.31298709120792184; Validation Loss = 0.3482895661169512\n",
            "Cost after 299601 iterations : Training Loss =  0.31298725296183383; Validation Loss = 0.3482890895279302\n",
            "Cost after 299602 iterations : Training Loss =  0.31298708565687194; Validation Loss = 0.3482893373782377\n",
            "Cost after 299603 iterations : Training Loss =  0.3129872056442428; Validation Loss = 0.34828958522854536\n",
            "Cost after 299604 iterations : Training Loss =  0.3129871320571687; Validation Loss = 0.3482891086395246\n",
            "Cost after 299605 iterations : Training Loss =  0.31298710637530486; Validation Loss = 0.3482893564898321\n",
            "Cost after 299606 iterations : Training Loss =  0.31298717845746515; Validation Loss = 0.3482888799008113\n",
            "Cost after 299607 iterations : Training Loss =  0.31298701115250366; Validation Loss = 0.3482891277511186\n",
            "Cost after 299608 iterations : Training Loss =  0.31298722081162533; Validation Loss = 0.3482893756014262\n",
            "Cost after 299609 iterations : Training Loss =  0.3129870575528004; Validation Loss = 0.34828889901240545\n",
            "Cost after 299610 iterations : Training Loss =  0.31298712154268743; Validation Loss = 0.3482891468627132\n",
            "Cost after 299611 iterations : Training Loss =  0.3129871039530969; Validation Loss = 0.3482886702736921\n",
            "Cost after 299612 iterations : Training Loss =  0.3129870222737496; Validation Loss = 0.34828891812399976\n",
            "Cost after 299613 iterations : Training Loss =  0.3129871503533934; Validation Loss = 0.3482884415349791\n",
            "Cost after 299614 iterations : Training Loss =  0.3129869830484318; Validation Loss = 0.34828868938528645\n",
            "Cost after 299615 iterations : Training Loss =  0.31298713671007; Validation Loss = 0.34828893723559406\n",
            "Cost after 299616 iterations : Training Loss =  0.31298702944872836; Validation Loss = 0.34828846064657315\n",
            "Cost after 299617 iterations : Training Loss =  0.3129870374411322; Validation Loss = 0.3482887084968806\n",
            "Cost after 299618 iterations : Training Loss =  0.31298707584902535; Validation Loss = 0.34828823190785996\n",
            "Cost after 299619 iterations : Training Loss =  0.3129869381721945; Validation Loss = 0.3482884797581675\n",
            "Cost after 299620 iterations : Training Loss =  0.3129871222493216; Validation Loss = 0.3482880031691466\n",
            "Cost after 299621 iterations : Training Loss =  0.31298695494436; Validation Loss = 0.3482882510194538\n",
            "Cost after 299622 iterations : Training Loss =  0.3129870526085148; Validation Loss = 0.3482884988697617\n",
            "Cost after 299623 iterations : Training Loss =  0.31298700134465657; Validation Loss = 0.3482880222807405\n",
            "Cost after 299624 iterations : Training Loss =  0.3129869533395771; Validation Loss = 0.3482882701310483\n",
            "Cost after 299625 iterations : Training Loss =  0.31298704774495323; Validation Loss = 0.3482877935420274\n",
            "Cost after 299626 iterations : Training Loss =  0.3129868804399915; Validation Loss = 0.348288041392335\n",
            "Cost after 299627 iterations : Training Loss =  0.31298706777589735; Validation Loss = 0.3482882892426423\n",
            "Cost after 299628 iterations : Training Loss =  0.3129869268402882; Validation Loss = 0.3482878126536215\n",
            "Cost after 299629 iterations : Training Loss =  0.31298696850695984; Validation Loss = 0.3482880605039292\n",
            "Cost after 299630 iterations : Training Loss =  0.31298697324058483; Validation Loss = 0.34828758391490866\n",
            "Cost after 299631 iterations : Training Loss =  0.3129868692380217; Validation Loss = 0.3482878317652157\n",
            "Cost after 299632 iterations : Training Loss =  0.3129870196408813; Validation Loss = 0.348287355176195\n",
            "Cost after 299633 iterations : Training Loss =  0.3129868523359199; Validation Loss = 0.3482876030265025\n",
            "Cost after 299634 iterations : Training Loss =  0.31298698367434236; Validation Loss = 0.3482878508768103\n",
            "Cost after 299635 iterations : Training Loss =  0.3129868987362164; Validation Loss = 0.3482873742877894\n",
            "Cost after 299636 iterations : Training Loss =  0.3129868844054042; Validation Loss = 0.348287622138097\n",
            "Cost after 299637 iterations : Training Loss =  0.31298694513651304; Validation Loss = 0.34828714554907597\n",
            "Cost after 299638 iterations : Training Loss =  0.3129867851364665; Validation Loss = 0.3482873933993836\n",
            "Cost after 299639 iterations : Training Loss =  0.3129869915368097; Validation Loss = 0.3482869168103626\n",
            "Cost after 299640 iterations : Training Loss =  0.31298682423184787; Validation Loss = 0.3482871646606705\n",
            "Cost after 299641 iterations : Training Loss =  0.31298689957278686; Validation Loss = 0.34828741251097795\n",
            "Cost after 299642 iterations : Training Loss =  0.3129868706321445; Validation Loss = 0.3482869359219572\n",
            "Cost after 299643 iterations : Training Loss =  0.31298680030384896; Validation Loss = 0.34828718377226436\n",
            "Cost after 299644 iterations : Training Loss =  0.3129869170324412; Validation Loss = 0.3482867071832438\n",
            "Cost after 299645 iterations : Training Loss =  0.3129867497274796; Validation Loss = 0.3482869550335515\n",
            "Cost after 299646 iterations : Training Loss =  0.3129869147401695; Validation Loss = 0.348287202883859\n",
            "Cost after 299647 iterations : Training Loss =  0.3129867961277762; Validation Loss = 0.34828672629483787\n",
            "Cost after 299648 iterations : Training Loss =  0.31298681547123147; Validation Loss = 0.34828697414514564\n",
            "Cost after 299649 iterations : Training Loss =  0.31298684252807285; Validation Loss = 0.34828649755612473\n",
            "Cost after 299650 iterations : Training Loss =  0.3129867162022938; Validation Loss = 0.3482867454064325\n",
            "Cost after 299651 iterations : Training Loss =  0.31298688892836934; Validation Loss = 0.3482862688174116\n",
            "Cost after 299652 iterations : Training Loss =  0.31298672162340774; Validation Loss = 0.34828651666771876\n",
            "Cost after 299653 iterations : Training Loss =  0.3129868306386144; Validation Loss = 0.34828676451802637\n",
            "Cost after 299654 iterations : Training Loss =  0.3129867680237043; Validation Loss = 0.34828628792900573\n",
            "Cost after 299655 iterations : Training Loss =  0.3129867313696764; Validation Loss = 0.3482865357793134\n",
            "Cost after 299656 iterations : Training Loss =  0.31298681442400106; Validation Loss = 0.3482860591902929\n",
            "Cost after 299657 iterations : Training Loss =  0.3129866471190392; Validation Loss = 0.3482863070405999\n",
            "Cost after 299658 iterations : Training Loss =  0.31298684580599717; Validation Loss = 0.3482865548909072\n",
            "Cost after 299659 iterations : Training Loss =  0.3129866935193358; Validation Loss = 0.34828607830188674\n",
            "Cost after 299660 iterations : Training Loss =  0.31298674653705916; Validation Loss = 0.34828632615219435\n",
            "Cost after 299661 iterations : Training Loss =  0.31298673991963233; Validation Loss = 0.34828584956317327\n",
            "Cost after 299662 iterations : Training Loss =  0.312986647268121; Validation Loss = 0.34828609741348104\n",
            "Cost after 299663 iterations : Training Loss =  0.31298678631992893; Validation Loss = 0.3482856208244603\n",
            "Cost after 299664 iterations : Training Loss =  0.31298661901496744; Validation Loss = 0.3482858686747677\n",
            "Cost after 299665 iterations : Training Loss =  0.31298676170444173; Validation Loss = 0.34828611652507524\n",
            "Cost after 299666 iterations : Training Loss =  0.31298666541526404; Validation Loss = 0.34828563993605416\n",
            "Cost after 299667 iterations : Training Loss =  0.3129866624355039; Validation Loss = 0.34828588778636205\n",
            "Cost after 299668 iterations : Training Loss =  0.3129867118155606; Validation Loss = 0.3482854111973409\n",
            "Cost after 299669 iterations : Training Loss =  0.3129865631665659; Validation Loss = 0.3482856590476485\n",
            "Cost after 299670 iterations : Training Loss =  0.31298675821585725; Validation Loss = 0.34828518245862755\n",
            "Cost after 299671 iterations : Training Loss =  0.31298659091089553; Validation Loss = 0.34828543030893516\n",
            "Cost after 299672 iterations : Training Loss =  0.3129866776028864; Validation Loss = 0.3482856781592426\n",
            "Cost after 299673 iterations : Training Loss =  0.31298663731119225; Validation Loss = 0.34828520157022175\n",
            "Cost after 299674 iterations : Training Loss =  0.31298657833394855; Validation Loss = 0.34828544942052925\n",
            "Cost after 299675 iterations : Training Loss =  0.3129866837114889; Validation Loss = 0.34828497283150894\n",
            "Cost after 299676 iterations : Training Loss =  0.312986516406527; Validation Loss = 0.3482852206818163\n",
            "Cost after 299677 iterations : Training Loss =  0.3129866927702691; Validation Loss = 0.34828546853212344\n",
            "Cost after 299678 iterations : Training Loss =  0.3129865628068237; Validation Loss = 0.3482849919431027\n",
            "Cost after 299679 iterations : Training Loss =  0.31298659350133123; Validation Loss = 0.3482852397934103\n",
            "Cost after 299680 iterations : Training Loss =  0.3129866092071204; Validation Loss = 0.3482847632043895\n",
            "Cost after 299681 iterations : Training Loss =  0.31298649423239316; Validation Loss = 0.3482850110546973\n",
            "Cost after 299682 iterations : Training Loss =  0.3129866556074172; Validation Loss = 0.3482845344656763\n",
            "Cost after 299683 iterations : Training Loss =  0.31298648830245535; Validation Loss = 0.3482847823159839\n",
            "Cost after 299684 iterations : Training Loss =  0.31298660866871375; Validation Loss = 0.3482850301662914\n",
            "Cost after 299685 iterations : Training Loss =  0.31298653470275184; Validation Loss = 0.3482845535772705\n",
            "Cost after 299686 iterations : Training Loss =  0.31298650939977607; Validation Loss = 0.3482848014275783\n",
            "Cost after 299687 iterations : Training Loss =  0.3129865811030487; Validation Loss = 0.3482843248385572\n",
            "Cost after 299688 iterations : Training Loss =  0.31298641379808695; Validation Loss = 0.3482845726888647\n",
            "Cost after 299689 iterations : Training Loss =  0.3129866238360964; Validation Loss = 0.3482848205391726\n",
            "Cost after 299690 iterations : Training Loss =  0.31298646019838366; Validation Loss = 0.3482843439501514\n",
            "Cost after 299691 iterations : Training Loss =  0.31298652456715853; Validation Loss = 0.3482845918004592\n",
            "Cost after 299692 iterations : Training Loss =  0.31298650659868027; Validation Loss = 0.34828411521143826\n",
            "Cost after 299693 iterations : Training Loss =  0.31298642529822057; Validation Loss = 0.34828436306174554\n",
            "Cost after 299694 iterations : Training Loss =  0.3129865529989769; Validation Loss = 0.3482838864727249\n",
            "Cost after 299695 iterations : Training Loss =  0.312986385694015; Validation Loss = 0.3482841343230325\n",
            "Cost after 299696 iterations : Training Loss =  0.31298653973454066; Validation Loss = 0.34828438217333985\n",
            "Cost after 299697 iterations : Training Loss =  0.31298643209431176; Validation Loss = 0.34828390558431904\n",
            "Cost after 299698 iterations : Training Loss =  0.3129864404656032; Validation Loss = 0.34828415343462693\n",
            "Cost after 299699 iterations : Training Loss =  0.31298647849460853; Validation Loss = 0.34828367684560574\n",
            "Cost after 299700 iterations : Training Loss =  0.3129863411966657; Validation Loss = 0.3482839246959133\n",
            "Cost after 299701 iterations : Training Loss =  0.312986524894905; Validation Loss = 0.3482834481068928\n",
            "Cost after 299702 iterations : Training Loss =  0.31298635758994314; Validation Loss = 0.3482836959572002\n",
            "Cost after 299703 iterations : Training Loss =  0.31298645563298594; Validation Loss = 0.3482839438075078\n",
            "Cost after 299704 iterations : Training Loss =  0.31298640399023986; Validation Loss = 0.3482834672184865\n",
            "Cost after 299705 iterations : Training Loss =  0.3129863563640479; Validation Loss = 0.3482837150687943\n",
            "Cost after 299706 iterations : Training Loss =  0.3129864503905366; Validation Loss = 0.34828323847977344\n",
            "Cost after 299707 iterations : Training Loss =  0.3129862830855748; Validation Loss = 0.3482834863300812\n",
            "Cost after 299708 iterations : Training Loss =  0.31298647080036857; Validation Loss = 0.34828373418038877\n",
            "Cost after 299709 iterations : Training Loss =  0.31298632948587146; Validation Loss = 0.348283257591368\n",
            "Cost after 299710 iterations : Training Loss =  0.3129863715314307; Validation Loss = 0.34828350544167536\n",
            "Cost after 299711 iterations : Training Loss =  0.3129863758861681; Validation Loss = 0.34828302885265444\n",
            "Cost after 299712 iterations : Training Loss =  0.3129862722624929; Validation Loss = 0.3482832767029619\n",
            "Cost after 299713 iterations : Training Loss =  0.3129864222864647; Validation Loss = 0.3482828001139416\n",
            "Cost after 299714 iterations : Training Loss =  0.31298625498150284; Validation Loss = 0.34828304796424886\n",
            "Cost after 299715 iterations : Training Loss =  0.3129863866988134; Validation Loss = 0.3482832958145561\n",
            "Cost after 299716 iterations : Training Loss =  0.3129863013817997; Validation Loss = 0.3482828192255353\n",
            "Cost after 299717 iterations : Training Loss =  0.3129862874298753; Validation Loss = 0.34828306707584306\n",
            "Cost after 299718 iterations : Training Loss =  0.3129863477820964; Validation Loss = 0.3482825904868225\n",
            "Cost after 299719 iterations : Training Loss =  0.3129861881609373; Validation Loss = 0.3482828383371298\n",
            "Cost after 299720 iterations : Training Loss =  0.312986394182393; Validation Loss = 0.34828236174810884\n",
            "Cost after 299721 iterations : Training Loss =  0.3129862268774312; Validation Loss = 0.3482826095984167\n",
            "Cost after 299722 iterations : Training Loss =  0.31298630259725796; Validation Loss = 0.34828285744872406\n",
            "Cost after 299723 iterations : Training Loss =  0.3129862732777276; Validation Loss = 0.3482823808597034\n",
            "Cost after 299724 iterations : Training Loss =  0.3129862033283201; Validation Loss = 0.34828262871001076\n",
            "Cost after 299725 iterations : Training Loss =  0.3129863196780245; Validation Loss = 0.34828215212098995\n",
            "Cost after 299726 iterations : Training Loss =  0.3129861523730626; Validation Loss = 0.34828239997129756\n",
            "Cost after 299727 iterations : Training Loss =  0.3129863177646408; Validation Loss = 0.34828264782160484\n",
            "Cost after 299728 iterations : Training Loss =  0.31298619877335954; Validation Loss = 0.3482821712325843\n",
            "Cost after 299729 iterations : Training Loss =  0.3129862184957028; Validation Loss = 0.3482824190828914\n",
            "Cost after 299730 iterations : Training Loss =  0.3129862451736559; Validation Loss = 0.3482819424938705\n",
            "Cost after 299731 iterations : Training Loss =  0.31298611922676495; Validation Loss = 0.3482821903441783\n",
            "Cost after 299732 iterations : Training Loss =  0.3129862915739527; Validation Loss = 0.34828171375515754\n",
            "Cost after 299733 iterations : Training Loss =  0.312986124268991; Validation Loss = 0.34828196160546526\n",
            "Cost after 299734 iterations : Training Loss =  0.3129862336630853; Validation Loss = 0.3482822094557726\n",
            "Cost after 299735 iterations : Training Loss =  0.3129861706692875; Validation Loss = 0.3482817328667517\n",
            "Cost after 299736 iterations : Training Loss =  0.3129861343941475; Validation Loss = 0.34828198071705935\n",
            "Cost after 299737 iterations : Training Loss =  0.31298621706958424; Validation Loss = 0.3482815041280383\n",
            "Cost after 299738 iterations : Training Loss =  0.31298604976462246; Validation Loss = 0.348281751978346\n",
            "Cost after 299739 iterations : Training Loss =  0.31298624883046783; Validation Loss = 0.3482819998286537\n",
            "Cost after 299740 iterations : Training Loss =  0.31298609616491924; Validation Loss = 0.3482815232396325\n",
            "Cost after 299741 iterations : Training Loss =  0.31298614956153026; Validation Loss = 0.3482817710899403\n",
            "Cost after 299742 iterations : Training Loss =  0.31298614256521573; Validation Loss = 0.34828129450091966\n",
            "Cost after 299743 iterations : Training Loss =  0.3129860502925922; Validation Loss = 0.34828154235122727\n",
            "Cost after 299744 iterations : Training Loss =  0.3129861889655124; Validation Loss = 0.3482810657622062\n",
            "Cost after 299745 iterations : Training Loss =  0.312986021660551; Validation Loss = 0.3482813136125136\n",
            "Cost after 299746 iterations : Training Loss =  0.3129861647289129; Validation Loss = 0.3482815614628214\n",
            "Cost after 299747 iterations : Training Loss =  0.3129860680608471; Validation Loss = 0.34828108487380066\n",
            "Cost after 299748 iterations : Training Loss =  0.312986065459975; Validation Loss = 0.3482813327241083\n",
            "Cost after 299749 iterations : Training Loss =  0.31298611446114394; Validation Loss = 0.3482808561350871\n",
            "Cost after 299750 iterations : Training Loss =  0.312985966191037; Validation Loss = 0.34828110398539464\n",
            "Cost after 299751 iterations : Training Loss =  0.3129861608614407; Validation Loss = 0.3482806273963738\n",
            "Cost after 299752 iterations : Training Loss =  0.3129859935564791; Validation Loss = 0.3482808752466815\n",
            "Cost after 299753 iterations : Training Loss =  0.3129860806273577; Validation Loss = 0.34828112309698883\n",
            "Cost after 299754 iterations : Training Loss =  0.3129860399567753; Validation Loss = 0.348280646507968\n",
            "Cost after 299755 iterations : Training Loss =  0.3129859813584196; Validation Loss = 0.3482808943582753\n",
            "Cost after 299756 iterations : Training Loss =  0.31298608635707226; Validation Loss = 0.3482804177692546\n",
            "Cost after 299757 iterations : Training Loss =  0.3129859190521103; Validation Loss = 0.34828066561956234\n",
            "Cost after 299758 iterations : Training Loss =  0.31298609579474035; Validation Loss = 0.34828091346986995\n",
            "Cost after 299759 iterations : Training Loss =  0.3129859654524071; Validation Loss = 0.34828043688084903\n",
            "Cost after 299760 iterations : Training Loss =  0.3129859965258023; Validation Loss = 0.34828068473115653\n",
            "Cost after 299761 iterations : Training Loss =  0.3129860118527039; Validation Loss = 0.3482802081421357\n",
            "Cost after 299762 iterations : Training Loss =  0.31298589725686443; Validation Loss = 0.34828045599244334\n",
            "Cost after 299763 iterations : Training Loss =  0.31298605825300047; Validation Loss = 0.3482799794034228\n",
            "Cost after 299764 iterations : Training Loss =  0.3129858909480387; Validation Loss = 0.34828022725373\n",
            "Cost after 299765 iterations : Training Loss =  0.31298601169318474; Validation Loss = 0.34828047510403737\n",
            "Cost after 299766 iterations : Training Loss =  0.31298593734833524; Validation Loss = 0.34827999851501673\n",
            "Cost after 299767 iterations : Training Loss =  0.3129859124242471; Validation Loss = 0.3482802463653242\n",
            "Cost after 299768 iterations : Training Loss =  0.3129859837486321; Validation Loss = 0.3482797697763035\n",
            "Cost after 299769 iterations : Training Loss =  0.31298581644367035; Validation Loss = 0.34828001762661087\n",
            "Cost after 299770 iterations : Training Loss =  0.3129860268605675; Validation Loss = 0.34828026547691865\n",
            "Cost after 299771 iterations : Training Loss =  0.31298586284396684; Validation Loss = 0.34827978888789746\n",
            "Cost after 299772 iterations : Training Loss =  0.3129859275916296; Validation Loss = 0.3482800367382054\n",
            "Cost after 299773 iterations : Training Loss =  0.31298590924426356; Validation Loss = 0.34827956014918454\n",
            "Cost after 299774 iterations : Training Loss =  0.3129858283226915; Validation Loss = 0.3482798079994919\n",
            "Cost after 299775 iterations : Training Loss =  0.3129859556445603; Validation Loss = 0.3482793314104707\n",
            "Cost after 299776 iterations : Training Loss =  0.3129857883395984; Validation Loss = 0.3482795792607783\n",
            "Cost after 299777 iterations : Training Loss =  0.31298594275901215; Validation Loss = 0.3482798271110862\n",
            "Cost after 299778 iterations : Training Loss =  0.3129858347398949; Validation Loss = 0.34827935052206555\n",
            "Cost after 299779 iterations : Training Loss =  0.31298584349007447; Validation Loss = 0.3482795983723727\n",
            "Cost after 299780 iterations : Training Loss =  0.3129858811401917; Validation Loss = 0.34827912178335213\n",
            "Cost after 299781 iterations : Training Loss =  0.3129857442211365; Validation Loss = 0.3482793696336594\n",
            "Cost after 299782 iterations : Training Loss =  0.31298592754048843; Validation Loss = 0.34827889304463844\n",
            "Cost after 299783 iterations : Training Loss =  0.31298576023552654; Validation Loss = 0.3482791408949461\n",
            "Cost after 299784 iterations : Training Loss =  0.312985858657457; Validation Loss = 0.3482793887452533\n",
            "Cost after 299785 iterations : Training Loss =  0.31298580663582315; Validation Loss = 0.3482789121562325\n",
            "Cost after 299786 iterations : Training Loss =  0.31298575938851925; Validation Loss = 0.3482791600065403\n",
            "Cost after 299787 iterations : Training Loss =  0.3129858530361199; Validation Loss = 0.3482786834175194\n",
            "Cost after 299788 iterations : Training Loss =  0.3129856857311583; Validation Loss = 0.34827893126782755\n",
            "Cost after 299789 iterations : Training Loss =  0.31298587382483967; Validation Loss = 0.3482791791181346\n",
            "Cost after 299790 iterations : Training Loss =  0.3129857321314547; Validation Loss = 0.3482787025291139\n",
            "Cost after 299791 iterations : Training Loss =  0.31298577455590193; Validation Loss = 0.34827895037942136\n",
            "Cost after 299792 iterations : Training Loss =  0.3129857785317514; Validation Loss = 0.3482784737904008\n",
            "Cost after 299793 iterations : Training Loss =  0.3129856752869638; Validation Loss = 0.3482787216407084\n",
            "Cost after 299794 iterations : Training Loss =  0.3129858249320479; Validation Loss = 0.3482782450516875\n",
            "Cost after 299795 iterations : Training Loss =  0.3129856576270864; Validation Loss = 0.3482784929019951\n",
            "Cost after 299796 iterations : Training Loss =  0.31298578972328434; Validation Loss = 0.3482787407523026\n",
            "Cost after 299797 iterations : Training Loss =  0.312985704027383; Validation Loss = 0.3482782641632818\n",
            "Cost after 299798 iterations : Training Loss =  0.3129856904543467; Validation Loss = 0.34827851201358895\n",
            "Cost after 299799 iterations : Training Loss =  0.31298575042767957; Validation Loss = 0.348278035424568\n",
            "Cost after 299800 iterations : Training Loss =  0.3129855911854086; Validation Loss = 0.34827828327487576\n",
            "Cost after 299801 iterations : Training Loss =  0.3129857968279762; Validation Loss = 0.34827780668585506\n",
            "Cost after 299802 iterations : Training Loss =  0.3129856295230146; Validation Loss = 0.3482780545361627\n",
            "Cost after 299803 iterations : Training Loss =  0.3129857056217293; Validation Loss = 0.3482783023864704\n",
            "Cost after 299804 iterations : Training Loss =  0.31298567592331134; Validation Loss = 0.3482778257974495\n",
            "Cost after 299805 iterations : Training Loss =  0.31298560635279116; Validation Loss = 0.3482780736477569\n",
            "Cost after 299806 iterations : Training Loss =  0.31298572232360794; Validation Loss = 0.3482775970587362\n",
            "Cost after 299807 iterations : Training Loss =  0.3129855550186462; Validation Loss = 0.3482778449090436\n",
            "Cost after 299808 iterations : Training Loss =  0.3129857207891118; Validation Loss = 0.34827809275935134\n",
            "Cost after 299809 iterations : Training Loss =  0.31298560141894255; Validation Loss = 0.34827761617033026\n",
            "Cost after 299810 iterations : Training Loss =  0.3129856215201739; Validation Loss = 0.3482778640206378\n",
            "Cost after 299811 iterations : Training Loss =  0.3129856478192395; Validation Loss = 0.3482773874316172\n",
            "Cost after 299812 iterations : Training Loss =  0.3129855222512361; Validation Loss = 0.34827763528192435\n",
            "Cost after 299813 iterations : Training Loss =  0.31298569421953626; Validation Loss = 0.3482771586929039\n",
            "Cost after 299814 iterations : Training Loss =  0.3129855269145743; Validation Loss = 0.3482774065432112\n",
            "Cost after 299815 iterations : Training Loss =  0.31298563668755647; Validation Loss = 0.3482776543935188\n",
            "Cost after 299816 iterations : Training Loss =  0.31298557331487087; Validation Loss = 0.34827717780449774\n",
            "Cost after 299817 iterations : Training Loss =  0.31298553741861856; Validation Loss = 0.3482774256548056\n",
            "Cost after 299818 iterations : Training Loss =  0.3129856197151675; Validation Loss = 0.34827694906578466\n",
            "Cost after 299819 iterations : Training Loss =  0.31298545241020576; Validation Loss = 0.3482771969160923\n",
            "Cost after 299820 iterations : Training Loss =  0.3129856518549391; Validation Loss = 0.3482774447663995\n",
            "Cost after 299821 iterations : Training Loss =  0.3129854988105027; Validation Loss = 0.34827696817737874\n",
            "Cost after 299822 iterations : Training Loss =  0.3129855525860013; Validation Loss = 0.34827721602768646\n",
            "Cost after 299823 iterations : Training Loss =  0.31298554521079924; Validation Loss = 0.3482767394386659\n",
            "Cost after 299824 iterations : Training Loss =  0.3129854533170633; Validation Loss = 0.3482769872889733\n",
            "Cost after 299825 iterations : Training Loss =  0.3129855916110958; Validation Loss = 0.3482765106999523\n",
            "Cost after 299826 iterations : Training Loss =  0.3129854243061342; Validation Loss = 0.3482767585502596\n",
            "Cost after 299827 iterations : Training Loss =  0.3129855677533837; Validation Loss = 0.34827700640056763\n",
            "Cost after 299828 iterations : Training Loss =  0.3129854707064307; Validation Loss = 0.348276529811546\n",
            "Cost after 299829 iterations : Training Loss =  0.3129854684844457; Validation Loss = 0.3482767776618538\n",
            "Cost after 299830 iterations : Training Loss =  0.31298551710672734; Validation Loss = 0.34827630107283286\n",
            "Cost after 299831 iterations : Training Loss =  0.3129853692155081; Validation Loss = 0.3482765489231406\n",
            "Cost after 299832 iterations : Training Loss =  0.31298556350702406; Validation Loss = 0.34827607233411984\n",
            "Cost after 299833 iterations : Training Loss =  0.3129853962020623; Validation Loss = 0.34827632018442756\n",
            "Cost after 299834 iterations : Training Loss =  0.31298548365182854; Validation Loss = 0.348276568034735\n",
            "Cost after 299835 iterations : Training Loss =  0.312985442602359; Validation Loss = 0.3482760914457142\n",
            "Cost after 299836 iterations : Training Loss =  0.31298538438289064; Validation Loss = 0.3482763392960218\n",
            "Cost after 299837 iterations : Training Loss =  0.3129854890026555; Validation Loss = 0.34827586270700056\n",
            "Cost after 299838 iterations : Training Loss =  0.3129853216976939; Validation Loss = 0.3482761105573081\n",
            "Cost after 299839 iterations : Training Loss =  0.3129854988192114; Validation Loss = 0.3482763584076159\n",
            "Cost after 299840 iterations : Training Loss =  0.3129853680979904; Validation Loss = 0.348275881818595\n",
            "Cost after 299841 iterations : Training Loss =  0.3129853995502735; Validation Loss = 0.3482761296689026\n",
            "Cost after 299842 iterations : Training Loss =  0.31298541449828693; Validation Loss = 0.3482756530798818\n",
            "Cost after 299843 iterations : Training Loss =  0.31298530028133525; Validation Loss = 0.348275900930189\n",
            "Cost after 299844 iterations : Training Loss =  0.3129854608985839; Validation Loss = 0.34827542434116815\n",
            "Cost after 299845 iterations : Training Loss =  0.31298529359362215; Validation Loss = 0.348275672191476\n",
            "Cost after 299846 iterations : Training Loss =  0.3129854147176559; Validation Loss = 0.34827592004178326\n",
            "Cost after 299847 iterations : Training Loss =  0.3129853399939188; Validation Loss = 0.3482754434527628\n",
            "Cost after 299848 iterations : Training Loss =  0.3129853154487184; Validation Loss = 0.3482756913030703\n",
            "Cost after 299849 iterations : Training Loss =  0.31298538639421514; Validation Loss = 0.3482752147140494\n",
            "Cost after 299850 iterations : Training Loss =  0.3129852190892535; Validation Loss = 0.348275462564357\n",
            "Cost after 299851 iterations : Training Loss =  0.3129854298850391; Validation Loss = 0.3482757104146647\n",
            "Cost after 299852 iterations : Training Loss =  0.31298526548955014; Validation Loss = 0.3482752338256435\n",
            "Cost after 299853 iterations : Training Loss =  0.3129853306161005; Validation Loss = 0.34827548167595107\n",
            "Cost after 299854 iterations : Training Loss =  0.3129853118898469; Validation Loss = 0.34827500508693054\n",
            "Cost after 299855 iterations : Training Loss =  0.31298523134716294; Validation Loss = 0.34827525293723793\n",
            "Cost after 299856 iterations : Training Loss =  0.3129853582901437; Validation Loss = 0.34827477634821746\n",
            "Cost after 299857 iterations : Training Loss =  0.31298519098518185; Validation Loss = 0.34827502419852463\n",
            "Cost after 299858 iterations : Training Loss =  0.31298534578348325; Validation Loss = 0.3482752720488322\n",
            "Cost after 299859 iterations : Training Loss =  0.3129852373854783; Validation Loss = 0.3482747954598112\n",
            "Cost after 299860 iterations : Training Loss =  0.31298524651454557; Validation Loss = 0.3482750433101191\n",
            "Cost after 299861 iterations : Training Loss =  0.31298528378577517; Validation Loss = 0.348274566721098\n",
            "Cost after 299862 iterations : Training Loss =  0.3129851472456074; Validation Loss = 0.3482748145714056\n",
            "Cost after 299863 iterations : Training Loss =  0.31298533018607166; Validation Loss = 0.34827433798238455\n",
            "Cost after 299864 iterations : Training Loss =  0.31298516288111006; Validation Loss = 0.34827458583269205\n",
            "Cost after 299865 iterations : Training Loss =  0.31298526168192814; Validation Loss = 0.3482748336830001\n",
            "Cost after 299866 iterations : Training Loss =  0.31298520928140666; Validation Loss = 0.3482743570939793\n",
            "Cost after 299867 iterations : Training Loss =  0.3129851624129901; Validation Loss = 0.3482746049442865\n",
            "Cost after 299868 iterations : Training Loss =  0.3129852556817033; Validation Loss = 0.3482741283552657\n",
            "Cost after 299869 iterations : Training Loss =  0.31298508837674144; Validation Loss = 0.34827437620557306\n",
            "Cost after 299870 iterations : Training Loss =  0.3129852768493106; Validation Loss = 0.34827462405588067\n",
            "Cost after 299871 iterations : Training Loss =  0.3129851347770379; Validation Loss = 0.3482741474668604\n",
            "Cost after 299872 iterations : Training Loss =  0.3129851775803729; Validation Loss = 0.3482743953171674\n",
            "Cost after 299873 iterations : Training Loss =  0.3129851811773346; Validation Loss = 0.3482739187281466\n",
            "Cost after 299874 iterations : Training Loss =  0.3129850783114347; Validation Loss = 0.3482741665784545\n",
            "Cost after 299875 iterations : Training Loss =  0.31298522757763136; Validation Loss = 0.3482736899894336\n",
            "Cost after 299876 iterations : Training Loss =  0.31298506027266976; Validation Loss = 0.3482739378397413\n",
            "Cost after 299877 iterations : Training Loss =  0.31298519274775566; Validation Loss = 0.34827418569004875\n",
            "Cost after 299878 iterations : Training Loss =  0.31298510667296653; Validation Loss = 0.34827370910102756\n",
            "Cost after 299879 iterations : Training Loss =  0.3129850934788175; Validation Loss = 0.34827395695133506\n",
            "Cost after 299880 iterations : Training Loss =  0.3129851530732631; Validation Loss = 0.3482734803623145\n",
            "Cost after 299881 iterations : Training Loss =  0.3129849942098798; Validation Loss = 0.34827372821262226\n",
            "Cost after 299882 iterations : Training Loss =  0.3129851994735597; Validation Loss = 0.3482732516236013\n",
            "Cost after 299883 iterations : Training Loss =  0.312985032168598; Validation Loss = 0.34827349947390873\n",
            "Cost after 299884 iterations : Training Loss =  0.31298510864620044; Validation Loss = 0.3482737473242159\n",
            "Cost after 299885 iterations : Training Loss =  0.31298507856889457; Validation Loss = 0.3482732707351954\n",
            "Cost after 299886 iterations : Training Loss =  0.312985009377262; Validation Loss = 0.34827351858550315\n",
            "Cost after 299887 iterations : Training Loss =  0.31298512496919123; Validation Loss = 0.3482730419964821\n",
            "Cost after 299888 iterations : Training Loss =  0.3129849576642294; Validation Loss = 0.34827328984678957\n",
            "Cost after 299889 iterations : Training Loss =  0.3129851238135827; Validation Loss = 0.34827353769709724\n",
            "Cost after 299890 iterations : Training Loss =  0.31298500406452595; Validation Loss = 0.3482730611080762\n",
            "Cost after 299891 iterations : Training Loss =  0.3129850245446449; Validation Loss = 0.3482733089583839\n",
            "Cost after 299892 iterations : Training Loss =  0.31298505046482283; Validation Loss = 0.34827283236936313\n",
            "Cost after 299893 iterations : Training Loss =  0.31298492527570704; Validation Loss = 0.3482730802196707\n",
            "Cost after 299894 iterations : Training Loss =  0.31298509686511916; Validation Loss = 0.34827260363064994\n",
            "Cost after 299895 iterations : Training Loss =  0.31298492956015767; Validation Loss = 0.3482728514809571\n",
            "Cost after 299896 iterations : Training Loss =  0.3129850397120274; Validation Loss = 0.3482730993312649\n",
            "Cost after 299897 iterations : Training Loss =  0.3129849759604544; Validation Loss = 0.34827262274224424\n",
            "Cost after 299898 iterations : Training Loss =  0.31298494044308944; Validation Loss = 0.34827287059255146\n",
            "Cost after 299899 iterations : Training Loss =  0.3129850223607509; Validation Loss = 0.3482723940035309\n",
            "Cost after 299900 iterations : Training Loss =  0.31298485505578927; Validation Loss = 0.3482726418538381\n",
            "Cost after 299901 iterations : Training Loss =  0.3129850548794102; Validation Loss = 0.3482728897041457\n",
            "Cost after 299902 iterations : Training Loss =  0.3129849014560859; Validation Loss = 0.3482724131151248\n",
            "Cost after 299903 iterations : Training Loss =  0.31298495561047235; Validation Loss = 0.34827266096543236\n",
            "Cost after 299904 iterations : Training Loss =  0.31298494785638237; Validation Loss = 0.34827218437641166\n",
            "Cost after 299905 iterations : Training Loss =  0.3129848563415344; Validation Loss = 0.3482724322267193\n",
            "Cost after 299906 iterations : Training Loss =  0.31298499425667925; Validation Loss = 0.34827195563769836\n",
            "Cost after 299907 iterations : Training Loss =  0.3129848269517176; Validation Loss = 0.34827220348800586\n",
            "Cost after 299908 iterations : Training Loss =  0.3129849707778549; Validation Loss = 0.34827245133831336\n",
            "Cost after 299909 iterations : Training Loss =  0.312984873352014; Validation Loss = 0.3482719747492925\n",
            "Cost after 299910 iterations : Training Loss =  0.31298487150891713; Validation Loss = 0.3482722225996004\n",
            "Cost after 299911 iterations : Training Loss =  0.31298491975231074; Validation Loss = 0.3482717460105795\n",
            "Cost after 299912 iterations : Training Loss =  0.31298477223997917; Validation Loss = 0.34827199386088664\n",
            "Cost after 299913 iterations : Training Loss =  0.31298496615260724; Validation Loss = 0.3482715172718657\n",
            "Cost after 299914 iterations : Training Loss =  0.3129847988476456; Validation Loss = 0.34827176512217334\n",
            "Cost after 299915 iterations : Training Loss =  0.3129848866762997; Validation Loss = 0.34827201297248095\n",
            "Cost after 299916 iterations : Training Loss =  0.3129848452479421; Validation Loss = 0.3482715363834604\n",
            "Cost after 299917 iterations : Training Loss =  0.31298478740736185; Validation Loss = 0.3482717842337674\n",
            "Cost after 299918 iterations : Training Loss =  0.3129848916482388; Validation Loss = 0.3482713076447469\n",
            "Cost after 299919 iterations : Training Loss =  0.31298472434327734; Validation Loss = 0.34827155549505434\n",
            "Cost after 299920 iterations : Training Loss =  0.3129849018436822; Validation Loss = 0.3482718033453618\n",
            "Cost after 299921 iterations : Training Loss =  0.3129847707435737; Validation Loss = 0.3482713267563412\n",
            "Cost after 299922 iterations : Training Loss =  0.31298480257474426; Validation Loss = 0.3482715746066485\n",
            "Cost after 299923 iterations : Training Loss =  0.31298481714387044; Validation Loss = 0.34827109801762746\n",
            "Cost after 299924 iterations : Training Loss =  0.3129847033058064; Validation Loss = 0.34827134586793507\n",
            "Cost after 299925 iterations : Training Loss =  0.31298486354416705; Validation Loss = 0.3482708692789146\n",
            "Cost after 299926 iterations : Training Loss =  0.31298469623920533; Validation Loss = 0.3482711171292221\n",
            "Cost after 299927 iterations : Training Loss =  0.3129848177421271; Validation Loss = 0.34827136497952965\n",
            "Cost after 299928 iterations : Training Loss =  0.3129847426395021; Validation Loss = 0.34827088839050874\n",
            "Cost after 299929 iterations : Training Loss =  0.3129847184731892; Validation Loss = 0.34827113624081607\n",
            "Cost after 299930 iterations : Training Loss =  0.3129847890397987; Validation Loss = 0.3482706596517957\n",
            "Cost after 299931 iterations : Training Loss =  0.3129846217348367; Validation Loss = 0.3482709075021033\n",
            "Cost after 299932 iterations : Training Loss =  0.3129848329095097; Validation Loss = 0.34827115535241066\n",
            "Cost after 299933 iterations : Training Loss =  0.31298466813513337; Validation Loss = 0.3482706787633896\n",
            "Cost after 299934 iterations : Training Loss =  0.312984733640572; Validation Loss = 0.34827092661369724\n",
            "Cost after 299935 iterations : Training Loss =  0.31298471453543025; Validation Loss = 0.3482704500246765\n",
            "Cost after 299936 iterations : Training Loss =  0.312984634371634; Validation Loss = 0.34827069787498416\n",
            "Cost after 299937 iterations : Training Loss =  0.3129847609357267; Validation Loss = 0.3482702212859634\n",
            "Cost after 299938 iterations : Training Loss =  0.3129845936307649; Validation Loss = 0.3482704691362706\n",
            "Cost after 299939 iterations : Training Loss =  0.3129847488079544; Validation Loss = 0.3482707169865782\n",
            "Cost after 299940 iterations : Training Loss =  0.31298464003106174; Validation Loss = 0.3482702403975573\n",
            "Cost after 299941 iterations : Training Loss =  0.31298464953901645; Validation Loss = 0.34827048824786533\n",
            "Cost after 299942 iterations : Training Loss =  0.3129846864313584; Validation Loss = 0.34827001165884414\n",
            "Cost after 299943 iterations : Training Loss =  0.31298455027007843; Validation Loss = 0.3482702595091522\n",
            "Cost after 299944 iterations : Training Loss =  0.3129847328316549; Validation Loss = 0.3482697829201307\n",
            "Cost after 299945 iterations : Training Loss =  0.31298456552669346; Validation Loss = 0.34827003077043855\n",
            "Cost after 299946 iterations : Training Loss =  0.31298466470639924; Validation Loss = 0.3482702786207459\n",
            "Cost after 299947 iterations : Training Loss =  0.31298461192698995; Validation Loss = 0.34826980203172525\n",
            "Cost after 299948 iterations : Training Loss =  0.3129845654374614; Validation Loss = 0.3482700498820327\n",
            "Cost after 299949 iterations : Training Loss =  0.3129846583272868; Validation Loss = 0.3482695732930121\n",
            "Cost after 299950 iterations : Training Loss =  0.31298449102232484; Validation Loss = 0.34826982114331956\n",
            "Cost after 299951 iterations : Training Loss =  0.31298467987378176; Validation Loss = 0.348270068993627\n",
            "Cost after 299952 iterations : Training Loss =  0.3129845374226214; Validation Loss = 0.3482695924046062\n",
            "Cost after 299953 iterations : Training Loss =  0.31298458060484385; Validation Loss = 0.3482698402549139\n",
            "Cost after 299954 iterations : Training Loss =  0.3129845838229177; Validation Loss = 0.34826936366589306\n",
            "Cost after 299955 iterations : Training Loss =  0.3129844813359061; Validation Loss = 0.3482696115162004\n",
            "Cost after 299956 iterations : Training Loss =  0.31298463022321477; Validation Loss = 0.34826913492717976\n",
            "Cost after 299957 iterations : Training Loss =  0.3129844629182529; Validation Loss = 0.34826938277748726\n",
            "Cost after 299958 iterations : Training Loss =  0.31298459577222637; Validation Loss = 0.3482696306277944\n",
            "Cost after 299959 iterations : Training Loss =  0.3129845093185497; Validation Loss = 0.3482691540387738\n",
            "Cost after 299960 iterations : Training Loss =  0.3129844965032888; Validation Loss = 0.34826940188908156\n",
            "Cost after 299961 iterations : Training Loss =  0.3129845557188463; Validation Loss = 0.34826892530006065\n",
            "Cost after 299962 iterations : Training Loss =  0.31298439723435095; Validation Loss = 0.3482691731503679\n",
            "Cost after 299963 iterations : Training Loss =  0.312984602119143; Validation Loss = 0.34826869656134735\n",
            "Cost after 299964 iterations : Training Loss =  0.3129844348141812; Validation Loss = 0.34826894441165496\n",
            "Cost after 299965 iterations : Training Loss =  0.312984511670671; Validation Loss = 0.3482691922619621\n",
            "Cost after 299966 iterations : Training Loss =  0.3129844812144778; Validation Loss = 0.3482687156729412\n",
            "Cost after 299967 iterations : Training Loss =  0.3129844124017335; Validation Loss = 0.3482689635232489\n",
            "Cost after 299968 iterations : Training Loss =  0.3129845276147745; Validation Loss = 0.34826848693422835\n",
            "Cost after 299969 iterations : Training Loss =  0.3129843603098129; Validation Loss = 0.3482687347845356\n",
            "Cost after 299970 iterations : Training Loss =  0.31298452683805394; Validation Loss = 0.34826898263484346\n",
            "Cost after 299971 iterations : Training Loss =  0.31298440671010935; Validation Loss = 0.34826850604582227\n",
            "Cost after 299972 iterations : Training Loss =  0.31298442756911576; Validation Loss = 0.3482687538961301\n",
            "Cost after 299973 iterations : Training Loss =  0.3129844531104059; Validation Loss = 0.34826827730710874\n",
            "Cost after 299974 iterations : Training Loss =  0.31298432830017797; Validation Loss = 0.34826852515741635\n",
            "Cost after 299975 iterations : Training Loss =  0.3129844995107025; Validation Loss = 0.3482680485683957\n",
            "Cost after 299976 iterations : Training Loss =  0.31298433220574107; Validation Loss = 0.3482682964187032\n",
            "Cost after 299977 iterations : Training Loss =  0.3129844427364985; Validation Loss = 0.34826854426901077\n",
            "Cost after 299978 iterations : Training Loss =  0.31298437860603767; Validation Loss = 0.3482680676799899\n",
            "Cost after 299979 iterations : Training Loss =  0.3129843434675607; Validation Loss = 0.3482683155302973\n",
            "Cost after 299980 iterations : Training Loss =  0.31298442500633433; Validation Loss = 0.3482678389412768\n",
            "Cost after 299981 iterations : Training Loss =  0.3129842577013725; Validation Loss = 0.34826808679158416\n",
            "Cost after 299982 iterations : Training Loss =  0.31298445790388146; Validation Loss = 0.34826833464189194\n",
            "Cost after 299983 iterations : Training Loss =  0.312984304101669; Validation Loss = 0.348267858052871\n",
            "Cost after 299984 iterations : Training Loss =  0.3129843586349434; Validation Loss = 0.3482681059031789\n",
            "Cost after 299985 iterations : Training Loss =  0.31298435050196605; Validation Loss = 0.3482676293141573\n",
            "Cost after 299986 iterations : Training Loss =  0.3129842593660057; Validation Loss = 0.34826787716446544\n",
            "Cost after 299987 iterations : Training Loss =  0.3129843969022625; Validation Loss = 0.3482674005754444\n",
            "Cost after 299988 iterations : Training Loss =  0.3129842295973007; Validation Loss = 0.3482676484257521\n",
            "Cost after 299989 iterations : Training Loss =  0.312984373802326; Validation Loss = 0.3482678962760594\n",
            "Cost after 299990 iterations : Training Loss =  0.31298427599759704; Validation Loss = 0.3482674196870387\n",
            "Cost after 299991 iterations : Training Loss =  0.31298427453338823; Validation Loss = 0.3482676675373457\n",
            "Cost after 299992 iterations : Training Loss =  0.3129843223978941; Validation Loss = 0.34826719094832526\n",
            "Cost after 299993 iterations : Training Loss =  0.31298417526445016; Validation Loss = 0.34826743879863287\n",
            "Cost after 299994 iterations : Training Loss =  0.3129843687981908; Validation Loss = 0.3482669622096124\n",
            "Cost after 299995 iterations : Training Loss =  0.3129842014932289; Validation Loss = 0.34826721005991956\n",
            "Cost after 299996 iterations : Training Loss =  0.3129842897007712; Validation Loss = 0.3482674579102269\n",
            "Cost after 299997 iterations : Training Loss =  0.3129842478935253; Validation Loss = 0.34826698132120615\n",
            "Cost after 299998 iterations : Training Loss =  0.31298419043183306; Validation Loss = 0.3482672291715138\n",
            "Cost after 299999 iterations : Training Loss =  0.31298429429382224; Validation Loss = 0.3482667525824931\n",
            "Cost after 300000 iterations : Training Loss =  0.31298412698886025; Validation Loss = 0.34826700043280073\n",
            "Training Complete : min_loss_achieved = 0.3482667525824931; W,B = (4.033558022446692, 11.865072000001538)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-pf_Yd7OZgL",
        "outputId": "f7b495e8-feeb-4c29-f398-e6945fdd8870"
      },
      "source": [
        "# Model-2 Training\n",
        "lin_results.append(lin_train(lin_data,L1,L0,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Cost after 295002 iterations : Training Loss =  0.10151099976374345; Validation Loss = 0.12394418111834485\n",
            "Cost after 295003 iterations : Training Loss =  0.10151099976374343; Validation Loss = 0.1239441811161366\n",
            "Cost after 295004 iterations : Training Loss =  0.1015109997637437; Validation Loss = 0.12394418111392852\n",
            "Cost after 295005 iterations : Training Loss =  0.1015109997637435; Validation Loss = 0.12394418111172095\n",
            "Cost after 295006 iterations : Training Loss =  0.10151099976374353; Validation Loss = 0.1239441811095133\n",
            "Cost after 295007 iterations : Training Loss =  0.10151099976374353; Validation Loss = 0.123944181107306\n",
            "Cost after 295008 iterations : Training Loss =  0.1015109997637435; Validation Loss = 0.12394418110509842\n",
            "Cost after 295009 iterations : Training Loss =  0.1015109997637436; Validation Loss = 0.12394418110289124\n",
            "Cost after 295010 iterations : Training Loss =  0.10151099976374349; Validation Loss = 0.12394418110068403\n",
            "Cost after 295011 iterations : Training Loss =  0.10151099976374334; Validation Loss = 0.123944181098477\n",
            "Cost after 295012 iterations : Training Loss =  0.10151099976374343; Validation Loss = 0.12394418109627006\n",
            "Cost after 295013 iterations : Training Loss =  0.10151099976374348; Validation Loss = 0.123944181094063\n",
            "Cost after 295014 iterations : Training Loss =  0.10151099976374342; Validation Loss = 0.12394418109185618\n",
            "Cost after 295015 iterations : Training Loss =  0.10151099976374367; Validation Loss = 0.12394418108965\n",
            "Cost after 295016 iterations : Training Loss =  0.10151099976374348; Validation Loss = 0.12394418108744303\n",
            "Cost after 295017 iterations : Training Loss =  0.10151099976374334; Validation Loss = 0.12394418108523687\n",
            "Cost after 295018 iterations : Training Loss =  0.10151099976374336; Validation Loss = 0.12394418108303062\n",
            "Cost after 295019 iterations : Training Loss =  0.10151099976374334; Validation Loss = 0.12394418108082451\n",
            "Cost after 295020 iterations : Training Loss =  0.10151099976374361; Validation Loss = 0.1239441810786184\n",
            "Cost after 295021 iterations : Training Loss =  0.10151099976374334; Validation Loss = 0.12394418107641218\n",
            "Cost after 295022 iterations : Training Loss =  0.10151099976374343; Validation Loss = 0.12394418107420674\n",
            "Cost after 295023 iterations : Training Loss =  0.10151099976374331; Validation Loss = 0.1239441810720005\n",
            "Cost after 295024 iterations : Training Loss =  0.1015109997637434; Validation Loss = 0.1239441810697948\n",
            "Cost after 295025 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.12394418106758956\n",
            "Cost after 295026 iterations : Training Loss =  0.10151099976374318; Validation Loss = 0.12394418106538431\n",
            "Cost after 295027 iterations : Training Loss =  0.10151099976374334; Validation Loss = 0.12394418106317899\n",
            "Cost after 295028 iterations : Training Loss =  0.10151099976374342; Validation Loss = 0.12394418106097405\n",
            "Cost after 295029 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.12394418105876857\n",
            "Cost after 295030 iterations : Training Loss =  0.1015109997637434; Validation Loss = 0.12394418105656378\n",
            "Cost after 295031 iterations : Training Loss =  0.10151099976374331; Validation Loss = 0.12394418105435893\n",
            "Cost after 295032 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.1239441810521541\n",
            "Cost after 295033 iterations : Training Loss =  0.10151099976374328; Validation Loss = 0.12394418104994959\n",
            "Cost after 295034 iterations : Training Loss =  0.10151099976374317; Validation Loss = 0.12394418104774504\n",
            "Cost after 295035 iterations : Training Loss =  0.10151099976374334; Validation Loss = 0.12394418104554046\n",
            "Cost after 295036 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.12394418104333615\n",
            "Cost after 295037 iterations : Training Loss =  0.10151099976374299; Validation Loss = 0.1239441810411322\n",
            "Cost after 295038 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.12394418103892826\n",
            "Cost after 295039 iterations : Training Loss =  0.10151099976374331; Validation Loss = 0.12394418103672428\n",
            "Cost after 295040 iterations : Training Loss =  0.10151099976374342; Validation Loss = 0.12394418103452036\n",
            "Cost after 295041 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.12394418103231665\n",
            "Cost after 295042 iterations : Training Loss =  0.1015109997637433; Validation Loss = 0.12394418103011295\n",
            "Cost after 295043 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.12394418102790945\n",
            "Cost after 295044 iterations : Training Loss =  0.10151099976374338; Validation Loss = 0.1239441810257063\n",
            "Cost after 295045 iterations : Training Loss =  0.10151099976374323; Validation Loss = 0.12394418102350294\n",
            "Cost after 295046 iterations : Training Loss =  0.1015109997637431; Validation Loss = 0.12394418102129956\n",
            "Cost after 295047 iterations : Training Loss =  0.10151099976374324; Validation Loss = 0.12394418101909653\n",
            "Cost after 295048 iterations : Training Loss =  0.10151099976374317; Validation Loss = 0.12394418101689395\n",
            "Cost after 295049 iterations : Training Loss =  0.10151099976374321; Validation Loss = 0.12394418101469105\n",
            "Cost after 295050 iterations : Training Loss =  0.1015109997637431; Validation Loss = 0.12394418101248812\n",
            "Cost after 295051 iterations : Training Loss =  0.10151099976374323; Validation Loss = 0.12394418101028569\n",
            "Cost after 295052 iterations : Training Loss =  0.10151099976374334; Validation Loss = 0.12394418100808309\n",
            "Cost after 295053 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.12394418100588077\n",
            "Cost after 295054 iterations : Training Loss =  0.10151099976374316; Validation Loss = 0.12394418100367853\n",
            "Cost after 295055 iterations : Training Loss =  0.1015109997637431; Validation Loss = 0.12394418100147686\n",
            "Cost after 295056 iterations : Training Loss =  0.10151099976374325; Validation Loss = 0.12394418099927436\n",
            "Cost after 295057 iterations : Training Loss =  0.10151099976374324; Validation Loss = 0.12394418099707247\n",
            "Cost after 295058 iterations : Training Loss =  0.1015109997637433; Validation Loss = 0.12394418099487052\n",
            "Cost after 295059 iterations : Training Loss =  0.10151099976374318; Validation Loss = 0.1239441809926689\n",
            "Cost after 295060 iterations : Training Loss =  0.10151099976374328; Validation Loss = 0.12394418099046724\n",
            "Cost after 295061 iterations : Training Loss =  0.10151099976374317; Validation Loss = 0.12394418098826564\n",
            "Cost after 295062 iterations : Training Loss =  0.1015109997637431; Validation Loss = 0.12394418098606406\n",
            "Cost after 295063 iterations : Training Loss =  0.10151099976374313; Validation Loss = 0.12394418098386295\n",
            "Cost after 295064 iterations : Training Loss =  0.101510999763743; Validation Loss = 0.12394418098166204\n",
            "Cost after 295065 iterations : Training Loss =  0.10151099976374311; Validation Loss = 0.12394418097946101\n",
            "Cost after 295066 iterations : Training Loss =  0.10151099976374316; Validation Loss = 0.12394418097726018\n",
            "Cost after 295067 iterations : Training Loss =  0.10151099976374321; Validation Loss = 0.1239441809750592\n",
            "Cost after 295068 iterations : Training Loss =  0.10151099976374309; Validation Loss = 0.12394418097285867\n",
            "Cost after 295069 iterations : Training Loss =  0.10151099976374317; Validation Loss = 0.12394418097065824\n",
            "Cost after 295070 iterations : Training Loss =  0.1015109997637433; Validation Loss = 0.1239441809684574\n",
            "Cost after 295071 iterations : Training Loss =  0.10151099976374323; Validation Loss = 0.12394418096625724\n",
            "Cost after 295072 iterations : Training Loss =  0.10151099976374303; Validation Loss = 0.12394418096405677\n",
            "Cost after 295073 iterations : Training Loss =  0.10151099976374311; Validation Loss = 0.12394418096185686\n",
            "Cost after 295074 iterations : Training Loss =  0.10151099976374311; Validation Loss = 0.12394418095965667\n",
            "Cost after 295075 iterations : Training Loss =  0.10151099976374303; Validation Loss = 0.12394418095745723\n",
            "Cost after 295076 iterations : Training Loss =  0.1015109997637431; Validation Loss = 0.123944180955257\n",
            "Cost after 295077 iterations : Training Loss =  0.10151099976374296; Validation Loss = 0.12394418095305772\n",
            "Cost after 295078 iterations : Training Loss =  0.101510999763743; Validation Loss = 0.12394418095085787\n",
            "Cost after 295079 iterations : Training Loss =  0.10151099976374311; Validation Loss = 0.12394418094865857\n",
            "Cost after 295080 iterations : Training Loss =  0.10151099976374309; Validation Loss = 0.12394418094645895\n",
            "Cost after 295081 iterations : Training Loss =  0.10151099976374309; Validation Loss = 0.12394418094425996\n",
            "Cost after 295082 iterations : Training Loss =  0.10151099976374296; Validation Loss = 0.12394418094206054\n",
            "Cost after 295083 iterations : Training Loss =  0.10151099976374277; Validation Loss = 0.1239441809398617\n",
            "Cost after 295084 iterations : Training Loss =  0.1015109997637428; Validation Loss = 0.12394418093766264\n",
            "Cost after 295085 iterations : Training Loss =  0.10151099976374296; Validation Loss = 0.12394418093546382\n",
            "Cost after 295086 iterations : Training Loss =  0.10151099976374292; Validation Loss = 0.12394418093326508\n",
            "Cost after 295087 iterations : Training Loss =  0.101510999763743; Validation Loss = 0.12394418093106677\n",
            "Cost after 295088 iterations : Training Loss =  0.1015109997637429; Validation Loss = 0.1239441809288682\n",
            "Cost after 295089 iterations : Training Loss =  0.10151099976374287; Validation Loss = 0.12394418092666973\n",
            "Cost after 295090 iterations : Training Loss =  0.10151099976374303; Validation Loss = 0.12394418092447143\n",
            "Cost after 295091 iterations : Training Loss =  0.10151099976374306; Validation Loss = 0.12394418092227348\n",
            "Cost after 295092 iterations : Training Loss =  0.10151099976374289; Validation Loss = 0.1239441809200753\n",
            "Cost after 295093 iterations : Training Loss =  0.10151099976374303; Validation Loss = 0.12394418091787747\n",
            "Cost after 295094 iterations : Training Loss =  0.10151099976374287; Validation Loss = 0.12394418091567957\n",
            "Cost after 295095 iterations : Training Loss =  0.10151099976374305; Validation Loss = 0.12394418091348194\n",
            "Cost after 295096 iterations : Training Loss =  0.10151099976374293; Validation Loss = 0.12394418091128458\n",
            "Cost after 295097 iterations : Training Loss =  0.10151099976374278; Validation Loss = 0.12394418090908703\n",
            "Cost after 295098 iterations : Training Loss =  0.10151099976374274; Validation Loss = 0.12394418090688925\n",
            "Cost after 295099 iterations : Training Loss =  0.101510999763743; Validation Loss = 0.12394418090469224\n",
            "Cost after 295100 iterations : Training Loss =  0.10151099976374296; Validation Loss = 0.12394418090249518\n",
            "Cost after 295101 iterations : Training Loss =  0.10151099976374296; Validation Loss = 0.12394418090029805\n",
            "Cost after 295102 iterations : Training Loss =  0.10151099976374284; Validation Loss = 0.12394418089810116\n",
            "Cost after 295103 iterations : Training Loss =  0.10151099976374281; Validation Loss = 0.12394418089590437\n",
            "Cost after 295104 iterations : Training Loss =  0.10151099976374289; Validation Loss = 0.12394418089370789\n",
            "Cost after 295105 iterations : Training Loss =  0.1015109997637428; Validation Loss = 0.12394418089151124\n",
            "Cost after 295106 iterations : Training Loss =  0.10151099976374292; Validation Loss = 0.12394418088931478\n",
            "Cost after 295107 iterations : Training Loss =  0.1015109997637428; Validation Loss = 0.12394418088711844\n",
            "Cost after 295108 iterations : Training Loss =  0.10151099976374278; Validation Loss = 0.1239441808849221\n",
            "Cost after 295109 iterations : Training Loss =  0.10151099976374278; Validation Loss = 0.1239441808827264\n",
            "Cost after 295110 iterations : Training Loss =  0.1015109997637429; Validation Loss = 0.12394418088052987\n",
            "Cost after 295111 iterations : Training Loss =  0.10151099976374273; Validation Loss = 0.12394418087833418\n",
            "Cost after 295112 iterations : Training Loss =  0.10151099976374285; Validation Loss = 0.12394418087613841\n",
            "Cost after 295113 iterations : Training Loss =  0.10151099976374298; Validation Loss = 0.12394418087394261\n",
            "Cost after 295114 iterations : Training Loss =  0.10151099976374274; Validation Loss = 0.12394418087174729\n",
            "Cost after 295115 iterations : Training Loss =  0.10151099976374277; Validation Loss = 0.12394418086955179\n",
            "Cost after 295116 iterations : Training Loss =  0.10151099976374274; Validation Loss = 0.12394418086735617\n",
            "Cost after 295117 iterations : Training Loss =  0.10151099976374274; Validation Loss = 0.12394418086516104\n",
            "Cost after 295118 iterations : Training Loss =  0.10151099976374264; Validation Loss = 0.12394418086296591\n",
            "Cost after 295119 iterations : Training Loss =  0.10151099976374277; Validation Loss = 0.1239441808607707\n",
            "Cost after 295120 iterations : Training Loss =  0.10151099976374271; Validation Loss = 0.12394418085857606\n",
            "Cost after 295121 iterations : Training Loss =  0.10151099976374268; Validation Loss = 0.12394418085638112\n",
            "Cost after 295122 iterations : Training Loss =  0.101510999763743; Validation Loss = 0.12394418085418639\n",
            "Cost after 295123 iterations : Training Loss =  0.10151099976374281; Validation Loss = 0.12394418085199176\n",
            "Cost after 295124 iterations : Training Loss =  0.10151099976374281; Validation Loss = 0.12394418084979748\n",
            "Cost after 295125 iterations : Training Loss =  0.1015109997637426; Validation Loss = 0.12394418084760345\n",
            "Cost after 295126 iterations : Training Loss =  0.10151099976374268; Validation Loss = 0.12394418084540901\n",
            "Cost after 295127 iterations : Training Loss =  0.10151099976374273; Validation Loss = 0.123944180843215\n",
            "Cost after 295128 iterations : Training Loss =  0.1015109997637429; Validation Loss = 0.12394418084102071\n",
            "Cost after 295129 iterations : Training Loss =  0.10151099976374274; Validation Loss = 0.12394418083882687\n",
            "Cost after 295130 iterations : Training Loss =  0.10151099976374274; Validation Loss = 0.12394418083663311\n",
            "Cost after 295131 iterations : Training Loss =  0.10151099976374287; Validation Loss = 0.12394418083443932\n",
            "Cost after 295132 iterations : Training Loss =  0.1015109997637426; Validation Loss = 0.12394418083224612\n",
            "Cost after 295133 iterations : Training Loss =  0.10151099976374264; Validation Loss = 0.12394418083005286\n",
            "Cost after 295134 iterations : Training Loss =  0.10151099976374268; Validation Loss = 0.1239441808278593\n",
            "Cost after 295135 iterations : Training Loss =  0.10151099976374277; Validation Loss = 0.12394418082566619\n",
            "Cost after 295136 iterations : Training Loss =  0.10151099976374264; Validation Loss = 0.12394418082347303\n",
            "Cost after 295137 iterations : Training Loss =  0.10151099976374267; Validation Loss = 0.12394418082127999\n",
            "Cost after 295138 iterations : Training Loss =  0.10151099976374273; Validation Loss = 0.1239441808190871\n",
            "Cost after 295139 iterations : Training Loss =  0.10151099976374259; Validation Loss = 0.1239441808168941\n",
            "Cost after 295140 iterations : Training Loss =  0.10151099976374266; Validation Loss = 0.12394418081470175\n",
            "Cost after 295141 iterations : Training Loss =  0.10151099976374278; Validation Loss = 0.12394418081250896\n",
            "Cost after 295142 iterations : Training Loss =  0.10151099976374259; Validation Loss = 0.12394418081031684\n",
            "Cost after 295143 iterations : Training Loss =  0.1015109997637426; Validation Loss = 0.12394418080812457\n",
            "Cost after 295144 iterations : Training Loss =  0.10151099976374264; Validation Loss = 0.1239441808059322\n",
            "Cost after 295145 iterations : Training Loss =  0.10151099976374253; Validation Loss = 0.12394418080373974\n",
            "Cost after 295146 iterations : Training Loss =  0.10151099976374277; Validation Loss = 0.12394418080154776\n",
            "Cost after 295147 iterations : Training Loss =  0.10151099976374256; Validation Loss = 0.1239441807993563\n",
            "Cost after 295148 iterations : Training Loss =  0.10151099976374249; Validation Loss = 0.12394418079716438\n",
            "Cost after 295149 iterations : Training Loss =  0.10151099976374253; Validation Loss = 0.12394418079497271\n",
            "Cost after 295150 iterations : Training Loss =  0.10151099976374266; Validation Loss = 0.12394418079278119\n",
            "Cost after 295151 iterations : Training Loss =  0.1015109997637426; Validation Loss = 0.12394418079058983\n",
            "Cost after 295152 iterations : Training Loss =  0.10151099976374248; Validation Loss = 0.1239441807883983\n",
            "Cost after 295153 iterations : Training Loss =  0.10151099976374267; Validation Loss = 0.12394418078620721\n",
            "Cost after 295154 iterations : Training Loss =  0.10151099976374252; Validation Loss = 0.12394418078401591\n",
            "Cost after 295155 iterations : Training Loss =  0.10151099976374253; Validation Loss = 0.12394418078182509\n",
            "Cost after 295156 iterations : Training Loss =  0.10151099976374262; Validation Loss = 0.12394418077963415\n",
            "Cost after 295157 iterations : Training Loss =  0.1015109997637426; Validation Loss = 0.12394418077744306\n",
            "Cost after 295158 iterations : Training Loss =  0.10151099976374249; Validation Loss = 0.12394418077525284\n",
            "Cost after 295159 iterations : Training Loss =  0.10151099976374262; Validation Loss = 0.12394418077306231\n",
            "Cost after 295160 iterations : Training Loss =  0.10151099976374267; Validation Loss = 0.12394418077087163\n",
            "Cost after 295161 iterations : Training Loss =  0.10151099976374248; Validation Loss = 0.12394418076868123\n",
            "Cost after 295162 iterations : Training Loss =  0.1015109997637424; Validation Loss = 0.12394418076649111\n",
            "Cost after 295163 iterations : Training Loss =  0.1015109997637424; Validation Loss = 0.12394418076430101\n",
            "Cost after 295164 iterations : Training Loss =  0.10151099976374248; Validation Loss = 0.12394418076211101\n",
            "Cost after 295165 iterations : Training Loss =  0.10151099976374248; Validation Loss = 0.12394418075992093\n",
            "Cost after 295166 iterations : Training Loss =  0.10151099976374248; Validation Loss = 0.12394418075773118\n",
            "Cost after 295167 iterations : Training Loss =  0.10151099976374246; Validation Loss = 0.12394418075554174\n",
            "Cost after 295168 iterations : Training Loss =  0.10151099976374246; Validation Loss = 0.12394418075335203\n",
            "Cost after 295169 iterations : Training Loss =  0.10151099976374262; Validation Loss = 0.12394418075116276\n",
            "Cost after 295170 iterations : Training Loss =  0.10151099976374249; Validation Loss = 0.12394418074897341\n",
            "Cost after 295171 iterations : Training Loss =  0.10151099976374255; Validation Loss = 0.1239441807467843\n",
            "Cost after 295172 iterations : Training Loss =  0.10151099976374249; Validation Loss = 0.12394418074459522\n",
            "Cost after 295173 iterations : Training Loss =  0.10151099976374255; Validation Loss = 0.12394418074240578\n",
            "Cost after 295174 iterations : Training Loss =  0.10151099976374241; Validation Loss = 0.12394418074021721\n",
            "Cost after 295175 iterations : Training Loss =  0.10151099976374237; Validation Loss = 0.12394418073802818\n",
            "Cost after 295176 iterations : Training Loss =  0.10151099976374232; Validation Loss = 0.12394418073583995\n",
            "Cost after 295177 iterations : Training Loss =  0.10151099976374221; Validation Loss = 0.1239441807336513\n",
            "Cost after 295178 iterations : Training Loss =  0.10151099976374223; Validation Loss = 0.12394418073146282\n",
            "Cost after 295179 iterations : Training Loss =  0.10151099976374242; Validation Loss = 0.12394418072927448\n",
            "Cost after 295180 iterations : Training Loss =  0.10151099976374246; Validation Loss = 0.12394418072708611\n",
            "Cost after 295181 iterations : Training Loss =  0.10151099976374237; Validation Loss = 0.12394418072489831\n",
            "Cost after 295182 iterations : Training Loss =  0.10151099976374248; Validation Loss = 0.12394418072271002\n",
            "Cost after 295183 iterations : Training Loss =  0.10151099976374241; Validation Loss = 0.12394418072052205\n",
            "Cost after 295184 iterations : Training Loss =  0.1015109997637423; Validation Loss = 0.12394418071833448\n",
            "Cost after 295185 iterations : Training Loss =  0.10151099976374264; Validation Loss = 0.12394418071614688\n",
            "Cost after 295186 iterations : Training Loss =  0.10151099976374232; Validation Loss = 0.12394418071395923\n",
            "Cost after 295187 iterations : Training Loss =  0.10151099976374248; Validation Loss = 0.12394418071177196\n",
            "Cost after 295188 iterations : Training Loss =  0.10151099976374227; Validation Loss = 0.12394418070958445\n",
            "Cost after 295189 iterations : Training Loss =  0.10151099976374237; Validation Loss = 0.12394418070739707\n",
            "Cost after 295190 iterations : Training Loss =  0.10151099976374232; Validation Loss = 0.12394418070521031\n",
            "Cost after 295191 iterations : Training Loss =  0.10151099976374216; Validation Loss = 0.12394418070302309\n",
            "Cost after 295192 iterations : Training Loss =  0.10151099976374235; Validation Loss = 0.12394418070083629\n",
            "Cost after 295193 iterations : Training Loss =  0.10151099976374232; Validation Loss = 0.12394418069864922\n",
            "Cost after 295194 iterations : Training Loss =  0.10151099976374221; Validation Loss = 0.12394418069646301\n",
            "Cost after 295195 iterations : Training Loss =  0.10151099976374223; Validation Loss = 0.1239441806942764\n",
            "Cost after 295196 iterations : Training Loss =  0.10151099976374228; Validation Loss = 0.12394418069208979\n",
            "Cost after 295197 iterations : Training Loss =  0.10151099976374228; Validation Loss = 0.1239441806899035\n",
            "Cost after 295198 iterations : Training Loss =  0.10151099976374224; Validation Loss = 0.12394418068771729\n",
            "Cost after 295199 iterations : Training Loss =  0.10151099976374228; Validation Loss = 0.12394418068553076\n",
            "Cost after 295200 iterations : Training Loss =  0.10151099976374221; Validation Loss = 0.12394418068334498\n",
            "Cost after 295201 iterations : Training Loss =  0.10151099976374228; Validation Loss = 0.12394418068115959\n",
            "Cost after 295202 iterations : Training Loss =  0.1015109997637424; Validation Loss = 0.12394418067897343\n",
            "Cost after 295203 iterations : Training Loss =  0.10151099976374217; Validation Loss = 0.12394418067678763\n",
            "Cost after 295204 iterations : Training Loss =  0.10151099976374237; Validation Loss = 0.12394418067460244\n",
            "Cost after 295205 iterations : Training Loss =  0.10151099976374227; Validation Loss = 0.12394418067241673\n",
            "Cost after 295206 iterations : Training Loss =  0.10151099976374221; Validation Loss = 0.12394418067023148\n",
            "Cost after 295207 iterations : Training Loss =  0.10151099976374209; Validation Loss = 0.12394418066804615\n",
            "Cost after 295208 iterations : Training Loss =  0.1015109997637423; Validation Loss = 0.12394418066586103\n",
            "Cost after 295209 iterations : Training Loss =  0.10151099976374227; Validation Loss = 0.12394418066367602\n",
            "Cost after 295210 iterations : Training Loss =  0.10151099976374235; Validation Loss = 0.12394418066149154\n",
            "Cost after 295211 iterations : Training Loss =  0.10151099976374221; Validation Loss = 0.12394418065930655\n",
            "Cost after 295212 iterations : Training Loss =  0.1015109997637422; Validation Loss = 0.12394418065712168\n",
            "Cost after 295213 iterations : Training Loss =  0.10151099976374223; Validation Loss = 0.12394418065493716\n",
            "Cost after 295214 iterations : Training Loss =  0.10151099976374209; Validation Loss = 0.1239441806527527\n",
            "Cost after 295215 iterations : Training Loss =  0.1015109997637421; Validation Loss = 0.1239441806505687\n",
            "Cost after 295216 iterations : Training Loss =  0.10151099976374217; Validation Loss = 0.12394418064838408\n",
            "Cost after 295217 iterations : Training Loss =  0.10151099976374228; Validation Loss = 0.12394418064619998\n",
            "Cost after 295218 iterations : Training Loss =  0.10151099976374224; Validation Loss = 0.12394418064401609\n",
            "Cost after 295219 iterations : Training Loss =  0.10151099976374228; Validation Loss = 0.12394418064183231\n",
            "Cost after 295220 iterations : Training Loss =  0.10151099976374221; Validation Loss = 0.12394418063964828\n",
            "Cost after 295221 iterations : Training Loss =  0.10151099976374216; Validation Loss = 0.12394418063746475\n",
            "Cost after 295222 iterations : Training Loss =  0.10151099976374217; Validation Loss = 0.12394418063528162\n",
            "Cost after 295223 iterations : Training Loss =  0.10151099976374216; Validation Loss = 0.12394418063309784\n",
            "Cost after 295224 iterations : Training Loss =  0.10151099976374202; Validation Loss = 0.1239441806309145\n",
            "Cost after 295225 iterations : Training Loss =  0.10151099976374207; Validation Loss = 0.12394418062873129\n",
            "Cost after 295226 iterations : Training Loss =  0.10151099976374207; Validation Loss = 0.12394418062654826\n",
            "Cost after 295227 iterations : Training Loss =  0.10151099976374214; Validation Loss = 0.12394418062436494\n",
            "Cost after 295228 iterations : Training Loss =  0.1015109997637422; Validation Loss = 0.12394418062218218\n",
            "Cost after 295229 iterations : Training Loss =  0.1015109997637421; Validation Loss = 0.12394418061999947\n",
            "Cost after 295230 iterations : Training Loss =  0.1015109997637421; Validation Loss = 0.12394418061781691\n",
            "Cost after 295231 iterations : Training Loss =  0.10151099976374212; Validation Loss = 0.12394418061563418\n",
            "Cost after 295232 iterations : Training Loss =  0.10151099976374207; Validation Loss = 0.12394418061345178\n",
            "Cost after 295233 iterations : Training Loss =  0.10151099976374216; Validation Loss = 0.12394418061126967\n",
            "Cost after 295234 iterations : Training Loss =  0.10151099976374217; Validation Loss = 0.12394418060908718\n",
            "Cost after 295235 iterations : Training Loss =  0.10151099976374212; Validation Loss = 0.12394418060690537\n",
            "Cost after 295236 iterations : Training Loss =  0.10151099976374198; Validation Loss = 0.12394418060472319\n",
            "Cost after 295237 iterations : Training Loss =  0.10151099976374203; Validation Loss = 0.1239441806025413\n",
            "Cost after 295238 iterations : Training Loss =  0.10151099976374192; Validation Loss = 0.12394418060035953\n",
            "Cost after 295239 iterations : Training Loss =  0.10151099976374205; Validation Loss = 0.12394418059817783\n",
            "Cost after 295240 iterations : Training Loss =  0.10151099976374188; Validation Loss = 0.12394418059599621\n",
            "Cost after 295241 iterations : Training Loss =  0.10151099976374196; Validation Loss = 0.12394418059381467\n",
            "Cost after 295242 iterations : Training Loss =  0.10151099976374207; Validation Loss = 0.12394418059163383\n",
            "Cost after 295243 iterations : Training Loss =  0.10151099976374217; Validation Loss = 0.1239441805894521\n",
            "Cost after 295244 iterations : Training Loss =  0.101510999763742; Validation Loss = 0.12394418058727105\n",
            "Cost after 295245 iterations : Training Loss =  0.10151099976374207; Validation Loss = 0.12394418058509021\n",
            "Cost after 295246 iterations : Training Loss =  0.10151099976374212; Validation Loss = 0.123944180582909\n",
            "Cost after 295247 iterations : Training Loss =  0.10151099976374207; Validation Loss = 0.12394418058072798\n",
            "Cost after 295248 iterations : Training Loss =  0.10151099976374217; Validation Loss = 0.12394418057854757\n",
            "Cost after 295249 iterations : Training Loss =  0.10151099976374192; Validation Loss = 0.12394418057636707\n",
            "Cost after 295250 iterations : Training Loss =  0.10151099976374196; Validation Loss = 0.1239441805741869\n",
            "Cost after 295251 iterations : Training Loss =  0.10151099976374195; Validation Loss = 0.12394418057200655\n",
            "Cost after 295252 iterations : Training Loss =  0.10151099976374195; Validation Loss = 0.1239441805698261\n",
            "Cost after 295253 iterations : Training Loss =  0.10151099976374196; Validation Loss = 0.12394418056764624\n",
            "Cost after 295254 iterations : Training Loss =  0.10151099976374192; Validation Loss = 0.12394418056546604\n",
            "Cost after 295255 iterations : Training Loss =  0.1015109997637419; Validation Loss = 0.12394418056328595\n",
            "Cost after 295256 iterations : Training Loss =  0.10151099976374207; Validation Loss = 0.12394418056110648\n",
            "Cost after 295257 iterations : Training Loss =  0.10151099976374198; Validation Loss = 0.1239441805589267\n",
            "Cost after 295258 iterations : Training Loss =  0.10151099976374192; Validation Loss = 0.12394418055674726\n",
            "Cost after 295259 iterations : Training Loss =  0.10151099976374209; Validation Loss = 0.12394418055456789\n",
            "Cost after 295260 iterations : Training Loss =  0.10151099976374195; Validation Loss = 0.12394418055238841\n",
            "Cost after 295261 iterations : Training Loss =  0.10151099976374203; Validation Loss = 0.12394418055020907\n",
            "Cost after 295262 iterations : Training Loss =  0.1015109997637418; Validation Loss = 0.12394418054802982\n",
            "Cost after 295263 iterations : Training Loss =  0.10151099976374196; Validation Loss = 0.12394418054585092\n",
            "Cost after 295264 iterations : Training Loss =  0.1015109997637419; Validation Loss = 0.12394418054367201\n",
            "Cost after 295265 iterations : Training Loss =  0.10151099976374188; Validation Loss = 0.12394418054149334\n",
            "Cost after 295266 iterations : Training Loss =  0.10151099976374203; Validation Loss = 0.12394418053931443\n",
            "Cost after 295267 iterations : Training Loss =  0.10151099976374177; Validation Loss = 0.12394418053713618\n",
            "Cost after 295268 iterations : Training Loss =  0.1015109997637417; Validation Loss = 0.12394418053495727\n",
            "Cost after 295269 iterations : Training Loss =  0.10151099976374196; Validation Loss = 0.12394418053277903\n",
            "Cost after 295270 iterations : Training Loss =  0.1015109997637418; Validation Loss = 0.12394418053060104\n",
            "Cost after 295271 iterations : Training Loss =  0.10151099976374192; Validation Loss = 0.12394418052842265\n",
            "Cost after 295272 iterations : Training Loss =  0.10151099976374188; Validation Loss = 0.12394418052624484\n",
            "Cost after 295273 iterations : Training Loss =  0.10151099976374173; Validation Loss = 0.12394418052406668\n",
            "Cost after 295274 iterations : Training Loss =  0.10151099976374178; Validation Loss = 0.12394418052188892\n",
            "Cost after 295275 iterations : Training Loss =  0.1015109997637416; Validation Loss = 0.12394418051971147\n",
            "Cost after 295276 iterations : Training Loss =  0.10151099976374188; Validation Loss = 0.12394418051753377\n",
            "Cost after 295277 iterations : Training Loss =  0.10151099976374184; Validation Loss = 0.12394418051535624\n",
            "Cost after 295278 iterations : Training Loss =  0.10151099976374188; Validation Loss = 0.12394418051317903\n",
            "Cost after 295279 iterations : Training Loss =  0.1015109997637418; Validation Loss = 0.12394418051100133\n",
            "Cost after 295280 iterations : Training Loss =  0.10151099976374177; Validation Loss = 0.12394418050882454\n",
            "Cost after 295281 iterations : Training Loss =  0.10151099976374166; Validation Loss = 0.12394418050664736\n",
            "Cost after 295282 iterations : Training Loss =  0.10151099976374164; Validation Loss = 0.12394418050447073\n",
            "Cost after 295283 iterations : Training Loss =  0.1015109997637418; Validation Loss = 0.12394418050229365\n",
            "Cost after 295284 iterations : Training Loss =  0.10151099976374173; Validation Loss = 0.12394418050011705\n",
            "Cost after 295285 iterations : Training Loss =  0.1015109997637418; Validation Loss = 0.123944180497941\n",
            "Cost after 295286 iterations : Training Loss =  0.10151099976374173; Validation Loss = 0.12394418049576404\n",
            "Cost after 295287 iterations : Training Loss =  0.1015109997637417; Validation Loss = 0.12394418049358777\n",
            "Cost after 295288 iterations : Training Loss =  0.1015109997637416; Validation Loss = 0.12394418049141141\n",
            "Cost after 295289 iterations : Training Loss =  0.10151099976374171; Validation Loss = 0.12394418048923543\n",
            "Cost after 295290 iterations : Training Loss =  0.10151099976374182; Validation Loss = 0.1239441804870593\n",
            "Cost after 295291 iterations : Training Loss =  0.10151099976374177; Validation Loss = 0.12394418048488322\n",
            "Cost after 295292 iterations : Training Loss =  0.10151099976374164; Validation Loss = 0.12394418048270753\n",
            "Cost after 295293 iterations : Training Loss =  0.1015109997637417; Validation Loss = 0.12394418048053182\n",
            "Cost after 295294 iterations : Training Loss =  0.10151099976374166; Validation Loss = 0.12394418047835626\n",
            "Cost after 295295 iterations : Training Loss =  0.10151099976374175; Validation Loss = 0.12394418047618061\n",
            "Cost after 295296 iterations : Training Loss =  0.10151099976374166; Validation Loss = 0.12394418047400522\n",
            "Cost after 295297 iterations : Training Loss =  0.1015109997637417; Validation Loss = 0.12394418047183006\n",
            "Cost after 295298 iterations : Training Loss =  0.1015109997637417; Validation Loss = 0.12394418046965479\n",
            "Cost after 295299 iterations : Training Loss =  0.10151099976374156; Validation Loss = 0.1239441804674798\n",
            "Cost after 295300 iterations : Training Loss =  0.1015109997637416; Validation Loss = 0.12394418046530521\n",
            "Cost after 295301 iterations : Training Loss =  0.1015109997637417; Validation Loss = 0.12394418046313022\n",
            "Cost after 295302 iterations : Training Loss =  0.1015109997637417; Validation Loss = 0.12394418046095552\n",
            "Cost after 295303 iterations : Training Loss =  0.10151099976374167; Validation Loss = 0.123944180458781\n",
            "Cost after 295304 iterations : Training Loss =  0.10151099976374153; Validation Loss = 0.12394418045660675\n",
            "Cost after 295305 iterations : Training Loss =  0.10151099976374166; Validation Loss = 0.12394418045443212\n",
            "Cost after 295306 iterations : Training Loss =  0.1015109997637418; Validation Loss = 0.12394418045225768\n",
            "Cost after 295307 iterations : Training Loss =  0.10151099976374166; Validation Loss = 0.12394418045008344\n",
            "Cost after 295308 iterations : Training Loss =  0.1015109997637416; Validation Loss = 0.12394418044790952\n",
            "Cost after 295309 iterations : Training Loss =  0.10151099976374175; Validation Loss = 0.12394418044573562\n",
            "Cost after 295310 iterations : Training Loss =  0.10151099976374164; Validation Loss = 0.1239441804435618\n",
            "Cost after 295311 iterations : Training Loss =  0.10151099976374159; Validation Loss = 0.12394418044138852\n",
            "Cost after 295312 iterations : Training Loss =  0.10151099976374157; Validation Loss = 0.12394418043921443\n",
            "Cost after 295313 iterations : Training Loss =  0.10151099976374159; Validation Loss = 0.12394418043704128\n",
            "Cost after 295314 iterations : Training Loss =  0.10151099976374156; Validation Loss = 0.12394418043486788\n",
            "Cost after 295315 iterations : Training Loss =  0.10151099976374156; Validation Loss = 0.1239441804326945\n",
            "Cost after 295316 iterations : Training Loss =  0.10151099976374152; Validation Loss = 0.1239441804305216\n",
            "Cost after 295317 iterations : Training Loss =  0.10151099976374153; Validation Loss = 0.12394418042834834\n",
            "Cost after 295318 iterations : Training Loss =  0.10151099976374152; Validation Loss = 0.12394418042617533\n",
            "Cost after 295319 iterations : Training Loss =  0.10151099976374152; Validation Loss = 0.12394418042400256\n",
            "Cost after 295320 iterations : Training Loss =  0.10151099976374148; Validation Loss = 0.12394418042182974\n",
            "Cost after 295321 iterations : Training Loss =  0.10151099976374166; Validation Loss = 0.12394418041965705\n",
            "Cost after 295322 iterations : Training Loss =  0.10151099976374167; Validation Loss = 0.1239441804174844\n",
            "Cost after 295323 iterations : Training Loss =  0.10151099976374167; Validation Loss = 0.12394418041531224\n",
            "Cost after 295324 iterations : Training Loss =  0.10151099976374148; Validation Loss = 0.12394418041313979\n",
            "Cost after 295325 iterations : Training Loss =  0.10151099976374153; Validation Loss = 0.12394418041096779\n",
            "Cost after 295326 iterations : Training Loss =  0.10151099976374153; Validation Loss = 0.12394418040879568\n",
            "Cost after 295327 iterations : Training Loss =  0.10151099976374153; Validation Loss = 0.12394418040662362\n",
            "Cost after 295328 iterations : Training Loss =  0.10151099976374153; Validation Loss = 0.12394418040445211\n",
            "Cost after 295329 iterations : Training Loss =  0.10151099976374146; Validation Loss = 0.12394418040227993\n",
            "Cost after 295330 iterations : Training Loss =  0.10151099976374144; Validation Loss = 0.12394418040010868\n",
            "Cost after 295331 iterations : Training Loss =  0.1015109997637415; Validation Loss = 0.12394418039793709\n",
            "Cost after 295332 iterations : Training Loss =  0.10151099976374144; Validation Loss = 0.12394418039576544\n",
            "Cost after 295333 iterations : Training Loss =  0.10151099976374135; Validation Loss = 0.12394418039359434\n",
            "Cost after 295334 iterations : Training Loss =  0.10151099976374138; Validation Loss = 0.12394418039142306\n",
            "Cost after 295335 iterations : Training Loss =  0.10151099976374141; Validation Loss = 0.12394418038925187\n",
            "Cost after 295336 iterations : Training Loss =  0.10151099976374156; Validation Loss = 0.12394418038708117\n",
            "Cost after 295337 iterations : Training Loss =  0.10151099976374135; Validation Loss = 0.12394418038491037\n",
            "Cost after 295338 iterations : Training Loss =  0.10151099976374146; Validation Loss = 0.12394418038273948\n",
            "Cost after 295339 iterations : Training Loss =  0.10151099976374148; Validation Loss = 0.12394418038056908\n",
            "Cost after 295340 iterations : Training Loss =  0.10151099976374145; Validation Loss = 0.1239441803783984\n",
            "Cost after 295341 iterations : Training Loss =  0.10151099976374135; Validation Loss = 0.12394418037622817\n",
            "Cost after 295342 iterations : Training Loss =  0.10151099976374144; Validation Loss = 0.12394418037405762\n",
            "Cost after 295343 iterations : Training Loss =  0.10151099976374152; Validation Loss = 0.12394418037188752\n",
            "Cost after 295344 iterations : Training Loss =  0.1015109997637414; Validation Loss = 0.12394418036971751\n",
            "Cost after 295345 iterations : Training Loss =  0.10151099976374132; Validation Loss = 0.12394418036754741\n",
            "Cost after 295346 iterations : Training Loss =  0.10151099976374156; Validation Loss = 0.12394418036537781\n",
            "Cost after 295347 iterations : Training Loss =  0.10151099976374134; Validation Loss = 0.12394418036320773\n",
            "Cost after 295348 iterations : Training Loss =  0.10151099976374134; Validation Loss = 0.12394418036103835\n",
            "Cost after 295349 iterations : Training Loss =  0.10151099976374141; Validation Loss = 0.12394418035886891\n",
            "Cost after 295350 iterations : Training Loss =  0.10151099976374128; Validation Loss = 0.12394418035669935\n",
            "Cost after 295351 iterations : Training Loss =  0.10151099976374141; Validation Loss = 0.12394418035453\n",
            "Cost after 295352 iterations : Training Loss =  0.10151099976374131; Validation Loss = 0.12394418035236089\n",
            "Cost after 295353 iterations : Training Loss =  0.10151099976374138; Validation Loss = 0.1239441803501916\n",
            "Cost after 295354 iterations : Training Loss =  0.10151099976374141; Validation Loss = 0.12394418034802292\n",
            "Cost after 295355 iterations : Training Loss =  0.10151099976374131; Validation Loss = 0.12394418034585368\n",
            "Cost after 295356 iterations : Training Loss =  0.10151099976374114; Validation Loss = 0.12394418034368494\n",
            "Cost after 295357 iterations : Training Loss =  0.10151099976374126; Validation Loss = 0.12394418034151622\n",
            "Cost after 295358 iterations : Training Loss =  0.10151099976374128; Validation Loss = 0.12394418033934766\n",
            "Cost after 295359 iterations : Training Loss =  0.10151099976374141; Validation Loss = 0.12394418033717931\n",
            "Cost after 295360 iterations : Training Loss =  0.1015109997637414; Validation Loss = 0.12394418033501106\n",
            "Cost after 295361 iterations : Training Loss =  0.10151099976374121; Validation Loss = 0.123944180332843\n",
            "Cost after 295362 iterations : Training Loss =  0.10151099976374123; Validation Loss = 0.12394418033067485\n",
            "Cost after 295363 iterations : Training Loss =  0.10151099976374127; Validation Loss = 0.12394418032850693\n",
            "Cost after 295364 iterations : Training Loss =  0.10151099976374113; Validation Loss = 0.123944180326339\n",
            "Cost after 295365 iterations : Training Loss =  0.10151099976374132; Validation Loss = 0.12394418032417118\n",
            "Cost after 295366 iterations : Training Loss =  0.10151099976374123; Validation Loss = 0.12394418032200377\n",
            "Cost after 295367 iterations : Training Loss =  0.10151099976374121; Validation Loss = 0.12394418031983588\n",
            "Cost after 295368 iterations : Training Loss =  0.10151099976374123; Validation Loss = 0.12394418031766886\n",
            "Cost after 295369 iterations : Training Loss =  0.10151099976374126; Validation Loss = 0.12394418031550149\n",
            "Cost after 295370 iterations : Training Loss =  0.10151099976374134; Validation Loss = 0.1239441803133343\n",
            "Cost after 295371 iterations : Training Loss =  0.10151099976374107; Validation Loss = 0.12394418031116722\n",
            "Cost after 295372 iterations : Training Loss =  0.10151099976374107; Validation Loss = 0.12394418030900024\n",
            "Cost after 295373 iterations : Training Loss =  0.10151099976374121; Validation Loss = 0.12394418030683323\n",
            "Cost after 295374 iterations : Training Loss =  0.1015109997637412; Validation Loss = 0.12394418030466664\n",
            "Cost after 295375 iterations : Training Loss =  0.10151099976374113; Validation Loss = 0.12394418030250001\n",
            "Cost after 295376 iterations : Training Loss =  0.10151099976374128; Validation Loss = 0.1239441803003332\n",
            "Cost after 295377 iterations : Training Loss =  0.10151099976374128; Validation Loss = 0.12394418029816699\n",
            "Cost after 295378 iterations : Training Loss =  0.10151099976374127; Validation Loss = 0.12394418029600056\n",
            "Cost after 295379 iterations : Training Loss =  0.1015109997637412; Validation Loss = 0.12394418029383453\n",
            "Cost after 295380 iterations : Training Loss =  0.10151099976374114; Validation Loss = 0.12394418029166833\n",
            "Cost after 295381 iterations : Training Loss =  0.10151099976374121; Validation Loss = 0.12394418028950233\n",
            "Cost after 295382 iterations : Training Loss =  0.10151099976374099; Validation Loss = 0.12394418028733627\n",
            "Cost after 295383 iterations : Training Loss =  0.10151099976374107; Validation Loss = 0.12394418028517043\n",
            "Cost after 295384 iterations : Training Loss =  0.10151099976374119; Validation Loss = 0.12394418028300479\n",
            "Cost after 295385 iterations : Training Loss =  0.10151099976374119; Validation Loss = 0.12394418028083934\n",
            "Cost after 295386 iterations : Training Loss =  0.10151099976374132; Validation Loss = 0.1239441802786739\n",
            "Cost after 295387 iterations : Training Loss =  0.10151099976374109; Validation Loss = 0.12394418027650835\n",
            "Cost after 295388 iterations : Training Loss =  0.10151099976374134; Validation Loss = 0.12394418027434319\n",
            "Cost after 295389 iterations : Training Loss =  0.10151099976374121; Validation Loss = 0.12394418027217796\n",
            "Cost after 295390 iterations : Training Loss =  0.10151099976374126; Validation Loss = 0.12394418027001332\n",
            "Cost after 295391 iterations : Training Loss =  0.10151099976374094; Validation Loss = 0.1239441802678481\n",
            "Cost after 295392 iterations : Training Loss =  0.10151099976374116; Validation Loss = 0.12394418026568353\n",
            "Cost after 295393 iterations : Training Loss =  0.10151099976374127; Validation Loss = 0.12394418026351875\n",
            "Cost after 295394 iterations : Training Loss =  0.10151099976374091; Validation Loss = 0.123944180261354\n",
            "Cost after 295395 iterations : Training Loss =  0.10151099976374123; Validation Loss = 0.12394418025919006\n",
            "Cost after 295396 iterations : Training Loss =  0.10151099976374109; Validation Loss = 0.12394418025702564\n",
            "Cost after 295397 iterations : Training Loss =  0.10151099976374119; Validation Loss = 0.12394418025486119\n",
            "Cost after 295398 iterations : Training Loss =  0.10151099976374102; Validation Loss = 0.12394418025269721\n",
            "Cost after 295399 iterations : Training Loss =  0.10151099976374102; Validation Loss = 0.12394418025053312\n",
            "Cost after 295400 iterations : Training Loss =  0.10151099976374094; Validation Loss = 0.12394418024836899\n",
            "Cost after 295401 iterations : Training Loss =  0.101510999763741; Validation Loss = 0.12394418024620522\n",
            "Cost after 295402 iterations : Training Loss =  0.10151099976374095; Validation Loss = 0.1239441802440416\n",
            "Cost after 295403 iterations : Training Loss =  0.10151099976374094; Validation Loss = 0.12394418024187825\n",
            "Cost after 295404 iterations : Training Loss =  0.10151099976374102; Validation Loss = 0.12394418023971468\n",
            "Cost after 295405 iterations : Training Loss =  0.101510999763741; Validation Loss = 0.12394418023755129\n",
            "Cost after 295406 iterations : Training Loss =  0.10151099976374113; Validation Loss = 0.12394418023538807\n",
            "Cost after 295407 iterations : Training Loss =  0.10151099976374096; Validation Loss = 0.12394418023322461\n",
            "Cost after 295408 iterations : Training Loss =  0.10151099976374095; Validation Loss = 0.12394418023106189\n",
            "Cost after 295409 iterations : Training Loss =  0.10151099976374106; Validation Loss = 0.12394418022889904\n",
            "Cost after 295410 iterations : Training Loss =  0.10151099976374095; Validation Loss = 0.12394418022673609\n",
            "Cost after 295411 iterations : Training Loss =  0.101510999763741; Validation Loss = 0.12394418022457344\n",
            "Cost after 295412 iterations : Training Loss =  0.10151099976374107; Validation Loss = 0.12394418022241079\n",
            "Cost after 295413 iterations : Training Loss =  0.10151099976374112; Validation Loss = 0.12394418022024838\n",
            "Cost after 295414 iterations : Training Loss =  0.10151099976374088; Validation Loss = 0.12394418021808636\n",
            "Cost after 295415 iterations : Training Loss =  0.10151099976374096; Validation Loss = 0.12394418021592359\n",
            "Cost after 295416 iterations : Training Loss =  0.10151099976374087; Validation Loss = 0.12394418021376172\n",
            "Cost after 295417 iterations : Training Loss =  0.10151099976374096; Validation Loss = 0.12394418021159974\n",
            "Cost after 295418 iterations : Training Loss =  0.10151099976374107; Validation Loss = 0.12394418020943747\n",
            "Cost after 295419 iterations : Training Loss =  0.10151099976374087; Validation Loss = 0.1239441802072756\n",
            "Cost after 295420 iterations : Training Loss =  0.10151099976374095; Validation Loss = 0.12394418020511404\n",
            "Cost after 295421 iterations : Training Loss =  0.10151099976374103; Validation Loss = 0.12394418020295252\n",
            "Cost after 295422 iterations : Training Loss =  0.10151099976374088; Validation Loss = 0.12394418020079094\n",
            "Cost after 295423 iterations : Training Loss =  0.1015109997637409; Validation Loss = 0.12394418019862954\n",
            "Cost after 295424 iterations : Training Loss =  0.10151099976374081; Validation Loss = 0.12394418019646836\n",
            "Cost after 295425 iterations : Training Loss =  0.10151099976374088; Validation Loss = 0.12394418019430722\n",
            "Cost after 295426 iterations : Training Loss =  0.10151099976374088; Validation Loss = 0.12394418019214597\n",
            "Cost after 295427 iterations : Training Loss =  0.1015109997637409; Validation Loss = 0.12394418018998536\n",
            "Cost after 295428 iterations : Training Loss =  0.10151099976374099; Validation Loss = 0.1239441801878244\n",
            "Cost after 295429 iterations : Training Loss =  0.10151099976374084; Validation Loss = 0.12394418018566367\n",
            "Cost after 295430 iterations : Training Loss =  0.10151099976374094; Validation Loss = 0.12394418018350307\n",
            "Cost after 295431 iterations : Training Loss =  0.10151099976374081; Validation Loss = 0.12394418018134239\n",
            "Cost after 295432 iterations : Training Loss =  0.10151099976374077; Validation Loss = 0.12394418017918209\n",
            "Cost after 295433 iterations : Training Loss =  0.10151099976374088; Validation Loss = 0.12394418017702169\n",
            "Cost after 295434 iterations : Training Loss =  0.10151099976374083; Validation Loss = 0.12394418017486164\n",
            "Cost after 295435 iterations : Training Loss =  0.10151099976374064; Validation Loss = 0.12394418017270145\n",
            "Cost after 295436 iterations : Training Loss =  0.10151099976374078; Validation Loss = 0.12394418017054178\n",
            "Cost after 295437 iterations : Training Loss =  0.10151099976374083; Validation Loss = 0.12394418016838196\n",
            "Cost after 295438 iterations : Training Loss =  0.10151099976374083; Validation Loss = 0.12394418016622219\n",
            "Cost after 295439 iterations : Training Loss =  0.10151099976374087; Validation Loss = 0.12394418016406253\n",
            "Cost after 295440 iterations : Training Loss =  0.10151099976374081; Validation Loss = 0.1239441801619028\n",
            "Cost after 295441 iterations : Training Loss =  0.10151099976374081; Validation Loss = 0.12394418015974341\n",
            "Cost after 295442 iterations : Training Loss =  0.10151099976374084; Validation Loss = 0.12394418015758396\n",
            "Cost after 295443 iterations : Training Loss =  0.10151099976374071; Validation Loss = 0.12394418015542509\n",
            "Cost after 295444 iterations : Training Loss =  0.10151099976374078; Validation Loss = 0.12394418015326591\n",
            "Cost after 295445 iterations : Training Loss =  0.1015109997637407; Validation Loss = 0.12394418015110685\n",
            "Cost after 295446 iterations : Training Loss =  0.10151099976374078; Validation Loss = 0.12394418014894788\n",
            "Cost after 295447 iterations : Training Loss =  0.10151099976374059; Validation Loss = 0.12394418014678946\n",
            "Cost after 295448 iterations : Training Loss =  0.10151099976374069; Validation Loss = 0.1239441801446307\n",
            "Cost after 295449 iterations : Training Loss =  0.10151099976374078; Validation Loss = 0.12394418014247212\n",
            "Cost after 295450 iterations : Training Loss =  0.10151099976374077; Validation Loss = 0.1239441801403138\n",
            "Cost after 295451 iterations : Training Loss =  0.10151099976374074; Validation Loss = 0.12394418013815538\n",
            "Cost after 295452 iterations : Training Loss =  0.10151099976374083; Validation Loss = 0.12394418013599733\n",
            "Cost after 295453 iterations : Training Loss =  0.10151099976374083; Validation Loss = 0.1239441801338391\n",
            "Cost after 295454 iterations : Training Loss =  0.10151099976374058; Validation Loss = 0.12394418013168135\n",
            "Cost after 295455 iterations : Training Loss =  0.10151099976374067; Validation Loss = 0.12394418012952314\n",
            "Cost after 295456 iterations : Training Loss =  0.10151099976374076; Validation Loss = 0.12394418012736559\n",
            "Cost after 295457 iterations : Training Loss =  0.1015109997637407; Validation Loss = 0.1239441801252079\n",
            "Cost after 295458 iterations : Training Loss =  0.10151099976374064; Validation Loss = 0.12394418012305052\n",
            "Cost after 295459 iterations : Training Loss =  0.10151099976374064; Validation Loss = 0.1239441801208929\n",
            "Cost after 295460 iterations : Training Loss =  0.10151099976374064; Validation Loss = 0.12394418011873558\n",
            "Cost after 295461 iterations : Training Loss =  0.10151099976374058; Validation Loss = 0.12394418011657851\n",
            "Cost after 295462 iterations : Training Loss =  0.1015109997637407; Validation Loss = 0.12394418011442129\n",
            "Cost after 295463 iterations : Training Loss =  0.10151099976374045; Validation Loss = 0.1239441801122644\n",
            "Cost after 295464 iterations : Training Loss =  0.10151099976374062; Validation Loss = 0.12394418011010754\n",
            "Cost after 295465 iterations : Training Loss =  0.10151099976374076; Validation Loss = 0.12394418010795048\n",
            "Cost after 295466 iterations : Training Loss =  0.10151099976374076; Validation Loss = 0.12394418010579418\n",
            "Cost after 295467 iterations : Training Loss =  0.10151099976374087; Validation Loss = 0.12394418010363752\n",
            "Cost after 295468 iterations : Training Loss =  0.10151099976374063; Validation Loss = 0.12394418010148094\n",
            "Cost after 295469 iterations : Training Loss =  0.10151099976374069; Validation Loss = 0.12394418009932455\n",
            "Cost after 295470 iterations : Training Loss =  0.10151099976374071; Validation Loss = 0.12394418009716826\n",
            "Cost after 295471 iterations : Training Loss =  0.10151099976374067; Validation Loss = 0.12394418009501232\n",
            "Cost after 295472 iterations : Training Loss =  0.1015109997637405; Validation Loss = 0.12394418009285606\n",
            "Cost after 295473 iterations : Training Loss =  0.10151099976374064; Validation Loss = 0.12394418009070028\n",
            "Cost after 295474 iterations : Training Loss =  0.10151099976374056; Validation Loss = 0.12394418008854452\n",
            "Cost after 295475 iterations : Training Loss =  0.10151099976374062; Validation Loss = 0.12394418008638887\n",
            "Cost after 295476 iterations : Training Loss =  0.10151099976374056; Validation Loss = 0.12394418008423305\n",
            "Cost after 295477 iterations : Training Loss =  0.10151099976374069; Validation Loss = 0.12394418008207787\n",
            "Cost after 295478 iterations : Training Loss =  0.10151099976374059; Validation Loss = 0.1239441800799225\n",
            "Cost after 295479 iterations : Training Loss =  0.1015109997637405; Validation Loss = 0.12394418007776724\n",
            "Cost after 295480 iterations : Training Loss =  0.10151099976374069; Validation Loss = 0.123944180075612\n",
            "Cost after 295481 iterations : Training Loss =  0.10151099976374063; Validation Loss = 0.12394418007345698\n",
            "Cost after 295482 iterations : Training Loss =  0.10151099976374056; Validation Loss = 0.12394418007130209\n",
            "Cost after 295483 iterations : Training Loss =  0.10151099976374067; Validation Loss = 0.1239441800691472\n",
            "Cost after 295484 iterations : Training Loss =  0.10151099976374058; Validation Loss = 0.12394418006699275\n",
            "Cost after 295485 iterations : Training Loss =  0.1015109997637405; Validation Loss = 0.12394418006483808\n",
            "Cost after 295486 iterations : Training Loss =  0.10151099976374052; Validation Loss = 0.12394418006268333\n",
            "Cost after 295487 iterations : Training Loss =  0.10151099976374056; Validation Loss = 0.12394418006052921\n",
            "Cost after 295488 iterations : Training Loss =  0.10151099976374067; Validation Loss = 0.12394418005837501\n",
            "Cost after 295489 iterations : Training Loss =  0.10151099976374055; Validation Loss = 0.12394418005622049\n",
            "Cost after 295490 iterations : Training Loss =  0.10151099976374058; Validation Loss = 0.12394418005406634\n",
            "Cost after 295491 iterations : Training Loss =  0.1015109997637405; Validation Loss = 0.1239441800519127\n",
            "Cost after 295492 iterations : Training Loss =  0.10151099976374067; Validation Loss = 0.12394418004975866\n",
            "Cost after 295493 iterations : Training Loss =  0.10151099976374056; Validation Loss = 0.1239441800476051\n",
            "Cost after 295494 iterations : Training Loss =  0.10151099976374042; Validation Loss = 0.12394418004545173\n",
            "Cost after 295495 iterations : Training Loss =  0.10151099976374059; Validation Loss = 0.12394418004329785\n",
            "Cost after 295496 iterations : Training Loss =  0.10151099976374056; Validation Loss = 0.12394418004114474\n",
            "Cost after 295497 iterations : Training Loss =  0.1015109997637407; Validation Loss = 0.1239441800389912\n",
            "Cost after 295498 iterations : Training Loss =  0.10151099976374063; Validation Loss = 0.12394418003683794\n",
            "Cost after 295499 iterations : Training Loss =  0.10151099976374049; Validation Loss = 0.12394418003468502\n",
            "Cost after 295500 iterations : Training Loss =  0.10151099976374062; Validation Loss = 0.12394418003253212\n",
            "Cost after 295501 iterations : Training Loss =  0.10151099976374046; Validation Loss = 0.12394418003037949\n",
            "Cost after 295502 iterations : Training Loss =  0.10151099976374055; Validation Loss = 0.12394418002822645\n",
            "Cost after 295503 iterations : Training Loss =  0.1015109997637404; Validation Loss = 0.12394418002607369\n",
            "Cost after 295504 iterations : Training Loss =  0.10151099976374037; Validation Loss = 0.12394418002392123\n",
            "Cost after 295505 iterations : Training Loss =  0.10151099976374049; Validation Loss = 0.12394418002176899\n",
            "Cost after 295506 iterations : Training Loss =  0.10151099976374031; Validation Loss = 0.12394418001961642\n",
            "Cost after 295507 iterations : Training Loss =  0.1015109997637403; Validation Loss = 0.12394418001746425\n",
            "Cost after 295508 iterations : Training Loss =  0.10151099976374055; Validation Loss = 0.1239441800153124\n",
            "Cost after 295509 iterations : Training Loss =  0.10151099976374037; Validation Loss = 0.12394418001316063\n",
            "Cost after 295510 iterations : Training Loss =  0.10151099976374034; Validation Loss = 0.12394418001100857\n",
            "Cost after 295511 iterations : Training Loss =  0.10151099976374044; Validation Loss = 0.1239441800088569\n",
            "Cost after 295512 iterations : Training Loss =  0.10151099976374023; Validation Loss = 0.12394418000670518\n",
            "Cost after 295513 iterations : Training Loss =  0.10151099976374042; Validation Loss = 0.12394418000455348\n",
            "Cost after 295514 iterations : Training Loss =  0.10151099976374038; Validation Loss = 0.12394418000240226\n",
            "Cost after 295515 iterations : Training Loss =  0.10151099976374046; Validation Loss = 0.12394418000025109\n",
            "Cost after 295516 iterations : Training Loss =  0.10151099976374027; Validation Loss = 0.12394417999809981\n",
            "Cost after 295517 iterations : Training Loss =  0.10151099976374034; Validation Loss = 0.12394417999594871\n",
            "Cost after 295518 iterations : Training Loss =  0.10151099976374046; Validation Loss = 0.12394417999379766\n",
            "Cost after 295519 iterations : Training Loss =  0.10151099976374034; Validation Loss = 0.12394417999164649\n",
            "Cost after 295520 iterations : Training Loss =  0.10151099976374042; Validation Loss = 0.12394417998949589\n",
            "Cost after 295521 iterations : Training Loss =  0.10151099976374031; Validation Loss = 0.12394417998734544\n",
            "Cost after 295522 iterations : Training Loss =  0.10151099976374038; Validation Loss = 0.1239441799851947\n",
            "Cost after 295523 iterations : Training Loss =  0.10151099976374034; Validation Loss = 0.1239441799830442\n",
            "Cost after 295524 iterations : Training Loss =  0.1015109997637403; Validation Loss = 0.12394417998089403\n",
            "Cost after 295525 iterations : Training Loss =  0.10151099976374037; Validation Loss = 0.12394417997874403\n",
            "Cost after 295526 iterations : Training Loss =  0.10151099976374031; Validation Loss = 0.12394417997659372\n",
            "Cost after 295527 iterations : Training Loss =  0.10151099976374044; Validation Loss = 0.12394417997444361\n",
            "Cost after 295528 iterations : Training Loss =  0.10151099976374042; Validation Loss = 0.12394417997229397\n",
            "Cost after 295529 iterations : Training Loss =  0.10151099976374037; Validation Loss = 0.12394417997014387\n",
            "Cost after 295530 iterations : Training Loss =  0.10151099976374026; Validation Loss = 0.12394417996799419\n",
            "Cost after 295531 iterations : Training Loss =  0.10151099976374038; Validation Loss = 0.12394417996584463\n",
            "Cost after 295532 iterations : Training Loss =  0.10151099976374031; Validation Loss = 0.12394417996369511\n",
            "Cost after 295533 iterations : Training Loss =  0.10151099976374031; Validation Loss = 0.12394417996154629\n",
            "Cost after 295534 iterations : Training Loss =  0.10151099976374027; Validation Loss = 0.12394417995939659\n",
            "Cost after 295535 iterations : Training Loss =  0.10151099976374013; Validation Loss = 0.12394417995724767\n",
            "Cost after 295536 iterations : Training Loss =  0.10151099976374013; Validation Loss = 0.1239441799550987\n",
            "Cost after 295537 iterations : Training Loss =  0.10151099976374027; Validation Loss = 0.12394417995294953\n",
            "Cost after 295538 iterations : Training Loss =  0.10151099976374024; Validation Loss = 0.12394417995080073\n",
            "Cost after 295539 iterations : Training Loss =  0.10151099976374024; Validation Loss = 0.12394417994865223\n",
            "Cost after 295540 iterations : Training Loss =  0.10151099976374027; Validation Loss = 0.1239441799465034\n",
            "Cost after 295541 iterations : Training Loss =  0.10151099976374013; Validation Loss = 0.12394417994435507\n",
            "Cost after 295542 iterations : Training Loss =  0.10151099976374017; Validation Loss = 0.12394417994220668\n",
            "Cost after 295543 iterations : Training Loss =  0.10151099976374027; Validation Loss = 0.12394417994005849\n",
            "Cost after 295544 iterations : Training Loss =  0.1015109997637403; Validation Loss = 0.1239441799379102\n",
            "Cost after 295545 iterations : Training Loss =  0.1015109997637403; Validation Loss = 0.12394417993576212\n",
            "Cost after 295546 iterations : Training Loss =  0.10151099976374012; Validation Loss = 0.12394417993361419\n",
            "Cost after 295547 iterations : Training Loss =  0.10151099976374012; Validation Loss = 0.12394417993146667\n",
            "Cost after 295548 iterations : Training Loss =  0.1015109997637401; Validation Loss = 0.12394417992931876\n",
            "Cost after 295549 iterations : Training Loss =  0.10151099976374006; Validation Loss = 0.1239441799271712\n",
            "Cost after 295550 iterations : Training Loss =  0.10151099976374023; Validation Loss = 0.12394417992502361\n",
            "Cost after 295551 iterations : Training Loss =  0.1015109997637403; Validation Loss = 0.12394417992287629\n",
            "Cost after 295552 iterations : Training Loss =  0.10151099976374005; Validation Loss = 0.12394417992072913\n",
            "Cost after 295553 iterations : Training Loss =  0.10151099976374023; Validation Loss = 0.12394417991858182\n",
            "Cost after 295554 iterations : Training Loss =  0.10151099976374019; Validation Loss = 0.1239441799164346\n",
            "Cost after 295555 iterations : Training Loss =  0.10151099976374013; Validation Loss = 0.12394417991428767\n",
            "Cost after 295556 iterations : Training Loss =  0.10151099976373999; Validation Loss = 0.12394417991214106\n",
            "Cost after 295557 iterations : Training Loss =  0.10151099976374019; Validation Loss = 0.1239441799099941\n",
            "Cost after 295558 iterations : Training Loss =  0.10151099976374023; Validation Loss = 0.1239441799078478\n",
            "Cost after 295559 iterations : Training Loss =  0.10151099976373999; Validation Loss = 0.12394417990570102\n",
            "Cost after 295560 iterations : Training Loss =  0.10151099976373999; Validation Loss = 0.1239441799035546\n",
            "Cost after 295561 iterations : Training Loss =  0.10151099976374008; Validation Loss = 0.12394417990140845\n",
            "Cost after 295562 iterations : Training Loss =  0.10151099976374023; Validation Loss = 0.1239441798992623\n",
            "Cost after 295563 iterations : Training Loss =  0.10151099976373999; Validation Loss = 0.123944179897116\n",
            "Cost after 295564 iterations : Training Loss =  0.10151099976373998; Validation Loss = 0.12394417989497054\n",
            "Cost after 295565 iterations : Training Loss =  0.10151099976374008; Validation Loss = 0.12394417989282448\n",
            "Cost after 295566 iterations : Training Loss =  0.10151099976373994; Validation Loss = 0.12394417989067856\n",
            "Cost after 295567 iterations : Training Loss =  0.10151099976374019; Validation Loss = 0.1239441798885329\n",
            "Cost after 295568 iterations : Training Loss =  0.10151099976374005; Validation Loss = 0.12394417988638722\n",
            "Cost after 295569 iterations : Training Loss =  0.10151099976373985; Validation Loss = 0.12394417988424188\n",
            "Cost after 295570 iterations : Training Loss =  0.1015109997637401; Validation Loss = 0.12394417988209651\n",
            "Cost after 295571 iterations : Training Loss =  0.10151099976374012; Validation Loss = 0.12394417987995128\n",
            "Cost after 295572 iterations : Training Loss =  0.10151099976373999; Validation Loss = 0.12394417987780627\n",
            "Cost after 295573 iterations : Training Loss =  0.1015109997637399; Validation Loss = 0.12394417987566132\n",
            "Cost after 295574 iterations : Training Loss =  0.10151099976373987; Validation Loss = 0.12394417987351648\n",
            "Cost after 295575 iterations : Training Loss =  0.10151099976374005; Validation Loss = 0.12394417987137131\n",
            "Cost after 295576 iterations : Training Loss =  0.10151099976374002; Validation Loss = 0.12394417986922701\n",
            "Cost after 295577 iterations : Training Loss =  0.10151099976373999; Validation Loss = 0.12394417986708217\n",
            "Cost after 295578 iterations : Training Loss =  0.10151099976374005; Validation Loss = 0.12394417986493814\n",
            "Cost after 295579 iterations : Training Loss =  0.10151099976374005; Validation Loss = 0.12394417986279355\n",
            "Cost after 295580 iterations : Training Loss =  0.1015109997637399; Validation Loss = 0.12394417986064958\n",
            "Cost after 295581 iterations : Training Loss =  0.10151099976374005; Validation Loss = 0.123944179858505\n",
            "Cost after 295582 iterations : Training Loss =  0.10151099976373995; Validation Loss = 0.1239441798563613\n",
            "Cost after 295583 iterations : Training Loss =  0.10151099976373994; Validation Loss = 0.12394417985421738\n",
            "Cost after 295584 iterations : Training Loss =  0.1015109997637398; Validation Loss = 0.12394417985207387\n",
            "Cost after 295585 iterations : Training Loss =  0.10151099976373992; Validation Loss = 0.12394417984993011\n",
            "Cost after 295586 iterations : Training Loss =  0.10151099976373998; Validation Loss = 0.12394417984778631\n",
            "Cost after 295587 iterations : Training Loss =  0.1015109997637398; Validation Loss = 0.12394417984564284\n",
            "Cost after 295588 iterations : Training Loss =  0.10151099976373992; Validation Loss = 0.12394417984349955\n",
            "Cost after 295589 iterations : Training Loss =  0.10151099976373988; Validation Loss = 0.12394417984135643\n",
            "Cost after 295590 iterations : Training Loss =  0.10151099976373974; Validation Loss = 0.12394417983921314\n",
            "Cost after 295591 iterations : Training Loss =  0.10151099976373995; Validation Loss = 0.12394417983707005\n",
            "Cost after 295592 iterations : Training Loss =  0.10151099976373976; Validation Loss = 0.12394417983492723\n",
            "Cost after 295593 iterations : Training Loss =  0.1015109997637398; Validation Loss = 0.1239441798327844\n",
            "Cost after 295594 iterations : Training Loss =  0.10151099976373992; Validation Loss = 0.12394417983064182\n",
            "Cost after 295595 iterations : Training Loss =  0.10151099976373992; Validation Loss = 0.12394417982849895\n",
            "Cost after 295596 iterations : Training Loss =  0.10151099976373992; Validation Loss = 0.1239441798263565\n",
            "Cost after 295597 iterations : Training Loss =  0.10151099976373998; Validation Loss = 0.12394417982421416\n",
            "Cost after 295598 iterations : Training Loss =  0.10151099976373999; Validation Loss = 0.1239441798220719\n",
            "Cost after 295599 iterations : Training Loss =  0.10151099976373985; Validation Loss = 0.1239441798199297\n",
            "Cost after 295600 iterations : Training Loss =  0.10151099976373973; Validation Loss = 0.12394417981778752\n",
            "Cost after 295601 iterations : Training Loss =  0.10151099976373987; Validation Loss = 0.12394417981564558\n",
            "Cost after 295602 iterations : Training Loss =  0.10151099976373976; Validation Loss = 0.12394417981350371\n",
            "Cost after 295603 iterations : Training Loss =  0.10151099976373985; Validation Loss = 0.12394417981136222\n",
            "Cost after 295604 iterations : Training Loss =  0.10151099976373962; Validation Loss = 0.12394417980922058\n",
            "Cost after 295605 iterations : Training Loss =  0.1015109997637398; Validation Loss = 0.12394417980707889\n",
            "Cost after 295606 iterations : Training Loss =  0.10151099976374002; Validation Loss = 0.12394417980493766\n",
            "Cost after 295607 iterations : Training Loss =  0.10151099976373966; Validation Loss = 0.12394417980279623\n",
            "Cost after 295608 iterations : Training Loss =  0.10151099976373992; Validation Loss = 0.12394417980065538\n",
            "Cost after 295609 iterations : Training Loss =  0.10151099976373995; Validation Loss = 0.12394417979851419\n",
            "Cost after 295610 iterations : Training Loss =  0.10151099976373981; Validation Loss = 0.12394417979637343\n",
            "Cost after 295611 iterations : Training Loss =  0.1015109997637397; Validation Loss = 0.12394417979423258\n",
            "Cost after 295612 iterations : Training Loss =  0.10151099976373981; Validation Loss = 0.12394417979209164\n",
            "Cost after 295613 iterations : Training Loss =  0.10151099976373981; Validation Loss = 0.12394417978995119\n",
            "Cost after 295614 iterations : Training Loss =  0.10151099976373973; Validation Loss = 0.12394417978781042\n",
            "Cost after 295615 iterations : Training Loss =  0.10151099976373983; Validation Loss = 0.12394417978566973\n",
            "Cost after 295616 iterations : Training Loss =  0.10151099976373988; Validation Loss = 0.12394417978352978\n",
            "Cost after 295617 iterations : Training Loss =  0.10151099976373976; Validation Loss = 0.12394417978138973\n",
            "Cost after 295618 iterations : Training Loss =  0.10151099976373998; Validation Loss = 0.12394417977924949\n",
            "Cost after 295619 iterations : Training Loss =  0.10151099976373969; Validation Loss = 0.12394417977710945\n",
            "Cost after 295620 iterations : Training Loss =  0.10151099976373978; Validation Loss = 0.12394417977496953\n",
            "Cost after 295621 iterations : Training Loss =  0.10151099976373969; Validation Loss = 0.12394417977282969\n",
            "Cost after 295622 iterations : Training Loss =  0.10151099976373974; Validation Loss = 0.12394417977069004\n",
            "Cost after 295623 iterations : Training Loss =  0.1015109997637398; Validation Loss = 0.1239441797685506\n",
            "Cost after 295624 iterations : Training Loss =  0.10151099976373974; Validation Loss = 0.12394417976641096\n",
            "Cost after 295625 iterations : Training Loss =  0.1015109997637398; Validation Loss = 0.12394417976427165\n",
            "Cost after 295626 iterations : Training Loss =  0.10151099976373967; Validation Loss = 0.12394417976213266\n",
            "Cost after 295627 iterations : Training Loss =  0.10151099976373974; Validation Loss = 0.12394417975999365\n",
            "Cost after 295628 iterations : Training Loss =  0.10151099976373966; Validation Loss = 0.12394417975785446\n",
            "Cost after 295629 iterations : Training Loss =  0.10151099976373966; Validation Loss = 0.1239441797557156\n",
            "Cost after 295630 iterations : Training Loss =  0.1015109997637396; Validation Loss = 0.1239441797535766\n",
            "Cost after 295631 iterations : Training Loss =  0.10151099976373963; Validation Loss = 0.12394417975143783\n",
            "Cost after 295632 iterations : Training Loss =  0.10151099976373966; Validation Loss = 0.12394417974929946\n",
            "Cost after 295633 iterations : Training Loss =  0.10151099976373955; Validation Loss = 0.12394417974716089\n",
            "Cost after 295634 iterations : Training Loss =  0.10151099976373958; Validation Loss = 0.12394417974502239\n",
            "Cost after 295635 iterations : Training Loss =  0.10151099976373973; Validation Loss = 0.12394417974288434\n",
            "Cost after 295636 iterations : Training Loss =  0.1015109997637395; Validation Loss = 0.1239441797407465\n",
            "Cost after 295637 iterations : Training Loss =  0.10151099976373969; Validation Loss = 0.12394417973860819\n",
            "Cost after 295638 iterations : Training Loss =  0.10151099976373969; Validation Loss = 0.12394417973647025\n",
            "Cost after 295639 iterations : Training Loss =  0.1015109997637395; Validation Loss = 0.12394417973433218\n",
            "Cost after 295640 iterations : Training Loss =  0.10151099976373962; Validation Loss = 0.12394417973219458\n",
            "Cost after 295641 iterations : Training Loss =  0.10151099976373963; Validation Loss = 0.12394417973005714\n",
            "Cost after 295642 iterations : Training Loss =  0.10151099976373969; Validation Loss = 0.12394417972791956\n",
            "Cost after 295643 iterations : Training Loss =  0.10151099976373974; Validation Loss = 0.12394417972578219\n",
            "Cost after 295644 iterations : Training Loss =  0.1015109997637396; Validation Loss = 0.12394417972364474\n",
            "Cost after 295645 iterations : Training Loss =  0.10151099976373967; Validation Loss = 0.12394417972150766\n",
            "Cost after 295646 iterations : Training Loss =  0.10151099976373967; Validation Loss = 0.12394417971937063\n",
            "Cost after 295647 iterations : Training Loss =  0.10151099976373962; Validation Loss = 0.12394417971723405\n",
            "Cost after 295648 iterations : Training Loss =  0.10151099976373934; Validation Loss = 0.12394417971509715\n",
            "Cost after 295649 iterations : Training Loss =  0.10151099976373963; Validation Loss = 0.12394417971296036\n",
            "Cost after 295650 iterations : Training Loss =  0.10151099976373953; Validation Loss = 0.12394417971082362\n",
            "Cost after 295651 iterations : Training Loss =  0.1015109997637396; Validation Loss = 0.12394417970868694\n",
            "Cost after 295652 iterations : Training Loss =  0.10151099976373937; Validation Loss = 0.12394417970655085\n",
            "Cost after 295653 iterations : Training Loss =  0.10151099976373974; Validation Loss = 0.12394417970441443\n",
            "Cost after 295654 iterations : Training Loss =  0.10151099976373958; Validation Loss = 0.12394417970227839\n",
            "Cost after 295655 iterations : Training Loss =  0.10151099976373949; Validation Loss = 0.12394417970014222\n",
            "Cost after 295656 iterations : Training Loss =  0.10151099976373949; Validation Loss = 0.12394417969800635\n",
            "Cost after 295657 iterations : Training Loss =  0.10151099976373966; Validation Loss = 0.12394417969587028\n",
            "Cost after 295658 iterations : Training Loss =  0.10151099976373942; Validation Loss = 0.1239441796937347\n",
            "Cost after 295659 iterations : Training Loss =  0.10151099976373955; Validation Loss = 0.12394417969159893\n",
            "Cost after 295660 iterations : Training Loss =  0.1015109997637395; Validation Loss = 0.12394417968946368\n",
            "Cost after 295661 iterations : Training Loss =  0.10151099976373949; Validation Loss = 0.12394417968732813\n",
            "Cost after 295662 iterations : Training Loss =  0.10151099976373962; Validation Loss = 0.12394417968519292\n",
            "Cost after 295663 iterations : Training Loss =  0.10151099976373931; Validation Loss = 0.12394417968305771\n",
            "Cost after 295664 iterations : Training Loss =  0.1015109997637395; Validation Loss = 0.12394417968092235\n",
            "Cost after 295665 iterations : Training Loss =  0.10151099976373958; Validation Loss = 0.1239441796787875\n",
            "Cost after 295666 iterations : Training Loss =  0.10151099976373962; Validation Loss = 0.1239441796766525\n",
            "Cost after 295667 iterations : Training Loss =  0.10151099976373942; Validation Loss = 0.12394417967451758\n",
            "Cost after 295668 iterations : Training Loss =  0.1015109997637395; Validation Loss = 0.12394417967238333\n",
            "Cost after 295669 iterations : Training Loss =  0.10151099976373949; Validation Loss = 0.12394417967024858\n",
            "Cost after 295670 iterations : Training Loss =  0.10151099976373942; Validation Loss = 0.12394417966811419\n",
            "Cost after 295671 iterations : Training Loss =  0.10151099976373945; Validation Loss = 0.12394417966597987\n",
            "Cost after 295672 iterations : Training Loss =  0.10151099976373937; Validation Loss = 0.12394417966384551\n",
            "Cost after 295673 iterations : Training Loss =  0.10151099976373942; Validation Loss = 0.1239441796617113\n",
            "Cost after 295674 iterations : Training Loss =  0.10151099976373945; Validation Loss = 0.1239441796595775\n",
            "Cost after 295675 iterations : Training Loss =  0.10151099976373935; Validation Loss = 0.12394417965744349\n",
            "Cost after 295676 iterations : Training Loss =  0.10151099976373944; Validation Loss = 0.12394417965530968\n",
            "Cost after 295677 iterations : Training Loss =  0.1015109997637393; Validation Loss = 0.12394417965317607\n",
            "Cost after 295678 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.12394417965104244\n",
            "Cost after 295679 iterations : Training Loss =  0.10151099976373942; Validation Loss = 0.12394417964890893\n",
            "Cost after 295680 iterations : Training Loss =  0.10151099976373937; Validation Loss = 0.12394417964677555\n",
            "Cost after 295681 iterations : Training Loss =  0.10151099976373935; Validation Loss = 0.12394417964464234\n",
            "Cost after 295682 iterations : Training Loss =  0.10151099976373909; Validation Loss = 0.12394417964250912\n",
            "Cost after 295683 iterations : Training Loss =  0.10151099976373934; Validation Loss = 0.12394417964037631\n",
            "Cost after 295684 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.12394417963824347\n",
            "Cost after 295685 iterations : Training Loss =  0.10151099976373935; Validation Loss = 0.12394417963611033\n",
            "Cost after 295686 iterations : Training Loss =  0.10151099976373941; Validation Loss = 0.12394417963397782\n",
            "Cost after 295687 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.1239441796318451\n",
            "Cost after 295688 iterations : Training Loss =  0.10151099976373935; Validation Loss = 0.12394417962971258\n",
            "Cost after 295689 iterations : Training Loss =  0.10151099976373944; Validation Loss = 0.12394417962758014\n",
            "Cost after 295690 iterations : Training Loss =  0.1015109997637393; Validation Loss = 0.12394417962544822\n",
            "Cost after 295691 iterations : Training Loss =  0.10151099976373926; Validation Loss = 0.12394417962331554\n",
            "Cost after 295692 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.1239441796211836\n",
            "Cost after 295693 iterations : Training Loss =  0.10151099976373924; Validation Loss = 0.12394417961905166\n",
            "Cost after 295694 iterations : Training Loss =  0.10151099976373909; Validation Loss = 0.12394417961691989\n",
            "Cost after 295695 iterations : Training Loss =  0.10151099976373919; Validation Loss = 0.12394417961478815\n",
            "Cost after 295696 iterations : Training Loss =  0.10151099976373917; Validation Loss = 0.12394417961265651\n",
            "Cost after 295697 iterations : Training Loss =  0.10151099976373931; Validation Loss = 0.12394417961052476\n",
            "Cost after 295698 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.12394417960839371\n",
            "Cost after 295699 iterations : Training Loss =  0.10151099976373922; Validation Loss = 0.1239441796062623\n",
            "Cost after 295700 iterations : Training Loss =  0.10151099976373912; Validation Loss = 0.12394417960413112\n",
            "Cost after 295701 iterations : Training Loss =  0.10151099976373926; Validation Loss = 0.123944179602\n",
            "Cost after 295702 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.12394417959986884\n",
            "Cost after 295703 iterations : Training Loss =  0.10151099976373913; Validation Loss = 0.12394417959773826\n",
            "Cost after 295704 iterations : Training Loss =  0.10151099976373923; Validation Loss = 0.1239441795956071\n",
            "Cost after 295705 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.12394417959347667\n",
            "Cost after 295706 iterations : Training Loss =  0.10151099976373924; Validation Loss = 0.12394417959134606\n",
            "Cost after 295707 iterations : Training Loss =  0.10151099976373931; Validation Loss = 0.12394417958921555\n",
            "Cost after 295708 iterations : Training Loss =  0.10151099976373931; Validation Loss = 0.12394417958708527\n",
            "Cost after 295709 iterations : Training Loss =  0.10151099976373894; Validation Loss = 0.12394417958495528\n",
            "Cost after 295710 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.12394417958282505\n",
            "Cost after 295711 iterations : Training Loss =  0.10151099976373913; Validation Loss = 0.12394417958069479\n",
            "Cost after 295712 iterations : Training Loss =  0.10151099976373913; Validation Loss = 0.1239441795785652\n",
            "Cost after 295713 iterations : Training Loss =  0.10151099976373909; Validation Loss = 0.12394417957643533\n",
            "Cost after 295714 iterations : Training Loss =  0.10151099976373923; Validation Loss = 0.1239441795743057\n",
            "Cost after 295715 iterations : Training Loss =  0.10151099976373928; Validation Loss = 0.12394417957217592\n",
            "Cost after 295716 iterations : Training Loss =  0.10151099976373923; Validation Loss = 0.12394417957004632\n",
            "Cost after 295717 iterations : Training Loss =  0.10151099976373919; Validation Loss = 0.123944179567917\n",
            "Cost after 295718 iterations : Training Loss =  0.10151099976373922; Validation Loss = 0.12394417956578807\n",
            "Cost after 295719 iterations : Training Loss =  0.10151099976373924; Validation Loss = 0.12394417956365858\n",
            "Cost after 295720 iterations : Training Loss =  0.101510999763739; Validation Loss = 0.12394417956152966\n",
            "Cost after 295721 iterations : Training Loss =  0.10151099976373909; Validation Loss = 0.1239441795594009\n",
            "Cost after 295722 iterations : Training Loss =  0.10151099976373917; Validation Loss = 0.12394417955727192\n",
            "Cost after 295723 iterations : Training Loss =  0.10151099976373917; Validation Loss = 0.12394417955514307\n",
            "Cost after 295724 iterations : Training Loss =  0.10151099976373906; Validation Loss = 0.12394417955301486\n",
            "Cost after 295725 iterations : Training Loss =  0.10151099976373916; Validation Loss = 0.12394417955088612\n",
            "Cost after 295726 iterations : Training Loss =  0.10151099976373916; Validation Loss = 0.12394417954875776\n",
            "Cost after 295727 iterations : Training Loss =  0.10151099976373897; Validation Loss = 0.12394417954662959\n",
            "Cost after 295728 iterations : Training Loss =  0.10151099976373913; Validation Loss = 0.12394417954450117\n",
            "Cost after 295729 iterations : Training Loss =  0.10151099976373919; Validation Loss = 0.1239441795423731\n",
            "Cost after 295730 iterations : Training Loss =  0.10151099976373903; Validation Loss = 0.12394417954024518\n",
            "Cost after 295731 iterations : Training Loss =  0.10151099976373919; Validation Loss = 0.12394417953811736\n",
            "Cost after 295732 iterations : Training Loss =  0.10151099976373897; Validation Loss = 0.12394417953598959\n",
            "Cost after 295733 iterations : Training Loss =  0.10151099976373903; Validation Loss = 0.12394417953386203\n",
            "Cost after 295734 iterations : Training Loss =  0.10151099976373917; Validation Loss = 0.12394417953173421\n",
            "Cost after 295735 iterations : Training Loss =  0.10151099976373901; Validation Loss = 0.12394417952960719\n",
            "Cost after 295736 iterations : Training Loss =  0.10151099976373905; Validation Loss = 0.12394417952747949\n",
            "Cost after 295737 iterations : Training Loss =  0.10151099976373898; Validation Loss = 0.12394417952535244\n",
            "Cost after 295738 iterations : Training Loss =  0.101510999763739; Validation Loss = 0.12394417952322544\n",
            "Cost after 295739 iterations : Training Loss =  0.10151099976373892; Validation Loss = 0.12394417952109851\n",
            "Cost after 295740 iterations : Training Loss =  0.10151099976373926; Validation Loss = 0.12394417951897178\n",
            "Cost after 295741 iterations : Training Loss =  0.10151099976373901; Validation Loss = 0.12394417951684489\n",
            "Cost after 295742 iterations : Training Loss =  0.1015109997637391; Validation Loss = 0.12394417951471827\n",
            "Cost after 295743 iterations : Training Loss =  0.10151099976373913; Validation Loss = 0.12394417951259168\n",
            "Cost after 295744 iterations : Training Loss =  0.10151099976373898; Validation Loss = 0.12394417951046523\n",
            "Cost after 295745 iterations : Training Loss =  0.10151099976373909; Validation Loss = 0.12394417950833876\n",
            "Cost after 295746 iterations : Training Loss =  0.10151099976373884; Validation Loss = 0.12394417950621271\n",
            "Cost after 295747 iterations : Training Loss =  0.10151099976373901; Validation Loss = 0.12394417950408647\n",
            "Cost after 295748 iterations : Training Loss =  0.10151099976373912; Validation Loss = 0.12394417950196068\n",
            "Cost after 295749 iterations : Training Loss =  0.10151099976373894; Validation Loss = 0.12394417949983462\n",
            "Cost after 295750 iterations : Training Loss =  0.1015109997637389; Validation Loss = 0.12394417949770885\n",
            "Cost after 295751 iterations : Training Loss =  0.10151099976373885; Validation Loss = 0.12394417949558298\n",
            "Cost after 295752 iterations : Training Loss =  0.10151099976373897; Validation Loss = 0.12394417949345726\n",
            "Cost after 295753 iterations : Training Loss =  0.10151099976373887; Validation Loss = 0.12394417949133228\n",
            "Cost after 295754 iterations : Training Loss =  0.101510999763739; Validation Loss = 0.12394417948920694\n",
            "Cost after 295755 iterations : Training Loss =  0.10151099976373898; Validation Loss = 0.12394417948708157\n",
            "Cost after 295756 iterations : Training Loss =  0.10151099976373898; Validation Loss = 0.12394417948495637\n",
            "Cost after 295757 iterations : Training Loss =  0.10151099976373894; Validation Loss = 0.12394417948283123\n",
            "Cost after 295758 iterations : Training Loss =  0.10151099976373913; Validation Loss = 0.12394417948070635\n",
            "Cost after 295759 iterations : Training Loss =  0.10151099976373898; Validation Loss = 0.12394417947858159\n",
            "Cost after 295760 iterations : Training Loss =  0.10151099976373897; Validation Loss = 0.12394417947645714\n",
            "Cost after 295761 iterations : Training Loss =  0.10151099976373891; Validation Loss = 0.1239441794743325\n",
            "Cost after 295762 iterations : Training Loss =  0.10151099976373897; Validation Loss = 0.12394417947220777\n",
            "Cost after 295763 iterations : Training Loss =  0.10151099976373892; Validation Loss = 0.12394417947008353\n",
            "Cost after 295764 iterations : Training Loss =  0.1015109997637389; Validation Loss = 0.12394417946795905\n",
            "Cost after 295765 iterations : Training Loss =  0.101510999763739; Validation Loss = 0.12394417946583482\n",
            "Cost after 295766 iterations : Training Loss =  0.1015109997637389; Validation Loss = 0.12394417946371063\n",
            "Cost after 295767 iterations : Training Loss =  0.10151099976373885; Validation Loss = 0.12394417946158687\n",
            "Cost after 295768 iterations : Training Loss =  0.101510999763739; Validation Loss = 0.12394417945946284\n",
            "Cost after 295769 iterations : Training Loss =  0.10151099976373916; Validation Loss = 0.12394417945733915\n",
            "Cost after 295770 iterations : Training Loss =  0.10151099976373873; Validation Loss = 0.12394417945521546\n",
            "Cost after 295771 iterations : Training Loss =  0.10151099976373887; Validation Loss = 0.12394417945309197\n",
            "Cost after 295772 iterations : Training Loss =  0.10151099976373881; Validation Loss = 0.12394417945096839\n",
            "Cost after 295773 iterations : Training Loss =  0.10151099976373887; Validation Loss = 0.12394417944884492\n",
            "Cost after 295774 iterations : Training Loss =  0.10151099976373887; Validation Loss = 0.1239441794467221\n",
            "Cost after 295775 iterations : Training Loss =  0.1015109997637388; Validation Loss = 0.12394417944459893\n",
            "Cost after 295776 iterations : Training Loss =  0.10151099976373892; Validation Loss = 0.12394417944247581\n",
            "Cost after 295777 iterations : Training Loss =  0.1015109997637388; Validation Loss = 0.12394417944035276\n",
            "Cost after 295778 iterations : Training Loss =  0.10151099976373874; Validation Loss = 0.12394417943823005\n",
            "Cost after 295779 iterations : Training Loss =  0.10151099976373885; Validation Loss = 0.1239441794361074\n",
            "Cost after 295780 iterations : Training Loss =  0.10151099976373887; Validation Loss = 0.12394417943398466\n",
            "Cost after 295781 iterations : Training Loss =  0.10151099976373884; Validation Loss = 0.12394417943186231\n",
            "Cost after 295782 iterations : Training Loss =  0.10151099976373877; Validation Loss = 0.1239441794297399\n",
            "Cost after 295783 iterations : Training Loss =  0.10151099976373894; Validation Loss = 0.12394417942761793\n",
            "Cost after 295784 iterations : Training Loss =  0.10151099976373887; Validation Loss = 0.12394417942549585\n",
            "Cost after 295785 iterations : Training Loss =  0.10151099976373872; Validation Loss = 0.12394417942337396\n",
            "Cost after 295786 iterations : Training Loss =  0.10151099976373891; Validation Loss = 0.12394417942125185\n",
            "Cost after 295787 iterations : Training Loss =  0.1015109997637388; Validation Loss = 0.12394417941913016\n",
            "Cost after 295788 iterations : Training Loss =  0.10151099976373884; Validation Loss = 0.12394417941700843\n",
            "Cost after 295789 iterations : Training Loss =  0.10151099976373873; Validation Loss = 0.12394417941488678\n",
            "Cost after 295790 iterations : Training Loss =  0.10151099976373874; Validation Loss = 0.12394417941276518\n",
            "Cost after 295791 iterations : Training Loss =  0.10151099976373869; Validation Loss = 0.12394417941064409\n",
            "Cost after 295792 iterations : Training Loss =  0.1015109997637386; Validation Loss = 0.12394417940852272\n",
            "Cost after 295793 iterations : Training Loss =  0.10151099976373859; Validation Loss = 0.12394417940640143\n",
            "Cost after 295794 iterations : Training Loss =  0.10151099976373877; Validation Loss = 0.12394417940428026\n",
            "Cost after 295795 iterations : Training Loss =  0.10151099976373865; Validation Loss = 0.12394417940215942\n",
            "Cost after 295796 iterations : Training Loss =  0.10151099976373873; Validation Loss = 0.12394417940003866\n",
            "Cost after 295797 iterations : Training Loss =  0.10151099976373869; Validation Loss = 0.12394417939791803\n",
            "Cost after 295798 iterations : Training Loss =  0.10151099976373859; Validation Loss = 0.1239441793957974\n",
            "Cost after 295799 iterations : Training Loss =  0.10151099976373859; Validation Loss = 0.12394417939367657\n",
            "Cost after 295800 iterations : Training Loss =  0.1015109997637386; Validation Loss = 0.1239441793915564\n",
            "Cost after 295801 iterations : Training Loss =  0.10151099976373873; Validation Loss = 0.12394417938943612\n",
            "Cost after 295802 iterations : Training Loss =  0.1015109997637386; Validation Loss = 0.12394417938731586\n",
            "Cost after 295803 iterations : Training Loss =  0.10151099976373874; Validation Loss = 0.12394417938519565\n",
            "Cost after 295804 iterations : Training Loss =  0.10151099976373874; Validation Loss = 0.12394417938307589\n",
            "Cost after 295805 iterations : Training Loss =  0.1015109997637385; Validation Loss = 0.12394417938095614\n",
            "Cost after 295806 iterations : Training Loss =  0.10151099976373854; Validation Loss = 0.12394417937883624\n",
            "Cost after 295807 iterations : Training Loss =  0.1015109997637386; Validation Loss = 0.12394417937671665\n",
            "Cost after 295808 iterations : Training Loss =  0.10151099976373859; Validation Loss = 0.12394417937459692\n",
            "Cost after 295809 iterations : Training Loss =  0.10151099976373867; Validation Loss = 0.12394417937247773\n",
            "Cost after 295810 iterations : Training Loss =  0.10151099976373873; Validation Loss = 0.12394417937035827\n",
            "Cost after 295811 iterations : Training Loss =  0.10151099976373847; Validation Loss = 0.12394417936823918\n",
            "Cost after 295812 iterations : Training Loss =  0.10151099976373855; Validation Loss = 0.12394417936612019\n",
            "Cost after 295813 iterations : Training Loss =  0.10151099976373866; Validation Loss = 0.12394417936400116\n",
            "Cost after 295814 iterations : Training Loss =  0.10151099976373854; Validation Loss = 0.12394417936188189\n",
            "Cost after 295815 iterations : Training Loss =  0.10151099976373867; Validation Loss = 0.12394417935976332\n",
            "Cost after 295816 iterations : Training Loss =  0.10151099976373866; Validation Loss = 0.12394417935764461\n",
            "Cost after 295817 iterations : Training Loss =  0.1015109997637385; Validation Loss = 0.12394417935552605\n",
            "Cost after 295818 iterations : Training Loss =  0.10151099976373865; Validation Loss = 0.12394417935340755\n",
            "Cost after 295819 iterations : Training Loss =  0.10151099976373872; Validation Loss = 0.1239441793512892\n",
            "Cost after 295820 iterations : Training Loss =  0.10151099976373865; Validation Loss = 0.12394417934917112\n",
            "Cost after 295821 iterations : Training Loss =  0.10151099976373865; Validation Loss = 0.12394417934705286\n",
            "Cost after 295822 iterations : Training Loss =  0.1015109997637386; Validation Loss = 0.12394417934493483\n",
            "Cost after 295823 iterations : Training Loss =  0.10151099976373859; Validation Loss = 0.12394417934281678\n",
            "Cost after 295824 iterations : Training Loss =  0.10151099976373855; Validation Loss = 0.12394417934069887\n",
            "Cost after 295825 iterations : Training Loss =  0.10151099976373855; Validation Loss = 0.12394417933858108\n",
            "Cost after 295826 iterations : Training Loss =  0.10151099976373845; Validation Loss = 0.12394417933646366\n",
            "Cost after 295827 iterations : Training Loss =  0.10151099976373855; Validation Loss = 0.12394417933434589\n",
            "Cost after 295828 iterations : Training Loss =  0.1015109997637385; Validation Loss = 0.12394417933222869\n",
            "Cost after 295829 iterations : Training Loss =  0.10151099976373855; Validation Loss = 0.12394417933011129\n",
            "Cost after 295830 iterations : Training Loss =  0.1015109997637383; Validation Loss = 0.12394417932799408\n",
            "Cost after 295831 iterations : Training Loss =  0.10151099976373854; Validation Loss = 0.12394417932587731\n",
            "Cost after 295832 iterations : Training Loss =  0.10151099976373842; Validation Loss = 0.12394417932376041\n",
            "Cost after 295833 iterations : Training Loss =  0.1015109997637384; Validation Loss = 0.12394417932164337\n",
            "Cost after 295834 iterations : Training Loss =  0.10151099976373852; Validation Loss = 0.12394417931952642\n",
            "Cost after 295835 iterations : Training Loss =  0.10151099976373848; Validation Loss = 0.12394417931740993\n",
            "Cost after 295836 iterations : Training Loss =  0.10151099976373824; Validation Loss = 0.12394417931529304\n",
            "Cost after 295837 iterations : Training Loss =  0.10151099976373852; Validation Loss = 0.12394417931317707\n",
            "Cost after 295838 iterations : Training Loss =  0.10151099976373841; Validation Loss = 0.12394417931106055\n",
            "Cost after 295839 iterations : Training Loss =  0.10151099976373847; Validation Loss = 0.12394417930894419\n",
            "Cost after 295840 iterations : Training Loss =  0.10151099976373845; Validation Loss = 0.12394417930682819\n",
            "Cost after 295841 iterations : Training Loss =  0.10151099976373837; Validation Loss = 0.12394417930471222\n",
            "Cost after 295842 iterations : Training Loss =  0.10151099976373841; Validation Loss = 0.12394417930259642\n",
            "Cost after 295843 iterations : Training Loss =  0.1015109997637385; Validation Loss = 0.12394417930048018\n",
            "Cost after 295844 iterations : Training Loss =  0.10151099976373837; Validation Loss = 0.12394417929836492\n",
            "Cost after 295845 iterations : Training Loss =  0.10151099976373845; Validation Loss = 0.12394417929624925\n",
            "Cost after 295846 iterations : Training Loss =  0.10151099976373847; Validation Loss = 0.12394417929413394\n",
            "Cost after 295847 iterations : Training Loss =  0.10151099976373847; Validation Loss = 0.12394417929201852\n",
            "Cost after 295848 iterations : Training Loss =  0.10151099976373855; Validation Loss = 0.1239441792899032\n",
            "Cost after 295849 iterations : Training Loss =  0.10151099976373833; Validation Loss = 0.1239441792877878\n",
            "Cost after 295850 iterations : Training Loss =  0.10151099976373845; Validation Loss = 0.12394417928567311\n",
            "Cost after 295851 iterations : Training Loss =  0.10151099976373845; Validation Loss = 0.12394417928355798\n",
            "Cost after 295852 iterations : Training Loss =  0.10151099976373842; Validation Loss = 0.12394417928144298\n",
            "Cost after 295853 iterations : Training Loss =  0.10151099976373834; Validation Loss = 0.12394417927932856\n",
            "Cost after 295854 iterations : Training Loss =  0.10151099976373842; Validation Loss = 0.12394417927721371\n",
            "Cost after 295855 iterations : Training Loss =  0.10151099976373841; Validation Loss = 0.12394417927509929\n",
            "Cost after 295856 iterations : Training Loss =  0.10151099976373845; Validation Loss = 0.12394417927298491\n",
            "Cost after 295857 iterations : Training Loss =  0.1015109997637384; Validation Loss = 0.12394417927087054\n",
            "Cost after 295858 iterations : Training Loss =  0.10151099976373842; Validation Loss = 0.12394417926875657\n",
            "Cost after 295859 iterations : Training Loss =  0.10151099976373817; Validation Loss = 0.12394417926664252\n",
            "Cost after 295860 iterations : Training Loss =  0.1015109997637384; Validation Loss = 0.12394417926452846\n",
            "Cost after 295861 iterations : Training Loss =  0.1015109997637383; Validation Loss = 0.12394417926241466\n",
            "Cost after 295862 iterations : Training Loss =  0.10151099976373824; Validation Loss = 0.12394417926030102\n",
            "Cost after 295863 iterations : Training Loss =  0.10151099976373824; Validation Loss = 0.1239441792581874\n",
            "Cost after 295864 iterations : Training Loss =  0.10151099976373822; Validation Loss = 0.12394417925607344\n",
            "Cost after 295865 iterations : Training Loss =  0.10151099976373829; Validation Loss = 0.12394417925396034\n",
            "Cost after 295866 iterations : Training Loss =  0.10151099976373834; Validation Loss = 0.12394417925184692\n",
            "Cost after 295867 iterations : Training Loss =  0.1015109997637383; Validation Loss = 0.12394417924973379\n",
            "Cost after 295868 iterations : Training Loss =  0.10151099976373845; Validation Loss = 0.12394417924762088\n",
            "Cost after 295869 iterations : Training Loss =  0.1015109997637383; Validation Loss = 0.12394417924550771\n",
            "Cost after 295870 iterations : Training Loss =  0.1015109997637383; Validation Loss = 0.12394417924339506\n",
            "Cost after 295871 iterations : Training Loss =  0.10151099976373824; Validation Loss = 0.12394417924128202\n",
            "Cost after 295872 iterations : Training Loss =  0.10151099976373817; Validation Loss = 0.12394417923916923\n",
            "Cost after 295873 iterations : Training Loss =  0.1015109997637384; Validation Loss = 0.12394417923705688\n",
            "Cost after 295874 iterations : Training Loss =  0.10151099976373841; Validation Loss = 0.12394417923494423\n",
            "Cost after 295875 iterations : Training Loss =  0.1015109997637382; Validation Loss = 0.12394417923283213\n",
            "Cost after 295876 iterations : Training Loss =  0.10151099976373822; Validation Loss = 0.12394417923072\n",
            "Cost after 295877 iterations : Training Loss =  0.10151099976373835; Validation Loss = 0.12394417922860793\n",
            "Cost after 295878 iterations : Training Loss =  0.1015109997637383; Validation Loss = 0.12394417922649584\n",
            "Cost after 295879 iterations : Training Loss =  0.10151099976373815; Validation Loss = 0.12394417922438393\n",
            "Cost after 295880 iterations : Training Loss =  0.10151099976373829; Validation Loss = 0.12394417922227213\n",
            "Cost after 295881 iterations : Training Loss =  0.1015109997637384; Validation Loss = 0.12394417922016039\n",
            "Cost after 295882 iterations : Training Loss =  0.10151099976373805; Validation Loss = 0.12394417921804866\n",
            "Cost after 295883 iterations : Training Loss =  0.10151099976373834; Validation Loss = 0.12394417921593716\n",
            "Cost after 295884 iterations : Training Loss =  0.10151099976373829; Validation Loss = 0.12394417921382588\n",
            "Cost after 295885 iterations : Training Loss =  0.10151099976373801; Validation Loss = 0.12394417921171454\n",
            "Cost after 295886 iterations : Training Loss =  0.10151099976373809; Validation Loss = 0.1239441792096034\n",
            "Cost after 295887 iterations : Training Loss =  0.10151099976373827; Validation Loss = 0.1239441792074923\n",
            "Cost after 295888 iterations : Training Loss =  0.10151099976373817; Validation Loss = 0.1239441792053814\n",
            "Cost after 295889 iterations : Training Loss =  0.10151099976373823; Validation Loss = 0.12394417920327051\n",
            "Cost after 295890 iterations : Training Loss =  0.1015109997637382; Validation Loss = 0.12394417920115987\n",
            "Cost after 295891 iterations : Training Loss =  0.10151099976373822; Validation Loss = 0.1239441791990492\n",
            "Cost after 295892 iterations : Training Loss =  0.10151099976373827; Validation Loss = 0.12394417919693851\n",
            "Cost after 295893 iterations : Training Loss =  0.1015109997637381; Validation Loss = 0.12394417919482828\n",
            "Cost after 295894 iterations : Training Loss =  0.10151099976373805; Validation Loss = 0.12394417919271782\n",
            "Cost after 295895 iterations : Training Loss =  0.10151099976373824; Validation Loss = 0.12394417919060778\n",
            "Cost after 295896 iterations : Training Loss =  0.1015109997637382; Validation Loss = 0.12394417918849775\n",
            "Cost after 295897 iterations : Training Loss =  0.10151099976373817; Validation Loss = 0.12394417918638782\n",
            "Cost after 295898 iterations : Training Loss =  0.10151099976373817; Validation Loss = 0.12394417918427805\n",
            "Cost after 295899 iterations : Training Loss =  0.10151099976373805; Validation Loss = 0.1239441791821679\n",
            "Cost after 295900 iterations : Training Loss =  0.10151099976373823; Validation Loss = 0.12394417918005847\n",
            "Cost after 295901 iterations : Training Loss =  0.1015109997637381; Validation Loss = 0.12394417917794914\n",
            "Cost after 295902 iterations : Training Loss =  0.10151099976373812; Validation Loss = 0.12394417917583933\n",
            "Cost after 295903 iterations : Training Loss =  0.10151099976373816; Validation Loss = 0.12394417917372995\n",
            "Cost after 295904 iterations : Training Loss =  0.10151099976373817; Validation Loss = 0.12394417917162068\n",
            "Cost after 295905 iterations : Training Loss =  0.10151099976373823; Validation Loss = 0.12394417916951174\n",
            "Cost after 295906 iterations : Training Loss =  0.10151099976373805; Validation Loss = 0.12394417916740265\n",
            "Cost after 295907 iterations : Training Loss =  0.10151099976373792; Validation Loss = 0.1239441791652938\n",
            "Cost after 295908 iterations : Training Loss =  0.10151099976373805; Validation Loss = 0.12394417916318513\n",
            "Cost after 295909 iterations : Training Loss =  0.10151099976373817; Validation Loss = 0.12394417916107646\n",
            "Cost after 295910 iterations : Training Loss =  0.10151099976373791; Validation Loss = 0.12394417915896774\n",
            "Cost after 295911 iterations : Training Loss =  0.10151099976373801; Validation Loss = 0.12394417915685908\n",
            "Cost after 295912 iterations : Training Loss =  0.10151099976373816; Validation Loss = 0.12394417915475069\n",
            "Cost after 295913 iterations : Training Loss =  0.10151099976373798; Validation Loss = 0.12394417915264275\n",
            "Cost after 295914 iterations : Training Loss =  0.10151099976373801; Validation Loss = 0.12394417915053438\n",
            "Cost after 295915 iterations : Training Loss =  0.10151099976373798; Validation Loss = 0.12394417914842625\n",
            "Cost after 295916 iterations : Training Loss =  0.10151099976373815; Validation Loss = 0.12394417914631838\n",
            "Cost after 295917 iterations : Training Loss =  0.10151099976373784; Validation Loss = 0.12394417914421055\n",
            "Cost after 295918 iterations : Training Loss =  0.10151099976373805; Validation Loss = 0.12394417914210258\n",
            "Cost after 295919 iterations : Training Loss =  0.1015109997637381; Validation Loss = 0.12394417913999534\n",
            "Cost after 295920 iterations : Training Loss =  0.10151099976373808; Validation Loss = 0.12394417913788758\n",
            "Cost after 295921 iterations : Training Loss =  0.10151099976373797; Validation Loss = 0.1239441791357803\n",
            "Cost after 295922 iterations : Training Loss =  0.10151099976373808; Validation Loss = 0.1239441791336728\n",
            "Cost after 295923 iterations : Training Loss =  0.10151099976373804; Validation Loss = 0.12394417913156602\n",
            "Cost after 295924 iterations : Training Loss =  0.10151099976373783; Validation Loss = 0.12394417912945872\n",
            "Cost after 295925 iterations : Training Loss =  0.10151099976373792; Validation Loss = 0.12394417912735174\n",
            "Cost after 295926 iterations : Training Loss =  0.10151099976373804; Validation Loss = 0.123944179125245\n",
            "Cost after 295927 iterations : Training Loss =  0.10151099976373788; Validation Loss = 0.12394417912313825\n",
            "Cost after 295928 iterations : Training Loss =  0.10151099976373808; Validation Loss = 0.12394417912103119\n",
            "Cost after 295929 iterations : Training Loss =  0.1015109997637379; Validation Loss = 0.12394417911892461\n",
            "Cost after 295930 iterations : Training Loss =  0.10151099976373802; Validation Loss = 0.12394417911681845\n",
            "Cost after 295931 iterations : Training Loss =  0.10151099976373791; Validation Loss = 0.12394417911471216\n",
            "Cost after 295932 iterations : Training Loss =  0.10151099976373784; Validation Loss = 0.12394417911260601\n",
            "Cost after 295933 iterations : Training Loss =  0.10151099976373795; Validation Loss = 0.12394417911049946\n",
            "Cost after 295934 iterations : Training Loss =  0.10151099976373792; Validation Loss = 0.12394417910839355\n",
            "Cost after 295935 iterations : Training Loss =  0.10151099976373797; Validation Loss = 0.12394417910628767\n",
            "Cost after 295936 iterations : Training Loss =  0.10151099976373801; Validation Loss = 0.12394417910418183\n",
            "Cost after 295937 iterations : Training Loss =  0.10151099976373777; Validation Loss = 0.12394417910207634\n",
            "Cost after 295938 iterations : Training Loss =  0.1015109997637379; Validation Loss = 0.12394417909997053\n",
            "Cost after 295939 iterations : Training Loss =  0.10151099976373783; Validation Loss = 0.12394417909786487\n",
            "Cost after 295940 iterations : Training Loss =  0.10151099976373802; Validation Loss = 0.12394417909575985\n",
            "Cost after 295941 iterations : Training Loss =  0.10151099976373784; Validation Loss = 0.12394417909365463\n",
            "Cost after 295942 iterations : Training Loss =  0.10151099976373773; Validation Loss = 0.12394417909154942\n",
            "Cost after 295943 iterations : Training Loss =  0.10151099976373797; Validation Loss = 0.12394417908944434\n",
            "Cost after 295944 iterations : Training Loss =  0.10151099976373797; Validation Loss = 0.12394417908733926\n",
            "Cost after 295945 iterations : Training Loss =  0.10151099976373783; Validation Loss = 0.12394417908523467\n",
            "Cost after 295946 iterations : Training Loss =  0.1015109997637379; Validation Loss = 0.12394417908312966\n",
            "Cost after 295947 iterations : Training Loss =  0.10151099976373788; Validation Loss = 0.1239441790810248\n",
            "Cost after 295948 iterations : Training Loss =  0.10151099976373786; Validation Loss = 0.12394417907892054\n",
            "Cost after 295949 iterations : Training Loss =  0.10151099976373779; Validation Loss = 0.12394417907681586\n",
            "Cost after 295950 iterations : Training Loss =  0.10151099976373784; Validation Loss = 0.12394417907471153\n",
            "Cost after 295951 iterations : Training Loss =  0.1015109997637378; Validation Loss = 0.1239441790726077\n",
            "Cost after 295952 iterations : Training Loss =  0.10151099976373765; Validation Loss = 0.12394417907050334\n",
            "Cost after 295953 iterations : Training Loss =  0.10151099976373763; Validation Loss = 0.12394417906839922\n",
            "Cost after 295954 iterations : Training Loss =  0.10151099976373797; Validation Loss = 0.12394417906629525\n",
            "Cost after 295955 iterations : Training Loss =  0.1015109997637379; Validation Loss = 0.12394417906419157\n",
            "Cost after 295956 iterations : Training Loss =  0.10151099976373772; Validation Loss = 0.12394417906208795\n",
            "Cost after 295957 iterations : Training Loss =  0.10151099976373772; Validation Loss = 0.12394417905998432\n",
            "Cost after 295958 iterations : Training Loss =  0.10151099976373773; Validation Loss = 0.123944179057881\n",
            "Cost after 295959 iterations : Training Loss =  0.10151099976373779; Validation Loss = 0.1239441790557776\n",
            "Cost after 295960 iterations : Training Loss =  0.1015109997637378; Validation Loss = 0.12394417905367412\n",
            "Cost after 295961 iterations : Training Loss =  0.10151099976373758; Validation Loss = 0.12394417905157104\n",
            "Cost after 295962 iterations : Training Loss =  0.10151099976373784; Validation Loss = 0.12394417904946803\n",
            "Cost after 295963 iterations : Training Loss =  0.1015109997637378; Validation Loss = 0.1239441790473649\n",
            "Cost after 295964 iterations : Training Loss =  0.10151099976373783; Validation Loss = 0.12394417904526212\n",
            "Cost after 295965 iterations : Training Loss =  0.10151099976373783; Validation Loss = 0.12394417904315948\n",
            "Cost after 295966 iterations : Training Loss =  0.10151099976373786; Validation Loss = 0.1239441790410569\n",
            "Cost after 295967 iterations : Training Loss =  0.10151099976373777; Validation Loss = 0.12394417903895448\n",
            "Cost after 295968 iterations : Training Loss =  0.1015109997637377; Validation Loss = 0.1239441790368517\n",
            "Cost after 295969 iterations : Training Loss =  0.1015109997637378; Validation Loss = 0.12394417903474965\n",
            "Cost after 295970 iterations : Training Loss =  0.10151099976373788; Validation Loss = 0.1239441790326473\n",
            "Cost after 295971 iterations : Training Loss =  0.10151099976373758; Validation Loss = 0.12394417903054518\n",
            "Cost after 295972 iterations : Training Loss =  0.1015109997637376; Validation Loss = 0.1239441790284433\n",
            "Cost after 295973 iterations : Training Loss =  0.10151099976373776; Validation Loss = 0.12394417902634144\n",
            "Cost after 295974 iterations : Training Loss =  0.10151099976373791; Validation Loss = 0.12394417902423968\n",
            "Cost after 295975 iterations : Training Loss =  0.10151099976373763; Validation Loss = 0.12394417902213807\n",
            "Cost after 295976 iterations : Training Loss =  0.10151099976373763; Validation Loss = 0.12394417902003639\n",
            "Cost after 295977 iterations : Training Loss =  0.1015109997637377; Validation Loss = 0.12394417901793524\n",
            "Cost after 295978 iterations : Training Loss =  0.10151099976373758; Validation Loss = 0.12394417901583367\n",
            "Cost after 295979 iterations : Training Loss =  0.10151099976373766; Validation Loss = 0.1239441790137325\n",
            "Cost after 295980 iterations : Training Loss =  0.10151099976373756; Validation Loss = 0.12394417901163139\n",
            "Cost after 295981 iterations : Training Loss =  0.10151099976373776; Validation Loss = 0.12394417900953031\n",
            "Cost after 295982 iterations : Training Loss =  0.1015109997637378; Validation Loss = 0.12394417900742948\n",
            "Cost after 295983 iterations : Training Loss =  0.10151099976373765; Validation Loss = 0.12394417900532854\n",
            "Cost after 295984 iterations : Training Loss =  0.10151099976373777; Validation Loss = 0.12394417900322773\n",
            "Cost after 295985 iterations : Training Loss =  0.10151099976373765; Validation Loss = 0.1239441790011272\n",
            "Cost after 295986 iterations : Training Loss =  0.10151099976373758; Validation Loss = 0.12394417899902689\n",
            "Cost after 295987 iterations : Training Loss =  0.10151099976373759; Validation Loss = 0.12394417899692652\n",
            "Cost after 295988 iterations : Training Loss =  0.10151099976373748; Validation Loss = 0.12394417899482592\n",
            "Cost after 295989 iterations : Training Loss =  0.10151099976373754; Validation Loss = 0.12394417899272604\n",
            "Cost after 295990 iterations : Training Loss =  0.10151099976373773; Validation Loss = 0.12394417899062594\n",
            "Cost after 295991 iterations : Training Loss =  0.1015109997637376; Validation Loss = 0.1239441789885261\n",
            "Cost after 295992 iterations : Training Loss =  0.10151099976373747; Validation Loss = 0.12394417898642608\n",
            "Cost after 295993 iterations : Training Loss =  0.10151099976373776; Validation Loss = 0.1239441789843264\n",
            "Cost after 295994 iterations : Training Loss =  0.10151099976373752; Validation Loss = 0.12394417898222695\n",
            "Cost after 295995 iterations : Training Loss =  0.10151099976373756; Validation Loss = 0.12394417898012744\n",
            "Cost after 295996 iterations : Training Loss =  0.1015109997637376; Validation Loss = 0.12394417897802808\n",
            "Cost after 295997 iterations : Training Loss =  0.1015109997637376; Validation Loss = 0.12394417897592848\n",
            "Cost after 295998 iterations : Training Loss =  0.10151099976373758; Validation Loss = 0.1239441789738295\n",
            "Cost after 295999 iterations : Training Loss =  0.10151099976373772; Validation Loss = 0.12394417897173007\n",
            "Cost after 296000 iterations : Training Loss =  0.10151099976373772; Validation Loss = 0.1239441789696313\n",
            "Cost after 296001 iterations : Training Loss =  0.10151099976373756; Validation Loss = 0.12394417896753243\n",
            "Cost after 296002 iterations : Training Loss =  0.10151099976373758; Validation Loss = 0.12394417896543353\n",
            "Cost after 296003 iterations : Training Loss =  0.10151099976373756; Validation Loss = 0.123944178963335\n",
            "Cost after 296004 iterations : Training Loss =  0.10151099976373765; Validation Loss = 0.12394417896123662\n",
            "Cost after 296005 iterations : Training Loss =  0.10151099976373754; Validation Loss = 0.12394417895913805\n",
            "Cost after 296006 iterations : Training Loss =  0.1015109997637374; Validation Loss = 0.12394417895703976\n",
            "Cost after 296007 iterations : Training Loss =  0.10151099976373741; Validation Loss = 0.12394417895494136\n",
            "Cost after 296008 iterations : Training Loss =  0.10151099976373741; Validation Loss = 0.12394417895284314\n",
            "Cost after 296009 iterations : Training Loss =  0.1015109997637374; Validation Loss = 0.12394417895074508\n",
            "Cost after 296010 iterations : Training Loss =  0.10151099976373744; Validation Loss = 0.12394417894864748\n",
            "Cost after 296011 iterations : Training Loss =  0.10151099976373763; Validation Loss = 0.12394417894654954\n",
            "Cost after 296012 iterations : Training Loss =  0.10151099976373741; Validation Loss = 0.12394417894445192\n",
            "Cost after 296013 iterations : Training Loss =  0.10151099976373748; Validation Loss = 0.12394417894235389\n",
            "Cost after 296014 iterations : Training Loss =  0.10151099976373736; Validation Loss = 0.12394417894025664\n",
            "Cost after 296015 iterations : Training Loss =  0.10151099976373748; Validation Loss = 0.12394417893815933\n",
            "Cost after 296016 iterations : Training Loss =  0.10151099976373751; Validation Loss = 0.12394417893606204\n",
            "Cost after 296017 iterations : Training Loss =  0.10151099976373734; Validation Loss = 0.12394417893396469\n",
            "Cost after 296018 iterations : Training Loss =  0.10151099976373756; Validation Loss = 0.12394417893186752\n",
            "Cost after 296019 iterations : Training Loss =  0.10151099976373744; Validation Loss = 0.1239441789297709\n",
            "Cost after 296020 iterations : Training Loss =  0.10151099976373731; Validation Loss = 0.12394417892767383\n",
            "Cost after 296021 iterations : Training Loss =  0.1015109997637376; Validation Loss = 0.12394417892557742\n",
            "Cost after 296022 iterations : Training Loss =  0.10151099976373767; Validation Loss = 0.1239441789234808\n",
            "Cost after 296023 iterations : Training Loss =  0.10151099976373748; Validation Loss = 0.12394417892138407\n",
            "Cost after 296024 iterations : Training Loss =  0.10151099976373744; Validation Loss = 0.1239441789192878\n",
            "Cost after 296025 iterations : Training Loss =  0.10151099976373744; Validation Loss = 0.12394417891719124\n",
            "Cost after 296026 iterations : Training Loss =  0.1015109997637374; Validation Loss = 0.12394417891509509\n",
            "Cost after 296027 iterations : Training Loss =  0.1015109997637374; Validation Loss = 0.12394417891299897\n",
            "Cost after 296028 iterations : Training Loss =  0.10151099976373736; Validation Loss = 0.12394417891090294\n",
            "Cost after 296029 iterations : Training Loss =  0.10151099976373729; Validation Loss = 0.12394417890880732\n",
            "Cost after 296030 iterations : Training Loss =  0.10151099976373731; Validation Loss = 0.1239441789067114\n",
            "Cost after 296031 iterations : Training Loss =  0.10151099976373726; Validation Loss = 0.1239441789046154\n",
            "Cost after 296032 iterations : Training Loss =  0.10151099976373729; Validation Loss = 0.12394417890251998\n",
            "Cost after 296033 iterations : Training Loss =  0.1015109997637374; Validation Loss = 0.12394417890042458\n",
            "Cost after 296034 iterations : Training Loss =  0.10151099976373729; Validation Loss = 0.12394417889832916\n",
            "Cost after 296035 iterations : Training Loss =  0.1015109997637374; Validation Loss = 0.12394417889623359\n",
            "Cost after 296036 iterations : Training Loss =  0.10151099976373731; Validation Loss = 0.12394417889413868\n",
            "Cost after 296037 iterations : Training Loss =  0.10151099976373738; Validation Loss = 0.12394417889204354\n",
            "Cost after 296038 iterations : Training Loss =  0.10151099976373738; Validation Loss = 0.12394417888994858\n",
            "Cost after 296039 iterations : Training Loss =  0.10151099976373731; Validation Loss = 0.12394417888785356\n",
            "Cost after 296040 iterations : Training Loss =  0.10151099976373722; Validation Loss = 0.12394417888575911\n",
            "Cost after 296041 iterations : Training Loss =  0.10151099976373726; Validation Loss = 0.12394417888366452\n",
            "Cost after 296042 iterations : Training Loss =  0.10151099976373733; Validation Loss = 0.1239441788815702\n",
            "Cost after 296043 iterations : Training Loss =  0.10151099976373731; Validation Loss = 0.12394417887947536\n",
            "Cost after 296044 iterations : Training Loss =  0.10151099976373736; Validation Loss = 0.12394417887738142\n",
            "Cost after 296045 iterations : Training Loss =  0.10151099976373734; Validation Loss = 0.12394417887528691\n",
            "Cost after 296046 iterations : Training Loss =  0.10151099976373729; Validation Loss = 0.12394417887319303\n",
            "Cost after 296047 iterations : Training Loss =  0.10151099976373729; Validation Loss = 0.12394417887109897\n",
            "Cost after 296048 iterations : Training Loss =  0.1015109997637372; Validation Loss = 0.12394417886900491\n",
            "Cost after 296049 iterations : Training Loss =  0.10151099976373709; Validation Loss = 0.12394417886691113\n",
            "Cost after 296050 iterations : Training Loss =  0.10151099976373715; Validation Loss = 0.12394417886481776\n",
            "Cost after 296051 iterations : Training Loss =  0.10151099976373709; Validation Loss = 0.12394417886272437\n",
            "Cost after 296052 iterations : Training Loss =  0.10151099976373731; Validation Loss = 0.12394417886063062\n",
            "Cost after 296053 iterations : Training Loss =  0.10151099976373731; Validation Loss = 0.12394417885853722\n",
            "Cost after 296054 iterations : Training Loss =  0.1015109997637372; Validation Loss = 0.12394417885644393\n",
            "Cost after 296055 iterations : Training Loss =  0.1015109997637372; Validation Loss = 0.12394417885435091\n",
            "Cost after 296056 iterations : Training Loss =  0.10151099976373726; Validation Loss = 0.12394417885225795\n",
            "Cost after 296057 iterations : Training Loss =  0.10151099976373716; Validation Loss = 0.12394417885016495\n",
            "Cost after 296058 iterations : Training Loss =  0.10151099976373738; Validation Loss = 0.12394417884807211\n",
            "Cost after 296059 iterations : Training Loss =  0.1015109997637372; Validation Loss = 0.12394417884597933\n",
            "Cost after 296060 iterations : Training Loss =  0.1015109997637372; Validation Loss = 0.12394417884388695\n",
            "Cost after 296061 iterations : Training Loss =  0.10151099976373719; Validation Loss = 0.12394417884179447\n",
            "Cost after 296062 iterations : Training Loss =  0.10151099976373726; Validation Loss = 0.12394417883970209\n",
            "Cost after 296063 iterations : Training Loss =  0.10151099976373719; Validation Loss = 0.12394417883760973\n",
            "Cost after 296064 iterations : Training Loss =  0.1015109997637372; Validation Loss = 0.12394417883551752\n",
            "Cost after 296065 iterations : Training Loss =  0.10151099976373702; Validation Loss = 0.1239441788334251\n",
            "Cost after 296066 iterations : Training Loss =  0.10151099976373716; Validation Loss = 0.1239441788313334\n",
            "Cost after 296067 iterations : Training Loss =  0.10151099976373726; Validation Loss = 0.12394417882924151\n",
            "Cost after 296068 iterations : Training Loss =  0.10151099976373719; Validation Loss = 0.12394417882714988\n",
            "Cost after 296069 iterations : Training Loss =  0.10151099976373715; Validation Loss = 0.12394417882505825\n",
            "Cost after 296070 iterations : Training Loss =  0.10151099976373716; Validation Loss = 0.12394417882296661\n",
            "Cost after 296071 iterations : Training Loss =  0.10151099976373715; Validation Loss = 0.12394417882087513\n",
            "Cost after 296072 iterations : Training Loss =  0.10151099976373723; Validation Loss = 0.1239441788187839\n",
            "Cost after 296073 iterations : Training Loss =  0.10151099976373706; Validation Loss = 0.12394417881669274\n",
            "Cost after 296074 iterations : Training Loss =  0.10151099976373715; Validation Loss = 0.12394417881460189\n",
            "Cost after 296075 iterations : Training Loss =  0.10151099976373704; Validation Loss = 0.12394417881251087\n",
            "Cost after 296076 iterations : Training Loss =  0.10151099976373715; Validation Loss = 0.1239441788104198\n",
            "Cost after 296077 iterations : Training Loss =  0.101510999763737; Validation Loss = 0.12394417880832923\n",
            "Cost after 296078 iterations : Training Loss =  0.1015109997637372; Validation Loss = 0.12394417880623806\n",
            "Cost after 296079 iterations : Training Loss =  0.10151099976373697; Validation Loss = 0.12394417880414771\n",
            "Cost after 296080 iterations : Training Loss =  0.10151099976373716; Validation Loss = 0.12394417880205684\n",
            "Cost after 296081 iterations : Training Loss =  0.10151099976373716; Validation Loss = 0.12394417879996673\n",
            "Cost after 296082 iterations : Training Loss =  0.10151099976373708; Validation Loss = 0.12394417879787611\n",
            "Cost after 296083 iterations : Training Loss =  0.10151099976373719; Validation Loss = 0.12394417879578626\n",
            "Cost after 296084 iterations : Training Loss =  0.10151099976373697; Validation Loss = 0.12394417879369611\n",
            "Cost after 296085 iterations : Training Loss =  0.10151099976373706; Validation Loss = 0.12394417879160646\n",
            "Cost after 296086 iterations : Training Loss =  0.10151099976373702; Validation Loss = 0.12394417878951611\n",
            "Cost after 296087 iterations : Training Loss =  0.10151099976373715; Validation Loss = 0.12394417878742672\n",
            "Cost after 296088 iterations : Training Loss =  0.10151099976373708; Validation Loss = 0.1239441787853372\n",
            "Cost after 296089 iterations : Training Loss =  0.10151099976373712; Validation Loss = 0.12394417878324741\n",
            "Cost after 296090 iterations : Training Loss =  0.101510999763737; Validation Loss = 0.12394417878115821\n",
            "Cost after 296091 iterations : Training Loss =  0.10151099976373715; Validation Loss = 0.12394417877906891\n",
            "Cost after 296092 iterations : Training Loss =  0.10151099976373702; Validation Loss = 0.12394417877697946\n",
            "Cost after 296093 iterations : Training Loss =  0.10151099976373709; Validation Loss = 0.12394417877489054\n",
            "Cost after 296094 iterations : Training Loss =  0.10151099976373697; Validation Loss = 0.12394417877280152\n",
            "Cost after 296095 iterations : Training Loss =  0.10151099976373691; Validation Loss = 0.1239441787707126\n",
            "Cost after 296096 iterations : Training Loss =  0.10151099976373695; Validation Loss = 0.1239441787686239\n",
            "Cost after 296097 iterations : Training Loss =  0.10151099976373695; Validation Loss = 0.12394417876653509\n",
            "Cost after 296098 iterations : Training Loss =  0.10151099976373704; Validation Loss = 0.12394417876444641\n",
            "Cost after 296099 iterations : Training Loss =  0.10151099976373688; Validation Loss = 0.12394417876235833\n",
            "Cost after 296100 iterations : Training Loss =  0.10151099976373715; Validation Loss = 0.12394417876026971\n",
            "Cost after 296101 iterations : Training Loss =  0.10151099976373684; Validation Loss = 0.12394417875818153\n",
            "Cost after 296102 iterations : Training Loss =  0.10151099976373708; Validation Loss = 0.12394417875609362\n",
            "Cost after 296103 iterations : Training Loss =  0.10151099976373681; Validation Loss = 0.12394417875400537\n",
            "Cost after 296104 iterations : Training Loss =  0.10151099976373697; Validation Loss = 0.12394417875191746\n",
            "Cost after 296105 iterations : Training Loss =  0.10151099976373697; Validation Loss = 0.1239441787498297\n",
            "Cost after 296106 iterations : Training Loss =  0.10151099976373687; Validation Loss = 0.12394417874774194\n",
            "Cost after 296107 iterations : Training Loss =  0.10151099976373702; Validation Loss = 0.1239441787456546\n",
            "Cost after 296108 iterations : Training Loss =  0.10151099976373684; Validation Loss = 0.1239441787435667\n",
            "Cost after 296109 iterations : Training Loss =  0.10151099976373691; Validation Loss = 0.12394417874147932\n",
            "Cost after 296110 iterations : Training Loss =  0.10151099976373691; Validation Loss = 0.12394417873939215\n",
            "Cost after 296111 iterations : Training Loss =  0.10151099976373694; Validation Loss = 0.12394417873730477\n",
            "Cost after 296112 iterations : Training Loss =  0.101510999763737; Validation Loss = 0.12394417873521783\n",
            "Cost after 296113 iterations : Training Loss =  0.10151099976373684; Validation Loss = 0.12394417873313064\n",
            "Cost after 296114 iterations : Training Loss =  0.10151099976373694; Validation Loss = 0.12394417873104396\n",
            "Cost after 296115 iterations : Training Loss =  0.1015109997637369; Validation Loss = 0.12394417872895705\n",
            "Cost after 296116 iterations : Training Loss =  0.10151099976373697; Validation Loss = 0.12394417872687022\n",
            "Cost after 296117 iterations : Training Loss =  0.10151099976373672; Validation Loss = 0.12394417872478383\n",
            "Cost after 296118 iterations : Training Loss =  0.10151099976373687; Validation Loss = 0.12394417872269718\n",
            "Cost after 296119 iterations : Training Loss =  0.10151099976373691; Validation Loss = 0.12394417872061098\n",
            "Cost after 296120 iterations : Training Loss =  0.10151099976373688; Validation Loss = 0.12394417871852471\n",
            "Cost after 296121 iterations : Training Loss =  0.10151099976373676; Validation Loss = 0.12394417871643867\n",
            "Cost after 296122 iterations : Training Loss =  0.10151099976373681; Validation Loss = 0.12394417871435248\n",
            "Cost after 296123 iterations : Training Loss =  0.10151099976373694; Validation Loss = 0.12394417871226651\n",
            "Cost after 296124 iterations : Training Loss =  0.10151099976373684; Validation Loss = 0.12394417871018053\n",
            "Cost after 296125 iterations : Training Loss =  0.10151099976373679; Validation Loss = 0.12394417870809506\n",
            "Cost after 296126 iterations : Training Loss =  0.1015109997637367; Validation Loss = 0.12394417870600943\n",
            "Cost after 296127 iterations : Training Loss =  0.10151099976373687; Validation Loss = 0.123944178703924\n",
            "Cost after 296128 iterations : Training Loss =  0.10151099976373684; Validation Loss = 0.12394417870183855\n",
            "Cost after 296129 iterations : Training Loss =  0.10151099976373679; Validation Loss = 0.12394417869975319\n",
            "Cost after 296130 iterations : Training Loss =  0.10151099976373681; Validation Loss = 0.12394417869766804\n",
            "Cost after 296131 iterations : Training Loss =  0.1015109997637369; Validation Loss = 0.12394417869558336\n",
            "Cost after 296132 iterations : Training Loss =  0.10151099976373684; Validation Loss = 0.12394417869349812\n",
            "Cost after 296133 iterations : Training Loss =  0.10151099976373688; Validation Loss = 0.1239441786914134\n",
            "Cost after 296134 iterations : Training Loss =  0.10151099976373683; Validation Loss = 0.12394417868932847\n",
            "Cost after 296135 iterations : Training Loss =  0.10151099976373694; Validation Loss = 0.12394417868724387\n",
            "Cost after 296136 iterations : Training Loss =  0.10151099976373676; Validation Loss = 0.1239441786851595\n",
            "Cost after 296137 iterations : Training Loss =  0.1015109997637367; Validation Loss = 0.12394417868307482\n",
            "Cost after 296138 iterations : Training Loss =  0.10151099976373675; Validation Loss = 0.12394417868099071\n",
            "Cost after 296139 iterations : Training Loss =  0.1015109997637367; Validation Loss = 0.12394417867890639\n",
            "Cost after 296140 iterations : Training Loss =  0.10151099976373677; Validation Loss = 0.12394417867682243\n",
            "Cost after 296141 iterations : Training Loss =  0.10151099976373679; Validation Loss = 0.1239441786747385\n",
            "Cost after 296142 iterations : Training Loss =  0.10151099976373687; Validation Loss = 0.12394417867265439\n",
            "Cost after 296143 iterations : Training Loss =  0.10151099976373687; Validation Loss = 0.12394417867057037\n",
            "Cost after 296144 iterations : Training Loss =  0.10151099976373679; Validation Loss = 0.1239441786684867\n",
            "Cost after 296145 iterations : Training Loss =  0.10151099976373668; Validation Loss = 0.12394417866640307\n",
            "Cost after 296146 iterations : Training Loss =  0.10151099976373676; Validation Loss = 0.12394417866431962\n",
            "Cost after 296147 iterations : Training Loss =  0.10151099976373669; Validation Loss = 0.12394417866223618\n",
            "Cost after 296148 iterations : Training Loss =  0.10151099976373675; Validation Loss = 0.12394417866015317\n",
            "Cost after 296149 iterations : Training Loss =  0.10151099976373679; Validation Loss = 0.12394417865806982\n",
            "Cost after 296150 iterations : Training Loss =  0.10151099976373669; Validation Loss = 0.12394417865598666\n",
            "Cost after 296151 iterations : Training Loss =  0.1015109997637367; Validation Loss = 0.12394417865390385\n",
            "Cost after 296152 iterations : Training Loss =  0.10151099976373659; Validation Loss = 0.12394417865182092\n",
            "Cost after 296153 iterations : Training Loss =  0.10151099976373669; Validation Loss = 0.12394417864973843\n",
            "Cost after 296154 iterations : Training Loss =  0.10151099976373679; Validation Loss = 0.12394417864765574\n",
            "Cost after 296155 iterations : Training Loss =  0.10151099976373658; Validation Loss = 0.12394417864557307\n",
            "Cost after 296156 iterations : Training Loss =  0.10151099976373675; Validation Loss = 0.12394417864349049\n",
            "Cost after 296157 iterations : Training Loss =  0.1015109997637367; Validation Loss = 0.12394417864140816\n",
            "Cost after 296158 iterations : Training Loss =  0.10151099976373668; Validation Loss = 0.12394417863932645\n",
            "Cost after 296159 iterations : Training Loss =  0.10151099976373662; Validation Loss = 0.12394417863724393\n",
            "Cost after 296160 iterations : Training Loss =  0.10151099976373672; Validation Loss = 0.12394417863516195\n",
            "Cost after 296161 iterations : Training Loss =  0.10151099976373658; Validation Loss = 0.12394417863308027\n",
            "Cost after 296162 iterations : Training Loss =  0.10151099976373658; Validation Loss = 0.12394417863099838\n",
            "Cost after 296163 iterations : Training Loss =  0.10151099976373675; Validation Loss = 0.1239441786289164\n",
            "Cost after 296164 iterations : Training Loss =  0.10151099976373658; Validation Loss = 0.12394417862683497\n",
            "Cost after 296165 iterations : Training Loss =  0.10151099976373662; Validation Loss = 0.12394417862475365\n",
            "Cost after 296166 iterations : Training Loss =  0.10151099976373663; Validation Loss = 0.12394417862267218\n",
            "Cost after 296167 iterations : Training Loss =  0.10151099976373656; Validation Loss = 0.12394417862059069\n",
            "Cost after 296168 iterations : Training Loss =  0.10151099976373669; Validation Loss = 0.12394417861850973\n",
            "Cost after 296169 iterations : Training Loss =  0.10151099976373662; Validation Loss = 0.1239441786164286\n",
            "Cost after 296170 iterations : Training Loss =  0.10151099976373658; Validation Loss = 0.12394417861434752\n",
            "Cost after 296171 iterations : Training Loss =  0.10151099976373669; Validation Loss = 0.12394417861226693\n",
            "Cost after 296172 iterations : Training Loss =  0.1015109997637365; Validation Loss = 0.12394417861018632\n",
            "Cost after 296173 iterations : Training Loss =  0.10151099976373668; Validation Loss = 0.1239441786081056\n",
            "Cost after 296174 iterations : Training Loss =  0.10151099976373662; Validation Loss = 0.12394417860602522\n",
            "Cost after 296175 iterations : Training Loss =  0.10151099976373647; Validation Loss = 0.12394417860394455\n",
            "Cost after 296176 iterations : Training Loss =  0.10151099976373672; Validation Loss = 0.12394417860186412\n",
            "Cost after 296177 iterations : Training Loss =  0.10151099976373652; Validation Loss = 0.12394417859978385\n",
            "Cost after 296178 iterations : Training Loss =  0.10151099976373659; Validation Loss = 0.12394417859770412\n",
            "Cost after 296179 iterations : Training Loss =  0.10151099976373656; Validation Loss = 0.12394417859562416\n",
            "Cost after 296180 iterations : Training Loss =  0.10151099976373656; Validation Loss = 0.1239441785935442\n",
            "Cost after 296181 iterations : Training Loss =  0.10151099976373651; Validation Loss = 0.12394417859146409\n",
            "Cost after 296182 iterations : Training Loss =  0.10151099976373658; Validation Loss = 0.12394417858938454\n",
            "Cost after 296183 iterations : Training Loss =  0.10151099976373647; Validation Loss = 0.1239441785873052\n",
            "Cost after 296184 iterations : Training Loss =  0.10151099976373645; Validation Loss = 0.12394417858522568\n",
            "Cost after 296185 iterations : Training Loss =  0.10151099976373643; Validation Loss = 0.12394417858314623\n",
            "Cost after 296186 iterations : Training Loss =  0.10151099976373655; Validation Loss = 0.12394417858106714\n",
            "Cost after 296187 iterations : Training Loss =  0.10151099976373645; Validation Loss = 0.12394417857898787\n",
            "Cost after 296188 iterations : Training Loss =  0.10151099976373655; Validation Loss = 0.12394417857690875\n",
            "Cost after 296189 iterations : Training Loss =  0.10151099976373644; Validation Loss = 0.12394417857482956\n",
            "Cost after 296190 iterations : Training Loss =  0.10151099976373647; Validation Loss = 0.1239441785727509\n",
            "Cost after 296191 iterations : Training Loss =  0.10151099976373669; Validation Loss = 0.12394417857067218\n",
            "Cost after 296192 iterations : Training Loss =  0.10151099976373638; Validation Loss = 0.12394417856859362\n",
            "Cost after 296193 iterations : Training Loss =  0.10151099976373634; Validation Loss = 0.12394417856651524\n",
            "Cost after 296194 iterations : Training Loss =  0.10151099976373645; Validation Loss = 0.12394417856443642\n",
            "Cost after 296195 iterations : Training Loss =  0.10151099976373651; Validation Loss = 0.12394417856235816\n",
            "Cost after 296196 iterations : Training Loss =  0.10151099976373645; Validation Loss = 0.12394417856028021\n",
            "Cost after 296197 iterations : Training Loss =  0.10151099976373651; Validation Loss = 0.12394417855820208\n",
            "Cost after 296198 iterations : Training Loss =  0.10151099976373658; Validation Loss = 0.12394417855612418\n",
            "Cost after 296199 iterations : Training Loss =  0.10151099976373659; Validation Loss = 0.12394417855404614\n",
            "Cost after 296200 iterations : Training Loss =  0.10151099976373644; Validation Loss = 0.12394417855196847\n",
            "Cost after 296201 iterations : Training Loss =  0.10151099976373645; Validation Loss = 0.12394417854989062\n",
            "Cost after 296202 iterations : Training Loss =  0.10151099976373645; Validation Loss = 0.12394417854781295\n",
            "Cost after 296203 iterations : Training Loss =  0.10151099976373655; Validation Loss = 0.12394417854573568\n",
            "Cost after 296204 iterations : Training Loss =  0.1015109997637365; Validation Loss = 0.12394417854365816\n",
            "Cost after 296205 iterations : Training Loss =  0.10151099976373645; Validation Loss = 0.12394417854158121\n",
            "Cost after 296206 iterations : Training Loss =  0.10151099976373651; Validation Loss = 0.12394417853950407\n",
            "Cost after 296207 iterations : Training Loss =  0.10151099976373645; Validation Loss = 0.12394417853742695\n",
            "Cost after 296208 iterations : Training Loss =  0.10151099976373633; Validation Loss = 0.12394417853535\n",
            "Cost after 296209 iterations : Training Loss =  0.10151099976373625; Validation Loss = 0.12394417853327326\n",
            "Cost after 296210 iterations : Training Loss =  0.10151099976373633; Validation Loss = 0.12394417853119638\n",
            "Cost after 296211 iterations : Training Loss =  0.10151099976373638; Validation Loss = 0.12394417852911982\n",
            "Cost after 296212 iterations : Training Loss =  0.10151099976373637; Validation Loss = 0.12394417852704344\n",
            "Cost after 296213 iterations : Training Loss =  0.10151099976373631; Validation Loss = 0.12394417852496681\n",
            "Cost after 296214 iterations : Training Loss =  0.10151099976373633; Validation Loss = 0.12394417852289039\n",
            "Cost after 296215 iterations : Training Loss =  0.10151099976373637; Validation Loss = 0.12394417852081432\n",
            "Cost after 296216 iterations : Training Loss =  0.10151099976373633; Validation Loss = 0.12394417851873822\n",
            "Cost after 296217 iterations : Training Loss =  0.10151099976373627; Validation Loss = 0.12394417851666212\n",
            "Cost after 296218 iterations : Training Loss =  0.10151099976373638; Validation Loss = 0.12394417851458606\n",
            "Cost after 296219 iterations : Training Loss =  0.10151099976373644; Validation Loss = 0.12394417851251033\n",
            "Cost after 296220 iterations : Training Loss =  0.1015109997637362; Validation Loss = 0.12394417851043493\n",
            "Cost after 296221 iterations : Training Loss =  0.10151099976373637; Validation Loss = 0.12394417850835934\n",
            "Cost after 296222 iterations : Training Loss =  0.10151099976373627; Validation Loss = 0.12394417850628385\n",
            "Cost after 296223 iterations : Training Loss =  0.10151099976373637; Validation Loss = 0.12394417850420823\n",
            "Cost after 296224 iterations : Training Loss =  0.10151099976373623; Validation Loss = 0.12394417850213327\n",
            "Cost after 296225 iterations : Training Loss =  0.10151099976373626; Validation Loss = 0.12394417850005807\n",
            "Cost after 296226 iterations : Training Loss =  0.10151099976373625; Validation Loss = 0.12394417849798296\n",
            "Cost after 296227 iterations : Training Loss =  0.10151099976373631; Validation Loss = 0.12394417849590815\n",
            "Cost after 296228 iterations : Training Loss =  0.10151099976373634; Validation Loss = 0.12394417849383313\n",
            "Cost after 296229 iterations : Training Loss =  0.1015109997637362; Validation Loss = 0.12394417849175834\n",
            "Cost after 296230 iterations : Training Loss =  0.10151099976373634; Validation Loss = 0.12394417848968357\n",
            "Cost after 296231 iterations : Training Loss =  0.10151099976373615; Validation Loss = 0.12394417848760948\n",
            "Cost after 296232 iterations : Training Loss =  0.10151099976373623; Validation Loss = 0.12394417848553531\n",
            "Cost after 296233 iterations : Training Loss =  0.10151099976373613; Validation Loss = 0.12394417848346073\n",
            "Cost after 296234 iterations : Training Loss =  0.10151099976373631; Validation Loss = 0.12394417848138656\n",
            "Cost after 296235 iterations : Training Loss =  0.10151099976373607; Validation Loss = 0.12394417847931244\n",
            "Cost after 296236 iterations : Training Loss =  0.10151099976373626; Validation Loss = 0.12394417847723867\n",
            "Cost after 296237 iterations : Training Loss =  0.10151099976373618; Validation Loss = 0.12394417847516476\n",
            "Cost after 296238 iterations : Training Loss =  0.10151099976373607; Validation Loss = 0.12394417847309068\n",
            "Cost after 296239 iterations : Training Loss =  0.1015109997637362; Validation Loss = 0.12394417847101727\n",
            "Cost after 296240 iterations : Training Loss =  0.10151099976373626; Validation Loss = 0.12394417846894333\n",
            "Cost after 296241 iterations : Training Loss =  0.10151099976373618; Validation Loss = 0.12394417846687017\n",
            "Cost after 296242 iterations : Training Loss =  0.10151099976373613; Validation Loss = 0.12394417846479652\n",
            "Cost after 296243 iterations : Training Loss =  0.10151099976373626; Validation Loss = 0.12394417846272344\n",
            "Cost after 296244 iterations : Training Loss =  0.10151099976373612; Validation Loss = 0.12394417846065046\n",
            "Cost after 296245 iterations : Training Loss =  0.10151099976373627; Validation Loss = 0.12394417845857705\n",
            "Cost after 296246 iterations : Training Loss =  0.10151099976373613; Validation Loss = 0.12394417845650443\n",
            "Cost after 296247 iterations : Training Loss =  0.10151099976373613; Validation Loss = 0.12394417845443163\n",
            "Cost after 296248 iterations : Training Loss =  0.10151099976373608; Validation Loss = 0.12394417845235875\n",
            "Cost after 296249 iterations : Training Loss =  0.10151099976373602; Validation Loss = 0.12394417845028638\n",
            "Cost after 296250 iterations : Training Loss =  0.10151099976373627; Validation Loss = 0.12394417844821351\n",
            "Cost after 296251 iterations : Training Loss =  0.1015109997637361; Validation Loss = 0.12394417844614147\n",
            "Cost after 296252 iterations : Training Loss =  0.10151099976373631; Validation Loss = 0.12394417844406895\n",
            "Cost after 296253 iterations : Training Loss =  0.10151099976373608; Validation Loss = 0.12394417844199702\n",
            "Cost after 296254 iterations : Training Loss =  0.10151099976373612; Validation Loss = 0.12394417843992456\n",
            "Cost after 296255 iterations : Training Loss =  0.10151099976373605; Validation Loss = 0.1239441784378525\n",
            "Cost after 296256 iterations : Training Loss =  0.10151099976373607; Validation Loss = 0.12394417843578087\n",
            "Cost after 296257 iterations : Training Loss =  0.10151099976373601; Validation Loss = 0.12394417843370897\n",
            "Cost after 296258 iterations : Training Loss =  0.10151099976373601; Validation Loss = 0.12394417843163721\n",
            "Cost after 296259 iterations : Training Loss =  0.10151099976373602; Validation Loss = 0.1239441784295654\n",
            "Cost after 296260 iterations : Training Loss =  0.101510999763736; Validation Loss = 0.1239441784274942\n",
            "Cost after 296261 iterations : Training Loss =  0.10151099976373615; Validation Loss = 0.1239441784254229\n",
            "Cost after 296262 iterations : Training Loss =  0.10151099976373613; Validation Loss = 0.1239441784233518\n",
            "Cost after 296263 iterations : Training Loss =  0.10151099976373618; Validation Loss = 0.12394417842128064\n",
            "Cost after 296264 iterations : Training Loss =  0.10151099976373608; Validation Loss = 0.12394417841920959\n",
            "Cost after 296265 iterations : Training Loss =  0.1015109997637361; Validation Loss = 0.12394417841713842\n",
            "Cost after 296266 iterations : Training Loss =  0.10151099976373601; Validation Loss = 0.1239441784150678\n",
            "Cost after 296267 iterations : Training Loss =  0.10151099976373598; Validation Loss = 0.12394417841299703\n",
            "Cost after 296268 iterations : Training Loss =  0.10151099976373607; Validation Loss = 0.12394417841092624\n",
            "Cost after 296269 iterations : Training Loss =  0.10151099976373618; Validation Loss = 0.12394417840885623\n",
            "Cost after 296270 iterations : Training Loss =  0.10151099976373602; Validation Loss = 0.12394417840678562\n",
            "Cost after 296271 iterations : Training Loss =  0.10151099976373602; Validation Loss = 0.12394417840471501\n",
            "Cost after 296272 iterations : Training Loss =  0.10151099976373601; Validation Loss = 0.12394417840264484\n",
            "Cost after 296273 iterations : Training Loss =  0.10151099976373595; Validation Loss = 0.12394417840057481\n",
            "Cost after 296274 iterations : Training Loss =  0.10151099976373605; Validation Loss = 0.12394417839850508\n",
            "Cost after 296275 iterations : Training Loss =  0.10151099976373587; Validation Loss = 0.12394417839643548\n",
            "Cost after 296276 iterations : Training Loss =  0.10151099976373608; Validation Loss = 0.12394417839436521\n",
            "Cost after 296277 iterations : Training Loss =  0.101510999763736; Validation Loss = 0.12394417839229557\n",
            "Cost after 296278 iterations : Training Loss =  0.10151099976373598; Validation Loss = 0.12394417839022578\n",
            "Cost after 296279 iterations : Training Loss =  0.1015109997637359; Validation Loss = 0.12394417838815666\n",
            "Cost after 296280 iterations : Training Loss =  0.101510999763736; Validation Loss = 0.12394417838608708\n",
            "Cost after 296281 iterations : Training Loss =  0.101510999763736; Validation Loss = 0.12394417838401822\n",
            "Cost after 296282 iterations : Training Loss =  0.10151099976373595; Validation Loss = 0.12394417838194897\n",
            "Cost after 296283 iterations : Training Loss =  0.10151099976373594; Validation Loss = 0.12394417837987945\n",
            "Cost after 296284 iterations : Training Loss =  0.10151099976373583; Validation Loss = 0.12394417837781081\n",
            "Cost after 296285 iterations : Training Loss =  0.10151099976373582; Validation Loss = 0.12394417837574186\n",
            "Cost after 296286 iterations : Training Loss =  0.10151099976373598; Validation Loss = 0.12394417837367325\n",
            "Cost after 296287 iterations : Training Loss =  0.1015109997637361; Validation Loss = 0.12394417837160472\n",
            "Cost after 296288 iterations : Training Loss =  0.10151099976373593; Validation Loss = 0.12394417836953622\n",
            "Cost after 296289 iterations : Training Loss =  0.10151099976373583; Validation Loss = 0.12394417836746799\n",
            "Cost after 296290 iterations : Training Loss =  0.10151099976373601; Validation Loss = 0.12394417836539923\n",
            "Cost after 296291 iterations : Training Loss =  0.10151099976373587; Validation Loss = 0.12394417836333116\n",
            "Cost after 296292 iterations : Training Loss =  0.101510999763736; Validation Loss = 0.12394417836126337\n",
            "Cost after 296293 iterations : Training Loss =  0.10151099976373579; Validation Loss = 0.12394417835919534\n",
            "Cost after 296294 iterations : Training Loss =  0.10151099976373588; Validation Loss = 0.12394417835712737\n",
            "Cost after 296295 iterations : Training Loss =  0.10151099976373588; Validation Loss = 0.12394417835505939\n",
            "Cost after 296296 iterations : Training Loss =  0.10151099976373593; Validation Loss = 0.12394417835299182\n",
            "Cost after 296297 iterations : Training Loss =  0.10151099976373588; Validation Loss = 0.12394417835092421\n",
            "Cost after 296298 iterations : Training Loss =  0.10151099976373602; Validation Loss = 0.12394417834885676\n",
            "Cost after 296299 iterations : Training Loss =  0.10151099976373588; Validation Loss = 0.12394417834678939\n",
            "Cost after 296300 iterations : Training Loss =  0.1015109997637358; Validation Loss = 0.1239441783447223\n",
            "Cost after 296301 iterations : Training Loss =  0.10151099976373575; Validation Loss = 0.12394417834265525\n",
            "Cost after 296302 iterations : Training Loss =  0.10151099976373601; Validation Loss = 0.123944178340588\n",
            "Cost after 296303 iterations : Training Loss =  0.10151099976373579; Validation Loss = 0.12394417833852096\n",
            "Cost after 296304 iterations : Training Loss =  0.10151099976373586; Validation Loss = 0.12394417833645426\n",
            "Cost after 296305 iterations : Training Loss =  0.10151099976373588; Validation Loss = 0.1239441783343875\n",
            "Cost after 296306 iterations : Training Loss =  0.10151099976373595; Validation Loss = 0.12394417833232094\n",
            "Cost after 296307 iterations : Training Loss =  0.10151099976373595; Validation Loss = 0.12394417833025446\n",
            "Cost after 296308 iterations : Training Loss =  0.10151099976373593; Validation Loss = 0.1239441783281879\n",
            "Cost after 296309 iterations : Training Loss =  0.10151099976373582; Validation Loss = 0.12394417832612124\n",
            "Cost after 296310 iterations : Training Loss =  0.10151099976373579; Validation Loss = 0.12394417832405528\n",
            "Cost after 296311 iterations : Training Loss =  0.10151099976373583; Validation Loss = 0.1239441783219892\n",
            "Cost after 296312 iterations : Training Loss =  0.10151099976373588; Validation Loss = 0.12394417831992334\n",
            "Cost after 296313 iterations : Training Loss =  0.10151099976373605; Validation Loss = 0.12394417831785756\n",
            "Cost after 296314 iterations : Training Loss =  0.10151099976373575; Validation Loss = 0.12394417831579128\n",
            "Cost after 296315 iterations : Training Loss =  0.10151099976373588; Validation Loss = 0.12394417831372553\n",
            "Cost after 296316 iterations : Training Loss =  0.10151099976373593; Validation Loss = 0.12394417831165995\n",
            "Cost after 296317 iterations : Training Loss =  0.10151099976373586; Validation Loss = 0.12394417830959496\n",
            "Cost after 296318 iterations : Training Loss =  0.10151099976373583; Validation Loss = 0.12394417830752943\n",
            "Cost after 296319 iterations : Training Loss =  0.10151099976373569; Validation Loss = 0.12394417830546425\n",
            "Cost after 296320 iterations : Training Loss =  0.10151099976373575; Validation Loss = 0.12394417830339911\n",
            "Cost after 296321 iterations : Training Loss =  0.10151099976373575; Validation Loss = 0.12394417830133389\n",
            "Cost after 296322 iterations : Training Loss =  0.10151099976373575; Validation Loss = 0.12394417829926908\n",
            "Cost after 296323 iterations : Training Loss =  0.10151099976373569; Validation Loss = 0.12394417829720418\n",
            "Cost after 296324 iterations : Training Loss =  0.10151099976373579; Validation Loss = 0.12394417829513923\n",
            "Cost after 296325 iterations : Training Loss =  0.1015109997637357; Validation Loss = 0.12394417829307472\n",
            "Cost after 296326 iterations : Training Loss =  0.10151099976373579; Validation Loss = 0.1239441782910103\n",
            "Cost after 296327 iterations : Training Loss =  0.1015109997637357; Validation Loss = 0.12394417828894568\n",
            "Cost after 296328 iterations : Training Loss =  0.10151099976373566; Validation Loss = 0.12394417828688135\n",
            "Cost after 296329 iterations : Training Loss =  0.10151099976373558; Validation Loss = 0.12394417828481742\n",
            "Cost after 296330 iterations : Training Loss =  0.10151099976373582; Validation Loss = 0.12394417828275316\n",
            "Cost after 296331 iterations : Training Loss =  0.10151099976373562; Validation Loss = 0.12394417828068917\n",
            "Cost after 296332 iterations : Training Loss =  0.10151099976373555; Validation Loss = 0.12394417827862529\n",
            "Cost after 296333 iterations : Training Loss =  0.10151099976373579; Validation Loss = 0.1239441782765614\n",
            "Cost after 296334 iterations : Training Loss =  0.10151099976373562; Validation Loss = 0.12394417827449786\n",
            "Cost after 296335 iterations : Training Loss =  0.10151099976373569; Validation Loss = 0.12394417827243412\n",
            "Cost after 296336 iterations : Training Loss =  0.10151099976373557; Validation Loss = 0.12394417827037056\n",
            "Cost after 296337 iterations : Training Loss =  0.10151099976373569; Validation Loss = 0.12394417826830773\n",
            "Cost after 296338 iterations : Training Loss =  0.10151099976373569; Validation Loss = 0.123944178266244\n",
            "Cost after 296339 iterations : Training Loss =  0.10151099976373562; Validation Loss = 0.12394417826418103\n",
            "Cost after 296340 iterations : Training Loss =  0.10151099976373555; Validation Loss = 0.12394417826211765\n",
            "Cost after 296341 iterations : Training Loss =  0.10151099976373573; Validation Loss = 0.12394417826005498\n",
            "Cost after 296342 iterations : Training Loss =  0.10151099976373557; Validation Loss = 0.1239441782579919\n",
            "Cost after 296343 iterations : Training Loss =  0.1015109997637357; Validation Loss = 0.1239441782559292\n",
            "Cost after 296344 iterations : Training Loss =  0.10151099976373561; Validation Loss = 0.12394417825386655\n",
            "Cost after 296345 iterations : Training Loss =  0.1015109997637357; Validation Loss = 0.12394417825180382\n",
            "Cost after 296346 iterations : Training Loss =  0.10151099976373562; Validation Loss = 0.12394417824974165\n",
            "Cost after 296347 iterations : Training Loss =  0.1015109997637355; Validation Loss = 0.1239441782476793\n",
            "Cost after 296348 iterations : Training Loss =  0.10151099976373568; Validation Loss = 0.123944178245617\n",
            "Cost after 296349 iterations : Training Loss =  0.10151099976373563; Validation Loss = 0.12394417824355465\n",
            "Cost after 296350 iterations : Training Loss =  0.10151099976373566; Validation Loss = 0.12394417824149283\n",
            "Cost after 296351 iterations : Training Loss =  0.10151099976373557; Validation Loss = 0.12394417823943109\n",
            "Cost after 296352 iterations : Training Loss =  0.10151099976373555; Validation Loss = 0.12394417823736888\n",
            "Cost after 296353 iterations : Training Loss =  0.10151099976373558; Validation Loss = 0.12394417823530761\n",
            "Cost after 296354 iterations : Training Loss =  0.10151099976373548; Validation Loss = 0.12394417823324583\n",
            "Cost after 296355 iterations : Training Loss =  0.10151099976373569; Validation Loss = 0.12394417823118442\n",
            "Cost after 296356 iterations : Training Loss =  0.10151099976373555; Validation Loss = 0.12394417822912283\n",
            "Cost after 296357 iterations : Training Loss =  0.10151099976373543; Validation Loss = 0.1239441782270617\n",
            "Cost after 296358 iterations : Training Loss =  0.10151099976373543; Validation Loss = 0.12394417822500056\n",
            "Cost after 296359 iterations : Training Loss =  0.10151099976373541; Validation Loss = 0.12394417822293961\n",
            "Cost after 296360 iterations : Training Loss =  0.10151099976373555; Validation Loss = 0.12394417822087876\n",
            "Cost after 296361 iterations : Training Loss =  0.10151099976373558; Validation Loss = 0.12394417821881781\n",
            "Cost after 296362 iterations : Training Loss =  0.10151099976373551; Validation Loss = 0.12394417821675681\n",
            "Cost after 296363 iterations : Training Loss =  0.10151099976373544; Validation Loss = 0.12394417821469642\n",
            "Cost after 296364 iterations : Training Loss =  0.10151099976373545; Validation Loss = 0.12394417821263598\n",
            "Cost after 296365 iterations : Training Loss =  0.10151099976373544; Validation Loss = 0.12394417821057514\n",
            "Cost after 296366 iterations : Training Loss =  0.10151099976373551; Validation Loss = 0.12394417820851474\n",
            "Cost after 296367 iterations : Training Loss =  0.10151099976373539; Validation Loss = 0.12394417820645466\n",
            "Cost after 296368 iterations : Training Loss =  0.10151099976373554; Validation Loss = 0.1239441782043944\n",
            "Cost after 296369 iterations : Training Loss =  0.10151099976373532; Validation Loss = 0.1239441782023345\n",
            "Cost after 296370 iterations : Training Loss =  0.1015109997637355; Validation Loss = 0.12394417820027455\n",
            "Cost after 296371 iterations : Training Loss =  0.10151099976373539; Validation Loss = 0.1239441781982146\n",
            "Cost after 296372 iterations : Training Loss =  0.10151099976373562; Validation Loss = 0.1239441781961548\n",
            "Cost after 296373 iterations : Training Loss =  0.10151099976373532; Validation Loss = 0.12394417819409506\n",
            "Cost after 296374 iterations : Training Loss =  0.10151099976373555; Validation Loss = 0.12394417819203576\n",
            "Cost after 296375 iterations : Training Loss =  0.10151099976373539; Validation Loss = 0.12394417818997677\n",
            "Cost after 296376 iterations : Training Loss =  0.10151099976373534; Validation Loss = 0.12394417818791695\n",
            "Cost after 296377 iterations : Training Loss =  0.10151099976373534; Validation Loss = 0.12394417818585791\n",
            "Cost after 296378 iterations : Training Loss =  0.10151099976373554; Validation Loss = 0.12394417818379902\n",
            "Cost after 296379 iterations : Training Loss =  0.10151099976373545; Validation Loss = 0.1239441781817398\n",
            "Cost after 296380 iterations : Training Loss =  0.10151099976373532; Validation Loss = 0.12394417817968108\n",
            "Cost after 296381 iterations : Training Loss =  0.10151099976373543; Validation Loss = 0.12394417817762207\n",
            "Cost after 296382 iterations : Training Loss =  0.10151099976373532; Validation Loss = 0.12394417817556348\n",
            "Cost after 296383 iterations : Training Loss =  0.10151099976373537; Validation Loss = 0.12394417817350503\n",
            "Cost after 296384 iterations : Training Loss =  0.10151099976373523; Validation Loss = 0.12394417817144646\n",
            "Cost after 296385 iterations : Training Loss =  0.10151099976373548; Validation Loss = 0.12394417816938802\n",
            "Cost after 296386 iterations : Training Loss =  0.10151099976373532; Validation Loss = 0.12394417816733004\n",
            "Cost after 296387 iterations : Training Loss =  0.10151099976373537; Validation Loss = 0.12394417816527184\n",
            "Cost after 296388 iterations : Training Loss =  0.10151099976373539; Validation Loss = 0.12394417816321335\n",
            "Cost after 296389 iterations : Training Loss =  0.10151099976373526; Validation Loss = 0.12394417816115566\n",
            "Cost after 296390 iterations : Training Loss =  0.10151099976373532; Validation Loss = 0.12394417815909804\n",
            "Cost after 296391 iterations : Training Loss =  0.10151099976373537; Validation Loss = 0.12394417815704029\n",
            "Cost after 296392 iterations : Training Loss =  0.10151099976373534; Validation Loss = 0.12394417815498229\n",
            "Cost after 296393 iterations : Training Loss =  0.10151099976373523; Validation Loss = 0.1239441781529252\n",
            "Cost after 296394 iterations : Training Loss =  0.10151099976373529; Validation Loss = 0.12394417815086765\n",
            "Cost after 296395 iterations : Training Loss =  0.10151099976373548; Validation Loss = 0.1239441781488103\n",
            "Cost after 296396 iterations : Training Loss =  0.10151099976373534; Validation Loss = 0.12394417814675315\n",
            "Cost after 296397 iterations : Training Loss =  0.10151099976373541; Validation Loss = 0.1239441781446959\n",
            "Cost after 296398 iterations : Training Loss =  0.10151099976373519; Validation Loss = 0.12394417814263907\n",
            "Cost after 296399 iterations : Training Loss =  0.10151099976373543; Validation Loss = 0.12394417814058223\n",
            "Cost after 296400 iterations : Training Loss =  0.10151099976373519; Validation Loss = 0.12394417813852522\n",
            "Cost after 296401 iterations : Training Loss =  0.10151099976373536; Validation Loss = 0.12394417813646832\n",
            "Cost after 296402 iterations : Training Loss =  0.10151099976373536; Validation Loss = 0.12394417813441205\n",
            "Cost after 296403 iterations : Training Loss =  0.10151099976373541; Validation Loss = 0.12394417813235581\n",
            "Cost after 296404 iterations : Training Loss =  0.10151099976373526; Validation Loss = 0.1239441781302991\n",
            "Cost after 296405 iterations : Training Loss =  0.10151099976373512; Validation Loss = 0.12394417812824302\n",
            "Cost after 296406 iterations : Training Loss =  0.10151099976373519; Validation Loss = 0.12394417812618701\n",
            "Cost after 296407 iterations : Training Loss =  0.10151099976373522; Validation Loss = 0.12394417812413075\n",
            "Cost after 296408 iterations : Training Loss =  0.10151099976373536; Validation Loss = 0.12394417812207469\n",
            "Cost after 296409 iterations : Training Loss =  0.1015109997637353; Validation Loss = 0.12394417812001901\n",
            "Cost after 296410 iterations : Training Loss =  0.10151099976373529; Validation Loss = 0.12394417811796317\n",
            "Cost after 296411 iterations : Training Loss =  0.10151099976373518; Validation Loss = 0.1239441781159075\n",
            "Cost after 296412 iterations : Training Loss =  0.1015109997637353; Validation Loss = 0.12394417811385179\n",
            "Cost after 296413 iterations : Training Loss =  0.10151099976373526; Validation Loss = 0.12394417811179675\n",
            "Cost after 296414 iterations : Training Loss =  0.10151099976373529; Validation Loss = 0.12394417810974126\n",
            "Cost after 296415 iterations : Training Loss =  0.10151099976373534; Validation Loss = 0.1239441781076862\n",
            "Cost after 296416 iterations : Training Loss =  0.1015109997637353; Validation Loss = 0.12394417810563084\n",
            "Cost after 296417 iterations : Training Loss =  0.1015109997637351; Validation Loss = 0.12394417810357602\n",
            "Cost after 296418 iterations : Training Loss =  0.10151099976373523; Validation Loss = 0.12394417810152104\n",
            "Cost after 296419 iterations : Training Loss =  0.10151099976373522; Validation Loss = 0.12394417809946644\n",
            "Cost after 296420 iterations : Training Loss =  0.10151099976373523; Validation Loss = 0.12394417809741144\n",
            "Cost after 296421 iterations : Training Loss =  0.10151099976373523; Validation Loss = 0.12394417809535689\n",
            "Cost after 296422 iterations : Training Loss =  0.10151099976373526; Validation Loss = 0.12394417809330278\n",
            "Cost after 296423 iterations : Training Loss =  0.10151099976373523; Validation Loss = 0.12394417809124798\n",
            "Cost after 296424 iterations : Training Loss =  0.10151099976373516; Validation Loss = 0.12394417808919346\n",
            "Cost after 296425 iterations : Training Loss =  0.10151099976373518; Validation Loss = 0.12394417808713916\n",
            "Cost after 296426 iterations : Training Loss =  0.10151099976373514; Validation Loss = 0.12394417808508541\n",
            "Cost after 296427 iterations : Training Loss =  0.10151099976373519; Validation Loss = 0.12394417808303139\n",
            "Cost after 296428 iterations : Training Loss =  0.10151099976373512; Validation Loss = 0.12394417808097757\n",
            "Cost after 296429 iterations : Training Loss =  0.10151099976373529; Validation Loss = 0.12394417807892387\n",
            "Cost after 296430 iterations : Training Loss =  0.10151099976373514; Validation Loss = 0.12394417807687023\n",
            "Cost after 296431 iterations : Training Loss =  0.10151099976373505; Validation Loss = 0.12394417807481664\n",
            "Cost after 296432 iterations : Training Loss =  0.10151099976373516; Validation Loss = 0.12394417807276298\n",
            "Cost after 296433 iterations : Training Loss =  0.10151099976373512; Validation Loss = 0.12394417807070988\n",
            "Cost after 296434 iterations : Training Loss =  0.10151099976373511; Validation Loss = 0.12394417806865647\n",
            "Cost after 296435 iterations : Training Loss =  0.10151099976373514; Validation Loss = 0.12394417806660346\n",
            "Cost after 296436 iterations : Training Loss =  0.10151099976373504; Validation Loss = 0.12394417806455044\n",
            "Cost after 296437 iterations : Training Loss =  0.10151099976373525; Validation Loss = 0.12394417806249763\n",
            "Cost after 296438 iterations : Training Loss =  0.10151099976373504; Validation Loss = 0.12394417806044457\n",
            "Cost after 296439 iterations : Training Loss =  0.10151099976373511; Validation Loss = 0.12394417805839196\n",
            "Cost after 296440 iterations : Training Loss =  0.10151099976373518; Validation Loss = 0.12394417805633902\n",
            "Cost after 296441 iterations : Training Loss =  0.10151099976373511; Validation Loss = 0.12394417805428681\n",
            "Cost after 296442 iterations : Training Loss =  0.10151099976373514; Validation Loss = 0.12394417805223423\n",
            "Cost after 296443 iterations : Training Loss =  0.10151099976373514; Validation Loss = 0.12394417805018179\n",
            "Cost after 296444 iterations : Training Loss =  0.10151099976373498; Validation Loss = 0.12394417804812992\n",
            "Cost after 296445 iterations : Training Loss =  0.10151099976373516; Validation Loss = 0.12394417804607784\n",
            "Cost after 296446 iterations : Training Loss =  0.10151099976373516; Validation Loss = 0.1239441780440258\n",
            "Cost after 296447 iterations : Training Loss =  0.10151099976373497; Validation Loss = 0.1239441780419742\n",
            "Cost after 296448 iterations : Training Loss =  0.10151099976373497; Validation Loss = 0.12394417803992192\n",
            "Cost after 296449 iterations : Training Loss =  0.10151099976373501; Validation Loss = 0.1239441780378704\n",
            "Cost after 296450 iterations : Training Loss =  0.10151099976373505; Validation Loss = 0.12394417803581897\n",
            "Cost after 296451 iterations : Training Loss =  0.10151099976373505; Validation Loss = 0.12394417803376719\n",
            "Cost after 296452 iterations : Training Loss =  0.10151099976373487; Validation Loss = 0.123944178031716\n",
            "Cost after 296453 iterations : Training Loss =  0.10151099976373486; Validation Loss = 0.12394417802966487\n",
            "Cost after 296454 iterations : Training Loss =  0.101510999763735; Validation Loss = 0.12394417802761355\n",
            "Cost after 296455 iterations : Training Loss =  0.10151099976373498; Validation Loss = 0.12394417802556248\n",
            "Cost after 296456 iterations : Training Loss =  0.10151099976373505; Validation Loss = 0.12394417802351154\n",
            "Cost after 296457 iterations : Training Loss =  0.10151099976373504; Validation Loss = 0.12394417802146078\n",
            "Cost after 296458 iterations : Training Loss =  0.10151099976373493; Validation Loss = 0.12394417801941007\n",
            "Cost after 296459 iterations : Training Loss =  0.10151099976373505; Validation Loss = 0.12394417801735934\n",
            "Cost after 296460 iterations : Training Loss =  0.10151099976373504; Validation Loss = 0.12394417801530874\n",
            "Cost after 296461 iterations : Training Loss =  0.10151099976373494; Validation Loss = 0.12394417801325833\n",
            "Cost after 296462 iterations : Training Loss =  0.10151099976373504; Validation Loss = 0.123944178011208\n",
            "Cost after 296463 iterations : Training Loss =  0.10151099976373491; Validation Loss = 0.12394417800915786\n",
            "Cost after 296464 iterations : Training Loss =  0.10151099976373498; Validation Loss = 0.12394417800710773\n",
            "Cost after 296465 iterations : Training Loss =  0.10151099976373494; Validation Loss = 0.12394417800505754\n",
            "Cost after 296466 iterations : Training Loss =  0.10151099976373497; Validation Loss = 0.12394417800300774\n",
            "Cost after 296467 iterations : Training Loss =  0.10151099976373497; Validation Loss = 0.12394417800095794\n",
            "Cost after 296468 iterations : Training Loss =  0.1015109997637349; Validation Loss = 0.12394417799890835\n",
            "Cost after 296469 iterations : Training Loss =  0.10151099976373494; Validation Loss = 0.12394417799685867\n",
            "Cost after 296470 iterations : Training Loss =  0.101510999763735; Validation Loss = 0.12394417799480918\n",
            "Cost after 296471 iterations : Training Loss =  0.10151099976373493; Validation Loss = 0.12394417799275967\n",
            "Cost after 296472 iterations : Training Loss =  0.10151099976373487; Validation Loss = 0.12394417799071035\n",
            "Cost after 296473 iterations : Training Loss =  0.10151099976373486; Validation Loss = 0.12394417798866128\n",
            "Cost after 296474 iterations : Training Loss =  0.10151099976373494; Validation Loss = 0.12394417798661196\n",
            "Cost after 296475 iterations : Training Loss =  0.10151099976373486; Validation Loss = 0.12394417798456343\n",
            "Cost after 296476 iterations : Training Loss =  0.101510999763735; Validation Loss = 0.1239441779825142\n",
            "Cost after 296477 iterations : Training Loss =  0.10151099976373487; Validation Loss = 0.12394417798046554\n",
            "Cost after 296478 iterations : Training Loss =  0.10151099976373494; Validation Loss = 0.1239441779784167\n",
            "Cost after 296479 iterations : Training Loss =  0.10151099976373491; Validation Loss = 0.12394417797636828\n",
            "Cost after 296480 iterations : Training Loss =  0.10151099976373484; Validation Loss = 0.12394417797431985\n",
            "Cost after 296481 iterations : Training Loss =  0.10151099976373498; Validation Loss = 0.12394417797227147\n",
            "Cost after 296482 iterations : Training Loss =  0.10151099976373487; Validation Loss = 0.12394417797022364\n",
            "Cost after 296483 iterations : Training Loss =  0.10151099976373487; Validation Loss = 0.12394417796817525\n",
            "Cost after 296484 iterations : Training Loss =  0.10151099976373498; Validation Loss = 0.12394417796612718\n",
            "Cost after 296485 iterations : Training Loss =  0.10151099976373487; Validation Loss = 0.12394417796407917\n",
            "Cost after 296486 iterations : Training Loss =  0.1015109997637349; Validation Loss = 0.12394417796203104\n",
            "Cost after 296487 iterations : Training Loss =  0.10151099976373477; Validation Loss = 0.12394417795998369\n",
            "Cost after 296488 iterations : Training Loss =  0.10151099976373491; Validation Loss = 0.12394417795793583\n",
            "Cost after 296489 iterations : Training Loss =  0.1015109997637348; Validation Loss = 0.12394417795588832\n",
            "Cost after 296490 iterations : Training Loss =  0.101510999763735; Validation Loss = 0.12394417795384093\n",
            "Cost after 296491 iterations : Training Loss =  0.10151099976373473; Validation Loss = 0.12394417795179369\n",
            "Cost after 296492 iterations : Training Loss =  0.10151099976373484; Validation Loss = 0.12394417794974669\n",
            "Cost after 296493 iterations : Training Loss =  0.10151099976373486; Validation Loss = 0.12394417794769963\n",
            "Cost after 296494 iterations : Training Loss =  0.10151099976373473; Validation Loss = 0.12394417794565252\n",
            "Cost after 296495 iterations : Training Loss =  0.10151099976373477; Validation Loss = 0.1239441779436055\n",
            "Cost after 296496 iterations : Training Loss =  0.10151099976373484; Validation Loss = 0.12394417794155839\n",
            "Cost after 296497 iterations : Training Loss =  0.1015109997637349; Validation Loss = 0.12394417793951219\n",
            "Cost after 296498 iterations : Training Loss =  0.10151099976373472; Validation Loss = 0.12394417793746522\n",
            "Cost after 296499 iterations : Training Loss =  0.1015109997637348; Validation Loss = 0.1239441779354191\n",
            "Cost after 296500 iterations : Training Loss =  0.10151099976373475; Validation Loss = 0.12394417793337269\n",
            "Cost after 296501 iterations : Training Loss =  0.10151099976373473; Validation Loss = 0.12394417793132619\n",
            "Cost after 296502 iterations : Training Loss =  0.10151099976373475; Validation Loss = 0.12394417792928017\n",
            "Cost after 296503 iterations : Training Loss =  0.10151099976373475; Validation Loss = 0.12394417792723406\n",
            "Cost after 296504 iterations : Training Loss =  0.10151099976373473; Validation Loss = 0.1239441779251881\n",
            "Cost after 296505 iterations : Training Loss =  0.10151099976373479; Validation Loss = 0.12394417792314229\n",
            "Cost after 296506 iterations : Training Loss =  0.10151099976373465; Validation Loss = 0.12394417792109665\n",
            "Cost after 296507 iterations : Training Loss =  0.10151099976373477; Validation Loss = 0.12394417791905092\n",
            "Cost after 296508 iterations : Training Loss =  0.10151099976373468; Validation Loss = 0.12394417791700534\n",
            "Cost after 296509 iterations : Training Loss =  0.10151099976373469; Validation Loss = 0.12394417791495983\n",
            "Cost after 296510 iterations : Training Loss =  0.10151099976373473; Validation Loss = 0.12394417791291458\n",
            "Cost after 296511 iterations : Training Loss =  0.10151099976373465; Validation Loss = 0.12394417791086941\n",
            "Cost after 296512 iterations : Training Loss =  0.10151099976373479; Validation Loss = 0.12394417790882417\n",
            "Cost after 296513 iterations : Training Loss =  0.10151099976373466; Validation Loss = 0.12394417790677921\n",
            "Cost after 296514 iterations : Training Loss =  0.10151099976373472; Validation Loss = 0.1239441779047344\n",
            "Cost after 296515 iterations : Training Loss =  0.10151099976373468; Validation Loss = 0.12394417790268912\n",
            "Cost after 296516 iterations : Training Loss =  0.10151099976373466; Validation Loss = 0.12394417790064473\n",
            "Cost after 296517 iterations : Training Loss =  0.10151099976373446; Validation Loss = 0.12394417789860013\n",
            "Cost after 296518 iterations : Training Loss =  0.10151099976373468; Validation Loss = 0.12394417789655576\n",
            "Cost after 296519 iterations : Training Loss =  0.10151099976373469; Validation Loss = 0.12394417789451137\n",
            "Cost after 296520 iterations : Training Loss =  0.10151099976373469; Validation Loss = 0.1239441778924668\n",
            "Cost after 296521 iterations : Training Loss =  0.10151099976373469; Validation Loss = 0.12394417789042265\n",
            "Cost after 296522 iterations : Training Loss =  0.10151099976373455; Validation Loss = 0.12394417788837865\n",
            "Cost after 296523 iterations : Training Loss =  0.1015109997637345; Validation Loss = 0.12394417788633477\n",
            "Cost after 296524 iterations : Training Loss =  0.10151099976373455; Validation Loss = 0.12394417788429092\n",
            "Cost after 296525 iterations : Training Loss =  0.1015109997637346; Validation Loss = 0.12394417788224693\n",
            "Cost after 296526 iterations : Training Loss =  0.10151099976373472; Validation Loss = 0.12394417788020341\n",
            "Cost after 296527 iterations : Training Loss =  0.10151099976373477; Validation Loss = 0.1239441778781596\n",
            "Cost after 296528 iterations : Training Loss =  0.1015109997637346; Validation Loss = 0.1239441778761164\n",
            "Cost after 296529 iterations : Training Loss =  0.10151099976373465; Validation Loss = 0.12394417787407294\n",
            "Cost after 296530 iterations : Training Loss =  0.10151099976373455; Validation Loss = 0.12394417787202988\n",
            "Cost after 296531 iterations : Training Loss =  0.1015109997637346; Validation Loss = 0.12394417786998642\n",
            "Cost after 296532 iterations : Training Loss =  0.10151099976373466; Validation Loss = 0.12394417786794358\n",
            "Cost after 296533 iterations : Training Loss =  0.10151099976373446; Validation Loss = 0.12394417786590045\n",
            "Cost after 296534 iterations : Training Loss =  0.10151099976373455; Validation Loss = 0.1239441778638577\n",
            "Cost after 296535 iterations : Training Loss =  0.10151099976373452; Validation Loss = 0.12394417786181519\n",
            "Cost after 296536 iterations : Training Loss =  0.10151099976373443; Validation Loss = 0.12394417785977226\n",
            "Cost after 296537 iterations : Training Loss =  0.1015109997637345; Validation Loss = 0.12394417785772993\n",
            "Cost after 296538 iterations : Training Loss =  0.1015109997637345; Validation Loss = 0.12394417785568744\n",
            "Cost after 296539 iterations : Training Loss =  0.10151099976373466; Validation Loss = 0.12394417785364499\n",
            "Cost after 296540 iterations : Training Loss =  0.10151099976373447; Validation Loss = 0.12394417785160278\n",
            "Cost after 296541 iterations : Training Loss =  0.10151099976373461; Validation Loss = 0.12394417784956079\n",
            "Cost after 296542 iterations : Training Loss =  0.10151099976373448; Validation Loss = 0.12394417784751884\n",
            "Cost after 296543 iterations : Training Loss =  0.10151099976373461; Validation Loss = 0.12394417784547682\n",
            "Cost after 296544 iterations : Training Loss =  0.1015109997637345; Validation Loss = 0.12394417784343517\n",
            "Cost after 296545 iterations : Training Loss =  0.10151099976373436; Validation Loss = 0.12394417784139361\n",
            "Cost after 296546 iterations : Training Loss =  0.10151099976373437; Validation Loss = 0.1239441778393517\n",
            "Cost after 296547 iterations : Training Loss =  0.10151099976373455; Validation Loss = 0.12394417783731042\n",
            "Cost after 296548 iterations : Training Loss =  0.10151099976373465; Validation Loss = 0.12394417783526873\n",
            "Cost after 296549 iterations : Training Loss =  0.10151099976373454; Validation Loss = 0.1239441778332271\n",
            "Cost after 296550 iterations : Training Loss =  0.1015109997637346; Validation Loss = 0.12394417783118629\n",
            "Cost after 296551 iterations : Training Loss =  0.10151099976373454; Validation Loss = 0.12394417782914531\n",
            "Cost after 296552 iterations : Training Loss =  0.1015109997637346; Validation Loss = 0.12394417782710399\n",
            "Cost after 296553 iterations : Training Loss =  0.10151099976373454; Validation Loss = 0.12394417782506345\n",
            "Cost after 296554 iterations : Training Loss =  0.10151099976373433; Validation Loss = 0.12394417782302304\n",
            "Cost after 296555 iterations : Training Loss =  0.10151099976373448; Validation Loss = 0.12394417782098187\n",
            "Cost after 296556 iterations : Training Loss =  0.10151099976373447; Validation Loss = 0.1239441778189414\n",
            "Cost after 296557 iterations : Training Loss =  0.1015109997637345; Validation Loss = 0.12394417781690081\n",
            "Cost after 296558 iterations : Training Loss =  0.10151099976373454; Validation Loss = 0.12394417781486046\n",
            "Cost after 296559 iterations : Training Loss =  0.10151099976373433; Validation Loss = 0.12394417781282041\n",
            "Cost after 296560 iterations : Training Loss =  0.10151099976373452; Validation Loss = 0.12394417781078003\n",
            "Cost after 296561 iterations : Training Loss =  0.1015109997637345; Validation Loss = 0.12394417780874012\n",
            "Cost after 296562 iterations : Training Loss =  0.10151099976373462; Validation Loss = 0.12394417780670018\n",
            "Cost after 296563 iterations : Training Loss =  0.10151099976373446; Validation Loss = 0.12394417780466027\n",
            "Cost after 296564 iterations : Training Loss =  0.10151099976373452; Validation Loss = 0.12394417780262043\n",
            "Cost after 296565 iterations : Training Loss =  0.10151099976373452; Validation Loss = 0.12394417780058081\n",
            "Cost after 296566 iterations : Training Loss =  0.10151099976373443; Validation Loss = 0.12394417779854144\n",
            "Cost after 296567 iterations : Training Loss =  0.10151099976373443; Validation Loss = 0.12394417779650219\n",
            "Cost after 296568 iterations : Training Loss =  0.10151099976373448; Validation Loss = 0.12394417779446261\n",
            "Cost after 296569 iterations : Training Loss =  0.10151099976373447; Validation Loss = 0.12394417779242338\n",
            "Cost after 296570 iterations : Training Loss =  0.10151099976373415; Validation Loss = 0.12394417779038461\n",
            "Cost after 296571 iterations : Training Loss =  0.1015109997637343; Validation Loss = 0.1239441777883454\n",
            "Cost after 296572 iterations : Training Loss =  0.10151099976373434; Validation Loss = 0.12394417778630654\n",
            "Cost after 296573 iterations : Training Loss =  0.10151099976373433; Validation Loss = 0.1239441777842678\n",
            "Cost after 296574 iterations : Training Loss =  0.10151099976373436; Validation Loss = 0.12394417778222896\n",
            "Cost after 296575 iterations : Training Loss =  0.10151099976373441; Validation Loss = 0.12394417778019022\n",
            "Cost after 296576 iterations : Training Loss =  0.1015109997637343; Validation Loss = 0.12394417777815195\n",
            "Cost after 296577 iterations : Training Loss =  0.10151099976373433; Validation Loss = 0.12394417777611344\n",
            "Cost after 296578 iterations : Training Loss =  0.10151099976373454; Validation Loss = 0.1239441777740751\n",
            "Cost after 296579 iterations : Training Loss =  0.10151099976373427; Validation Loss = 0.1239441777720369\n",
            "Cost after 296580 iterations : Training Loss =  0.10151099976373441; Validation Loss = 0.1239441777699988\n",
            "Cost after 296581 iterations : Training Loss =  0.10151099976373423; Validation Loss = 0.12394417776796064\n",
            "Cost after 296582 iterations : Training Loss =  0.1015109997637344; Validation Loss = 0.12394417776592313\n",
            "Cost after 296583 iterations : Training Loss =  0.10151099976373425; Validation Loss = 0.12394417776388521\n",
            "Cost after 296584 iterations : Training Loss =  0.10151099976373448; Validation Loss = 0.12394417776184785\n",
            "Cost after 296585 iterations : Training Loss =  0.10151099976373429; Validation Loss = 0.12394417775981006\n",
            "Cost after 296586 iterations : Training Loss =  0.10151099976373429; Validation Loss = 0.12394417775777244\n",
            "Cost after 296587 iterations : Training Loss =  0.1015109997637343; Validation Loss = 0.12394417775573524\n",
            "Cost after 296588 iterations : Training Loss =  0.10151099976373427; Validation Loss = 0.12394417775369806\n",
            "Cost after 296589 iterations : Training Loss =  0.1015109997637344; Validation Loss = 0.12394417775166078\n",
            "Cost after 296590 iterations : Training Loss =  0.1015109997637343; Validation Loss = 0.12394417774962371\n",
            "Cost after 296591 iterations : Training Loss =  0.10151099976373416; Validation Loss = 0.12394417774758693\n",
            "Cost after 296592 iterations : Training Loss =  0.10151099976373437; Validation Loss = 0.1239441777455498\n",
            "Cost after 296593 iterations : Training Loss =  0.10151099976373422; Validation Loss = 0.12394417774351311\n",
            "Cost after 296594 iterations : Training Loss =  0.10151099976373427; Validation Loss = 0.12394417774147636\n",
            "Cost after 296595 iterations : Training Loss =  0.10151099976373415; Validation Loss = 0.12394417773944001\n",
            "Cost after 296596 iterations : Training Loss =  0.1015109997637343; Validation Loss = 0.12394417773740352\n",
            "Cost after 296597 iterations : Training Loss =  0.10151099976373415; Validation Loss = 0.12394417773536764\n",
            "Cost after 296598 iterations : Training Loss =  0.10151099976373416; Validation Loss = 0.12394417773333118\n",
            "Cost after 296599 iterations : Training Loss =  0.10151099976373425; Validation Loss = 0.12394417773129496\n",
            "Cost after 296600 iterations : Training Loss =  0.10151099976373422; Validation Loss = 0.12394417772925927\n",
            "Cost after 296601 iterations : Training Loss =  0.10151099976373408; Validation Loss = 0.12394417772722319\n",
            "Cost after 296602 iterations : Training Loss =  0.1015109997637342; Validation Loss = 0.12394417772518733\n",
            "Cost after 296603 iterations : Training Loss =  0.10151099976373418; Validation Loss = 0.12394417772315175\n",
            "Cost after 296604 iterations : Training Loss =  0.1015109997637341; Validation Loss = 0.12394417772111606\n",
            "Cost after 296605 iterations : Training Loss =  0.1015109997637342; Validation Loss = 0.12394417771908091\n",
            "Cost after 296606 iterations : Training Loss =  0.10151099976373437; Validation Loss = 0.12394417771704518\n",
            "Cost after 296607 iterations : Training Loss =  0.10151099976373437; Validation Loss = 0.12394417771501001\n",
            "Cost after 296608 iterations : Training Loss =  0.10151099976373408; Validation Loss = 0.12394417771297475\n",
            "Cost after 296609 iterations : Training Loss =  0.10151099976373423; Validation Loss = 0.12394417771093949\n",
            "Cost after 296610 iterations : Training Loss =  0.10151099976373416; Validation Loss = 0.12394417770890463\n",
            "Cost after 296611 iterations : Training Loss =  0.10151099976373434; Validation Loss = 0.12394417770686973\n",
            "Cost after 296612 iterations : Training Loss =  0.10151099976373403; Validation Loss = 0.12394417770483497\n",
            "Cost after 296613 iterations : Training Loss =  0.1015109997637342; Validation Loss = 0.12394417770280036\n",
            "Cost after 296614 iterations : Training Loss =  0.1015109997637341; Validation Loss = 0.12394417770076586\n",
            "Cost after 296615 iterations : Training Loss =  0.10151099976373418; Validation Loss = 0.12394417769873144\n",
            "Cost after 296616 iterations : Training Loss =  0.1015109997637342; Validation Loss = 0.12394417769669684\n",
            "Cost after 296617 iterations : Training Loss =  0.10151099976373404; Validation Loss = 0.1239441776946626\n",
            "Cost after 296618 iterations : Training Loss =  0.10151099976373429; Validation Loss = 0.1239441776926285\n",
            "Cost after 296619 iterations : Training Loss =  0.10151099976373422; Validation Loss = 0.12394417769059445\n",
            "Cost after 296620 iterations : Training Loss =  0.10151099976373415; Validation Loss = 0.1239441776885606\n",
            "Cost after 296621 iterations : Training Loss =  0.10151099976373408; Validation Loss = 0.12394417768652678\n",
            "Cost after 296622 iterations : Training Loss =  0.10151099976373398; Validation Loss = 0.12394417768449315\n",
            "Cost after 296623 iterations : Training Loss =  0.10151099976373405; Validation Loss = 0.12394417768245936\n",
            "Cost after 296624 iterations : Training Loss =  0.10151099976373412; Validation Loss = 0.12394417768042607\n",
            "Cost after 296625 iterations : Training Loss =  0.10151099976373411; Validation Loss = 0.12394417767839234\n",
            "Cost after 296626 iterations : Training Loss =  0.10151099976373391; Validation Loss = 0.12394417767635911\n",
            "Cost after 296627 iterations : Training Loss =  0.10151099976373415; Validation Loss = 0.12394417767432565\n",
            "Cost after 296628 iterations : Training Loss =  0.10151099976373405; Validation Loss = 0.12394417767229268\n",
            "Cost after 296629 iterations : Training Loss =  0.10151099976373416; Validation Loss = 0.12394417767025977\n",
            "Cost after 296630 iterations : Training Loss =  0.10151099976373411; Validation Loss = 0.12394417766822687\n",
            "Cost after 296631 iterations : Training Loss =  0.1015109997637341; Validation Loss = 0.12394417766619385\n",
            "Cost after 296632 iterations : Training Loss =  0.10151099976373411; Validation Loss = 0.12394417766416126\n",
            "Cost after 296633 iterations : Training Loss =  0.10151099976373397; Validation Loss = 0.12394417766212824\n",
            "Cost after 296634 iterations : Training Loss =  0.10151099976373404; Validation Loss = 0.12394417766009617\n",
            "Cost after 296635 iterations : Training Loss =  0.10151099976373403; Validation Loss = 0.12394417765806401\n",
            "Cost after 296636 iterations : Training Loss =  0.10151099976373401; Validation Loss = 0.12394417765603137\n",
            "Cost after 296637 iterations : Training Loss =  0.10151099976373384; Validation Loss = 0.12394417765399914\n",
            "Cost after 296638 iterations : Training Loss =  0.10151099976373405; Validation Loss = 0.12394417765196714\n",
            "Cost after 296639 iterations : Training Loss =  0.10151099976373411; Validation Loss = 0.1239441776499351\n",
            "Cost after 296640 iterations : Training Loss =  0.1015109997637341; Validation Loss = 0.12394417764790323\n",
            "Cost after 296641 iterations : Training Loss =  0.1015109997637341; Validation Loss = 0.12394417764587164\n",
            "Cost after 296642 iterations : Training Loss =  0.10151099976373415; Validation Loss = 0.12394417764383991\n",
            "Cost after 296643 iterations : Training Loss =  0.10151099976373404; Validation Loss = 0.12394417764180846\n",
            "Cost after 296644 iterations : Training Loss =  0.10151099976373396; Validation Loss = 0.12394417763977687\n",
            "Cost after 296645 iterations : Training Loss =  0.10151099976373384; Validation Loss = 0.12394417763774529\n",
            "Cost after 296646 iterations : Training Loss =  0.10151099976373397; Validation Loss = 0.12394417763571429\n",
            "Cost after 296647 iterations : Training Loss =  0.10151099976373396; Validation Loss = 0.12394417763368283\n",
            "Cost after 296648 iterations : Training Loss =  0.10151099976373415; Validation Loss = 0.12394417763165215\n",
            "Cost after 296649 iterations : Training Loss =  0.10151099976373411; Validation Loss = 0.12394417762962126\n",
            "Cost after 296650 iterations : Training Loss =  0.10151099976373393; Validation Loss = 0.12394417762759022\n",
            "Cost after 296651 iterations : Training Loss =  0.1015109997637338; Validation Loss = 0.12394417762555972\n",
            "Cost after 296652 iterations : Training Loss =  0.10151099976373411; Validation Loss = 0.12394417762352905\n",
            "Cost after 296653 iterations : Training Loss =  0.10151099976373398; Validation Loss = 0.1239441776214984\n",
            "Cost after 296654 iterations : Training Loss =  0.10151099976373401; Validation Loss = 0.12394417761946791\n",
            "Cost after 296655 iterations : Training Loss =  0.10151099976373393; Validation Loss = 0.12394417761743746\n",
            "Cost after 296656 iterations : Training Loss =  0.10151099976373376; Validation Loss = 0.12394417761540752\n",
            "Cost after 296657 iterations : Training Loss =  0.10151099976373384; Validation Loss = 0.12394417761337717\n",
            "Cost after 296658 iterations : Training Loss =  0.10151099976373397; Validation Loss = 0.12394417761134731\n",
            "Cost after 296659 iterations : Training Loss =  0.10151099976373415; Validation Loss = 0.12394417760931745\n",
            "Cost after 296660 iterations : Training Loss =  0.10151099976373379; Validation Loss = 0.1239441776072875\n",
            "Cost after 296661 iterations : Training Loss =  0.10151099976373379; Validation Loss = 0.12394417760525783\n",
            "Cost after 296662 iterations : Training Loss =  0.10151099976373401; Validation Loss = 0.12394417760322819\n",
            "Cost after 296663 iterations : Training Loss =  0.10151099976373393; Validation Loss = 0.12394417760119858\n",
            "Cost after 296664 iterations : Training Loss =  0.1015109997637338; Validation Loss = 0.1239441775991694\n",
            "Cost after 296665 iterations : Training Loss =  0.10151099976373383; Validation Loss = 0.12394417759713998\n",
            "Cost after 296666 iterations : Training Loss =  0.10151099976373383; Validation Loss = 0.12394417759511071\n",
            "Cost after 296667 iterations : Training Loss =  0.10151099976373389; Validation Loss = 0.12394417759308163\n",
            "Cost after 296668 iterations : Training Loss =  0.1015109997637339; Validation Loss = 0.12394417759105267\n",
            "Cost after 296669 iterations : Training Loss =  0.10151099976373401; Validation Loss = 0.12394417758902382\n",
            "Cost after 296670 iterations : Training Loss =  0.10151099976373373; Validation Loss = 0.12394417758699518\n",
            "Cost after 296671 iterations : Training Loss =  0.10151099976373379; Validation Loss = 0.12394417758496648\n",
            "Cost after 296672 iterations : Training Loss =  0.10151099976373386; Validation Loss = 0.12394417758293762\n",
            "Cost after 296673 iterations : Training Loss =  0.1015109997637339; Validation Loss = 0.1239441775809095\n",
            "Cost after 296674 iterations : Training Loss =  0.10151099976373372; Validation Loss = 0.12394417757888099\n",
            "Cost after 296675 iterations : Training Loss =  0.10151099976373361; Validation Loss = 0.12394417757685265\n",
            "Cost after 296676 iterations : Training Loss =  0.1015109997637339; Validation Loss = 0.12394417757482432\n",
            "Cost after 296677 iterations : Training Loss =  0.10151099976373384; Validation Loss = 0.12394417757279665\n",
            "Cost after 296678 iterations : Training Loss =  0.10151099976373376; Validation Loss = 0.12394417757076866\n",
            "Cost after 296679 iterations : Training Loss =  0.10151099976373379; Validation Loss = 0.12394417756874074\n",
            "Cost after 296680 iterations : Training Loss =  0.1015109997637339; Validation Loss = 0.12394417756671292\n",
            "Cost after 296681 iterations : Training Loss =  0.10151099976373386; Validation Loss = 0.12394417756468531\n",
            "Cost after 296682 iterations : Training Loss =  0.10151099976373378; Validation Loss = 0.12394417756265774\n",
            "Cost after 296683 iterations : Training Loss =  0.10151099976373372; Validation Loss = 0.12394417756063013\n",
            "Cost after 296684 iterations : Training Loss =  0.1015109997637338; Validation Loss = 0.12394417755860301\n",
            "Cost after 296685 iterations : Training Loss =  0.10151099976373364; Validation Loss = 0.12394417755657566\n",
            "Cost after 296686 iterations : Training Loss =  0.10151099976373383; Validation Loss = 0.12394417755454847\n",
            "Cost after 296687 iterations : Training Loss =  0.10151099976373384; Validation Loss = 0.12394417755252166\n",
            "Cost after 296688 iterations : Training Loss =  0.10151099976373365; Validation Loss = 0.12394417755049451\n",
            "Cost after 296689 iterations : Training Loss =  0.10151099976373373; Validation Loss = 0.12394417754846757\n",
            "Cost after 296690 iterations : Training Loss =  0.1015109997637338; Validation Loss = 0.12394417754644087\n",
            "Cost after 296691 iterations : Training Loss =  0.10151099976373396; Validation Loss = 0.12394417754441436\n",
            "Cost after 296692 iterations : Training Loss =  0.10151099976373372; Validation Loss = 0.12394417754238796\n",
            "Cost after 296693 iterations : Training Loss =  0.10151099976373373; Validation Loss = 0.12394417754036145\n",
            "Cost after 296694 iterations : Training Loss =  0.1015109997637337; Validation Loss = 0.1239441775383351\n",
            "Cost after 296695 iterations : Training Loss =  0.10151099976373383; Validation Loss = 0.12394417753630897\n",
            "Cost after 296696 iterations : Training Loss =  0.10151099976373366; Validation Loss = 0.12394417753428263\n",
            "Cost after 296697 iterations : Training Loss =  0.10151099976373372; Validation Loss = 0.12394417753225678\n",
            "Cost after 296698 iterations : Training Loss =  0.10151099976373372; Validation Loss = 0.12394417753023086\n",
            "Cost after 296699 iterations : Training Loss =  0.10151099976373364; Validation Loss = 0.12394417752820502\n",
            "Cost after 296700 iterations : Training Loss =  0.10151099976373386; Validation Loss = 0.12394417752617948\n",
            "Cost after 296701 iterations : Training Loss =  0.10151099976373373; Validation Loss = 0.12394417752415397\n",
            "Cost after 296702 iterations : Training Loss =  0.10151099976373351; Validation Loss = 0.12394417752212819\n",
            "Cost after 296703 iterations : Training Loss =  0.10151099976373347; Validation Loss = 0.12394417752010271\n",
            "Cost after 296704 iterations : Training Loss =  0.10151099976373364; Validation Loss = 0.12394417751807767\n",
            "Cost after 296705 iterations : Training Loss =  0.10151099976373379; Validation Loss = 0.12394417751605236\n",
            "Cost after 296706 iterations : Training Loss =  0.10151099976373366; Validation Loss = 0.12394417751402745\n",
            "Cost after 296707 iterations : Training Loss =  0.10151099976373378; Validation Loss = 0.12394417751200243\n",
            "Cost after 296708 iterations : Training Loss =  0.10151099976373365; Validation Loss = 0.12394417750997759\n",
            "Cost after 296709 iterations : Training Loss =  0.10151099976373368; Validation Loss = 0.12394417750795278\n",
            "Cost after 296710 iterations : Training Loss =  0.10151099976373348; Validation Loss = 0.12394417750592802\n",
            "Cost after 296711 iterations : Training Loss =  0.10151099976373373; Validation Loss = 0.12394417750390341\n",
            "Cost after 296712 iterations : Training Loss =  0.10151099976373365; Validation Loss = 0.12394417750187912\n",
            "Cost after 296713 iterations : Training Loss =  0.10151099976373364; Validation Loss = 0.12394417749985476\n",
            "Cost after 296714 iterations : Training Loss =  0.10151099976373346; Validation Loss = 0.12394417749783063\n",
            "Cost after 296715 iterations : Training Loss =  0.10151099976373361; Validation Loss = 0.12394417749580625\n",
            "Cost after 296716 iterations : Training Loss =  0.10151099976373354; Validation Loss = 0.12394417749378217\n",
            "Cost after 296717 iterations : Training Loss =  0.10151099976373368; Validation Loss = 0.12394417749175828\n",
            "Cost after 296718 iterations : Training Loss =  0.10151099976373348; Validation Loss = 0.12394417748973444\n",
            "Cost after 296719 iterations : Training Loss =  0.1015109997637337; Validation Loss = 0.1239441774877109\n",
            "Cost after 296720 iterations : Training Loss =  0.10151099976373373; Validation Loss = 0.123944177485687\n",
            "Cost after 296721 iterations : Training Loss =  0.10151099976373361; Validation Loss = 0.12394417748366365\n",
            "Cost after 296722 iterations : Training Loss =  0.10151099976373354; Validation Loss = 0.12394417748164042\n",
            "Cost after 296723 iterations : Training Loss =  0.10151099976373364; Validation Loss = 0.12394417747961665\n",
            "Cost after 296724 iterations : Training Loss =  0.10151099976373358; Validation Loss = 0.12394417747759373\n",
            "Cost after 296725 iterations : Training Loss =  0.10151099976373339; Validation Loss = 0.12394417747557036\n",
            "Cost after 296726 iterations : Training Loss =  0.10151099976373357; Validation Loss = 0.12394417747354747\n",
            "Cost after 296727 iterations : Training Loss =  0.10151099976373357; Validation Loss = 0.12394417747152447\n",
            "Cost after 296728 iterations : Training Loss =  0.10151099976373348; Validation Loss = 0.1239441774695017\n",
            "Cost after 296729 iterations : Training Loss =  0.10151099976373348; Validation Loss = 0.12394417746747911\n",
            "Cost after 296730 iterations : Training Loss =  0.10151099976373354; Validation Loss = 0.12394417746545643\n",
            "Cost after 296731 iterations : Training Loss =  0.10151099976373339; Validation Loss = 0.12394417746343404\n",
            "Cost after 296732 iterations : Training Loss =  0.10151099976373353; Validation Loss = 0.12394417746141144\n",
            "Cost after 296733 iterations : Training Loss =  0.10151099976373353; Validation Loss = 0.12394417745938897\n",
            "Cost after 296734 iterations : Training Loss =  0.10151099976373344; Validation Loss = 0.1239441774573671\n",
            "Cost after 296735 iterations : Training Loss =  0.10151099976373341; Validation Loss = 0.12394417745534476\n",
            "Cost after 296736 iterations : Training Loss =  0.10151099976373353; Validation Loss = 0.1239441774533228\n",
            "Cost after 296737 iterations : Training Loss =  0.10151099976373354; Validation Loss = 0.1239441774513007\n",
            "Cost after 296738 iterations : Training Loss =  0.10151099976373347; Validation Loss = 0.12394417744927895\n",
            "Cost after 296739 iterations : Training Loss =  0.10151099976373347; Validation Loss = 0.12394417744725728\n",
            "Cost after 296740 iterations : Training Loss =  0.10151099976373348; Validation Loss = 0.12394417744523566\n",
            "Cost after 296741 iterations : Training Loss =  0.10151099976373354; Validation Loss = 0.12394417744321425\n",
            "Cost after 296742 iterations : Training Loss =  0.10151099976373336; Validation Loss = 0.12394417744119268\n",
            "Cost after 296743 iterations : Training Loss =  0.10151099976373348; Validation Loss = 0.12394417743917165\n",
            "Cost after 296744 iterations : Training Loss =  0.1015109997637336; Validation Loss = 0.12394417743715043\n",
            "Cost after 296745 iterations : Training Loss =  0.10151099976373346; Validation Loss = 0.12394417743512925\n",
            "Cost after 296746 iterations : Training Loss =  0.10151099976373344; Validation Loss = 0.12394417743310843\n",
            "Cost after 296747 iterations : Training Loss =  0.10151099976373358; Validation Loss = 0.1239441774310874\n",
            "Cost after 296748 iterations : Training Loss =  0.10151099976373336; Validation Loss = 0.12394417742906658\n",
            "Cost after 296749 iterations : Training Loss =  0.1015109997637334; Validation Loss = 0.12394417742704623\n",
            "Cost after 296750 iterations : Training Loss =  0.10151099976373339; Validation Loss = 0.12394417742502556\n",
            "Cost after 296751 iterations : Training Loss =  0.10151099976373341; Validation Loss = 0.12394417742300479\n",
            "Cost after 296752 iterations : Training Loss =  0.10151099976373339; Validation Loss = 0.12394417742098465\n",
            "Cost after 296753 iterations : Training Loss =  0.10151099976373358; Validation Loss = 0.12394417741896455\n",
            "Cost after 296754 iterations : Training Loss =  0.10151099976373336; Validation Loss = 0.12394417741694425\n",
            "Cost after 296755 iterations : Training Loss =  0.10151099976373344; Validation Loss = 0.12394417741492418\n",
            "Cost after 296756 iterations : Training Loss =  0.10151099976373326; Validation Loss = 0.12394417741290421\n",
            "Cost after 296757 iterations : Training Loss =  0.10151099976373344; Validation Loss = 0.12394417741088443\n",
            "Cost after 296758 iterations : Training Loss =  0.10151099976373348; Validation Loss = 0.12394417740886443\n",
            "Cost after 296759 iterations : Training Loss =  0.10151099976373334; Validation Loss = 0.12394417740684499\n",
            "Cost after 296760 iterations : Training Loss =  0.10151099976373344; Validation Loss = 0.12394417740482569\n",
            "Cost after 296761 iterations : Training Loss =  0.10151099976373322; Validation Loss = 0.12394417740280615\n",
            "Cost after 296762 iterations : Training Loss =  0.10151099976373323; Validation Loss = 0.12394417740078673\n",
            "Cost after 296763 iterations : Training Loss =  0.10151099976373334; Validation Loss = 0.12394417739876758\n",
            "Cost after 296764 iterations : Training Loss =  0.10151099976373357; Validation Loss = 0.12394417739674835\n",
            "Cost after 296765 iterations : Training Loss =  0.10151099976373339; Validation Loss = 0.12394417739472952\n",
            "Cost after 296766 iterations : Training Loss =  0.1015109997637334; Validation Loss = 0.12394417739271023\n",
            "Cost after 296767 iterations : Training Loss =  0.1015109997637334; Validation Loss = 0.12394417739069148\n",
            "Cost after 296768 iterations : Training Loss =  0.10151099976373319; Validation Loss = 0.12394417738867299\n",
            "Cost after 296769 iterations : Training Loss =  0.1015109997637334; Validation Loss = 0.1239441773866542\n",
            "Cost after 296770 iterations : Training Loss =  0.10151099976373357; Validation Loss = 0.12394417738463581\n",
            "Cost after 296771 iterations : Training Loss =  0.1015109997637334; Validation Loss = 0.12394417738261729\n",
            "Cost after 296772 iterations : Training Loss =  0.10151099976373314; Validation Loss = 0.123944177380599\n",
            "Cost after 296773 iterations : Training Loss =  0.1015109997637332; Validation Loss = 0.12394417737858074\n",
            "Cost after 296774 iterations : Training Loss =  0.10151099976373339; Validation Loss = 0.12394417737656253\n",
            "Cost after 296775 iterations : Training Loss =  0.10151099976373329; Validation Loss = 0.12394417737454484\n",
            "Cost after 296776 iterations : Training Loss =  0.10151099976373351; Validation Loss = 0.12394417737252639\n",
            "Cost after 296777 iterations : Training Loss =  0.10151099976373347; Validation Loss = 0.12394417737050904\n",
            "Cost after 296778 iterations : Training Loss =  0.10151099976373336; Validation Loss = 0.12394417736849145\n",
            "Cost after 296779 iterations : Training Loss =  0.10151099976373333; Validation Loss = 0.12394417736647366\n",
            "Cost after 296780 iterations : Training Loss =  0.10151099976373336; Validation Loss = 0.12394417736445609\n",
            "Cost after 296781 iterations : Training Loss =  0.10151099976373316; Validation Loss = 0.12394417736243889\n",
            "Cost after 296782 iterations : Training Loss =  0.10151099976373339; Validation Loss = 0.12394417736042158\n",
            "Cost after 296783 iterations : Training Loss =  0.10151099976373334; Validation Loss = 0.12394417735840454\n",
            "Cost after 296784 iterations : Training Loss =  0.10151099976373329; Validation Loss = 0.12394417735638716\n",
            "Cost after 296785 iterations : Training Loss =  0.10151099976373323; Validation Loss = 0.12394417735437027\n",
            "Cost after 296786 iterations : Training Loss =  0.10151099976373322; Validation Loss = 0.12394417735235347\n",
            "Cost after 296787 iterations : Training Loss =  0.10151099976373332; Validation Loss = 0.12394417735033701\n",
            "Cost after 296788 iterations : Training Loss =  0.10151099976373326; Validation Loss = 0.12394417734832024\n",
            "Cost after 296789 iterations : Training Loss =  0.1015109997637332; Validation Loss = 0.12394417734630336\n",
            "Cost after 296790 iterations : Training Loss =  0.10151099976373323; Validation Loss = 0.12394417734428702\n",
            "Cost after 296791 iterations : Training Loss =  0.10151099976373329; Validation Loss = 0.12394417734227062\n",
            "Cost after 296792 iterations : Training Loss =  0.10151099976373319; Validation Loss = 0.12394417734025412\n",
            "Cost after 296793 iterations : Training Loss =  0.10151099976373328; Validation Loss = 0.12394417733823833\n",
            "Cost after 296794 iterations : Training Loss =  0.10151099976373332; Validation Loss = 0.12394417733622215\n",
            "Cost after 296795 iterations : Training Loss =  0.10151099976373322; Validation Loss = 0.12394417733420598\n",
            "Cost after 296796 iterations : Training Loss =  0.10151099976373323; Validation Loss = 0.1239441773321902\n",
            "Cost after 296797 iterations : Training Loss =  0.1015109997637332; Validation Loss = 0.12394417733017489\n",
            "Cost after 296798 iterations : Training Loss =  0.10151099976373319; Validation Loss = 0.123944177328159\n",
            "Cost after 296799 iterations : Training Loss =  0.10151099976373319; Validation Loss = 0.12394417732614345\n",
            "Cost after 296800 iterations : Training Loss =  0.10151099976373312; Validation Loss = 0.12394417732412809\n",
            "Cost after 296801 iterations : Training Loss =  0.1015109997637332; Validation Loss = 0.12394417732211276\n",
            "Cost after 296802 iterations : Training Loss =  0.10151099976373312; Validation Loss = 0.12394417732009773\n",
            "Cost after 296803 iterations : Training Loss =  0.10151099976373312; Validation Loss = 0.12394417731808222\n",
            "Cost after 296804 iterations : Training Loss =  0.10151099976373315; Validation Loss = 0.12394417731606741\n",
            "Cost after 296805 iterations : Training Loss =  0.10151099976373316; Validation Loss = 0.12394417731405252\n",
            "Cost after 296806 iterations : Training Loss =  0.10151099976373319; Validation Loss = 0.12394417731203745\n",
            "Cost after 296807 iterations : Training Loss =  0.1015109997637331; Validation Loss = 0.12394417731002288\n",
            "Cost after 296808 iterations : Training Loss =  0.10151099976373326; Validation Loss = 0.12394417730800818\n",
            "Cost after 296809 iterations : Training Loss =  0.10151099976373308; Validation Loss = 0.12394417730599383\n",
            "Cost after 296810 iterations : Training Loss =  0.10151099976373308; Validation Loss = 0.12394417730397933\n",
            "Cost after 296811 iterations : Training Loss =  0.10151099976373316; Validation Loss = 0.12394417730196489\n",
            "Cost after 296812 iterations : Training Loss =  0.1015109997637332; Validation Loss = 0.12394417729995066\n",
            "Cost after 296813 iterations : Training Loss =  0.10151099976373328; Validation Loss = 0.12394417729793658\n",
            "Cost after 296814 iterations : Training Loss =  0.10151099976373314; Validation Loss = 0.1239441772959228\n",
            "Cost after 296815 iterations : Training Loss =  0.10151099976373315; Validation Loss = 0.12394417729390869\n",
            "Cost after 296816 iterations : Training Loss =  0.10151099976373319; Validation Loss = 0.12394417729189477\n",
            "Cost after 296817 iterations : Training Loss =  0.10151099976373326; Validation Loss = 0.12394417728988127\n",
            "Cost after 296818 iterations : Training Loss =  0.10151099976373326; Validation Loss = 0.12394417728786757\n",
            "Cost after 296819 iterations : Training Loss =  0.10151099976373297; Validation Loss = 0.12394417728585402\n",
            "Cost after 296820 iterations : Training Loss =  0.10151099976373307; Validation Loss = 0.12394417728384062\n",
            "Cost after 296821 iterations : Training Loss =  0.10151099976373307; Validation Loss = 0.1239441772818274\n",
            "Cost after 296822 iterations : Training Loss =  0.10151099976373296; Validation Loss = 0.12394417727981419\n",
            "Cost after 296823 iterations : Training Loss =  0.10151099976373307; Validation Loss = 0.12394417727780133\n",
            "Cost after 296824 iterations : Training Loss =  0.10151099976373307; Validation Loss = 0.12394417727578798\n",
            "Cost after 296825 iterations : Training Loss =  0.10151099976373315; Validation Loss = 0.12394417727377546\n",
            "Cost after 296826 iterations : Training Loss =  0.10151099976373301; Validation Loss = 0.1239441772717625\n",
            "Cost after 296827 iterations : Training Loss =  0.10151099976373303; Validation Loss = 0.12394417726975\n",
            "Cost after 296828 iterations : Training Loss =  0.10151099976373294; Validation Loss = 0.12394417726773721\n",
            "Cost after 296829 iterations : Training Loss =  0.10151099976373296; Validation Loss = 0.12394417726572464\n",
            "Cost after 296830 iterations : Training Loss =  0.10151099976373307; Validation Loss = 0.12394417726371243\n",
            "Cost after 296831 iterations : Training Loss =  0.10151099976373307; Validation Loss = 0.12394417726170037\n",
            "Cost after 296832 iterations : Training Loss =  0.10151099976373285; Validation Loss = 0.12394417725968766\n",
            "Cost after 296833 iterations : Training Loss =  0.10151099976373297; Validation Loss = 0.12394417725767583\n",
            "Cost after 296834 iterations : Training Loss =  0.10151099976373308; Validation Loss = 0.12394417725566383\n",
            "Cost after 296835 iterations : Training Loss =  0.101510999763733; Validation Loss = 0.12394417725365196\n",
            "Cost after 296836 iterations : Training Loss =  0.10151099976373294; Validation Loss = 0.12394417725164018\n",
            "Cost after 296837 iterations : Training Loss =  0.10151099976373303; Validation Loss = 0.1239441772496286\n",
            "Cost after 296838 iterations : Training Loss =  0.10151099976373301; Validation Loss = 0.12394417724761692\n",
            "Cost after 296839 iterations : Training Loss =  0.10151099976373291; Validation Loss = 0.12394417724560555\n",
            "Cost after 296840 iterations : Training Loss =  0.10151099976373294; Validation Loss = 0.12394417724359422\n",
            "Cost after 296841 iterations : Training Loss =  0.10151099976373283; Validation Loss = 0.123944177241583\n",
            "Cost after 296842 iterations : Training Loss =  0.10151099976373276; Validation Loss = 0.1239441772395719\n",
            "Cost after 296843 iterations : Training Loss =  0.10151099976373296; Validation Loss = 0.12394417723756095\n",
            "Cost after 296844 iterations : Training Loss =  0.10151099976373289; Validation Loss = 0.12394417723555\n",
            "Cost after 296845 iterations : Training Loss =  0.10151099976373279; Validation Loss = 0.12394417723353904\n",
            "Cost after 296846 iterations : Training Loss =  0.1015109997637331; Validation Loss = 0.12394417723152845\n",
            "Cost after 296847 iterations : Training Loss =  0.10151099976373275; Validation Loss = 0.12394417722951756\n",
            "Cost after 296848 iterations : Training Loss =  0.10151099976373297; Validation Loss = 0.12394417722750727\n",
            "Cost after 296849 iterations : Training Loss =  0.10151099976373289; Validation Loss = 0.12394417722549672\n",
            "Cost after 296850 iterations : Training Loss =  0.10151099976373297; Validation Loss = 0.12394417722348645\n",
            "Cost after 296851 iterations : Training Loss =  0.10151099976373272; Validation Loss = 0.12394417722147613\n",
            "Cost after 296852 iterations : Training Loss =  0.10151099976373291; Validation Loss = 0.12394417721946598\n",
            "Cost after 296853 iterations : Training Loss =  0.10151099976373287; Validation Loss = 0.12394417721745603\n",
            "Cost after 296854 iterations : Training Loss =  0.10151099976373291; Validation Loss = 0.12394417721544607\n",
            "Cost after 296855 iterations : Training Loss =  0.10151099976373282; Validation Loss = 0.12394417721343612\n",
            "Cost after 296856 iterations : Training Loss =  0.10151099976373294; Validation Loss = 0.1239441772114265\n",
            "Cost after 296857 iterations : Training Loss =  0.10151099976373279; Validation Loss = 0.12394417720941674\n",
            "Cost after 296858 iterations : Training Loss =  0.10151099976373297; Validation Loss = 0.12394417720740766\n",
            "Cost after 296859 iterations : Training Loss =  0.10151099976373289; Validation Loss = 0.12394417720539791\n",
            "Cost after 296860 iterations : Training Loss =  0.10151099976373289; Validation Loss = 0.12394417720338834\n",
            "Cost after 296861 iterations : Training Loss =  0.10151099976373282; Validation Loss = 0.1239441772013793\n",
            "Cost after 296862 iterations : Training Loss =  0.1015109997637327; Validation Loss = 0.12394417719937036\n",
            "Cost after 296863 iterations : Training Loss =  0.10151099976373283; Validation Loss = 0.12394417719736124\n",
            "Cost after 296864 iterations : Training Loss =  0.1015109997637329; Validation Loss = 0.12394417719535245\n",
            "Cost after 296865 iterations : Training Loss =  0.10151099976373287; Validation Loss = 0.12394417719334345\n",
            "Cost after 296866 iterations : Training Loss =  0.10151099976373276; Validation Loss = 0.123944177191335\n",
            "Cost after 296867 iterations : Training Loss =  0.10151099976373289; Validation Loss = 0.12394417718932642\n",
            "Cost after 296868 iterations : Training Loss =  0.10151099976373268; Validation Loss = 0.12394417718731796\n",
            "Cost after 296869 iterations : Training Loss =  0.10151099976373289; Validation Loss = 0.12394417718530946\n",
            "Cost after 296870 iterations : Training Loss =  0.10151099976373278; Validation Loss = 0.12394417718330125\n",
            "Cost after 296871 iterations : Training Loss =  0.1015109997637329; Validation Loss = 0.12394417718129315\n",
            "Cost after 296872 iterations : Training Loss =  0.1015109997637329; Validation Loss = 0.12394417717928459\n",
            "Cost after 296873 iterations : Training Loss =  0.10151099976373275; Validation Loss = 0.1239441771772769\n",
            "Cost after 296874 iterations : Training Loss =  0.10151099976373269; Validation Loss = 0.12394417717526907\n",
            "Cost after 296875 iterations : Training Loss =  0.10151099976373279; Validation Loss = 0.12394417717326113\n",
            "Cost after 296876 iterations : Training Loss =  0.10151099976373276; Validation Loss = 0.12394417717125362\n",
            "Cost after 296877 iterations : Training Loss =  0.10151099976373276; Validation Loss = 0.12394417716924615\n",
            "Cost after 296878 iterations : Training Loss =  0.10151099976373279; Validation Loss = 0.12394417716723859\n",
            "Cost after 296879 iterations : Training Loss =  0.10151099976373268; Validation Loss = 0.12394417716523105\n",
            "Cost after 296880 iterations : Training Loss =  0.10151099976373265; Validation Loss = 0.12394417716322394\n",
            "Cost after 296881 iterations : Training Loss =  0.10151099976373258; Validation Loss = 0.12394417716121699\n",
            "Cost after 296882 iterations : Training Loss =  0.10151099976373278; Validation Loss = 0.12394417715920979\n",
            "Cost after 296883 iterations : Training Loss =  0.10151099976373275; Validation Loss = 0.12394417715720284\n",
            "Cost after 296884 iterations : Training Loss =  0.10151099976373282; Validation Loss = 0.12394417715519586\n",
            "Cost after 296885 iterations : Training Loss =  0.10151099976373283; Validation Loss = 0.12394417715318909\n",
            "Cost after 296886 iterations : Training Loss =  0.10151099976373279; Validation Loss = 0.12394417715118262\n",
            "Cost after 296887 iterations : Training Loss =  0.10151099976373257; Validation Loss = 0.12394417714917559\n",
            "Cost after 296888 iterations : Training Loss =  0.10151099976373275; Validation Loss = 0.12394417714716939\n",
            "Cost after 296889 iterations : Training Loss =  0.10151099976373275; Validation Loss = 0.12394417714516308\n",
            "Cost after 296890 iterations : Training Loss =  0.10151099976373276; Validation Loss = 0.12394417714315656\n",
            "Cost after 296891 iterations : Training Loss =  0.10151099976373272; Validation Loss = 0.1239441771411504\n",
            "Cost after 296892 iterations : Training Loss =  0.10151099976373278; Validation Loss = 0.12394417713914477\n",
            "Cost after 296893 iterations : Training Loss =  0.10151099976373264; Validation Loss = 0.12394417713713875\n",
            "Cost after 296894 iterations : Training Loss =  0.10151099976373264; Validation Loss = 0.12394417713513273\n",
            "Cost after 296895 iterations : Training Loss =  0.10151099976373258; Validation Loss = 0.12394417713312712\n",
            "Cost after 296896 iterations : Training Loss =  0.10151099976373269; Validation Loss = 0.12394417713112128\n",
            "Cost after 296897 iterations : Training Loss =  0.1015109997637326; Validation Loss = 0.12394417712911604\n",
            "Cost after 296898 iterations : Training Loss =  0.10151099976373265; Validation Loss = 0.12394417712711049\n",
            "Cost after 296899 iterations : Training Loss =  0.10151099976373275; Validation Loss = 0.12394417712510508\n",
            "Cost after 296900 iterations : Training Loss =  0.1015109997637327; Validation Loss = 0.12394417712309988\n",
            "Cost after 296901 iterations : Training Loss =  0.1015109997637326; Validation Loss = 0.12394417712109487\n",
            "Cost after 296902 iterations : Training Loss =  0.10151099976373255; Validation Loss = 0.1239441771190898\n",
            "Cost after 296903 iterations : Training Loss =  0.10151099976373251; Validation Loss = 0.12394417711708493\n",
            "Cost after 296904 iterations : Training Loss =  0.10151099976373258; Validation Loss = 0.12394417711508002\n",
            "Cost after 296905 iterations : Training Loss =  0.10151099976373276; Validation Loss = 0.12394417711307507\n",
            "Cost after 296906 iterations : Training Loss =  0.10151099976373262; Validation Loss = 0.12394417711107082\n",
            "Cost after 296907 iterations : Training Loss =  0.10151099976373278; Validation Loss = 0.12394417710906605\n",
            "Cost after 296908 iterations : Training Loss =  0.10151099976373246; Validation Loss = 0.12394417710706172\n",
            "Cost after 296909 iterations : Training Loss =  0.10151099976373246; Validation Loss = 0.12394417710505747\n",
            "Cost after 296910 iterations : Training Loss =  0.10151099976373253; Validation Loss = 0.12394417710305285\n",
            "Cost after 296911 iterations : Training Loss =  0.1015109997637326; Validation Loss = 0.12394417710104874\n",
            "Cost after 296912 iterations : Training Loss =  0.10151099976373265; Validation Loss = 0.12394417709904502\n",
            "Cost after 296913 iterations : Training Loss =  0.1015109997637326; Validation Loss = 0.12394417709704096\n",
            "Cost after 296914 iterations : Training Loss =  0.10151099976373268; Validation Loss = 0.12394417709503723\n",
            "Cost after 296915 iterations : Training Loss =  0.10151099976373258; Validation Loss = 0.12394417709303347\n",
            "Cost after 296916 iterations : Training Loss =  0.10151099976373253; Validation Loss = 0.12394417709102991\n",
            "Cost after 296917 iterations : Training Loss =  0.10151099976373258; Validation Loss = 0.12394417708902639\n",
            "Cost after 296918 iterations : Training Loss =  0.10151099976373244; Validation Loss = 0.1239441770870229\n",
            "Cost after 296919 iterations : Training Loss =  0.1015109997637326; Validation Loss = 0.12394417708501962\n",
            "Cost after 296920 iterations : Training Loss =  0.10151099976373257; Validation Loss = 0.12394417708301646\n",
            "Cost after 296921 iterations : Training Loss =  0.10151099976373253; Validation Loss = 0.12394417708101334\n",
            "Cost after 296922 iterations : Training Loss =  0.10151099976373247; Validation Loss = 0.12394417707901038\n",
            "Cost after 296923 iterations : Training Loss =  0.10151099976373243; Validation Loss = 0.12394417707700757\n",
            "Cost after 296924 iterations : Training Loss =  0.10151099976373264; Validation Loss = 0.12394417707500494\n",
            "Cost after 296925 iterations : Training Loss =  0.1015109997637325; Validation Loss = 0.12394417707300219\n",
            "Cost after 296926 iterations : Training Loss =  0.10151099976373246; Validation Loss = 0.12394417707099954\n",
            "Cost after 296927 iterations : Training Loss =  0.10151099976373255; Validation Loss = 0.1239441770689967\n",
            "Cost after 296928 iterations : Training Loss =  0.10151099976373262; Validation Loss = 0.12394417706699486\n",
            "Cost after 296929 iterations : Training Loss =  0.10151099976373243; Validation Loss = 0.12394417706499242\n",
            "Cost after 296930 iterations : Training Loss =  0.10151099976373237; Validation Loss = 0.12394417706299038\n",
            "Cost after 296931 iterations : Training Loss =  0.10151099976373251; Validation Loss = 0.12394417706098841\n",
            "Cost after 296932 iterations : Training Loss =  0.10151099976373257; Validation Loss = 0.12394417705898622\n",
            "Cost after 296933 iterations : Training Loss =  0.1015109997637324; Validation Loss = 0.12394417705698434\n",
            "Cost after 296934 iterations : Training Loss =  0.10151099976373243; Validation Loss = 0.12394417705498235\n",
            "Cost after 296935 iterations : Training Loss =  0.10151099976373243; Validation Loss = 0.12394417705298075\n",
            "Cost after 296936 iterations : Training Loss =  0.10151099976373262; Validation Loss = 0.12394417705097918\n",
            "Cost after 296937 iterations : Training Loss =  0.10151099976373239; Validation Loss = 0.1239441770489777\n",
            "Cost after 296938 iterations : Training Loss =  0.10151099976373255; Validation Loss = 0.12394417704697626\n",
            "Cost after 296939 iterations : Training Loss =  0.10151099976373226; Validation Loss = 0.12394417704497528\n",
            "Cost after 296940 iterations : Training Loss =  0.10151099976373232; Validation Loss = 0.12394417704297396\n",
            "Cost after 296941 iterations : Training Loss =  0.10151099976373233; Validation Loss = 0.12394417704097292\n",
            "Cost after 296942 iterations : Training Loss =  0.10151099976373257; Validation Loss = 0.1239441770389718\n",
            "Cost after 296943 iterations : Training Loss =  0.10151099976373253; Validation Loss = 0.12394417703697087\n",
            "Cost after 296944 iterations : Training Loss =  0.10151099976373247; Validation Loss = 0.12394417703497031\n",
            "Cost after 296945 iterations : Training Loss =  0.10151099976373226; Validation Loss = 0.12394417703296975\n",
            "Cost after 296946 iterations : Training Loss =  0.10151099976373247; Validation Loss = 0.12394417703096888\n",
            "Cost after 296947 iterations : Training Loss =  0.10151099976373228; Validation Loss = 0.12394417702896866\n",
            "Cost after 296948 iterations : Training Loss =  0.10151099976373232; Validation Loss = 0.1239441770269679\n",
            "Cost after 296949 iterations : Training Loss =  0.10151099976373223; Validation Loss = 0.12394417702496775\n",
            "Cost after 296950 iterations : Training Loss =  0.10151099976373239; Validation Loss = 0.12394417702296791\n",
            "Cost after 296951 iterations : Training Loss =  0.10151099976373232; Validation Loss = 0.12394417702096784\n",
            "Cost after 296952 iterations : Training Loss =  0.1015109997637323; Validation Loss = 0.12394417701896811\n",
            "Cost after 296953 iterations : Training Loss =  0.1015109997637324; Validation Loss = 0.1239441770169681\n",
            "Cost after 296954 iterations : Training Loss =  0.10151099976373244; Validation Loss = 0.12394417701496835\n",
            "Cost after 296955 iterations : Training Loss =  0.1015109997637323; Validation Loss = 0.12394417701296881\n",
            "Cost after 296956 iterations : Training Loss =  0.10151099976373223; Validation Loss = 0.12394417701096895\n",
            "Cost after 296957 iterations : Training Loss =  0.1015109997637323; Validation Loss = 0.12394417700896977\n",
            "Cost after 296958 iterations : Training Loss =  0.10151099976373226; Validation Loss = 0.12394417700697037\n",
            "Cost after 296959 iterations : Training Loss =  0.10151099976373228; Validation Loss = 0.12394417700497115\n",
            "Cost after 296960 iterations : Training Loss =  0.1015109997637324; Validation Loss = 0.12394417700297201\n",
            "Cost after 296961 iterations : Training Loss =  0.10151099976373233; Validation Loss = 0.12394417700097288\n",
            "Cost after 296962 iterations : Training Loss =  0.10151099976373243; Validation Loss = 0.12394417699897409\n",
            "Cost after 296963 iterations : Training Loss =  0.1015109997637323; Validation Loss = 0.1239441769969751\n",
            "Cost after 296964 iterations : Training Loss =  0.10151099976373235; Validation Loss = 0.12394417699497638\n",
            "Cost after 296965 iterations : Training Loss =  0.10151099976373239; Validation Loss = 0.12394417699297788\n",
            "Cost after 296966 iterations : Training Loss =  0.10151099976373233; Validation Loss = 0.12394417699097923\n",
            "Cost after 296967 iterations : Training Loss =  0.1015109997637324; Validation Loss = 0.12394417698898096\n",
            "Cost after 296968 iterations : Training Loss =  0.10151099976373235; Validation Loss = 0.12394417698698218\n",
            "Cost after 296969 iterations : Training Loss =  0.10151099976373244; Validation Loss = 0.1239441769849842\n",
            "Cost after 296970 iterations : Training Loss =  0.1015109997637323; Validation Loss = 0.12394417698298629\n",
            "Cost after 296971 iterations : Training Loss =  0.10151099976373225; Validation Loss = 0.12394417698098827\n",
            "Cost after 296972 iterations : Training Loss =  0.10151099976373235; Validation Loss = 0.1239441769789902\n",
            "Cost after 296973 iterations : Training Loss =  0.10151099976373237; Validation Loss = 0.12394417697699223\n",
            "Cost after 296974 iterations : Training Loss =  0.10151099976373233; Validation Loss = 0.12394417697499475\n",
            "Cost after 296975 iterations : Training Loss =  0.10151099976373223; Validation Loss = 0.12394417697299699\n",
            "Cost after 296976 iterations : Training Loss =  0.10151099976373215; Validation Loss = 0.12394417697099966\n",
            "Cost after 296977 iterations : Training Loss =  0.10151099976373218; Validation Loss = 0.12394417696900169\n",
            "Cost after 296978 iterations : Training Loss =  0.10151099976373233; Validation Loss = 0.12394417696700473\n",
            "Cost after 296979 iterations : Training Loss =  0.10151099976373233; Validation Loss = 0.12394417696500779\n",
            "Cost after 296980 iterations : Training Loss =  0.10151099976373237; Validation Loss = 0.1239441769630103\n",
            "Cost after 296981 iterations : Training Loss =  0.10151099976373223; Validation Loss = 0.12394417696101358\n",
            "Cost after 296982 iterations : Training Loss =  0.10151099976373235; Validation Loss = 0.12394417695901634\n",
            "Cost after 296983 iterations : Training Loss =  0.1015109997637323; Validation Loss = 0.12394417695701976\n",
            "Cost after 296984 iterations : Training Loss =  0.1015109997637322; Validation Loss = 0.12394417695502308\n",
            "Cost after 296985 iterations : Training Loss =  0.10151099976373218; Validation Loss = 0.12394417695302626\n",
            "Cost after 296986 iterations : Training Loss =  0.10151099976373208; Validation Loss = 0.12394417695102988\n",
            "Cost after 296987 iterations : Training Loss =  0.1015109997637323; Validation Loss = 0.1239441769490336\n",
            "Cost after 296988 iterations : Training Loss =  0.10151099976373214; Validation Loss = 0.12394417694703735\n",
            "Cost after 296989 iterations : Training Loss =  0.10151099976373215; Validation Loss = 0.12394417694504126\n",
            "Cost after 296990 iterations : Training Loss =  0.10151099976373207; Validation Loss = 0.12394417694304484\n",
            "Cost after 296991 iterations : Training Loss =  0.10151099976373226; Validation Loss = 0.12394417694104895\n",
            "Cost after 296992 iterations : Training Loss =  0.10151099976373208; Validation Loss = 0.12394417693905296\n",
            "Cost after 296993 iterations : Training Loss =  0.10151099976373208; Validation Loss = 0.12394417693705716\n",
            "Cost after 296994 iterations : Training Loss =  0.10151099976373226; Validation Loss = 0.12394417693506167\n",
            "Cost after 296995 iterations : Training Loss =  0.10151099976373221; Validation Loss = 0.12394417693306586\n",
            "Cost after 296996 iterations : Training Loss =  0.10151099976373201; Validation Loss = 0.12394417693107083\n",
            "Cost after 296997 iterations : Training Loss =  0.10151099976373212; Validation Loss = 0.12394417692907524\n",
            "Cost after 296998 iterations : Training Loss =  0.10151099976373205; Validation Loss = 0.12394417692707994\n",
            "Cost after 296999 iterations : Training Loss =  0.10151099976373214; Validation Loss = 0.12394417692508451\n",
            "Cost after 297000 iterations : Training Loss =  0.10151099976373211; Validation Loss = 0.1239441769230896\n",
            "Cost after 297001 iterations : Training Loss =  0.10151099976373232; Validation Loss = 0.12394417692109475\n",
            "Cost after 297002 iterations : Training Loss =  0.10151099976373196; Validation Loss = 0.12394417691909995\n",
            "Cost after 297003 iterations : Training Loss =  0.10151099976373201; Validation Loss = 0.12394417691710498\n",
            "Cost after 297004 iterations : Training Loss =  0.10151099976373207; Validation Loss = 0.12394417691511021\n",
            "Cost after 297005 iterations : Training Loss =  0.10151099976373226; Validation Loss = 0.12394417691311552\n",
            "Cost after 297006 iterations : Training Loss =  0.10151099976373215; Validation Loss = 0.12394417691112118\n",
            "Cost after 297007 iterations : Training Loss =  0.101510999763732; Validation Loss = 0.12394417690912708\n",
            "Cost after 297008 iterations : Training Loss =  0.10151099976373205; Validation Loss = 0.12394417690713255\n",
            "Cost after 297009 iterations : Training Loss =  0.10151099976373211; Validation Loss = 0.12394417690513844\n",
            "Cost after 297010 iterations : Training Loss =  0.10151099976373214; Validation Loss = 0.12394417690314426\n",
            "Cost after 297011 iterations : Training Loss =  0.10151099976373205; Validation Loss = 0.12394417690115023\n",
            "Cost after 297012 iterations : Training Loss =  0.10151099976373207; Validation Loss = 0.12394417689915634\n",
            "Cost after 297013 iterations : Training Loss =  0.10151099976373214; Validation Loss = 0.12394417689716275\n",
            "Cost after 297014 iterations : Training Loss =  0.10151099976373194; Validation Loss = 0.12394417689516903\n",
            "Cost after 297015 iterations : Training Loss =  0.101510999763732; Validation Loss = 0.12394417689317544\n",
            "Cost after 297016 iterations : Training Loss =  0.10151099976373211; Validation Loss = 0.12394417689118192\n",
            "Cost after 297017 iterations : Training Loss =  0.10151099976373196; Validation Loss = 0.12394417688918873\n",
            "Cost after 297018 iterations : Training Loss =  0.10151099976373189; Validation Loss = 0.12394417688719532\n",
            "Cost after 297019 iterations : Training Loss =  0.10151099976373205; Validation Loss = 0.123944176885202\n",
            "Cost after 297020 iterations : Training Loss =  0.10151099976373201; Validation Loss = 0.12394417688320922\n",
            "Cost after 297021 iterations : Training Loss =  0.10151099976373203; Validation Loss = 0.12394417688121614\n",
            "Cost after 297022 iterations : Training Loss =  0.10151099976373218; Validation Loss = 0.12394417687922324\n",
            "Cost after 297023 iterations : Training Loss =  0.10151099976373207; Validation Loss = 0.12394417687723057\n",
            "Cost after 297024 iterations : Training Loss =  0.10151099976373201; Validation Loss = 0.12394417687523789\n",
            "Cost after 297025 iterations : Training Loss =  0.101510999763732; Validation Loss = 0.12394417687324545\n",
            "Cost after 297026 iterations : Training Loss =  0.10151099976373214; Validation Loss = 0.12394417687125321\n",
            "Cost after 297027 iterations : Training Loss =  0.10151099976373198; Validation Loss = 0.12394417686926057\n",
            "Cost after 297028 iterations : Training Loss =  0.10151099976373198; Validation Loss = 0.12394417686726844\n",
            "Cost after 297029 iterations : Training Loss =  0.10151099976373203; Validation Loss = 0.12394417686527617\n",
            "Cost after 297030 iterations : Training Loss =  0.10151099976373194; Validation Loss = 0.12394417686328439\n",
            "Cost after 297031 iterations : Training Loss =  0.10151099976373208; Validation Loss = 0.12394417686129246\n",
            "Cost after 297032 iterations : Training Loss =  0.10151099976373203; Validation Loss = 0.12394417685930038\n",
            "Cost after 297033 iterations : Training Loss =  0.10151099976373186; Validation Loss = 0.12394417685730863\n",
            "Cost after 297034 iterations : Training Loss =  0.10151099976373194; Validation Loss = 0.1239441768553172\n",
            "Cost after 297035 iterations : Training Loss =  0.10151099976373193; Validation Loss = 0.12394417685332554\n",
            "Cost after 297036 iterations : Training Loss =  0.10151099976373194; Validation Loss = 0.12394417685133415\n",
            "Cost after 297037 iterations : Training Loss =  0.1015109997637319; Validation Loss = 0.12394417684934282\n",
            "Cost after 297038 iterations : Training Loss =  0.10151099976373196; Validation Loss = 0.12394417684735143\n",
            "Cost after 297039 iterations : Training Loss =  0.10151099976373179; Validation Loss = 0.12394417684536004\n",
            "Cost after 297040 iterations : Training Loss =  0.10151099976373196; Validation Loss = 0.12394417684336967\n",
            "Cost after 297041 iterations : Training Loss =  0.10151099976373205; Validation Loss = 0.12394417684137853\n",
            "Cost after 297042 iterations : Training Loss =  0.1015109997637319; Validation Loss = 0.12394417683938784\n",
            "Cost after 297043 iterations : Training Loss =  0.10151099976373193; Validation Loss = 0.1239441768373969\n",
            "Cost after 297044 iterations : Training Loss =  0.101510999763732; Validation Loss = 0.12394417683540622\n",
            "Cost after 297045 iterations : Training Loss =  0.10151099976373186; Validation Loss = 0.1239441768334158\n",
            "Cost after 297046 iterations : Training Loss =  0.10151099976373175; Validation Loss = 0.12394417683142513\n",
            "Cost after 297047 iterations : Training Loss =  0.1015109997637319; Validation Loss = 0.12394417682943498\n",
            "Cost after 297048 iterations : Training Loss =  0.10151099976373196; Validation Loss = 0.12394417682744487\n",
            "Cost after 297049 iterations : Training Loss =  0.1015109997637319; Validation Loss = 0.12394417682545462\n",
            "Cost after 297050 iterations : Training Loss =  0.10151099976373186; Validation Loss = 0.12394417682346481\n",
            "Cost after 297051 iterations : Training Loss =  0.10151099976373183; Validation Loss = 0.12394417682147481\n",
            "Cost after 297052 iterations : Training Loss =  0.10151099976373211; Validation Loss = 0.12394417681948473\n",
            "Cost after 297053 iterations : Training Loss =  0.1015109997637319; Validation Loss = 0.1239441768174952\n",
            "Cost after 297054 iterations : Training Loss =  0.10151099976373182; Validation Loss = 0.12394417681550532\n",
            "Cost after 297055 iterations : Training Loss =  0.10151099976373189; Validation Loss = 0.12394417681351609\n",
            "Cost after 297056 iterations : Training Loss =  0.10151099976373187; Validation Loss = 0.12394417681152668\n",
            "Cost after 297057 iterations : Training Loss =  0.10151099976373171; Validation Loss = 0.12394417680953726\n",
            "Cost after 297058 iterations : Training Loss =  0.10151099976373179; Validation Loss = 0.12394417680754777\n",
            "Cost after 297059 iterations : Training Loss =  0.10151099976373174; Validation Loss = 0.12394417680555898\n",
            "Cost after 297060 iterations : Training Loss =  0.10151099976373186; Validation Loss = 0.12394417680356984\n",
            "Cost after 297061 iterations : Training Loss =  0.1015109997637319; Validation Loss = 0.12394417680158093\n",
            "Cost after 297062 iterations : Training Loss =  0.10151099976373179; Validation Loss = 0.12394417679959199\n",
            "Cost after 297063 iterations : Training Loss =  0.10151099976373183; Validation Loss = 0.12394417679760336\n",
            "Cost after 297064 iterations : Training Loss =  0.10151099976373187; Validation Loss = 0.1239441767956148\n",
            "Cost after 297065 iterations : Training Loss =  0.10151099976373187; Validation Loss = 0.12394417679362611\n",
            "Cost after 297066 iterations : Training Loss =  0.10151099976373175; Validation Loss = 0.1239441767916379\n",
            "Cost after 297067 iterations : Training Loss =  0.10151099976373175; Validation Loss = 0.12394417678964981\n",
            "Cost after 297068 iterations : Training Loss =  0.10151099976373174; Validation Loss = 0.12394417678766155\n",
            "Cost after 297069 iterations : Training Loss =  0.10151099976373183; Validation Loss = 0.12394417678567304\n",
            "Cost after 297070 iterations : Training Loss =  0.10151099976373182; Validation Loss = 0.12394417678368547\n",
            "Cost after 297071 iterations : Training Loss =  0.10151099976373176; Validation Loss = 0.12394417678169749\n",
            "Cost after 297072 iterations : Training Loss =  0.10151099976373187; Validation Loss = 0.12394417677970987\n",
            "Cost after 297073 iterations : Training Loss =  0.10151099976373158; Validation Loss = 0.12394417677772206\n",
            "Cost after 297074 iterations : Training Loss =  0.10151099976373179; Validation Loss = 0.1239441767757343\n",
            "Cost after 297075 iterations : Training Loss =  0.10151099976373168; Validation Loss = 0.1239441767737468\n",
            "Cost after 297076 iterations : Training Loss =  0.10151099976373183; Validation Loss = 0.1239441767717595\n",
            "Cost after 297077 iterations : Training Loss =  0.10151099976373174; Validation Loss = 0.12394417676977211\n",
            "Cost after 297078 iterations : Training Loss =  0.1015109997637318; Validation Loss = 0.12394417676778499\n",
            "Cost after 297079 iterations : Training Loss =  0.10151099976373175; Validation Loss = 0.1239441767657979\n",
            "Cost after 297080 iterations : Training Loss =  0.10151099976373174; Validation Loss = 0.12394417676381082\n",
            "Cost after 297081 iterations : Training Loss =  0.10151099976373176; Validation Loss = 0.12394417676182405\n",
            "Cost after 297082 iterations : Training Loss =  0.10151099976373164; Validation Loss = 0.12394417675983736\n",
            "Cost after 297083 iterations : Training Loss =  0.1015109997637319; Validation Loss = 0.12394417675785059\n",
            "Cost after 297084 iterations : Training Loss =  0.10151099976373168; Validation Loss = 0.1239441767558639\n",
            "Cost after 297085 iterations : Training Loss =  0.10151099976373175; Validation Loss = 0.12394417675387769\n",
            "Cost after 297086 iterations : Training Loss =  0.10151099976373174; Validation Loss = 0.12394417675189104\n",
            "Cost after 297087 iterations : Training Loss =  0.10151099976373167; Validation Loss = 0.12394417674990481\n",
            "Cost after 297088 iterations : Training Loss =  0.10151099976373171; Validation Loss = 0.12394417674791873\n",
            "Cost after 297089 iterations : Training Loss =  0.1015109997637318; Validation Loss = 0.12394417674593236\n",
            "Cost after 297090 iterations : Training Loss =  0.10151099976373158; Validation Loss = 0.12394417674394667\n",
            "Cost after 297091 iterations : Training Loss =  0.10151099976373149; Validation Loss = 0.1239441767419607\n",
            "Cost after 297092 iterations : Training Loss =  0.10151099976373168; Validation Loss = 0.12394417673997513\n",
            "Cost after 297093 iterations : Training Loss =  0.10151099976373168; Validation Loss = 0.12394417673798919\n",
            "Cost after 297094 iterations : Training Loss =  0.10151099976373164; Validation Loss = 0.12394417673600344\n",
            "Cost after 297095 iterations : Training Loss =  0.10151099976373176; Validation Loss = 0.12394417673401815\n",
            "Cost after 297096 iterations : Training Loss =  0.1015109997637317; Validation Loss = 0.12394417673203266\n",
            "Cost after 297097 iterations : Training Loss =  0.10151099976373162; Validation Loss = 0.12394417673004764\n",
            "Cost after 297098 iterations : Training Loss =  0.10151099976373151; Validation Loss = 0.1239441767280626\n",
            "Cost after 297099 iterations : Training Loss =  0.10151099976373154; Validation Loss = 0.12394417672607698\n",
            "Cost after 297100 iterations : Training Loss =  0.10151099976373154; Validation Loss = 0.12394417672409226\n",
            "Cost after 297101 iterations : Training Loss =  0.10151099976373162; Validation Loss = 0.12394417672210714\n",
            "Cost after 297102 iterations : Training Loss =  0.10151099976373161; Validation Loss = 0.12394417672012249\n",
            "Cost after 297103 iterations : Training Loss =  0.10151099976373161; Validation Loss = 0.12394417671813798\n",
            "Cost after 297104 iterations : Training Loss =  0.10151099976373142; Validation Loss = 0.12394417671615336\n",
            "Cost after 297105 iterations : Training Loss =  0.10151099976373162; Validation Loss = 0.12394417671416864\n",
            "Cost after 297106 iterations : Training Loss =  0.1015109997637315; Validation Loss = 0.12394417671218431\n",
            "Cost after 297107 iterations : Training Loss =  0.1015109997637317; Validation Loss = 0.12394417671020012\n",
            "Cost after 297108 iterations : Training Loss =  0.10151099976373157; Validation Loss = 0.12394417670821595\n",
            "Cost after 297109 iterations : Training Loss =  0.10151099976373149; Validation Loss = 0.12394417670623215\n",
            "Cost after 297110 iterations : Training Loss =  0.10151099976373149; Validation Loss = 0.12394417670424793\n",
            "Cost after 297111 iterations : Training Loss =  0.10151099976373143; Validation Loss = 0.12394417670226375\n",
            "Cost after 297112 iterations : Training Loss =  0.10151099976373161; Validation Loss = 0.12394417670028014\n",
            "Cost after 297113 iterations : Training Loss =  0.10151099976373162; Validation Loss = 0.1239441766982965\n",
            "Cost after 297114 iterations : Training Loss =  0.10151099976373162; Validation Loss = 0.12394417669631291\n",
            "Cost after 297115 iterations : Training Loss =  0.10151099976373157; Validation Loss = 0.12394417669432958\n",
            "Cost after 297116 iterations : Training Loss =  0.10151099976373164; Validation Loss = 0.12394417669234599\n",
            "Cost after 297117 iterations : Training Loss =  0.10151099976373167; Validation Loss = 0.12394417669036324\n",
            "Cost after 297118 iterations : Training Loss =  0.10151099976373142; Validation Loss = 0.12394417668837998\n",
            "Cost after 297119 iterations : Training Loss =  0.10151099976373167; Validation Loss = 0.12394417668639712\n",
            "Cost after 297120 iterations : Training Loss =  0.10151099976373137; Validation Loss = 0.12394417668441392\n",
            "Cost after 297121 iterations : Training Loss =  0.10151099976373154; Validation Loss = 0.12394417668243105\n",
            "Cost after 297122 iterations : Training Loss =  0.10151099976373146; Validation Loss = 0.12394417668044837\n",
            "Cost after 297123 iterations : Training Loss =  0.10151099976373146; Validation Loss = 0.12394417667846544\n",
            "Cost after 297124 iterations : Training Loss =  0.10151099976373157; Validation Loss = 0.12394417667648305\n",
            "Cost after 297125 iterations : Training Loss =  0.10151099976373151; Validation Loss = 0.12394417667450056\n",
            "Cost after 297126 iterations : Training Loss =  0.10151099976373158; Validation Loss = 0.12394417667251836\n",
            "Cost after 297127 iterations : Training Loss =  0.10151099976373162; Validation Loss = 0.1239441766705362\n",
            "Cost after 297128 iterations : Training Loss =  0.10151099976373142; Validation Loss = 0.12394417666855377\n",
            "Cost after 297129 iterations : Training Loss =  0.1015109997637315; Validation Loss = 0.12394417666657216\n",
            "Cost after 297130 iterations : Training Loss =  0.10151099976373151; Validation Loss = 0.12394417666458982\n",
            "Cost after 297131 iterations : Training Loss =  0.10151099976373132; Validation Loss = 0.12394417666260797\n",
            "Cost after 297132 iterations : Training Loss =  0.10151099976373144; Validation Loss = 0.12394417666062639\n",
            "Cost after 297133 iterations : Training Loss =  0.10151099976373161; Validation Loss = 0.12394417665864456\n",
            "Cost after 297134 iterations : Training Loss =  0.10151099976373155; Validation Loss = 0.1239441766566631\n",
            "Cost after 297135 iterations : Training Loss =  0.10151099976373144; Validation Loss = 0.12394417665468188\n",
            "Cost after 297136 iterations : Training Loss =  0.10151099976373139; Validation Loss = 0.1239441766527004\n",
            "Cost after 297137 iterations : Training Loss =  0.10151099976373158; Validation Loss = 0.12394417665071888\n",
            "Cost after 297138 iterations : Training Loss =  0.1015109997637315; Validation Loss = 0.12394417664873791\n",
            "Cost after 297139 iterations : Training Loss =  0.1015109997637313; Validation Loss = 0.12394417664675697\n",
            "Cost after 297140 iterations : Training Loss =  0.10151099976373144; Validation Loss = 0.12394417664477576\n",
            "Cost after 297141 iterations : Training Loss =  0.10151099976373151; Validation Loss = 0.12394417664279525\n",
            "Cost after 297142 iterations : Training Loss =  0.10151099976373158; Validation Loss = 0.1239441766408143\n",
            "Cost after 297143 iterations : Training Loss =  0.10151099976373143; Validation Loss = 0.12394417663883381\n",
            "Cost after 297144 iterations : Training Loss =  0.10151099976373151; Validation Loss = 0.12394417663685332\n",
            "Cost after 297145 iterations : Training Loss =  0.1015109997637313; Validation Loss = 0.12394417663487285\n",
            "Cost after 297146 iterations : Training Loss =  0.10151099976373135; Validation Loss = 0.12394417663289259\n",
            "Cost after 297147 iterations : Training Loss =  0.10151099976373142; Validation Loss = 0.12394417663091253\n",
            "Cost after 297148 iterations : Training Loss =  0.10151099976373151; Validation Loss = 0.1239441766289321\n",
            "Cost after 297149 iterations : Training Loss =  0.10151099976373132; Validation Loss = 0.12394417662695202\n",
            "Cost after 297150 iterations : Training Loss =  0.1015109997637312; Validation Loss = 0.12394417662497213\n",
            "Cost after 297151 iterations : Training Loss =  0.10151099976373157; Validation Loss = 0.12394417662299223\n",
            "Cost after 297152 iterations : Training Loss =  0.10151099976373139; Validation Loss = 0.12394417662101243\n",
            "Cost after 297153 iterations : Training Loss =  0.10151099976373132; Validation Loss = 0.12394417661903281\n",
            "Cost after 297154 iterations : Training Loss =  0.10151099976373126; Validation Loss = 0.1239441766170534\n",
            "Cost after 297155 iterations : Training Loss =  0.10151099976373126; Validation Loss = 0.12394417661507365\n",
            "Cost after 297156 iterations : Training Loss =  0.10151099976373125; Validation Loss = 0.12394417661309447\n",
            "Cost after 297157 iterations : Training Loss =  0.1015109997637312; Validation Loss = 0.12394417661111533\n",
            "Cost after 297158 iterations : Training Loss =  0.1015109997637311; Validation Loss = 0.12394417660913597\n",
            "Cost after 297159 iterations : Training Loss =  0.1015109997637312; Validation Loss = 0.12394417660715722\n",
            "Cost after 297160 iterations : Training Loss =  0.1015109997637313; Validation Loss = 0.12394417660517824\n",
            "Cost after 297161 iterations : Training Loss =  0.10151099976373126; Validation Loss = 0.1239441766031994\n",
            "Cost after 297162 iterations : Training Loss =  0.10151099976373135; Validation Loss = 0.12394417660122034\n",
            "Cost after 297163 iterations : Training Loss =  0.10151099976373122; Validation Loss = 0.1239441765992418\n",
            "Cost after 297164 iterations : Training Loss =  0.10151099976373126; Validation Loss = 0.12394417659726348\n",
            "Cost after 297165 iterations : Training Loss =  0.1015109997637313; Validation Loss = 0.12394417659528506\n",
            "Cost after 297166 iterations : Training Loss =  0.10151099976373135; Validation Loss = 0.12394417659330657\n",
            "Cost after 297167 iterations : Training Loss =  0.10151099976373122; Validation Loss = 0.12394417659132831\n",
            "Cost after 297168 iterations : Training Loss =  0.10151099976373135; Validation Loss = 0.1239441765893499\n",
            "Cost after 297169 iterations : Training Loss =  0.10151099976373149; Validation Loss = 0.12394417658737242\n",
            "Cost after 297170 iterations : Training Loss =  0.10151099976373126; Validation Loss = 0.1239441765853943\n",
            "Cost after 297171 iterations : Training Loss =  0.10151099976373111; Validation Loss = 0.12394417658341639\n",
            "Cost after 297172 iterations : Training Loss =  0.10151099976373112; Validation Loss = 0.12394417658143882\n",
            "Cost after 297173 iterations : Training Loss =  0.10151099976373129; Validation Loss = 0.12394417657946104\n",
            "Cost after 297174 iterations : Training Loss =  0.1015109997637313; Validation Loss = 0.12394417657748338\n",
            "Cost after 297175 iterations : Training Loss =  0.10151099976373122; Validation Loss = 0.12394417657550619\n",
            "Cost after 297176 iterations : Training Loss =  0.10151099976373126; Validation Loss = 0.12394417657352863\n",
            "Cost after 297177 iterations : Training Loss =  0.10151099976373125; Validation Loss = 0.12394417657155131\n",
            "Cost after 297178 iterations : Training Loss =  0.10151099976373136; Validation Loss = 0.12394417656957428\n",
            "Cost after 297179 iterations : Training Loss =  0.10151099976373124; Validation Loss = 0.12394417656759732\n",
            "Cost after 297180 iterations : Training Loss =  0.10151099976373118; Validation Loss = 0.12394417656562048\n",
            "Cost after 297181 iterations : Training Loss =  0.10151099976373139; Validation Loss = 0.12394417656364323\n",
            "Cost after 297182 iterations : Training Loss =  0.10151099976373117; Validation Loss = 0.12394417656166674\n",
            "Cost after 297183 iterations : Training Loss =  0.10151099976373112; Validation Loss = 0.12394417655969016\n",
            "Cost after 297184 iterations : Training Loss =  0.10151099976373118; Validation Loss = 0.12394417655771356\n",
            "Cost after 297185 iterations : Training Loss =  0.10151099976373118; Validation Loss = 0.12394417655573704\n",
            "Cost after 297186 iterations : Training Loss =  0.10151099976373139; Validation Loss = 0.123944176553761\n",
            "Cost after 297187 iterations : Training Loss =  0.10151099976373114; Validation Loss = 0.12394417655178458\n",
            "Cost after 297188 iterations : Training Loss =  0.1015109997637312; Validation Loss = 0.12394417654980848\n",
            "Cost after 297189 iterations : Training Loss =  0.10151099976373124; Validation Loss = 0.12394417654783245\n",
            "Cost after 297190 iterations : Training Loss =  0.1015109997637312; Validation Loss = 0.12394417654585645\n",
            "Cost after 297191 iterations : Training Loss =  0.10151099976373117; Validation Loss = 0.12394417654388061\n",
            "Cost after 297192 iterations : Training Loss =  0.10151099976373125; Validation Loss = 0.1239441765419049\n",
            "Cost after 297193 iterations : Training Loss =  0.10151099976373125; Validation Loss = 0.1239441765399291\n",
            "Cost after 297194 iterations : Training Loss =  0.10151099976373122; Validation Loss = 0.12394417653795374\n",
            "Cost after 297195 iterations : Training Loss =  0.10151099976373112; Validation Loss = 0.12394417653597842\n",
            "Cost after 297196 iterations : Training Loss =  0.10151099976373114; Validation Loss = 0.12394417653400291\n",
            "Cost after 297197 iterations : Training Loss =  0.10151099976373122; Validation Loss = 0.12394417653202754\n",
            "Cost after 297198 iterations : Training Loss =  0.10151099976373129; Validation Loss = 0.12394417653005267\n",
            "Cost after 297199 iterations : Training Loss =  0.10151099976373122; Validation Loss = 0.12394417652807743\n",
            "Cost after 297200 iterations : Training Loss =  0.10151099976373107; Validation Loss = 0.12394417652610244\n",
            "Cost after 297201 iterations : Training Loss =  0.10151099976373101; Validation Loss = 0.12394417652412777\n",
            "Cost after 297202 iterations : Training Loss =  0.10151099976373106; Validation Loss = 0.12394417652215291\n",
            "Cost after 297203 iterations : Training Loss =  0.10151099976373118; Validation Loss = 0.1239441765201783\n",
            "Cost after 297204 iterations : Training Loss =  0.10151099976373101; Validation Loss = 0.12394417651820383\n",
            "Cost after 297205 iterations : Training Loss =  0.10151099976373118; Validation Loss = 0.12394417651622945\n",
            "Cost after 297206 iterations : Training Loss =  0.101510999763731; Validation Loss = 0.12394417651425482\n",
            "Cost after 297207 iterations : Training Loss =  0.10151099976373118; Validation Loss = 0.12394417651228072\n",
            "Cost after 297208 iterations : Training Loss =  0.1015109997637311; Validation Loss = 0.12394417651030655\n",
            "Cost after 297209 iterations : Training Loss =  0.10151099976373099; Validation Loss = 0.12394417650833253\n",
            "Cost after 297210 iterations : Training Loss =  0.101510999763731; Validation Loss = 0.12394417650635904\n",
            "Cost after 297211 iterations : Training Loss =  0.10151099976373104; Validation Loss = 0.12394417650438506\n",
            "Cost after 297212 iterations : Training Loss =  0.10151099976373104; Validation Loss = 0.12394417650241145\n",
            "Cost after 297213 iterations : Training Loss =  0.10151099976373107; Validation Loss = 0.1239441765004375\n",
            "Cost after 297214 iterations : Training Loss =  0.10151099976373114; Validation Loss = 0.12394417649846418\n",
            "Cost after 297215 iterations : Training Loss =  0.10151099976373092; Validation Loss = 0.12394417649649078\n",
            "Cost after 297216 iterations : Training Loss =  0.10151099976373114; Validation Loss = 0.12394417649451756\n",
            "Cost after 297217 iterations : Training Loss =  0.1015109997637311; Validation Loss = 0.12394417649254436\n",
            "Cost after 297218 iterations : Training Loss =  0.10151099976373114; Validation Loss = 0.12394417649057102\n",
            "Cost after 297219 iterations : Training Loss =  0.10151099976373107; Validation Loss = 0.12394417648859817\n",
            "Cost after 297220 iterations : Training Loss =  0.10151099976373101; Validation Loss = 0.12394417648662531\n",
            "Cost after 297221 iterations : Training Loss =  0.10151099976373101; Validation Loss = 0.12394417648465254\n",
            "Cost after 297222 iterations : Training Loss =  0.10151099976373107; Validation Loss = 0.12394417648267972\n",
            "Cost after 297223 iterations : Training Loss =  0.1015109997637312; Validation Loss = 0.12394417648070688\n",
            "Cost after 297224 iterations : Training Loss =  0.10151099976373112; Validation Loss = 0.12394417647873443\n",
            "Cost after 297225 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417647676231\n",
            "Cost after 297226 iterations : Training Loss =  0.10151099976373094; Validation Loss = 0.12394417647479004\n",
            "Cost after 297227 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417647281769\n",
            "Cost after 297228 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417647084563\n",
            "Cost after 297229 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417646887357\n",
            "Cost after 297230 iterations : Training Loss =  0.10151099976373094; Validation Loss = 0.1239441764669013\n",
            "Cost after 297231 iterations : Training Loss =  0.10151099976373087; Validation Loss = 0.12394417646492975\n",
            "Cost after 297232 iterations : Training Loss =  0.10151099976373099; Validation Loss = 0.12394417646295834\n",
            "Cost after 297233 iterations : Training Loss =  0.10151099976373093; Validation Loss = 0.12394417646098671\n",
            "Cost after 297234 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417645901509\n",
            "Cost after 297235 iterations : Training Loss =  0.10151099976373111; Validation Loss = 0.12394417645704353\n",
            "Cost after 297236 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417645507255\n",
            "Cost after 297237 iterations : Training Loss =  0.10151099976373093; Validation Loss = 0.12394417645310139\n",
            "Cost after 297238 iterations : Training Loss =  0.10151099976373093; Validation Loss = 0.12394417645113011\n",
            "Cost after 297239 iterations : Training Loss =  0.1015109997637309; Validation Loss = 0.12394417644915898\n",
            "Cost after 297240 iterations : Training Loss =  0.10151099976373082; Validation Loss = 0.12394417644718778\n",
            "Cost after 297241 iterations : Training Loss =  0.10151099976373082; Validation Loss = 0.12394417644521706\n",
            "Cost after 297242 iterations : Training Loss =  0.10151099976373093; Validation Loss = 0.12394417644324658\n",
            "Cost after 297243 iterations : Training Loss =  0.10151099976373087; Validation Loss = 0.12394417644127591\n",
            "Cost after 297244 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417643930532\n",
            "Cost after 297245 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417643733516\n",
            "Cost after 297246 iterations : Training Loss =  0.10151099976373087; Validation Loss = 0.12394417643536469\n",
            "Cost after 297247 iterations : Training Loss =  0.1015109997637308; Validation Loss = 0.12394417643339452\n",
            "Cost after 297248 iterations : Training Loss =  0.10151099976373094; Validation Loss = 0.12394417643142445\n",
            "Cost after 297249 iterations : Training Loss =  0.1015109997637309; Validation Loss = 0.12394417642945442\n",
            "Cost after 297250 iterations : Training Loss =  0.10151099976373092; Validation Loss = 0.12394417642748455\n",
            "Cost after 297251 iterations : Training Loss =  0.10151099976373094; Validation Loss = 0.12394417642551474\n",
            "Cost after 297252 iterations : Training Loss =  0.10151099976373079; Validation Loss = 0.12394417642354548\n",
            "Cost after 297253 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.1239441764215755\n",
            "Cost after 297254 iterations : Training Loss =  0.1015109997637309; Validation Loss = 0.12394417641960603\n",
            "Cost after 297255 iterations : Training Loss =  0.10151099976373085; Validation Loss = 0.12394417641763647\n",
            "Cost after 297256 iterations : Training Loss =  0.10151099976373078; Validation Loss = 0.12394417641566738\n",
            "Cost after 297257 iterations : Training Loss =  0.10151099976373085; Validation Loss = 0.12394417641369809\n",
            "Cost after 297258 iterations : Training Loss =  0.10151099976373085; Validation Loss = 0.1239441764117289\n",
            "Cost after 297259 iterations : Training Loss =  0.10151099976373075; Validation Loss = 0.12394417640976016\n",
            "Cost after 297260 iterations : Training Loss =  0.10151099976373062; Validation Loss = 0.12394417640779098\n",
            "Cost after 297261 iterations : Training Loss =  0.10151099976373085; Validation Loss = 0.12394417640582232\n",
            "Cost after 297262 iterations : Training Loss =  0.10151099976373079; Validation Loss = 0.12394417640385359\n",
            "Cost after 297263 iterations : Training Loss =  0.10151099976373085; Validation Loss = 0.12394417640188504\n",
            "Cost after 297264 iterations : Training Loss =  0.10151099976373072; Validation Loss = 0.12394417639991664\n",
            "Cost after 297265 iterations : Training Loss =  0.10151099976373097; Validation Loss = 0.12394417639794802\n",
            "Cost after 297266 iterations : Training Loss =  0.10151099976373068; Validation Loss = 0.12394417639597982\n",
            "Cost after 297267 iterations : Training Loss =  0.10151099976373067; Validation Loss = 0.12394417639401159\n",
            "Cost after 297268 iterations : Training Loss =  0.10151099976373087; Validation Loss = 0.12394417639204328\n",
            "Cost after 297269 iterations : Training Loss =  0.1015109997637308; Validation Loss = 0.12394417639007581\n",
            "Cost after 297270 iterations : Training Loss =  0.10151099976373068; Validation Loss = 0.12394417638810756\n",
            "Cost after 297271 iterations : Training Loss =  0.1015109997637307; Validation Loss = 0.12394417638613967\n",
            "Cost after 297272 iterations : Training Loss =  0.10151099976373078; Validation Loss = 0.12394417638417199\n",
            "Cost after 297273 iterations : Training Loss =  0.1015109997637309; Validation Loss = 0.12394417638220459\n",
            "Cost after 297274 iterations : Training Loss =  0.10151099976373074; Validation Loss = 0.12394417638023698\n",
            "Cost after 297275 iterations : Training Loss =  0.10151099976373082; Validation Loss = 0.12394417637826954\n",
            "Cost after 297276 iterations : Training Loss =  0.10151099976373078; Validation Loss = 0.12394417637630238\n",
            "Cost after 297277 iterations : Training Loss =  0.10151099976373067; Validation Loss = 0.12394417637433514\n",
            "Cost after 297278 iterations : Training Loss =  0.1015109997637307; Validation Loss = 0.12394417637236804\n",
            "Cost after 297279 iterations : Training Loss =  0.1015109997637308; Validation Loss = 0.12394417637040114\n",
            "Cost after 297280 iterations : Training Loss =  0.10151099976373068; Validation Loss = 0.12394417636843401\n",
            "Cost after 297281 iterations : Training Loss =  0.10151099976373072; Validation Loss = 0.12394417636646729\n",
            "Cost after 297282 iterations : Training Loss =  0.10151099976373087; Validation Loss = 0.12394417636450072\n",
            "Cost after 297283 iterations : Training Loss =  0.10151099976373062; Validation Loss = 0.12394417636253387\n",
            "Cost after 297284 iterations : Training Loss =  0.1015109997637308; Validation Loss = 0.12394417636056751\n",
            "Cost after 297285 iterations : Training Loss =  0.1015109997637308; Validation Loss = 0.12394417635860108\n",
            "Cost after 297286 iterations : Training Loss =  0.10151099976373078; Validation Loss = 0.12394417635663477\n",
            "Cost after 297287 iterations : Training Loss =  0.10151099976373072; Validation Loss = 0.1239441763546687\n",
            "Cost after 297288 iterations : Training Loss =  0.1015109997637305; Validation Loss = 0.12394417635270254\n",
            "Cost after 297289 iterations : Training Loss =  0.1015109997637306; Validation Loss = 0.1239441763507363\n",
            "Cost after 297290 iterations : Training Loss =  0.10151099976373072; Validation Loss = 0.12394417634877054\n",
            "Cost after 297291 iterations : Training Loss =  0.10151099976373078; Validation Loss = 0.1239441763468046\n",
            "Cost after 297292 iterations : Training Loss =  0.10151099976373068; Validation Loss = 0.12394417634483905\n",
            "Cost after 297293 iterations : Training Loss =  0.1015109997637306; Validation Loss = 0.12394417634287341\n",
            "Cost after 297294 iterations : Training Loss =  0.10151099976373065; Validation Loss = 0.12394417634090803\n",
            "Cost after 297295 iterations : Training Loss =  0.10151099976373068; Validation Loss = 0.1239441763389424\n",
            "Cost after 297296 iterations : Training Loss =  0.10151099976373065; Validation Loss = 0.12394417633697721\n",
            "Cost after 297297 iterations : Training Loss =  0.10151099976373057; Validation Loss = 0.12394417633501215\n",
            "Cost after 297298 iterations : Training Loss =  0.10151099976373074; Validation Loss = 0.12394417633304693\n",
            "Cost after 297299 iterations : Training Loss =  0.1015109997637304; Validation Loss = 0.12394417633108193\n",
            "Cost after 297300 iterations : Training Loss =  0.10151099976373061; Validation Loss = 0.12394417632911696\n",
            "Cost after 297301 iterations : Training Loss =  0.10151099976373061; Validation Loss = 0.12394417632715224\n",
            "Cost after 297302 iterations : Training Loss =  0.10151099976373049; Validation Loss = 0.12394417632518728\n",
            "Cost after 297303 iterations : Training Loss =  0.10151099976373067; Validation Loss = 0.12394417632322284\n",
            "Cost after 297304 iterations : Training Loss =  0.10151099976373061; Validation Loss = 0.12394417632125809\n",
            "Cost after 297305 iterations : Training Loss =  0.10151099976373049; Validation Loss = 0.12394417631929389\n",
            "Cost after 297306 iterations : Training Loss =  0.10151099976373062; Validation Loss = 0.12394417631732939\n",
            "Cost after 297307 iterations : Training Loss =  0.10151099976373067; Validation Loss = 0.12394417631536538\n",
            "Cost after 297308 iterations : Training Loss =  0.1015109997637306; Validation Loss = 0.12394417631340124\n",
            "Cost after 297309 iterations : Training Loss =  0.10151099976373065; Validation Loss = 0.12394417631143746\n",
            "Cost after 297310 iterations : Training Loss =  0.10151099976373061; Validation Loss = 0.12394417630947346\n",
            "Cost after 297311 iterations : Training Loss =  0.10151099976373065; Validation Loss = 0.12394417630750966\n",
            "Cost after 297312 iterations : Training Loss =  0.10151099976373067; Validation Loss = 0.12394417630554574\n",
            "Cost after 297313 iterations : Training Loss =  0.10151099976373056; Validation Loss = 0.12394417630358204\n",
            "Cost after 297314 iterations : Training Loss =  0.10151099976373053; Validation Loss = 0.12394417630161882\n",
            "Cost after 297315 iterations : Training Loss =  0.10151099976373043; Validation Loss = 0.12394417629965514\n",
            "Cost after 297316 iterations : Training Loss =  0.10151099976373068; Validation Loss = 0.12394417629769149\n",
            "Cost after 297317 iterations : Training Loss =  0.10151099976373047; Validation Loss = 0.1239441762957285\n",
            "Cost after 297318 iterations : Training Loss =  0.10151099976373047; Validation Loss = 0.12394417629376524\n",
            "Cost after 297319 iterations : Training Loss =  0.10151099976373054; Validation Loss = 0.12394417629180243\n",
            "Cost after 297320 iterations : Training Loss =  0.10151099976373042; Validation Loss = 0.12394417628983959\n",
            "Cost after 297321 iterations : Training Loss =  0.10151099976373057; Validation Loss = 0.12394417628787655\n",
            "Cost after 297322 iterations : Training Loss =  0.1015109997637304; Validation Loss = 0.1239441762859138\n",
            "Cost after 297323 iterations : Training Loss =  0.10151099976373049; Validation Loss = 0.12394417628395117\n",
            "Cost after 297324 iterations : Training Loss =  0.10151099976373042; Validation Loss = 0.12394417628198845\n",
            "Cost after 297325 iterations : Training Loss =  0.10151099976373046; Validation Loss = 0.12394417628002621\n",
            "Cost after 297326 iterations : Training Loss =  0.1015109997637306; Validation Loss = 0.12394417627806392\n",
            "Cost after 297327 iterations : Training Loss =  0.1015109997637305; Validation Loss = 0.12394417627610164\n",
            "Cost after 297328 iterations : Training Loss =  0.1015109997637304; Validation Loss = 0.12394417627413955\n",
            "Cost after 297329 iterations : Training Loss =  0.10151099976373047; Validation Loss = 0.12394417627217708\n",
            "Cost after 297330 iterations : Training Loss =  0.10151099976373043; Validation Loss = 0.12394417627021553\n",
            "Cost after 297331 iterations : Training Loss =  0.10151099976373046; Validation Loss = 0.12394417626825366\n",
            "Cost after 297332 iterations : Training Loss =  0.1015109997637306; Validation Loss = 0.12394417626629205\n",
            "Cost after 297333 iterations : Training Loss =  0.1015109997637305; Validation Loss = 0.1239441762643303\n",
            "Cost after 297334 iterations : Training Loss =  0.10151099976373054; Validation Loss = 0.12394417626236873\n",
            "Cost after 297335 iterations : Training Loss =  0.10151099976373046; Validation Loss = 0.12394417626040738\n",
            "Cost after 297336 iterations : Training Loss =  0.10151099976373028; Validation Loss = 0.12394417625844588\n",
            "Cost after 297337 iterations : Training Loss =  0.1015109997637305; Validation Loss = 0.12394417625648477\n",
            "Cost after 297338 iterations : Training Loss =  0.10151099976373049; Validation Loss = 0.12394417625452311\n",
            "Cost after 297339 iterations : Training Loss =  0.10151099976373053; Validation Loss = 0.12394417625256261\n",
            "Cost after 297340 iterations : Training Loss =  0.10151099976373053; Validation Loss = 0.1239441762506015\n",
            "Cost after 297341 iterations : Training Loss =  0.10151099976373038; Validation Loss = 0.12394417624864054\n",
            "Cost after 297342 iterations : Training Loss =  0.10151099976373024; Validation Loss = 0.12394417624667997\n",
            "Cost after 297343 iterations : Training Loss =  0.10151099976373046; Validation Loss = 0.12394417624471897\n",
            "Cost after 297344 iterations : Training Loss =  0.10151099976373043; Validation Loss = 0.12394417624275879\n",
            "Cost after 297345 iterations : Training Loss =  0.10151099976373021; Validation Loss = 0.12394417624079845\n",
            "Cost after 297346 iterations : Training Loss =  0.1015109997637303; Validation Loss = 0.12394417623883792\n",
            "Cost after 297347 iterations : Training Loss =  0.10151099976373036; Validation Loss = 0.12394417623687762\n",
            "Cost after 297348 iterations : Training Loss =  0.1015109997637303; Validation Loss = 0.12394417623491726\n",
            "Cost after 297349 iterations : Training Loss =  0.1015109997637305; Validation Loss = 0.12394417623295731\n",
            "Cost after 297350 iterations : Training Loss =  0.10151099976373053; Validation Loss = 0.12394417623099754\n",
            "Cost after 297351 iterations : Training Loss =  0.10151099976373043; Validation Loss = 0.1239441762290378\n",
            "Cost after 297352 iterations : Training Loss =  0.10151099976373024; Validation Loss = 0.1239441762270779\n",
            "Cost after 297353 iterations : Training Loss =  0.1015109997637305; Validation Loss = 0.12394417622511812\n",
            "Cost after 297354 iterations : Training Loss =  0.10151099976373042; Validation Loss = 0.12394417622315881\n",
            "Cost after 297355 iterations : Training Loss =  0.10151099976373038; Validation Loss = 0.12394417622119966\n",
            "Cost after 297356 iterations : Training Loss =  0.10151099976373033; Validation Loss = 0.12394417621923967\n",
            "Cost after 297357 iterations : Training Loss =  0.10151099976373036; Validation Loss = 0.12394417621728065\n",
            "Cost after 297358 iterations : Training Loss =  0.10151099976373021; Validation Loss = 0.12394417621532157\n",
            "Cost after 297359 iterations : Training Loss =  0.10151099976373049; Validation Loss = 0.12394417621336234\n",
            "Cost after 297360 iterations : Training Loss =  0.10151099976373025; Validation Loss = 0.12394417621140331\n",
            "Cost after 297361 iterations : Training Loss =  0.1015109997637303; Validation Loss = 0.12394417620944491\n",
            "Cost after 297362 iterations : Training Loss =  0.10151099976373046; Validation Loss = 0.12394417620748595\n",
            "Cost after 297363 iterations : Training Loss =  0.1015109997637304; Validation Loss = 0.12394417620552727\n",
            "Cost after 297364 iterations : Training Loss =  0.10151099976373036; Validation Loss = 0.12394417620356876\n",
            "Cost after 297365 iterations : Training Loss =  0.10151099976373025; Validation Loss = 0.12394417620161031\n",
            "Cost after 297366 iterations : Training Loss =  0.10151099976373018; Validation Loss = 0.12394417619965194\n",
            "Cost after 297367 iterations : Training Loss =  0.10151099976373013; Validation Loss = 0.12394417619769353\n",
            "Cost after 297368 iterations : Training Loss =  0.10151099976373003; Validation Loss = 0.12394417619573546\n",
            "Cost after 297369 iterations : Training Loss =  0.10151099976373046; Validation Loss = 0.12394417619377748\n",
            "Cost after 297370 iterations : Training Loss =  0.10151099976373024; Validation Loss = 0.12394417619181958\n",
            "Cost after 297371 iterations : Training Loss =  0.10151099976373017; Validation Loss = 0.1239441761898615\n",
            "Cost after 297372 iterations : Training Loss =  0.10151099976373028; Validation Loss = 0.12394417618790383\n",
            "Cost after 297373 iterations : Training Loss =  0.10151099976373022; Validation Loss = 0.1239441761859462\n",
            "Cost after 297374 iterations : Training Loss =  0.1015109997637303; Validation Loss = 0.12394417618398905\n",
            "Cost after 297375 iterations : Training Loss =  0.10151099976373022; Validation Loss = 0.12394417618203124\n",
            "Cost after 297376 iterations : Training Loss =  0.1015109997637304; Validation Loss = 0.12394417618007378\n",
            "Cost after 297377 iterations : Training Loss =  0.10151099976373022; Validation Loss = 0.12394417617811676\n",
            "Cost after 297378 iterations : Training Loss =  0.10151099976373017; Validation Loss = 0.12394417617615933\n",
            "Cost after 297379 iterations : Training Loss =  0.10151099976373036; Validation Loss = 0.1239441761742026\n",
            "Cost after 297380 iterations : Training Loss =  0.10151099976373028; Validation Loss = 0.12394417617224548\n",
            "Cost after 297381 iterations : Training Loss =  0.10151099976373029; Validation Loss = 0.12394417617028886\n",
            "Cost after 297382 iterations : Training Loss =  0.10151099976372999; Validation Loss = 0.12394417616833185\n",
            "Cost after 297383 iterations : Training Loss =  0.10151099976373013; Validation Loss = 0.12394417616637535\n",
            "Cost after 297384 iterations : Training Loss =  0.10151099976373018; Validation Loss = 0.12394417616441879\n",
            "Cost after 297385 iterations : Training Loss =  0.10151099976373029; Validation Loss = 0.12394417616246209\n",
            "Cost after 297386 iterations : Training Loss =  0.10151099976373004; Validation Loss = 0.12394417616050575\n",
            "Cost after 297387 iterations : Training Loss =  0.10151099976373013; Validation Loss = 0.12394417615854962\n",
            "Cost after 297388 iterations : Training Loss =  0.10151099976373006; Validation Loss = 0.12394417615659353\n",
            "Cost after 297389 iterations : Training Loss =  0.10151099976373025; Validation Loss = 0.12394417615463761\n",
            "Cost after 297390 iterations : Training Loss =  0.10151099976373033; Validation Loss = 0.1239441761526814\n",
            "Cost after 297391 iterations : Training Loss =  0.10151099976373022; Validation Loss = 0.12394417615072541\n",
            "Cost after 297392 iterations : Training Loss =  0.10151099976373029; Validation Loss = 0.12394417614876968\n",
            "Cost after 297393 iterations : Training Loss =  0.10151099976373008; Validation Loss = 0.12394417614681409\n",
            "Cost after 297394 iterations : Training Loss =  0.1015109997637299; Validation Loss = 0.12394417614485853\n",
            "Cost after 297395 iterations : Training Loss =  0.10151099976373008; Validation Loss = 0.12394417614290315\n",
            "Cost after 297396 iterations : Training Loss =  0.10151099976373025; Validation Loss = 0.12394417614094778\n",
            "Cost after 297397 iterations : Training Loss =  0.10151099976373018; Validation Loss = 0.12394417613899275\n",
            "Cost after 297398 iterations : Training Loss =  0.10151099976373013; Validation Loss = 0.1239441761370373\n",
            "Cost after 297399 iterations : Training Loss =  0.10151099976373021; Validation Loss = 0.12394417613508246\n",
            "Cost after 297400 iterations : Training Loss =  0.10151099976373018; Validation Loss = 0.12394417613312703\n",
            "Cost after 297401 iterations : Training Loss =  0.10151099976373017; Validation Loss = 0.12394417613117241\n",
            "Cost after 297402 iterations : Training Loss =  0.10151099976373021; Validation Loss = 0.12394417612921778\n",
            "Cost after 297403 iterations : Training Loss =  0.10151099976373011; Validation Loss = 0.12394417612726322\n",
            "Cost after 297404 iterations : Training Loss =  0.10151099976372999; Validation Loss = 0.12394417612530836\n",
            "Cost after 297405 iterations : Training Loss =  0.10151099976373015; Validation Loss = 0.12394417612335412\n",
            "Cost after 297406 iterations : Training Loss =  0.10151099976373013; Validation Loss = 0.12394417612139953\n",
            "Cost after 297407 iterations : Training Loss =  0.10151099976373003; Validation Loss = 0.12394417611944544\n",
            "Cost after 297408 iterations : Training Loss =  0.10151099976373011; Validation Loss = 0.12394417611749123\n",
            "Cost after 297409 iterations : Training Loss =  0.10151099976373004; Validation Loss = 0.12394417611553708\n",
            "Cost after 297410 iterations : Training Loss =  0.10151099976373017; Validation Loss = 0.12394417611358337\n",
            "Cost after 297411 iterations : Training Loss =  0.10151099976373024; Validation Loss = 0.12394417611162921\n",
            "Cost after 297412 iterations : Training Loss =  0.10151099976373004; Validation Loss = 0.12394417610967551\n",
            "Cost after 297413 iterations : Training Loss =  0.10151099976373025; Validation Loss = 0.12394417610772201\n",
            "Cost after 297414 iterations : Training Loss =  0.10151099976373001; Validation Loss = 0.12394417610576838\n",
            "Cost after 297415 iterations : Training Loss =  0.10151099976373003; Validation Loss = 0.12394417610381486\n",
            "Cost after 297416 iterations : Training Loss =  0.10151099976372993; Validation Loss = 0.12394417610186131\n",
            "Cost after 297417 iterations : Training Loss =  0.10151099976372993; Validation Loss = 0.123944176099908\n",
            "Cost after 297418 iterations : Training Loss =  0.1015109997637301; Validation Loss = 0.12394417609795515\n",
            "Cost after 297419 iterations : Training Loss =  0.10151099976373001; Validation Loss = 0.12394417609600172\n",
            "Cost after 297420 iterations : Training Loss =  0.1015109997637301; Validation Loss = 0.12394417609404892\n",
            "Cost after 297421 iterations : Training Loss =  0.10151099976372997; Validation Loss = 0.12394417609209558\n",
            "Cost after 297422 iterations : Training Loss =  0.1015109997637301; Validation Loss = 0.12394417609014324\n",
            "Cost after 297423 iterations : Training Loss =  0.10151099976372972; Validation Loss = 0.1239441760881903\n",
            "Cost after 297424 iterations : Training Loss =  0.10151099976373025; Validation Loss = 0.12394417608623777\n",
            "Cost after 297425 iterations : Training Loss =  0.10151099976372997; Validation Loss = 0.12394417608428528\n",
            "Cost after 297426 iterations : Training Loss =  0.10151099976373011; Validation Loss = 0.12394417608233262\n",
            "Cost after 297427 iterations : Training Loss =  0.10151099976373011; Validation Loss = 0.1239441760803806\n",
            "Cost after 297428 iterations : Training Loss =  0.10151099976372996; Validation Loss = 0.1239441760784283\n",
            "Cost after 297429 iterations : Training Loss =  0.10151099976372997; Validation Loss = 0.12394417607647597\n",
            "Cost after 297430 iterations : Training Loss =  0.10151099976373006; Validation Loss = 0.1239441760745245\n",
            "Cost after 297431 iterations : Training Loss =  0.10151099976373004; Validation Loss = 0.12394417607257253\n",
            "Cost after 297432 iterations : Training Loss =  0.10151099976372992; Validation Loss = 0.12394417607062078\n",
            "Cost after 297433 iterations : Training Loss =  0.10151099976373004; Validation Loss = 0.12394417606866913\n",
            "Cost after 297434 iterations : Training Loss =  0.10151099976373001; Validation Loss = 0.12394417606671723\n",
            "Cost after 297435 iterations : Training Loss =  0.1015109997637299; Validation Loss = 0.12394417606476595\n",
            "Cost after 297436 iterations : Training Loss =  0.10151099976372993; Validation Loss = 0.12394417606281444\n",
            "Cost after 297437 iterations : Training Loss =  0.10151099976372983; Validation Loss = 0.1239441760608629\n",
            "Cost after 297438 iterations : Training Loss =  0.10151099976372992; Validation Loss = 0.12394417605891186\n",
            "Cost after 297439 iterations : Training Loss =  0.10151099976372992; Validation Loss = 0.12394417605696094\n",
            "Cost after 297440 iterations : Training Loss =  0.10151099976372989; Validation Loss = 0.1239441760550098\n",
            "Cost after 297441 iterations : Training Loss =  0.10151099976372978; Validation Loss = 0.12394417605305864\n",
            "Cost after 297442 iterations : Training Loss =  0.10151099976372997; Validation Loss = 0.12394417605110791\n",
            "Cost after 297443 iterations : Training Loss =  0.10151099976372999; Validation Loss = 0.12394417604915708\n",
            "Cost after 297444 iterations : Training Loss =  0.10151099976372996; Validation Loss = 0.12394417604720688\n",
            "Cost after 297445 iterations : Training Loss =  0.10151099976372996; Validation Loss = 0.12394417604525618\n",
            "Cost after 297446 iterations : Training Loss =  0.10151099976372983; Validation Loss = 0.12394417604330592\n",
            "Cost after 297447 iterations : Training Loss =  0.10151099976372996; Validation Loss = 0.12394417604135562\n",
            "Cost after 297448 iterations : Training Loss =  0.10151099976372986; Validation Loss = 0.12394417603940541\n",
            "Cost after 297449 iterations : Training Loss =  0.10151099976372974; Validation Loss = 0.12394417603745506\n",
            "Cost after 297450 iterations : Training Loss =  0.10151099976372974; Validation Loss = 0.12394417603550509\n",
            "Cost after 297451 iterations : Training Loss =  0.10151099976372993; Validation Loss = 0.12394417603355543\n",
            "Cost after 297452 iterations : Training Loss =  0.10151099976372999; Validation Loss = 0.12394417603160568\n",
            "Cost after 297453 iterations : Training Loss =  0.10151099976372986; Validation Loss = 0.12394417602965567\n",
            "Cost after 297454 iterations : Training Loss =  0.10151099976372985; Validation Loss = 0.12394417602770622\n",
            "Cost after 297455 iterations : Training Loss =  0.10151099976372974; Validation Loss = 0.1239441760257564\n",
            "Cost after 297456 iterations : Training Loss =  0.10151099976372992; Validation Loss = 0.12394417602380728\n",
            "Cost after 297457 iterations : Training Loss =  0.10151099976372989; Validation Loss = 0.12394417602185807\n",
            "Cost after 297458 iterations : Training Loss =  0.10151099976372986; Validation Loss = 0.12394417601990868\n",
            "Cost after 297459 iterations : Training Loss =  0.10151099976372971; Validation Loss = 0.12394417601795946\n",
            "Cost after 297460 iterations : Training Loss =  0.10151099976372978; Validation Loss = 0.12394417601601056\n",
            "Cost after 297461 iterations : Training Loss =  0.10151099976372958; Validation Loss = 0.12394417601406169\n",
            "Cost after 297462 iterations : Training Loss =  0.10151099976372989; Validation Loss = 0.12394417601211294\n",
            "Cost after 297463 iterations : Training Loss =  0.10151099976372972; Validation Loss = 0.12394417601016425\n",
            "Cost after 297464 iterations : Training Loss =  0.10151099976372999; Validation Loss = 0.1239441760082153\n",
            "Cost after 297465 iterations : Training Loss =  0.10151099976372974; Validation Loss = 0.12394417600626723\n",
            "Cost after 297466 iterations : Training Loss =  0.10151099976372979; Validation Loss = 0.12394417600431855\n",
            "Cost after 297467 iterations : Training Loss =  0.10151099976372974; Validation Loss = 0.12394417600237038\n",
            "Cost after 297468 iterations : Training Loss =  0.10151099976372968; Validation Loss = 0.12394417600042194\n",
            "Cost after 297469 iterations : Training Loss =  0.10151099976372985; Validation Loss = 0.12394417599847392\n",
            "Cost after 297470 iterations : Training Loss =  0.10151099976372976; Validation Loss = 0.12394417599652585\n",
            "Cost after 297471 iterations : Training Loss =  0.10151099976372986; Validation Loss = 0.12394417599457809\n",
            "Cost after 297472 iterations : Training Loss =  0.10151099976372978; Validation Loss = 0.12394417599263002\n",
            "Cost after 297473 iterations : Training Loss =  0.10151099976372986; Validation Loss = 0.12394417599068253\n",
            "Cost after 297474 iterations : Training Loss =  0.10151099976372978; Validation Loss = 0.12394417598873472\n",
            "Cost after 297475 iterations : Training Loss =  0.10151099976372972; Validation Loss = 0.12394417598678718\n",
            "Cost after 297476 iterations : Training Loss =  0.10151099976372985; Validation Loss = 0.1239441759848398\n",
            "Cost after 297477 iterations : Training Loss =  0.10151099976372974; Validation Loss = 0.12394417598289238\n",
            "Cost after 297478 iterations : Training Loss =  0.10151099976372961; Validation Loss = 0.12394417598094505\n",
            "Cost after 297479 iterations : Training Loss =  0.10151099976372983; Validation Loss = 0.12394417597899807\n",
            "Cost after 297480 iterations : Training Loss =  0.1015109997637298; Validation Loss = 0.12394417597705072\n",
            "Cost after 297481 iterations : Training Loss =  0.10151099976372971; Validation Loss = 0.12394417597510395\n",
            "Cost after 297482 iterations : Training Loss =  0.10151099976372976; Validation Loss = 0.12394417597315709\n",
            "Cost after 297483 iterations : Training Loss =  0.10151099976372964; Validation Loss = 0.12394417597121025\n",
            "Cost after 297484 iterations : Training Loss =  0.10151099976372965; Validation Loss = 0.12394417596926337\n",
            "Cost after 297485 iterations : Training Loss =  0.10151099976372974; Validation Loss = 0.12394417596731698\n",
            "Cost after 297486 iterations : Training Loss =  0.10151099976372983; Validation Loss = 0.12394417596537072\n",
            "Cost after 297487 iterations : Training Loss =  0.10151099976372968; Validation Loss = 0.12394417596342411\n",
            "Cost after 297488 iterations : Training Loss =  0.10151099976372968; Validation Loss = 0.12394417596147812\n",
            "Cost after 297489 iterations : Training Loss =  0.10151099976372976; Validation Loss = 0.12394417595953161\n",
            "Cost after 297490 iterations : Training Loss =  0.10151099976372971; Validation Loss = 0.12394417595758589\n",
            "Cost after 297491 iterations : Training Loss =  0.10151099976372978; Validation Loss = 0.12394417595563981\n",
            "Cost after 297492 iterations : Training Loss =  0.10151099976372967; Validation Loss = 0.12394417595369385\n",
            "Cost after 297493 iterations : Training Loss =  0.1015109997637296; Validation Loss = 0.12394417595174846\n",
            "Cost after 297494 iterations : Training Loss =  0.10151099976372961; Validation Loss = 0.12394417594980257\n",
            "Cost after 297495 iterations : Training Loss =  0.10151099976372985; Validation Loss = 0.12394417594785712\n",
            "Cost after 297496 iterations : Training Loss =  0.10151099976372965; Validation Loss = 0.12394417594591164\n",
            "Cost after 297497 iterations : Training Loss =  0.10151099976372967; Validation Loss = 0.12394417594396641\n",
            "Cost after 297498 iterations : Training Loss =  0.10151099976372972; Validation Loss = 0.12394417594202113\n",
            "Cost after 297499 iterations : Training Loss =  0.1015109997637298; Validation Loss = 0.12394417594007594\n",
            "Cost after 297500 iterations : Training Loss =  0.10151099976372976; Validation Loss = 0.12394417593813067\n",
            "Cost after 297501 iterations : Training Loss =  0.10151099976372967; Validation Loss = 0.12394417593618562\n",
            "Cost after 297502 iterations : Training Loss =  0.1015109997637296; Validation Loss = 0.12394417593424088\n",
            "Cost after 297503 iterations : Training Loss =  0.10151099976372957; Validation Loss = 0.12394417593229631\n",
            "Cost after 297504 iterations : Training Loss =  0.10151099976372957; Validation Loss = 0.12394417593035165\n",
            "Cost after 297505 iterations : Training Loss =  0.10151099976372954; Validation Loss = 0.12394417592840686\n",
            "Cost after 297506 iterations : Training Loss =  0.10151099976372964; Validation Loss = 0.12394417592646234\n",
            "Cost after 297507 iterations : Training Loss =  0.10151099976372972; Validation Loss = 0.12394417592451816\n",
            "Cost after 297508 iterations : Training Loss =  0.10151099976372957; Validation Loss = 0.12394417592257394\n",
            "Cost after 297509 iterations : Training Loss =  0.10151099976372951; Validation Loss = 0.12394417592062983\n",
            "Cost after 297510 iterations : Training Loss =  0.1015109997637296; Validation Loss = 0.1239441759186856\n",
            "Cost after 297511 iterations : Training Loss =  0.10151099976372949; Validation Loss = 0.12394417591674169\n",
            "Cost after 297512 iterations : Training Loss =  0.10151099976372968; Validation Loss = 0.12394417591479762\n",
            "Cost after 297513 iterations : Training Loss =  0.10151099976372954; Validation Loss = 0.12394417591285393\n",
            "Cost after 297514 iterations : Training Loss =  0.10151099976372968; Validation Loss = 0.1239441759109104\n",
            "Cost after 297515 iterations : Training Loss =  0.10151099976372953; Validation Loss = 0.12394417590896648\n",
            "Cost after 297516 iterations : Training Loss =  0.10151099976372945; Validation Loss = 0.12394417590702322\n",
            "Cost after 297517 iterations : Training Loss =  0.10151099976372961; Validation Loss = 0.12394417590507978\n",
            "Cost after 297518 iterations : Training Loss =  0.10151099976372953; Validation Loss = 0.1239441759031367\n",
            "Cost after 297519 iterations : Training Loss =  0.10151099976372953; Validation Loss = 0.12394417590119322\n",
            "Cost after 297520 iterations : Training Loss =  0.10151099976372958; Validation Loss = 0.12394417589925015\n",
            "Cost after 297521 iterations : Training Loss =  0.10151099976372953; Validation Loss = 0.12394417589730729\n",
            "Cost after 297522 iterations : Training Loss =  0.10151099976372949; Validation Loss = 0.12394417589536413\n",
            "Cost after 297523 iterations : Training Loss =  0.10151099976372954; Validation Loss = 0.12394417589342155\n",
            "Cost after 297524 iterations : Training Loss =  0.10151099976372953; Validation Loss = 0.12394417589147867\n",
            "Cost after 297525 iterations : Training Loss =  0.10151099976372933; Validation Loss = 0.123944175889536\n",
            "Cost after 297526 iterations : Training Loss =  0.10151099976372946; Validation Loss = 0.12394417588759363\n",
            "Cost after 297527 iterations : Training Loss =  0.10151099976372967; Validation Loss = 0.12394417588565115\n",
            "Cost after 297528 iterations : Training Loss =  0.1015109997637294; Validation Loss = 0.12394417588370907\n",
            "Cost after 297529 iterations : Training Loss =  0.10151099976372964; Validation Loss = 0.12394417588176686\n",
            "Cost after 297530 iterations : Training Loss =  0.10151099976372942; Validation Loss = 0.12394417587982481\n",
            "Cost after 297531 iterations : Training Loss =  0.10151099976372949; Validation Loss = 0.12394417587788262\n",
            "Cost after 297532 iterations : Training Loss =  0.10151099976372949; Validation Loss = 0.12394417587594067\n",
            "Cost after 297533 iterations : Training Loss =  0.10151099976372939; Validation Loss = 0.1239441758739988\n",
            "Cost after 297534 iterations : Training Loss =  0.10151099976372953; Validation Loss = 0.12394417587205699\n",
            "Cost after 297535 iterations : Training Loss =  0.10151099976372947; Validation Loss = 0.12394417587011533\n",
            "Cost after 297536 iterations : Training Loss =  0.10151099976372961; Validation Loss = 0.12394417586817388\n",
            "Cost after 297537 iterations : Training Loss =  0.10151099976372949; Validation Loss = 0.1239441758662325\n",
            "Cost after 297538 iterations : Training Loss =  0.1015109997637294; Validation Loss = 0.12394417586429153\n",
            "Cost after 297539 iterations : Training Loss =  0.10151099976372949; Validation Loss = 0.12394417586234979\n",
            "Cost after 297540 iterations : Training Loss =  0.1015109997637294; Validation Loss = 0.12394417586040886\n",
            "Cost after 297541 iterations : Training Loss =  0.10151099976372945; Validation Loss = 0.12394417585846793\n",
            "Cost after 297542 iterations : Training Loss =  0.10151099976372946; Validation Loss = 0.12394417585652708\n",
            "Cost after 297543 iterations : Training Loss =  0.10151099976372932; Validation Loss = 0.12394417585458642\n",
            "Cost after 297544 iterations : Training Loss =  0.10151099976372942; Validation Loss = 0.12394417585264542\n",
            "Cost after 297545 iterations : Training Loss =  0.10151099976372951; Validation Loss = 0.12394417585070494\n",
            "Cost after 297546 iterations : Training Loss =  0.10151099976372935; Validation Loss = 0.1239441758487642\n",
            "Cost after 297547 iterations : Training Loss =  0.10151099976372935; Validation Loss = 0.12394417584682395\n",
            "Cost after 297548 iterations : Training Loss =  0.10151099976372949; Validation Loss = 0.12394417584488347\n",
            "Cost after 297549 iterations : Training Loss =  0.10151099976372928; Validation Loss = 0.12394417584294345\n",
            "Cost after 297550 iterations : Training Loss =  0.10151099976372932; Validation Loss = 0.1239441758410036\n",
            "Cost after 297551 iterations : Training Loss =  0.10151099976372945; Validation Loss = 0.12394417583906307\n",
            "Cost after 297552 iterations : Training Loss =  0.10151099976372935; Validation Loss = 0.12394417583712337\n",
            "Cost after 297553 iterations : Training Loss =  0.10151099976372928; Validation Loss = 0.12394417583518344\n",
            "Cost after 297554 iterations : Training Loss =  0.10151099976372946; Validation Loss = 0.12394417583324353\n",
            "Cost after 297555 iterations : Training Loss =  0.10151099976372935; Validation Loss = 0.12394417583130407\n",
            "Cost after 297556 iterations : Training Loss =  0.10151099976372932; Validation Loss = 0.12394417582936458\n",
            "Cost after 297557 iterations : Training Loss =  0.10151099976372946; Validation Loss = 0.12394417582742522\n",
            "Cost after 297558 iterations : Training Loss =  0.10151099976372933; Validation Loss = 0.12394417582548599\n",
            "Cost after 297559 iterations : Training Loss =  0.10151099976372926; Validation Loss = 0.12394417582354657\n",
            "Cost after 297560 iterations : Training Loss =  0.10151099976372914; Validation Loss = 0.12394417582160744\n",
            "Cost after 297561 iterations : Training Loss =  0.10151099976372945; Validation Loss = 0.12394417581966817\n",
            "Cost after 297562 iterations : Training Loss =  0.1015109997637294; Validation Loss = 0.12394417581772928\n",
            "Cost after 297563 iterations : Training Loss =  0.10151099976372922; Validation Loss = 0.1239441758157904\n",
            "Cost after 297564 iterations : Training Loss =  0.10151099976372954; Validation Loss = 0.1239441758138515\n",
            "Cost after 297565 iterations : Training Loss =  0.10151099976372924; Validation Loss = 0.12394417581191308\n",
            "Cost after 297566 iterations : Training Loss =  0.10151099976372933; Validation Loss = 0.1239441758099746\n",
            "Cost after 297567 iterations : Training Loss =  0.10151099976372946; Validation Loss = 0.12394417580803581\n",
            "Cost after 297568 iterations : Training Loss =  0.10151099976372932; Validation Loss = 0.12394417580609755\n",
            "Cost after 297569 iterations : Training Loss =  0.10151099976372928; Validation Loss = 0.12394417580415916\n",
            "Cost after 297570 iterations : Training Loss =  0.10151099976372926; Validation Loss = 0.12394417580222104\n",
            "Cost after 297571 iterations : Training Loss =  0.10151099976372928; Validation Loss = 0.12394417580028268\n",
            "Cost after 297572 iterations : Training Loss =  0.10151099976372921; Validation Loss = 0.1239441757983449\n",
            "Cost after 297573 iterations : Training Loss =  0.10151099976372921; Validation Loss = 0.12394417579640742\n",
            "Cost after 297574 iterations : Training Loss =  0.10151099976372924; Validation Loss = 0.12394417579446941\n",
            "Cost after 297575 iterations : Training Loss =  0.10151099976372926; Validation Loss = 0.12394417579253172\n",
            "Cost after 297576 iterations : Training Loss =  0.10151099976372933; Validation Loss = 0.12394417579059411\n",
            "Cost after 297577 iterations : Training Loss =  0.10151099976372939; Validation Loss = 0.12394417578865671\n",
            "Cost after 297578 iterations : Training Loss =  0.10151099976372921; Validation Loss = 0.12394417578671946\n",
            "Cost after 297579 iterations : Training Loss =  0.10151099976372913; Validation Loss = 0.12394417578478194\n",
            "Cost after 297580 iterations : Training Loss =  0.10151099976372915; Validation Loss = 0.12394417578284479\n",
            "Cost after 297581 iterations : Training Loss =  0.1015109997637291; Validation Loss = 0.12394417578090781\n",
            "Cost after 297582 iterations : Training Loss =  0.10151099976372922; Validation Loss = 0.12394417577897093\n",
            "Cost after 297583 iterations : Training Loss =  0.10151099976372917; Validation Loss = 0.12394417577703369\n",
            "Cost after 297584 iterations : Training Loss =  0.10151099976372917; Validation Loss = 0.12394417577509743\n",
            "Cost after 297585 iterations : Training Loss =  0.10151099976372921; Validation Loss = 0.12394417577316036\n",
            "Cost after 297586 iterations : Training Loss =  0.10151099976372936; Validation Loss = 0.12394417577122376\n",
            "Cost after 297587 iterations : Training Loss =  0.10151099976372929; Validation Loss = 0.12394417576928714\n",
            "Cost after 297588 iterations : Training Loss =  0.10151099976372917; Validation Loss = 0.1239441757673506\n",
            "Cost after 297589 iterations : Training Loss =  0.10151099976372917; Validation Loss = 0.12394417576541467\n",
            "Cost after 297590 iterations : Training Loss =  0.10151099976372921; Validation Loss = 0.12394417576347852\n",
            "Cost after 297591 iterations : Training Loss =  0.10151099976372928; Validation Loss = 0.12394417576154204\n",
            "Cost after 297592 iterations : Training Loss =  0.10151099976372936; Validation Loss = 0.12394417575960626\n",
            "Cost after 297593 iterations : Training Loss =  0.1015109997637291; Validation Loss = 0.12394417575767026\n",
            "Cost after 297594 iterations : Training Loss =  0.10151099976372935; Validation Loss = 0.12394417575573467\n",
            "Cost after 297595 iterations : Training Loss =  0.10151099976372914; Validation Loss = 0.1239441757537987\n",
            "Cost after 297596 iterations : Training Loss =  0.10151099976372928; Validation Loss = 0.12394417575186287\n",
            "Cost after 297597 iterations : Training Loss =  0.10151099976372917; Validation Loss = 0.12394417574992776\n",
            "Cost after 297598 iterations : Training Loss =  0.10151099976372915; Validation Loss = 0.12394417574799205\n",
            "Cost after 297599 iterations : Training Loss =  0.10151099976372904; Validation Loss = 0.12394417574605679\n",
            "Cost after 297600 iterations : Training Loss =  0.10151099976372924; Validation Loss = 0.12394417574412168\n",
            "Cost after 297601 iterations : Training Loss =  0.10151099976372928; Validation Loss = 0.1239441757421866\n",
            "Cost after 297602 iterations : Training Loss =  0.10151099976372897; Validation Loss = 0.12394417574025168\n",
            "Cost after 297603 iterations : Training Loss =  0.10151099976372902; Validation Loss = 0.12394417573831686\n",
            "Cost after 297604 iterations : Training Loss =  0.1015109997637291; Validation Loss = 0.12394417573638195\n",
            "Cost after 297605 iterations : Training Loss =  0.10151099976372915; Validation Loss = 0.12394417573444727\n",
            "Cost after 297606 iterations : Training Loss =  0.10151099976372903; Validation Loss = 0.12394417573251247\n",
            "Cost after 297607 iterations : Training Loss =  0.1015109997637291; Validation Loss = 0.12394417573057805\n",
            "Cost after 297608 iterations : Training Loss =  0.10151099976372913; Validation Loss = 0.12394417572864361\n",
            "Cost after 297609 iterations : Training Loss =  0.10151099976372913; Validation Loss = 0.12394417572670938\n",
            "Cost after 297610 iterations : Training Loss =  0.10151099976372917; Validation Loss = 0.12394417572477487\n",
            "Cost after 297611 iterations : Training Loss =  0.1015109997637292; Validation Loss = 0.12394417572284099\n",
            "Cost after 297612 iterations : Training Loss =  0.10151099976372889; Validation Loss = 0.1239441757209069\n",
            "Cost after 297613 iterations : Training Loss =  0.10151099976372904; Validation Loss = 0.12394417571897279\n",
            "Cost after 297614 iterations : Training Loss =  0.10151099976372903; Validation Loss = 0.12394417571703928\n",
            "Cost after 297615 iterations : Training Loss =  0.10151099976372913; Validation Loss = 0.12394417571510542\n",
            "Cost after 297616 iterations : Training Loss =  0.10151099976372903; Validation Loss = 0.12394417571317172\n",
            "Cost after 297617 iterations : Training Loss =  0.10151099976372915; Validation Loss = 0.12394417571123809\n",
            "Cost after 297618 iterations : Training Loss =  0.10151099976372922; Validation Loss = 0.12394417570930479\n",
            "Cost after 297619 iterations : Training Loss =  0.10151099976372904; Validation Loss = 0.12394417570737158\n",
            "Cost after 297620 iterations : Training Loss =  0.10151099976372914; Validation Loss = 0.12394417570543793\n",
            "Cost after 297621 iterations : Training Loss =  0.10151099976372908; Validation Loss = 0.12394417570350498\n",
            "Cost after 297622 iterations : Training Loss =  0.10151099976372902; Validation Loss = 0.12394417570157196\n",
            "Cost after 297623 iterations : Training Loss =  0.101510999763729; Validation Loss = 0.12394417569963886\n",
            "Cost after 297624 iterations : Training Loss =  0.1015109997637291; Validation Loss = 0.12394417569770572\n",
            "Cost after 297625 iterations : Training Loss =  0.10151099976372897; Validation Loss = 0.12394417569577333\n",
            "Cost after 297626 iterations : Training Loss =  0.1015109997637291; Validation Loss = 0.12394417569384018\n",
            "Cost after 297627 iterations : Training Loss =  0.10151099976372921; Validation Loss = 0.12394417569190808\n",
            "Cost after 297628 iterations : Training Loss =  0.10151099976372897; Validation Loss = 0.12394417568997543\n",
            "Cost after 297629 iterations : Training Loss =  0.10151099976372902; Validation Loss = 0.12394417568804311\n",
            "Cost after 297630 iterations : Training Loss =  0.10151099976372896; Validation Loss = 0.12394417568611074\n",
            "Cost after 297631 iterations : Training Loss =  0.10151099976372896; Validation Loss = 0.12394417568417851\n",
            "Cost after 297632 iterations : Training Loss =  0.10151099976372902; Validation Loss = 0.1239441756822465\n",
            "Cost after 297633 iterations : Training Loss =  0.10151099976372889; Validation Loss = 0.1239441756803143\n",
            "Cost after 297634 iterations : Training Loss =  0.10151099976372892; Validation Loss = 0.12394417567838273\n",
            "Cost after 297635 iterations : Training Loss =  0.10151099976372892; Validation Loss = 0.12394417567645104\n",
            "Cost after 297636 iterations : Training Loss =  0.10151099976372903; Validation Loss = 0.12394417567451914\n",
            "Cost after 297637 iterations : Training Loss =  0.10151099976372889; Validation Loss = 0.12394417567258786\n",
            "Cost after 297638 iterations : Training Loss =  0.10151099976372896; Validation Loss = 0.12394417567065613\n",
            "Cost after 297639 iterations : Training Loss =  0.10151099976372883; Validation Loss = 0.12394417566872475\n",
            "Cost after 297640 iterations : Training Loss =  0.10151099976372885; Validation Loss = 0.12394417566679353\n",
            "Cost after 297641 iterations : Training Loss =  0.10151099976372904; Validation Loss = 0.12394417566486247\n",
            "Cost after 297642 iterations : Training Loss =  0.10151099976372889; Validation Loss = 0.12394417566293096\n",
            "Cost after 297643 iterations : Training Loss =  0.10151099976372888; Validation Loss = 0.12394417566100006\n",
            "Cost after 297644 iterations : Training Loss =  0.1015109997637291; Validation Loss = 0.12394417565906936\n",
            "Cost after 297645 iterations : Training Loss =  0.10151099976372895; Validation Loss = 0.12394417565713844\n",
            "Cost after 297646 iterations : Training Loss =  0.10151099976372885; Validation Loss = 0.12394417565520773\n",
            "Cost after 297647 iterations : Training Loss =  0.10151099976372897; Validation Loss = 0.12394417565327719\n",
            "Cost after 297648 iterations : Training Loss =  0.101510999763729; Validation Loss = 0.12394417565134673\n",
            "Cost after 297649 iterations : Training Loss =  0.10151099976372888; Validation Loss = 0.12394417564941644\n",
            "Cost after 297650 iterations : Training Loss =  0.1015109997637289; Validation Loss = 0.12394417564748607\n",
            "Cost after 297651 iterations : Training Loss =  0.10151099976372896; Validation Loss = 0.12394417564555564\n",
            "Cost after 297652 iterations : Training Loss =  0.10151099976372883; Validation Loss = 0.12394417564362571\n",
            "Cost after 297653 iterations : Training Loss =  0.10151099976372888; Validation Loss = 0.12394417564169583\n",
            "Cost after 297654 iterations : Training Loss =  0.10151099976372878; Validation Loss = 0.12394417563976565\n",
            "Cost after 297655 iterations : Training Loss =  0.10151099976372889; Validation Loss = 0.12394417563783573\n",
            "Cost after 297656 iterations : Training Loss =  0.1015109997637289; Validation Loss = 0.12394417563590601\n",
            "Cost after 297657 iterations : Training Loss =  0.10151099976372877; Validation Loss = 0.12394417563397646\n",
            "Cost after 297658 iterations : Training Loss =  0.10151099976372888; Validation Loss = 0.12394417563204696\n",
            "Cost after 297659 iterations : Training Loss =  0.10151099976372883; Validation Loss = 0.12394417563011731\n",
            "Cost after 297660 iterations : Training Loss =  0.10151099976372885; Validation Loss = 0.12394417562818831\n",
            "Cost after 297661 iterations : Training Loss =  0.10151099976372882; Validation Loss = 0.12394417562625913\n",
            "Cost after 297662 iterations : Training Loss =  0.10151099976372877; Validation Loss = 0.12394417562432972\n",
            "Cost after 297663 iterations : Training Loss =  0.10151099976372892; Validation Loss = 0.1239441756224008\n",
            "Cost after 297664 iterations : Training Loss =  0.1015109997637289; Validation Loss = 0.12394417562047187\n",
            "Cost after 297665 iterations : Training Loss =  0.1015109997637288; Validation Loss = 0.123944175618543\n",
            "Cost after 297666 iterations : Training Loss =  0.10151099976372892; Validation Loss = 0.12394417561661432\n",
            "Cost after 297667 iterations : Training Loss =  0.101510999763729; Validation Loss = 0.12394417561468561\n",
            "Cost after 297668 iterations : Training Loss =  0.10151099976372897; Validation Loss = 0.1239441756127572\n",
            "Cost after 297669 iterations : Training Loss =  0.10151099976372903; Validation Loss = 0.12394417561082886\n",
            "Cost after 297670 iterations : Training Loss =  0.10151099976372888; Validation Loss = 0.12394417560890031\n",
            "Cost after 297671 iterations : Training Loss =  0.10151099976372889; Validation Loss = 0.12394417560697177\n",
            "Cost after 297672 iterations : Training Loss =  0.10151099976372895; Validation Loss = 0.12394417560504367\n",
            "Cost after 297673 iterations : Training Loss =  0.10151099976372871; Validation Loss = 0.12394417560311574\n",
            "Cost after 297674 iterations : Training Loss =  0.10151099976372875; Validation Loss = 0.12394417560118791\n",
            "Cost after 297675 iterations : Training Loss =  0.10151099976372872; Validation Loss = 0.12394417559925984\n",
            "Cost after 297676 iterations : Training Loss =  0.10151099976372878; Validation Loss = 0.12394417559733217\n",
            "Cost after 297677 iterations : Training Loss =  0.1015109997637287; Validation Loss = 0.12394417559540431\n",
            "Cost after 297678 iterations : Training Loss =  0.10151099976372882; Validation Loss = 0.12394417559347684\n",
            "Cost after 297679 iterations : Training Loss =  0.10151099976372882; Validation Loss = 0.12394417559154945\n",
            "Cost after 297680 iterations : Training Loss =  0.10151099976372878; Validation Loss = 0.12394417558962208\n",
            "Cost after 297681 iterations : Training Loss =  0.1015109997637287; Validation Loss = 0.1239441755876949\n",
            "Cost after 297682 iterations : Training Loss =  0.1015109997637288; Validation Loss = 0.1239441755857676\n",
            "Cost after 297683 iterations : Training Loss =  0.1015109997637288; Validation Loss = 0.12394417558384069\n",
            "Cost after 297684 iterations : Training Loss =  0.10151099976372858; Validation Loss = 0.12394417558191353\n",
            "Cost after 297685 iterations : Training Loss =  0.10151099976372868; Validation Loss = 0.12394417557998681\n",
            "Cost after 297686 iterations : Training Loss =  0.1015109997637289; Validation Loss = 0.12394417557806009\n",
            "Cost after 297687 iterations : Training Loss =  0.10151099976372895; Validation Loss = 0.12394417557613333\n",
            "Cost after 297688 iterations : Training Loss =  0.10151099976372863; Validation Loss = 0.12394417557420692\n",
            "Cost after 297689 iterations : Training Loss =  0.10151099976372863; Validation Loss = 0.12394417557228016\n",
            "Cost after 297690 iterations : Training Loss =  0.10151099976372865; Validation Loss = 0.12394417557035406\n",
            "Cost after 297691 iterations : Training Loss =  0.10151099976372868; Validation Loss = 0.12394417556842754\n",
            "Cost after 297692 iterations : Training Loss =  0.10151099976372871; Validation Loss = 0.12394417556650127\n",
            "Cost after 297693 iterations : Training Loss =  0.10151099976372857; Validation Loss = 0.12394417556457542\n",
            "Cost after 297694 iterations : Training Loss =  0.10151099976372872; Validation Loss = 0.12394417556264938\n",
            "Cost after 297695 iterations : Training Loss =  0.10151099976372868; Validation Loss = 0.12394417556072299\n",
            "Cost after 297696 iterations : Training Loss =  0.10151099976372885; Validation Loss = 0.12394417555879746\n",
            "Cost after 297697 iterations : Training Loss =  0.10151099976372852; Validation Loss = 0.12394417555687155\n",
            "Cost after 297698 iterations : Training Loss =  0.1015109997637287; Validation Loss = 0.12394417555494602\n",
            "Cost after 297699 iterations : Training Loss =  0.10151099976372877; Validation Loss = 0.12394417555302065\n",
            "Cost after 297700 iterations : Training Loss =  0.10151099976372858; Validation Loss = 0.12394417555109534\n",
            "Cost after 297701 iterations : Training Loss =  0.10151099976372878; Validation Loss = 0.12394417554917006\n",
            "Cost after 297702 iterations : Training Loss =  0.10151099976372872; Validation Loss = 0.12394417554724452\n",
            "Cost after 297703 iterations : Training Loss =  0.10151099976372868; Validation Loss = 0.12394417554531945\n",
            "Cost after 297704 iterations : Training Loss =  0.10151099976372871; Validation Loss = 0.12394417554339451\n",
            "Cost after 297705 iterations : Training Loss =  0.10151099976372877; Validation Loss = 0.12394417554146951\n",
            "Cost after 297706 iterations : Training Loss =  0.10151099976372871; Validation Loss = 0.12394417553954475\n",
            "Cost after 297707 iterations : Training Loss =  0.10151099976372852; Validation Loss = 0.12394417553762002\n",
            "Cost after 297708 iterations : Training Loss =  0.10151099976372868; Validation Loss = 0.12394417553569537\n",
            "Cost after 297709 iterations : Training Loss =  0.10151099976372872; Validation Loss = 0.12394417553377103\n",
            "Cost after 297710 iterations : Training Loss =  0.10151099976372853; Validation Loss = 0.1239441755318464\n",
            "Cost after 297711 iterations : Training Loss =  0.10151099976372857; Validation Loss = 0.12394417552992192\n",
            "Cost after 297712 iterations : Training Loss =  0.10151099976372863; Validation Loss = 0.12394417552799787\n",
            "Cost after 297713 iterations : Training Loss =  0.10151099976372863; Validation Loss = 0.12394417552607363\n",
            "Cost after 297714 iterations : Training Loss =  0.10151099976372868; Validation Loss = 0.12394417552415002\n",
            "Cost after 297715 iterations : Training Loss =  0.1015109997637287; Validation Loss = 0.12394417552222592\n",
            "Cost after 297716 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.12394417552030178\n",
            "Cost after 297717 iterations : Training Loss =  0.1015109997637287; Validation Loss = 0.12394417551837814\n",
            "Cost after 297718 iterations : Training Loss =  0.10151099976372853; Validation Loss = 0.12394417551645404\n",
            "Cost after 297719 iterations : Training Loss =  0.10151099976372864; Validation Loss = 0.12394417551453088\n",
            "Cost after 297720 iterations : Training Loss =  0.10151099976372853; Validation Loss = 0.1239441755126075\n",
            "Cost after 297721 iterations : Training Loss =  0.10151099976372878; Validation Loss = 0.12394417551068404\n",
            "Cost after 297722 iterations : Training Loss =  0.10151099976372856; Validation Loss = 0.12394417550876081\n",
            "Cost after 297723 iterations : Training Loss =  0.1015109997637285; Validation Loss = 0.12394417550683784\n",
            "Cost after 297724 iterations : Training Loss =  0.10151099976372858; Validation Loss = 0.12394417550491442\n",
            "Cost after 297725 iterations : Training Loss =  0.10151099976372871; Validation Loss = 0.12394417550299153\n",
            "Cost after 297726 iterations : Training Loss =  0.10151099976372847; Validation Loss = 0.12394417550106863\n",
            "Cost after 297727 iterations : Training Loss =  0.1015109997637285; Validation Loss = 0.12394417549914591\n",
            "Cost after 297728 iterations : Training Loss =  0.10151099976372863; Validation Loss = 0.12394417549722324\n",
            "Cost after 297729 iterations : Training Loss =  0.10151099976372853; Validation Loss = 0.12394417549530047\n",
            "Cost after 297730 iterations : Training Loss =  0.10151099976372863; Validation Loss = 0.12394417549337833\n",
            "Cost after 297731 iterations : Training Loss =  0.10151099976372857; Validation Loss = 0.12394417549145577\n",
            "Cost after 297732 iterations : Training Loss =  0.10151099976372835; Validation Loss = 0.12394417548953346\n",
            "Cost after 297733 iterations : Training Loss =  0.10151099976372856; Validation Loss = 0.12394417548761145\n",
            "Cost after 297734 iterations : Training Loss =  0.10151099976372852; Validation Loss = 0.12394417548568906\n",
            "Cost after 297735 iterations : Training Loss =  0.10151099976372831; Validation Loss = 0.12394417548376704\n",
            "Cost after 297736 iterations : Training Loss =  0.10151099976372857; Validation Loss = 0.1239441754818455\n",
            "Cost after 297737 iterations : Training Loss =  0.10151099976372839; Validation Loss = 0.12394417547992338\n",
            "Cost after 297738 iterations : Training Loss =  0.10151099976372845; Validation Loss = 0.1239441754780016\n",
            "Cost after 297739 iterations : Training Loss =  0.10151099976372871; Validation Loss = 0.1239441754760799\n",
            "Cost after 297740 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.12394417547415851\n",
            "Cost after 297741 iterations : Training Loss =  0.10151099976372838; Validation Loss = 0.12394417547223709\n",
            "Cost after 297742 iterations : Training Loss =  0.10151099976372847; Validation Loss = 0.12394417547031594\n",
            "Cost after 297743 iterations : Training Loss =  0.10151099976372845; Validation Loss = 0.12394417546839442\n",
            "Cost after 297744 iterations : Training Loss =  0.10151099976372824; Validation Loss = 0.12394417546647316\n",
            "Cost after 297745 iterations : Training Loss =  0.10151099976372846; Validation Loss = 0.12394417546455247\n",
            "Cost after 297746 iterations : Training Loss =  0.10151099976372838; Validation Loss = 0.12394417546263135\n",
            "Cost after 297747 iterations : Training Loss =  0.10151099976372834; Validation Loss = 0.12394417546071079\n",
            "Cost after 297748 iterations : Training Loss =  0.1015109997637284; Validation Loss = 0.12394417545879016\n",
            "Cost after 297749 iterations : Training Loss =  0.10151099976372846; Validation Loss = 0.12394417545686928\n",
            "Cost after 297750 iterations : Training Loss =  0.10151099976372847; Validation Loss = 0.12394417545494885\n",
            "Cost after 297751 iterations : Training Loss =  0.1015109997637284; Validation Loss = 0.12394417545302833\n",
            "Cost after 297752 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.12394417545110792\n",
            "Cost after 297753 iterations : Training Loss =  0.10151099976372839; Validation Loss = 0.12394417544918819\n",
            "Cost after 297754 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.1239441754472676\n",
            "Cost after 297755 iterations : Training Loss =  0.1015109997637285; Validation Loss = 0.1239441754453477\n",
            "Cost after 297756 iterations : Training Loss =  0.1015109997637285; Validation Loss = 0.12394417544342805\n",
            "Cost after 297757 iterations : Training Loss =  0.10151099976372832; Validation Loss = 0.1239441754415076\n",
            "Cost after 297758 iterations : Training Loss =  0.10151099976372856; Validation Loss = 0.12394417543958827\n",
            "Cost after 297759 iterations : Training Loss =  0.10151099976372838; Validation Loss = 0.12394417543766847\n",
            "Cost after 297760 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.12394417543574873\n",
            "Cost after 297761 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.12394417543382924\n",
            "Cost after 297762 iterations : Training Loss =  0.10151099976372835; Validation Loss = 0.12394417543191003\n",
            "Cost after 297763 iterations : Training Loss =  0.10151099976372828; Validation Loss = 0.12394417542999059\n",
            "Cost after 297764 iterations : Training Loss =  0.10151099976372828; Validation Loss = 0.1239441754280715\n",
            "Cost after 297765 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.12394417542615227\n",
            "Cost after 297766 iterations : Training Loss =  0.10151099976372839; Validation Loss = 0.12394417542423349\n",
            "Cost after 297767 iterations : Training Loss =  0.10151099976372832; Validation Loss = 0.12394417542231441\n",
            "Cost after 297768 iterations : Training Loss =  0.1015109997637284; Validation Loss = 0.12394417542039612\n",
            "Cost after 297769 iterations : Training Loss =  0.10151099976372852; Validation Loss = 0.12394417541847692\n",
            "Cost after 297770 iterations : Training Loss =  0.10151099976372839; Validation Loss = 0.12394417541655856\n",
            "Cost after 297771 iterations : Training Loss =  0.10151099976372831; Validation Loss = 0.12394417541463988\n",
            "Cost after 297772 iterations : Training Loss =  0.10151099976372825; Validation Loss = 0.12394417541272164\n",
            "Cost after 297773 iterations : Training Loss =  0.1015109997637282; Validation Loss = 0.12394417541080335\n",
            "Cost after 297774 iterations : Training Loss =  0.10151099976372845; Validation Loss = 0.12394417540888533\n",
            "Cost after 297775 iterations : Training Loss =  0.1015109997637284; Validation Loss = 0.12394417540696712\n",
            "Cost after 297776 iterations : Training Loss =  0.10151099976372834; Validation Loss = 0.12394417540504908\n",
            "Cost after 297777 iterations : Training Loss =  0.10151099976372834; Validation Loss = 0.1239441754031309\n",
            "Cost after 297778 iterations : Training Loss =  0.10151099976372831; Validation Loss = 0.1239441754012135\n",
            "Cost after 297779 iterations : Training Loss =  0.10151099976372827; Validation Loss = 0.1239441753992956\n",
            "Cost after 297780 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.12394417539737772\n",
            "Cost after 297781 iterations : Training Loss =  0.10151099976372827; Validation Loss = 0.1239441753954602\n",
            "Cost after 297782 iterations : Training Loss =  0.10151099976372843; Validation Loss = 0.12394417539354258\n",
            "Cost after 297783 iterations : Training Loss =  0.10151099976372835; Validation Loss = 0.1239441753916255\n",
            "Cost after 297784 iterations : Training Loss =  0.10151099976372834; Validation Loss = 0.12394417538970823\n",
            "Cost after 297785 iterations : Training Loss =  0.10151099976372835; Validation Loss = 0.1239441753877912\n",
            "Cost after 297786 iterations : Training Loss =  0.10151099976372828; Validation Loss = 0.1239441753858742\n",
            "Cost after 297787 iterations : Training Loss =  0.10151099976372811; Validation Loss = 0.12394417538395722\n",
            "Cost after 297788 iterations : Training Loss =  0.10151099976372825; Validation Loss = 0.12394417538204032\n",
            "Cost after 297789 iterations : Training Loss =  0.10151099976372824; Validation Loss = 0.1239441753801237\n",
            "Cost after 297790 iterations : Training Loss =  0.10151099976372821; Validation Loss = 0.12394417537820704\n",
            "Cost after 297791 iterations : Training Loss =  0.10151099976372828; Validation Loss = 0.1239441753762905\n",
            "Cost after 297792 iterations : Training Loss =  0.10151099976372827; Validation Loss = 0.12394417537437398\n",
            "Cost after 297793 iterations : Training Loss =  0.10151099976372824; Validation Loss = 0.12394417537245776\n",
            "Cost after 297794 iterations : Training Loss =  0.10151099976372818; Validation Loss = 0.12394417537054153\n",
            "Cost after 297795 iterations : Training Loss =  0.10151099976372824; Validation Loss = 0.12394417536862559\n",
            "Cost after 297796 iterations : Training Loss =  0.10151099976372824; Validation Loss = 0.1239441753667096\n",
            "Cost after 297797 iterations : Training Loss =  0.10151099976372818; Validation Loss = 0.12394417536479332\n",
            "Cost after 297798 iterations : Training Loss =  0.10151099976372809; Validation Loss = 0.12394417536287768\n",
            "Cost after 297799 iterations : Training Loss =  0.10151099976372811; Validation Loss = 0.12394417536096153\n",
            "Cost after 297800 iterations : Training Loss =  0.10151099976372821; Validation Loss = 0.12394417535904616\n",
            "Cost after 297801 iterations : Training Loss =  0.10151099976372815; Validation Loss = 0.12394417535713068\n",
            "Cost after 297802 iterations : Training Loss =  0.10151099976372818; Validation Loss = 0.12394417535521508\n",
            "Cost after 297803 iterations : Training Loss =  0.10151099976372803; Validation Loss = 0.12394417535329944\n",
            "Cost after 297804 iterations : Training Loss =  0.10151099976372815; Validation Loss = 0.12394417535138445\n",
            "Cost after 297805 iterations : Training Loss =  0.10151099976372818; Validation Loss = 0.12394417534946918\n",
            "Cost after 297806 iterations : Training Loss =  0.10151099976372811; Validation Loss = 0.12394417534755406\n",
            "Cost after 297807 iterations : Training Loss =  0.10151099976372809; Validation Loss = 0.12394417534563917\n",
            "Cost after 297808 iterations : Training Loss =  0.1015109997637282; Validation Loss = 0.12394417534372446\n",
            "Cost after 297809 iterations : Training Loss =  0.1015109997637282; Validation Loss = 0.12394417534180946\n",
            "Cost after 297810 iterations : Training Loss =  0.10151099976372814; Validation Loss = 0.12394417533989457\n",
            "Cost after 297811 iterations : Training Loss =  0.10151099976372811; Validation Loss = 0.12394417533798015\n",
            "Cost after 297812 iterations : Training Loss =  0.10151099976372811; Validation Loss = 0.12394417533606582\n",
            "Cost after 297813 iterations : Training Loss =  0.10151099976372811; Validation Loss = 0.12394417533415139\n",
            "Cost after 297814 iterations : Training Loss =  0.101510999763728; Validation Loss = 0.1239441753322372\n",
            "Cost after 297815 iterations : Training Loss =  0.10151099976372824; Validation Loss = 0.12394417533032273\n",
            "Cost after 297816 iterations : Training Loss =  0.10151099976372799; Validation Loss = 0.12394417532840855\n",
            "Cost after 297817 iterations : Training Loss =  0.10151099976372821; Validation Loss = 0.12394417532649467\n",
            "Cost after 297818 iterations : Training Loss =  0.10151099976372821; Validation Loss = 0.12394417532458084\n",
            "Cost after 297819 iterations : Training Loss =  0.101510999763728; Validation Loss = 0.1239441753226669\n",
            "Cost after 297820 iterations : Training Loss =  0.10151099976372811; Validation Loss = 0.12394417532075325\n",
            "Cost after 297821 iterations : Training Loss =  0.10151099976372824; Validation Loss = 0.12394417531883918\n",
            "Cost after 297822 iterations : Training Loss =  0.10151099976372803; Validation Loss = 0.1239441753169262\n",
            "Cost after 297823 iterations : Training Loss =  0.10151099976372793; Validation Loss = 0.12394417531501262\n",
            "Cost after 297824 iterations : Training Loss =  0.10151099976372807; Validation Loss = 0.12394417531309918\n",
            "Cost after 297825 iterations : Training Loss =  0.10151099976372784; Validation Loss = 0.1239441753111863\n",
            "Cost after 297826 iterations : Training Loss =  0.101510999763728; Validation Loss = 0.12394417530927275\n",
            "Cost after 297827 iterations : Training Loss =  0.10151099976372793; Validation Loss = 0.12394417530735986\n",
            "Cost after 297828 iterations : Training Loss =  0.10151099976372821; Validation Loss = 0.12394417530544663\n",
            "Cost after 297829 iterations : Training Loss =  0.10151099976372803; Validation Loss = 0.12394417530353419\n",
            "Cost after 297830 iterations : Training Loss =  0.10151099976372803; Validation Loss = 0.12394417530162129\n",
            "Cost after 297831 iterations : Training Loss =  0.101510999763728; Validation Loss = 0.12394417529970866\n",
            "Cost after 297832 iterations : Training Loss =  0.10151099976372796; Validation Loss = 0.12394417529779603\n",
            "Cost after 297833 iterations : Training Loss =  0.10151099976372806; Validation Loss = 0.12394417529588349\n",
            "Cost after 297834 iterations : Training Loss =  0.10151099976372807; Validation Loss = 0.12394417529397117\n",
            "Cost after 297835 iterations : Training Loss =  0.10151099976372815; Validation Loss = 0.12394417529205899\n",
            "Cost after 297836 iterations : Training Loss =  0.10151099976372809; Validation Loss = 0.12394417529014677\n",
            "Cost after 297837 iterations : Training Loss =  0.10151099976372793; Validation Loss = 0.12394417528823481\n",
            "Cost after 297838 iterations : Training Loss =  0.10151099976372795; Validation Loss = 0.12394417528632297\n",
            "Cost after 297839 iterations : Training Loss =  0.10151099976372795; Validation Loss = 0.12394417528441089\n",
            "Cost after 297840 iterations : Training Loss =  0.101510999763728; Validation Loss = 0.12394417528249912\n",
            "Cost after 297841 iterations : Training Loss =  0.10151099976372806; Validation Loss = 0.1239441752805874\n",
            "Cost after 297842 iterations : Training Loss =  0.10151099976372799; Validation Loss = 0.12394417527867595\n",
            "Cost after 297843 iterations : Training Loss =  0.10151099976372803; Validation Loss = 0.12394417527676442\n",
            "Cost after 297844 iterations : Training Loss =  0.10151099976372803; Validation Loss = 0.1239441752748531\n",
            "Cost after 297845 iterations : Training Loss =  0.1015109997637279; Validation Loss = 0.12394417527294171\n",
            "Cost after 297846 iterations : Training Loss =  0.10151099976372806; Validation Loss = 0.1239441752710306\n",
            "Cost after 297847 iterations : Training Loss =  0.1015109997637279; Validation Loss = 0.12394417526911944\n",
            "Cost after 297848 iterations : Training Loss =  0.10151099976372802; Validation Loss = 0.12394417526720843\n",
            "Cost after 297849 iterations : Training Loss =  0.10151099976372807; Validation Loss = 0.1239441752652974\n",
            "Cost after 297850 iterations : Training Loss =  0.10151099976372782; Validation Loss = 0.12394417526338693\n",
            "Cost after 297851 iterations : Training Loss =  0.101510999763728; Validation Loss = 0.12394417526147618\n",
            "Cost after 297852 iterations : Training Loss =  0.10151099976372803; Validation Loss = 0.12394417525956548\n",
            "Cost after 297853 iterations : Training Loss =  0.101510999763728; Validation Loss = 0.1239441752576547\n",
            "Cost after 297854 iterations : Training Loss =  0.10151099976372799; Validation Loss = 0.12394417525574446\n",
            "Cost after 297855 iterations : Training Loss =  0.10151099976372795; Validation Loss = 0.12394417525383436\n",
            "Cost after 297856 iterations : Training Loss =  0.1015109997637279; Validation Loss = 0.12394417525192414\n",
            "Cost after 297857 iterations : Training Loss =  0.10151099976372784; Validation Loss = 0.12394417525001376\n",
            "Cost after 297858 iterations : Training Loss =  0.10151099976372795; Validation Loss = 0.1239441752481039\n",
            "Cost after 297859 iterations : Training Loss =  0.10151099976372799; Validation Loss = 0.12394417524619383\n",
            "Cost after 297860 iterations : Training Loss =  0.10151099976372795; Validation Loss = 0.12394417524428405\n",
            "Cost after 297861 iterations : Training Loss =  0.10151099976372799; Validation Loss = 0.12394417524237433\n",
            "Cost after 297862 iterations : Training Loss =  0.10151099976372811; Validation Loss = 0.12394417524046464\n",
            "Cost after 297863 iterations : Training Loss =  0.10151099976372799; Validation Loss = 0.12394417523855535\n",
            "Cost after 297864 iterations : Training Loss =  0.10151099976372799; Validation Loss = 0.12394417523664569\n",
            "Cost after 297865 iterations : Training Loss =  0.10151099976372796; Validation Loss = 0.12394417523473612\n",
            "Cost after 297866 iterations : Training Loss =  0.10151099976372788; Validation Loss = 0.12394417523282711\n",
            "Cost after 297867 iterations : Training Loss =  0.10151099976372784; Validation Loss = 0.12394417523091765\n",
            "Cost after 297868 iterations : Training Loss =  0.10151099976372795; Validation Loss = 0.12394417522900852\n",
            "Cost after 297869 iterations : Training Loss =  0.10151099976372786; Validation Loss = 0.12394417522709976\n",
            "Cost after 297870 iterations : Training Loss =  0.10151099976372782; Validation Loss = 0.12394417522519093\n",
            "Cost after 297871 iterations : Training Loss =  0.1015109997637279; Validation Loss = 0.12394417522328192\n",
            "Cost after 297872 iterations : Training Loss =  0.10151099976372774; Validation Loss = 0.12394417522137338\n",
            "Cost after 297873 iterations : Training Loss =  0.10151099976372788; Validation Loss = 0.12394417521946464\n",
            "Cost after 297874 iterations : Training Loss =  0.10151099976372782; Validation Loss = 0.1239441752175566\n",
            "Cost after 297875 iterations : Training Loss =  0.1015109997637278; Validation Loss = 0.12394417521564816\n",
            "Cost after 297876 iterations : Training Loss =  0.1015109997637279; Validation Loss = 0.12394417521373977\n",
            "Cost after 297877 iterations : Training Loss =  0.10151099976372782; Validation Loss = 0.12394417521183121\n",
            "Cost after 297878 iterations : Training Loss =  0.10151099976372777; Validation Loss = 0.12394417520992351\n",
            "Cost after 297879 iterations : Training Loss =  0.10151099976372793; Validation Loss = 0.12394417520801546\n",
            "Cost after 297880 iterations : Training Loss =  0.10151099976372788; Validation Loss = 0.12394417520610748\n",
            "Cost after 297881 iterations : Training Loss =  0.10151099976372782; Validation Loss = 0.12394417520419969\n",
            "Cost after 297882 iterations : Training Loss =  0.1015109997637278; Validation Loss = 0.12394417520229206\n",
            "Cost after 297883 iterations : Training Loss =  0.10151099976372771; Validation Loss = 0.12394417520038431\n",
            "Cost after 297884 iterations : Training Loss =  0.10151099976372774; Validation Loss = 0.12394417519847674\n",
            "Cost after 297885 iterations : Training Loss =  0.1015109997637278; Validation Loss = 0.12394417519656954\n",
            "Cost after 297886 iterations : Training Loss =  0.10151099976372788; Validation Loss = 0.12394417519466203\n",
            "Cost after 297887 iterations : Training Loss =  0.10151099976372782; Validation Loss = 0.12394417519275486\n",
            "Cost after 297888 iterations : Training Loss =  0.10151099976372789; Validation Loss = 0.12394417519084763\n",
            "Cost after 297889 iterations : Training Loss =  0.10151099976372784; Validation Loss = 0.12394417518894066\n",
            "Cost after 297890 iterations : Training Loss =  0.10151099976372775; Validation Loss = 0.12394417518703367\n",
            "Cost after 297891 iterations : Training Loss =  0.10151099976372771; Validation Loss = 0.1239441751851268\n",
            "Cost after 297892 iterations : Training Loss =  0.10151099976372777; Validation Loss = 0.12394417518322026\n",
            "Cost after 297893 iterations : Training Loss =  0.1015109997637278; Validation Loss = 0.12394417518131358\n",
            "Cost after 297894 iterations : Training Loss =  0.10151099976372771; Validation Loss = 0.12394417517940695\n",
            "Cost after 297895 iterations : Training Loss =  0.10151099976372789; Validation Loss = 0.12394417517750052\n",
            "Cost after 297896 iterations : Training Loss =  0.10151099976372775; Validation Loss = 0.12394417517559418\n",
            "Cost after 297897 iterations : Training Loss =  0.10151099976372761; Validation Loss = 0.12394417517368782\n",
            "Cost after 297898 iterations : Training Loss =  0.1015109997637278; Validation Loss = 0.12394417517178187\n",
            "Cost after 297899 iterations : Training Loss =  0.10151099976372768; Validation Loss = 0.12394417516987576\n",
            "Cost after 297900 iterations : Training Loss =  0.10151099976372767; Validation Loss = 0.12394417516796956\n",
            "Cost after 297901 iterations : Training Loss =  0.10151099976372771; Validation Loss = 0.12394417516606371\n",
            "Cost after 297902 iterations : Training Loss =  0.10151099976372756; Validation Loss = 0.12394417516415807\n",
            "Cost after 297903 iterations : Training Loss =  0.1015109997637278; Validation Loss = 0.12394417516225244\n",
            "Cost after 297904 iterations : Training Loss =  0.10151099976372757; Validation Loss = 0.12394417516034674\n",
            "Cost after 297905 iterations : Training Loss =  0.10151099976372763; Validation Loss = 0.12394417515844137\n",
            "Cost after 297906 iterations : Training Loss =  0.10151099976372767; Validation Loss = 0.12394417515653584\n",
            "Cost after 297907 iterations : Training Loss =  0.10151099976372759; Validation Loss = 0.12394417515463063\n",
            "Cost after 297908 iterations : Training Loss =  0.10151099976372757; Validation Loss = 0.12394417515272566\n",
            "Cost after 297909 iterations : Training Loss =  0.10151099976372782; Validation Loss = 0.12394417515082054\n",
            "Cost after 297910 iterations : Training Loss =  0.1015109997637277; Validation Loss = 0.12394417514891551\n",
            "Cost after 297911 iterations : Training Loss =  0.10151099976372774; Validation Loss = 0.12394417514701049\n",
            "Cost after 297912 iterations : Training Loss =  0.10151099976372775; Validation Loss = 0.12394417514510588\n",
            "Cost after 297913 iterations : Training Loss =  0.10151099976372774; Validation Loss = 0.12394417514320108\n",
            "Cost after 297914 iterations : Training Loss =  0.10151099976372768; Validation Loss = 0.12394417514129653\n",
            "Cost after 297915 iterations : Training Loss =  0.10151099976372781; Validation Loss = 0.12394417513939197\n",
            "Cost after 297916 iterations : Training Loss =  0.10151099976372775; Validation Loss = 0.12394417513748757\n",
            "Cost after 297917 iterations : Training Loss =  0.10151099976372757; Validation Loss = 0.12394417513558319\n",
            "Cost after 297918 iterations : Training Loss =  0.10151099976372754; Validation Loss = 0.12394417513367882\n",
            "Cost after 297919 iterations : Training Loss =  0.10151099976372752; Validation Loss = 0.12394417513177461\n",
            "Cost after 297920 iterations : Training Loss =  0.10151099976372764; Validation Loss = 0.1239441751298707\n",
            "Cost after 297921 iterations : Training Loss =  0.10151099976372775; Validation Loss = 0.12394417512796668\n",
            "Cost after 297922 iterations : Training Loss =  0.10151099976372788; Validation Loss = 0.12394417512606296\n",
            "Cost after 297923 iterations : Training Loss =  0.10151099976372756; Validation Loss = 0.12394417512415952\n",
            "Cost after 297924 iterations : Training Loss =  0.10151099976372754; Validation Loss = 0.12394417512225545\n",
            "Cost after 297925 iterations : Training Loss =  0.10151099976372759; Validation Loss = 0.12394417512035219\n",
            "Cost after 297926 iterations : Training Loss =  0.10151099976372757; Validation Loss = 0.1239441751184487\n",
            "Cost after 297927 iterations : Training Loss =  0.10151099976372767; Validation Loss = 0.12394417511654529\n",
            "Cost after 297928 iterations : Training Loss =  0.10151099976372767; Validation Loss = 0.12394417511464198\n",
            "Cost after 297929 iterations : Training Loss =  0.10151099976372764; Validation Loss = 0.12394417511273913\n",
            "Cost after 297930 iterations : Training Loss =  0.10151099976372761; Validation Loss = 0.12394417511083587\n",
            "Cost after 297931 iterations : Training Loss =  0.10151099976372763; Validation Loss = 0.12394417510893259\n",
            "Cost after 297932 iterations : Training Loss =  0.10151099976372771; Validation Loss = 0.12394417510702992\n",
            "Cost after 297933 iterations : Training Loss =  0.10151099976372752; Validation Loss = 0.12394417510512712\n",
            "Cost after 297934 iterations : Training Loss =  0.10151099976372756; Validation Loss = 0.1239441751032245\n",
            "Cost after 297935 iterations : Training Loss =  0.10151099976372767; Validation Loss = 0.12394417510132183\n",
            "Cost after 297936 iterations : Training Loss =  0.10151099976372761; Validation Loss = 0.12394417509941956\n",
            "Cost after 297937 iterations : Training Loss =  0.10151099976372752; Validation Loss = 0.12394417509751712\n",
            "Cost after 297938 iterations : Training Loss =  0.1015109997637277; Validation Loss = 0.12394417509561487\n",
            "Cost after 297939 iterations : Training Loss =  0.1015109997637275; Validation Loss = 0.12394417509371257\n",
            "Cost after 297940 iterations : Training Loss =  0.10151099976372752; Validation Loss = 0.12394417509181054\n",
            "Cost after 297941 iterations : Training Loss =  0.10151099976372754; Validation Loss = 0.12394417508990853\n",
            "Cost after 297942 iterations : Training Loss =  0.10151099976372754; Validation Loss = 0.1239441750880065\n",
            "Cost after 297943 iterations : Training Loss =  0.10151099976372754; Validation Loss = 0.12394417508610481\n",
            "Cost after 297944 iterations : Training Loss =  0.10151099976372754; Validation Loss = 0.12394417508420319\n",
            "Cost after 297945 iterations : Training Loss =  0.10151099976372743; Validation Loss = 0.1239441750823013\n",
            "Cost after 297946 iterations : Training Loss =  0.10151099976372759; Validation Loss = 0.12394417508040008\n",
            "Cost after 297947 iterations : Training Loss =  0.10151099976372746; Validation Loss = 0.12394417507849838\n",
            "Cost after 297948 iterations : Training Loss =  0.10151099976372746; Validation Loss = 0.12394417507659719\n",
            "Cost after 297949 iterations : Training Loss =  0.10151099976372761; Validation Loss = 0.12394417507469609\n",
            "Cost after 297950 iterations : Training Loss =  0.10151099976372759; Validation Loss = 0.12394417507279475\n",
            "Cost after 297951 iterations : Training Loss =  0.10151099976372742; Validation Loss = 0.12394417507089373\n",
            "Cost after 297952 iterations : Training Loss =  0.10151099976372759; Validation Loss = 0.1239441750689927\n",
            "Cost after 297953 iterations : Training Loss =  0.10151099976372756; Validation Loss = 0.12394417506709188\n",
            "Cost after 297954 iterations : Training Loss =  0.10151099976372754; Validation Loss = 0.12394417506519116\n",
            "Cost after 297955 iterations : Training Loss =  0.10151099976372743; Validation Loss = 0.12394417506329071\n",
            "Cost after 297956 iterations : Training Loss =  0.10151099976372767; Validation Loss = 0.12394417506139004\n",
            "Cost after 297957 iterations : Training Loss =  0.10151099976372743; Validation Loss = 0.12394417505948965\n",
            "Cost after 297958 iterations : Training Loss =  0.10151099976372757; Validation Loss = 0.123944175057589\n",
            "Cost after 297959 iterations : Training Loss =  0.10151099976372727; Validation Loss = 0.12394417505568876\n",
            "Cost after 297960 iterations : Training Loss =  0.10151099976372756; Validation Loss = 0.12394417505378874\n",
            "Cost after 297961 iterations : Training Loss =  0.1015109997637275; Validation Loss = 0.12394417505188862\n",
            "Cost after 297962 iterations : Training Loss =  0.10151099976372738; Validation Loss = 0.1239441750499887\n",
            "Cost after 297963 iterations : Training Loss =  0.10151099976372749; Validation Loss = 0.12394417504808856\n",
            "Cost after 297964 iterations : Training Loss =  0.10151099976372742; Validation Loss = 0.12394417504618918\n",
            "Cost after 297965 iterations : Training Loss =  0.10151099976372736; Validation Loss = 0.12394417504428934\n",
            "Cost after 297966 iterations : Training Loss =  0.10151099976372754; Validation Loss = 0.12394417504238993\n",
            "Cost after 297967 iterations : Training Loss =  0.1015109997637275; Validation Loss = 0.12394417504049039\n",
            "Cost after 297968 iterations : Training Loss =  0.10151099976372743; Validation Loss = 0.12394417503859066\n",
            "Cost after 297969 iterations : Training Loss =  0.10151099976372732; Validation Loss = 0.12394417503669175\n",
            "Cost after 297970 iterations : Training Loss =  0.10151099976372749; Validation Loss = 0.12394417503479199\n",
            "Cost after 297971 iterations : Training Loss =  0.10151099976372756; Validation Loss = 0.1239441750328934\n",
            "Cost after 297972 iterations : Training Loss =  0.1015109997637275; Validation Loss = 0.12394417503099411\n",
            "Cost after 297973 iterations : Training Loss =  0.10151099976372731; Validation Loss = 0.12394417502909544\n",
            "Cost after 297974 iterations : Training Loss =  0.10151099976372735; Validation Loss = 0.12394417502719689\n",
            "Cost after 297975 iterations : Training Loss =  0.10151099976372727; Validation Loss = 0.12394417502529777\n",
            "Cost after 297976 iterations : Training Loss =  0.10151099976372752; Validation Loss = 0.12394417502339891\n",
            "Cost after 297977 iterations : Training Loss =  0.10151099976372725; Validation Loss = 0.12394417502150062\n",
            "Cost after 297978 iterations : Training Loss =  0.10151099976372731; Validation Loss = 0.12394417501960217\n",
            "Cost after 297979 iterations : Training Loss =  0.1015109997637272; Validation Loss = 0.12394417501770377\n",
            "Cost after 297980 iterations : Training Loss =  0.10151099976372742; Validation Loss = 0.12394417501580565\n",
            "Cost after 297981 iterations : Training Loss =  0.10151099976372742; Validation Loss = 0.12394417501390766\n",
            "Cost after 297982 iterations : Training Loss =  0.10151099976372743; Validation Loss = 0.12394417501200973\n",
            "Cost after 297983 iterations : Training Loss =  0.10151099976372746; Validation Loss = 0.12394417501011143\n",
            "Cost after 297984 iterations : Training Loss =  0.10151099976372742; Validation Loss = 0.12394417500821354\n",
            "Cost after 297985 iterations : Training Loss =  0.10151099976372732; Validation Loss = 0.12394417500631613\n",
            "Cost after 297986 iterations : Training Loss =  0.10151099976372735; Validation Loss = 0.12394417500441808\n",
            "Cost after 297987 iterations : Training Loss =  0.10151099976372738; Validation Loss = 0.12394417500252068\n",
            "Cost after 297988 iterations : Training Loss =  0.1015109997637271; Validation Loss = 0.12394417500062327\n",
            "Cost after 297989 iterations : Training Loss =  0.10151099976372725; Validation Loss = 0.12394417499872579\n",
            "Cost after 297990 iterations : Training Loss =  0.10151099976372724; Validation Loss = 0.12394417499682829\n",
            "Cost after 297991 iterations : Training Loss =  0.10151099976372724; Validation Loss = 0.12394417499493131\n",
            "Cost after 297992 iterations : Training Loss =  0.10151099976372736; Validation Loss = 0.12394417499303421\n",
            "Cost after 297993 iterations : Training Loss =  0.10151099976372717; Validation Loss = 0.12394417499113734\n",
            "Cost after 297994 iterations : Training Loss =  0.10151099976372732; Validation Loss = 0.12394417498924046\n",
            "Cost after 297995 iterations : Training Loss =  0.10151099976372731; Validation Loss = 0.12394417498734374\n",
            "Cost after 297996 iterations : Training Loss =  0.10151099976372727; Validation Loss = 0.12394417498544708\n",
            "Cost after 297997 iterations : Training Loss =  0.10151099976372722; Validation Loss = 0.12394417498355025\n",
            "Cost after 297998 iterations : Training Loss =  0.10151099976372717; Validation Loss = 0.12394417498165407\n",
            "Cost after 297999 iterations : Training Loss =  0.10151099976372717; Validation Loss = 0.12394417497975757\n",
            "Cost after 298000 iterations : Training Loss =  0.10151099976372724; Validation Loss = 0.12394417497786116\n",
            "Cost after 298001 iterations : Training Loss =  0.1015109997637272; Validation Loss = 0.12394417497596485\n",
            "Cost after 298002 iterations : Training Loss =  0.10151099976372731; Validation Loss = 0.12394417497406869\n",
            "Cost after 298003 iterations : Training Loss =  0.10151099976372731; Validation Loss = 0.12394417497217265\n",
            "Cost after 298004 iterations : Training Loss =  0.10151099976372725; Validation Loss = 0.12394417497027663\n",
            "Cost after 298005 iterations : Training Loss =  0.10151099976372714; Validation Loss = 0.12394417496838099\n",
            "Cost after 298006 iterations : Training Loss =  0.10151099976372722; Validation Loss = 0.12394417496648523\n",
            "Cost after 298007 iterations : Training Loss =  0.10151099976372731; Validation Loss = 0.1239441749645897\n",
            "Cost after 298008 iterations : Training Loss =  0.10151099976372731; Validation Loss = 0.1239441749626943\n",
            "Cost after 298009 iterations : Training Loss =  0.10151099976372711; Validation Loss = 0.12394417496079839\n",
            "Cost after 298010 iterations : Training Loss =  0.10151099976372713; Validation Loss = 0.12394417495890332\n",
            "Cost after 298011 iterations : Training Loss =  0.1015109997637272; Validation Loss = 0.12394417495700827\n",
            "Cost after 298012 iterations : Training Loss =  0.1015109997637271; Validation Loss = 0.12394417495511294\n",
            "Cost after 298013 iterations : Training Loss =  0.10151099976372704; Validation Loss = 0.12394417495321772\n",
            "Cost after 298014 iterations : Training Loss =  0.10151099976372717; Validation Loss = 0.12394417495132289\n",
            "Cost after 298015 iterations : Training Loss =  0.10151099976372711; Validation Loss = 0.12394417494942804\n",
            "Cost after 298016 iterations : Training Loss =  0.10151099976372713; Validation Loss = 0.12394417494753313\n",
            "Cost after 298017 iterations : Training Loss =  0.10151099976372713; Validation Loss = 0.12394417494563849\n",
            "Cost after 298018 iterations : Training Loss =  0.10151099976372727; Validation Loss = 0.12394417494374399\n",
            "Cost after 298019 iterations : Training Loss =  0.10151099976372707; Validation Loss = 0.1239441749418496\n",
            "Cost after 298020 iterations : Training Loss =  0.1015109997637271; Validation Loss = 0.12394417493995505\n",
            "Cost after 298021 iterations : Training Loss =  0.10151099976372722; Validation Loss = 0.12394417493806055\n",
            "Cost after 298022 iterations : Training Loss =  0.10151099976372724; Validation Loss = 0.12394417493616625\n",
            "Cost after 298023 iterations : Training Loss =  0.10151099976372706; Validation Loss = 0.12394417493427234\n",
            "Cost after 298024 iterations : Training Loss =  0.1015109997637272; Validation Loss = 0.12394417493237841\n",
            "Cost after 298025 iterations : Training Loss =  0.10151099976372702; Validation Loss = 0.12394417493048437\n",
            "Cost after 298026 iterations : Training Loss =  0.10151099976372714; Validation Loss = 0.12394417492859099\n",
            "Cost after 298027 iterations : Training Loss =  0.10151099976372717; Validation Loss = 0.12394417492669707\n",
            "Cost after 298028 iterations : Training Loss =  0.10151099976372711; Validation Loss = 0.12394417492480306\n",
            "Cost after 298029 iterations : Training Loss =  0.10151099976372714; Validation Loss = 0.12394417492290963\n",
            "Cost after 298030 iterations : Training Loss =  0.10151099976372711; Validation Loss = 0.12394417492101635\n",
            "Cost after 298031 iterations : Training Loss =  0.10151099976372702; Validation Loss = 0.12394417491912314\n",
            "Cost after 298032 iterations : Training Loss =  0.10151099976372707; Validation Loss = 0.12394417491722984\n",
            "Cost after 298033 iterations : Training Loss =  0.10151099976372717; Validation Loss = 0.12394417491533685\n",
            "Cost after 298034 iterations : Training Loss =  0.10151099976372722; Validation Loss = 0.12394417491344378\n",
            "Cost after 298035 iterations : Training Loss =  0.10151099976372699; Validation Loss = 0.12394417491155064\n",
            "Cost after 298036 iterations : Training Loss =  0.10151099976372713; Validation Loss = 0.12394417490965787\n",
            "Cost after 298037 iterations : Training Loss =  0.10151099976372724; Validation Loss = 0.12394417490776541\n",
            "Cost after 298038 iterations : Training Loss =  0.10151099976372698; Validation Loss = 0.12394417490587259\n",
            "Cost after 298039 iterations : Training Loss =  0.1015109997637271; Validation Loss = 0.12394417490397995\n",
            "Cost after 298040 iterations : Training Loss =  0.10151099976372702; Validation Loss = 0.12394417490208755\n",
            "Cost after 298041 iterations : Training Loss =  0.101510999763727; Validation Loss = 0.12394417490019483\n",
            "Cost after 298042 iterations : Training Loss =  0.10151099976372711; Validation Loss = 0.12394417489830271\n",
            "Cost after 298043 iterations : Training Loss =  0.10151099976372706; Validation Loss = 0.12394417489641053\n",
            "Cost after 298044 iterations : Training Loss =  0.10151099976372713; Validation Loss = 0.12394417489451869\n",
            "Cost after 298045 iterations : Training Loss =  0.10151099976372714; Validation Loss = 0.12394417489262638\n",
            "Cost after 298046 iterations : Training Loss =  0.10151099976372713; Validation Loss = 0.12394417489073493\n",
            "Cost after 298047 iterations : Training Loss =  0.10151099976372699; Validation Loss = 0.1239441748888429\n",
            "Cost after 298048 iterations : Training Loss =  0.10151099976372707; Validation Loss = 0.12394417488695111\n",
            "Cost after 298049 iterations : Training Loss =  0.10151099976372717; Validation Loss = 0.12394417488505927\n",
            "Cost after 298050 iterations : Training Loss =  0.10151099976372702; Validation Loss = 0.12394417488316802\n",
            "Cost after 298051 iterations : Training Loss =  0.10151099976372686; Validation Loss = 0.12394417488127663\n",
            "Cost after 298052 iterations : Training Loss =  0.10151099976372695; Validation Loss = 0.12394417487938551\n",
            "Cost after 298053 iterations : Training Loss =  0.10151099976372692; Validation Loss = 0.12394417487749422\n",
            "Cost after 298054 iterations : Training Loss =  0.10151099976372699; Validation Loss = 0.12394417487560276\n",
            "Cost after 298055 iterations : Training Loss =  0.10151099976372707; Validation Loss = 0.12394417487371193\n",
            "Cost after 298056 iterations : Training Loss =  0.10151099976372702; Validation Loss = 0.12394417487182122\n",
            "Cost after 298057 iterations : Training Loss =  0.10151099976372714; Validation Loss = 0.12394417486993015\n",
            "Cost after 298058 iterations : Training Loss =  0.10151099976372699; Validation Loss = 0.12394417486803988\n",
            "Cost after 298059 iterations : Training Loss =  0.10151099976372693; Validation Loss = 0.12394417486614863\n",
            "Cost after 298060 iterations : Training Loss =  0.101510999763727; Validation Loss = 0.12394417486425821\n",
            "Cost after 298061 iterations : Training Loss =  0.10151099976372693; Validation Loss = 0.12394417486236779\n",
            "Cost after 298062 iterations : Training Loss =  0.1015109997637269; Validation Loss = 0.12394417486047718\n",
            "Cost after 298063 iterations : Training Loss =  0.10151099976372707; Validation Loss = 0.12394417485858714\n",
            "Cost after 298064 iterations : Training Loss =  0.10151099976372698; Validation Loss = 0.12394417485669697\n",
            "Cost after 298065 iterations : Training Loss =  0.10151099976372706; Validation Loss = 0.1239441748548069\n",
            "Cost after 298066 iterations : Training Loss =  0.10151099976372675; Validation Loss = 0.1239441748529171\n",
            "Cost after 298067 iterations : Training Loss =  0.10151099976372693; Validation Loss = 0.12394417485102699\n",
            "Cost after 298068 iterations : Training Loss =  0.10151099976372718; Validation Loss = 0.12394417484913753\n",
            "Cost after 298069 iterations : Training Loss =  0.10151099976372693; Validation Loss = 0.12394417484724748\n",
            "Cost after 298070 iterations : Training Loss =  0.10151099976372688; Validation Loss = 0.12394417484535812\n",
            "Cost after 298071 iterations : Training Loss =  0.10151099976372706; Validation Loss = 0.12394417484346885\n",
            "Cost after 298072 iterations : Training Loss =  0.10151099976372693; Validation Loss = 0.1239441748415792\n",
            "Cost after 298073 iterations : Training Loss =  0.10151099976372675; Validation Loss = 0.12394417483968966\n",
            "Cost after 298074 iterations : Training Loss =  0.10151099976372699; Validation Loss = 0.12394417483780046\n",
            "Cost after 298075 iterations : Training Loss =  0.1015109997637268; Validation Loss = 0.12394417483591161\n",
            "Cost after 298076 iterations : Training Loss =  0.10151099976372682; Validation Loss = 0.1239441748340227\n",
            "Cost after 298077 iterations : Training Loss =  0.10151099976372681; Validation Loss = 0.1239441748321336\n",
            "Cost after 298078 iterations : Training Loss =  0.10151099976372702; Validation Loss = 0.12394417483024478\n",
            "Cost after 298079 iterations : Training Loss =  0.10151099976372686; Validation Loss = 0.12394417482835625\n",
            "Cost after 298080 iterations : Training Loss =  0.10151099976372698; Validation Loss = 0.12394417482646758\n",
            "Cost after 298081 iterations : Training Loss =  0.10151099976372681; Validation Loss = 0.123944174824579\n",
            "Cost after 298082 iterations : Training Loss =  0.10151099976372692; Validation Loss = 0.12394417482269061\n",
            "Cost after 298083 iterations : Training Loss =  0.10151099976372681; Validation Loss = 0.12394417482080201\n",
            "Cost after 298084 iterations : Training Loss =  0.10151099976372698; Validation Loss = 0.12394417481891398\n",
            "Cost after 298085 iterations : Training Loss =  0.10151099976372686; Validation Loss = 0.12394417481702574\n",
            "Cost after 298086 iterations : Training Loss =  0.10151099976372688; Validation Loss = 0.12394417481513774\n",
            "Cost after 298087 iterations : Training Loss =  0.10151099976372682; Validation Loss = 0.12394417481324972\n",
            "Cost after 298088 iterations : Training Loss =  0.10151099976372693; Validation Loss = 0.12394417481136212\n",
            "Cost after 298089 iterations : Training Loss =  0.10151099976372698; Validation Loss = 0.12394417480947421\n",
            "Cost after 298090 iterations : Training Loss =  0.1015109997637269; Validation Loss = 0.12394417480758677\n",
            "Cost after 298091 iterations : Training Loss =  0.10151099976372682; Validation Loss = 0.12394417480569907\n",
            "Cost after 298092 iterations : Training Loss =  0.10151099976372678; Validation Loss = 0.12394417480381162\n",
            "Cost after 298093 iterations : Training Loss =  0.10151099976372682; Validation Loss = 0.1239441748019242\n",
            "Cost after 298094 iterations : Training Loss =  0.10151099976372661; Validation Loss = 0.12394417480003686\n",
            "Cost after 298095 iterations : Training Loss =  0.10151099976372695; Validation Loss = 0.12394417479814976\n",
            "Cost after 298096 iterations : Training Loss =  0.10151099976372681; Validation Loss = 0.12394417479626285\n",
            "Cost after 298097 iterations : Training Loss =  0.1015109997637268; Validation Loss = 0.12394417479437575\n",
            "Cost after 298098 iterations : Training Loss =  0.10151099976372674; Validation Loss = 0.12394417479248901\n",
            "Cost after 298099 iterations : Training Loss =  0.10151099976372688; Validation Loss = 0.12394417479060181\n",
            "Cost after 298100 iterations : Training Loss =  0.10151099976372688; Validation Loss = 0.12394417478871518\n",
            "Cost after 298101 iterations : Training Loss =  0.1015109997637267; Validation Loss = 0.12394417478682872\n",
            "Cost after 298102 iterations : Training Loss =  0.10151099976372675; Validation Loss = 0.12394417478494205\n",
            "Cost after 298103 iterations : Training Loss =  0.10151099976372682; Validation Loss = 0.12394417478305564\n",
            "Cost after 298104 iterations : Training Loss =  0.1015109997637267; Validation Loss = 0.12394417478116929\n",
            "Cost after 298105 iterations : Training Loss =  0.10151099976372675; Validation Loss = 0.12394417477928336\n",
            "Cost after 298106 iterations : Training Loss =  0.10151099976372685; Validation Loss = 0.1239441747773968\n",
            "Cost after 298107 iterations : Training Loss =  0.10151099976372678; Validation Loss = 0.12394417477551083\n",
            "Cost after 298108 iterations : Training Loss =  0.10151099976372685; Validation Loss = 0.1239441747736251\n",
            "Cost after 298109 iterations : Training Loss =  0.10151099976372692; Validation Loss = 0.12394417477173898\n",
            "Cost after 298110 iterations : Training Loss =  0.10151099976372682; Validation Loss = 0.12394417476985328\n",
            "Cost after 298111 iterations : Training Loss =  0.10151099976372673; Validation Loss = 0.12394417476796772\n",
            "Cost after 298112 iterations : Training Loss =  0.10151099976372674; Validation Loss = 0.1239441747660823\n",
            "Cost after 298113 iterations : Training Loss =  0.10151099976372682; Validation Loss = 0.12394417476419671\n",
            "Cost after 298114 iterations : Training Loss =  0.10151099976372673; Validation Loss = 0.12394417476231112\n",
            "Cost after 298115 iterations : Training Loss =  0.10151099976372668; Validation Loss = 0.12394417476042606\n",
            "Cost after 298116 iterations : Training Loss =  0.10151099976372673; Validation Loss = 0.12394417475854085\n",
            "Cost after 298117 iterations : Training Loss =  0.10151099976372668; Validation Loss = 0.123944174756656\n",
            "Cost after 298118 iterations : Training Loss =  0.1015109997637268; Validation Loss = 0.12394417475477065\n",
            "Cost after 298119 iterations : Training Loss =  0.10151099976372675; Validation Loss = 0.1239441747528857\n",
            "Cost after 298120 iterations : Training Loss =  0.1015109997637266; Validation Loss = 0.12394417475100106\n",
            "Cost after 298121 iterations : Training Loss =  0.1015109997637266; Validation Loss = 0.12394417474911613\n",
            "Cost after 298122 iterations : Training Loss =  0.10151099976372666; Validation Loss = 0.12394417474723177\n",
            "Cost after 298123 iterations : Training Loss =  0.10151099976372667; Validation Loss = 0.12394417474534748\n",
            "Cost after 298124 iterations : Training Loss =  0.10151099976372661; Validation Loss = 0.12394417474346289\n",
            "Cost after 298125 iterations : Training Loss =  0.10151099976372673; Validation Loss = 0.12394417474157886\n",
            "Cost after 298126 iterations : Training Loss =  0.10151099976372661; Validation Loss = 0.12394417473969448\n",
            "Cost after 298127 iterations : Training Loss =  0.1015109997637266; Validation Loss = 0.12394417473781032\n",
            "Cost after 298128 iterations : Training Loss =  0.10151099976372666; Validation Loss = 0.12394417473592638\n",
            "Cost after 298129 iterations : Training Loss =  0.10151099976372654; Validation Loss = 0.1239441747340423\n",
            "Cost after 298130 iterations : Training Loss =  0.1015109997637266; Validation Loss = 0.12394417473215837\n",
            "Cost after 298131 iterations : Training Loss =  0.1015109997637266; Validation Loss = 0.12394417473027453\n",
            "Cost after 298132 iterations : Training Loss =  0.10151099976372646; Validation Loss = 0.12394417472839077\n",
            "Cost after 298133 iterations : Training Loss =  0.10151099976372653; Validation Loss = 0.12394417472650777\n",
            "Cost after 298134 iterations : Training Loss =  0.1015109997637267; Validation Loss = 0.1239441747246239\n",
            "Cost after 298135 iterations : Training Loss =  0.10151099976372674; Validation Loss = 0.12394417472274062\n",
            "Cost after 298136 iterations : Training Loss =  0.10151099976372686; Validation Loss = 0.12394417472085756\n",
            "Cost after 298137 iterations : Training Loss =  0.10151099976372656; Validation Loss = 0.12394417471897395\n",
            "Cost after 298138 iterations : Training Loss =  0.10151099976372657; Validation Loss = 0.12394417471709136\n",
            "Cost after 298139 iterations : Training Loss =  0.1015109997637267; Validation Loss = 0.12394417471520804\n",
            "Cost after 298140 iterations : Training Loss =  0.10151099976372675; Validation Loss = 0.12394417471332521\n",
            "Cost after 298141 iterations : Training Loss =  0.10151099976372667; Validation Loss = 0.12394417471144248\n",
            "Cost after 298142 iterations : Training Loss =  0.10151099976372654; Validation Loss = 0.12394417470955965\n",
            "Cost after 298143 iterations : Training Loss =  0.10151099976372668; Validation Loss = 0.12394417470767724\n",
            "Cost after 298144 iterations : Training Loss =  0.10151099976372657; Validation Loss = 0.12394417470579452\n",
            "Cost after 298145 iterations : Training Loss =  0.10151099976372654; Validation Loss = 0.1239441747039123\n",
            "Cost after 298146 iterations : Training Loss =  0.10151099976372673; Validation Loss = 0.12394417470202992\n",
            "Cost after 298147 iterations : Training Loss =  0.10151099976372661; Validation Loss = 0.12394417470014783\n",
            "Cost after 298148 iterations : Training Loss =  0.1015109997637266; Validation Loss = 0.1239441746982655\n",
            "Cost after 298149 iterations : Training Loss =  0.10151099976372678; Validation Loss = 0.12394417469638327\n",
            "Cost after 298150 iterations : Training Loss =  0.10151099976372643; Validation Loss = 0.1239441746945015\n",
            "Cost after 298151 iterations : Training Loss =  0.10151099976372661; Validation Loss = 0.1239441746926197\n",
            "Cost after 298152 iterations : Training Loss =  0.10151099976372656; Validation Loss = 0.12394417469073789\n",
            "Cost after 298153 iterations : Training Loss =  0.10151099976372653; Validation Loss = 0.12394417468885639\n",
            "Cost after 298154 iterations : Training Loss =  0.10151099976372656; Validation Loss = 0.12394417468697495\n",
            "Cost after 298155 iterations : Training Loss =  0.10151099976372653; Validation Loss = 0.12394417468509326\n",
            "Cost after 298156 iterations : Training Loss =  0.10151099976372656; Validation Loss = 0.12394417468321198\n",
            "Cost after 298157 iterations : Training Loss =  0.10151099976372649; Validation Loss = 0.12394417468133105\n",
            "Cost after 298158 iterations : Training Loss =  0.10151099976372656; Validation Loss = 0.1239441746794496\n",
            "Cost after 298159 iterations : Training Loss =  0.10151099976372648; Validation Loss = 0.12394417467756869\n",
            "Cost after 298160 iterations : Training Loss =  0.10151099976372648; Validation Loss = 0.12394417467568743\n",
            "Cost after 298161 iterations : Training Loss =  0.10151099976372663; Validation Loss = 0.12394417467380675\n",
            "Cost after 298162 iterations : Training Loss =  0.10151099976372654; Validation Loss = 0.12394417467192587\n",
            "Cost after 298163 iterations : Training Loss =  0.10151099976372657; Validation Loss = 0.12394417467004522\n",
            "Cost after 298164 iterations : Training Loss =  0.10151099976372635; Validation Loss = 0.12394417466816464\n",
            "Cost after 298165 iterations : Training Loss =  0.10151099976372667; Validation Loss = 0.123944174666284\n",
            "Cost after 298166 iterations : Training Loss =  0.1015109997637265; Validation Loss = 0.12394417466440372\n",
            "Cost after 298167 iterations : Training Loss =  0.10151099976372638; Validation Loss = 0.12394417466252353\n",
            "Cost after 298168 iterations : Training Loss =  0.10151099976372648; Validation Loss = 0.12394417466064316\n",
            "Cost after 298169 iterations : Training Loss =  0.10151099976372646; Validation Loss = 0.12394417465876298\n",
            "Cost after 298170 iterations : Training Loss =  0.10151099976372636; Validation Loss = 0.1239441746568827\n",
            "Cost after 298171 iterations : Training Loss =  0.10151099976372638; Validation Loss = 0.12394417465500325\n",
            "Cost after 298172 iterations : Training Loss =  0.10151099976372656; Validation Loss = 0.12394417465312327\n",
            "Cost after 298173 iterations : Training Loss =  0.10151099976372643; Validation Loss = 0.12394417465124363\n",
            "Cost after 298174 iterations : Training Loss =  0.10151099976372635; Validation Loss = 0.1239441746493642\n",
            "Cost after 298175 iterations : Training Loss =  0.10151099976372649; Validation Loss = 0.1239441746474845\n",
            "Cost after 298176 iterations : Training Loss =  0.10151099976372646; Validation Loss = 0.12394417464560512\n",
            "Cost after 298177 iterations : Training Loss =  0.10151099976372646; Validation Loss = 0.1239441746437257\n",
            "Cost after 298178 iterations : Training Loss =  0.10151099976372643; Validation Loss = 0.12394417464184633\n",
            "Cost after 298179 iterations : Training Loss =  0.1015109997637264; Validation Loss = 0.12394417463996708\n",
            "Cost after 298180 iterations : Training Loss =  0.10151099976372648; Validation Loss = 0.12394417463808821\n",
            "Cost after 298181 iterations : Training Loss =  0.10151099976372653; Validation Loss = 0.1239441746362092\n",
            "Cost after 298182 iterations : Training Loss =  0.1015109997637264; Validation Loss = 0.12394417463433036\n",
            "Cost after 298183 iterations : Training Loss =  0.1015109997637266; Validation Loss = 0.12394417463245137\n",
            "Cost after 298184 iterations : Training Loss =  0.10151099976372618; Validation Loss = 0.12394417463057282\n",
            "Cost after 298185 iterations : Training Loss =  0.10151099976372635; Validation Loss = 0.1239441746286941\n",
            "Cost after 298186 iterations : Training Loss =  0.10151099976372649; Validation Loss = 0.12394417462681552\n",
            "Cost after 298187 iterations : Training Loss =  0.10151099976372636; Validation Loss = 0.12394417462493731\n",
            "Cost after 298188 iterations : Training Loss =  0.10151099976372623; Validation Loss = 0.12394417462305903\n",
            "Cost after 298189 iterations : Training Loss =  0.10151099976372643; Validation Loss = 0.12394417462118072\n",
            "Cost after 298190 iterations : Training Loss =  0.1015109997637264; Validation Loss = 0.12394417461930267\n",
            "Cost after 298191 iterations : Training Loss =  0.10151099976372635; Validation Loss = 0.12394417461742452\n",
            "Cost after 298192 iterations : Training Loss =  0.10151099976372628; Validation Loss = 0.12394417461554669\n",
            "Cost after 298193 iterations : Training Loss =  0.10151099976372643; Validation Loss = 0.12394417461366906\n",
            "Cost after 298194 iterations : Training Loss =  0.1015109997637264; Validation Loss = 0.12394417461179137\n",
            "Cost after 298195 iterations : Training Loss =  0.10151099976372631; Validation Loss = 0.12394417460991349\n",
            "Cost after 298196 iterations : Training Loss =  0.10151099976372642; Validation Loss = 0.12394417460803608\n",
            "Cost after 298197 iterations : Training Loss =  0.10151099976372636; Validation Loss = 0.12394417460615843\n",
            "Cost after 298198 iterations : Training Loss =  0.1015109997637263; Validation Loss = 0.12394417460428134\n",
            "Cost after 298199 iterations : Training Loss =  0.10151099976372635; Validation Loss = 0.12394417460240409\n",
            "Cost after 298200 iterations : Training Loss =  0.1015109997637265; Validation Loss = 0.12394417460052694\n",
            "Cost after 298201 iterations : Training Loss =  0.1015109997637263; Validation Loss = 0.12394417459864981\n",
            "Cost after 298202 iterations : Training Loss =  0.10151099976372642; Validation Loss = 0.12394417459677268\n",
            "Cost after 298203 iterations : Training Loss =  0.10151099976372618; Validation Loss = 0.12394417459489603\n",
            "Cost after 298204 iterations : Training Loss =  0.10151099976372611; Validation Loss = 0.12394417459301928\n",
            "Cost after 298205 iterations : Training Loss =  0.10151099976372642; Validation Loss = 0.12394417459114253\n",
            "Cost after 298206 iterations : Training Loss =  0.10151099976372625; Validation Loss = 0.12394417458926596\n",
            "Cost after 298207 iterations : Training Loss =  0.10151099976372624; Validation Loss = 0.12394417458738934\n",
            "Cost after 298208 iterations : Training Loss =  0.10151099976372617; Validation Loss = 0.12394417458551286\n",
            "Cost after 298209 iterations : Training Loss =  0.10151099976372636; Validation Loss = 0.123944174583637\n",
            "Cost after 298210 iterations : Training Loss =  0.10151099976372624; Validation Loss = 0.12394417458176055\n",
            "Cost after 298211 iterations : Training Loss =  0.10151099976372634; Validation Loss = 0.12394417457988448\n",
            "Cost after 298212 iterations : Training Loss =  0.10151099976372638; Validation Loss = 0.12394417457800845\n",
            "Cost after 298213 iterations : Training Loss =  0.1015109997637264; Validation Loss = 0.12394417457613262\n",
            "Cost after 298214 iterations : Training Loss =  0.10151099976372624; Validation Loss = 0.12394417457425676\n",
            "Cost after 298215 iterations : Training Loss =  0.10151099976372618; Validation Loss = 0.12394417457238077\n",
            "Cost after 298216 iterations : Training Loss =  0.10151099976372624; Validation Loss = 0.12394417457050524\n",
            "Cost after 298217 iterations : Training Loss =  0.10151099976372631; Validation Loss = 0.12394417456862958\n",
            "Cost after 298218 iterations : Training Loss =  0.10151099976372631; Validation Loss = 0.12394417456675427\n",
            "Cost after 298219 iterations : Training Loss =  0.10151099976372635; Validation Loss = 0.12394417456487951\n",
            "Cost after 298220 iterations : Training Loss =  0.10151099976372631; Validation Loss = 0.1239441745630037\n",
            "Cost after 298221 iterations : Training Loss =  0.10151099976372643; Validation Loss = 0.12394417456112841\n",
            "Cost after 298222 iterations : Training Loss =  0.10151099976372631; Validation Loss = 0.12394417455925333\n",
            "Cost after 298223 iterations : Training Loss =  0.10151099976372617; Validation Loss = 0.12394417455737827\n",
            "Cost after 298224 iterations : Training Loss =  0.10151099976372634; Validation Loss = 0.1239441745555037\n",
            "Cost after 298225 iterations : Training Loss =  0.10151099976372638; Validation Loss = 0.12394417455362901\n",
            "Cost after 298226 iterations : Training Loss =  0.1015109997637264; Validation Loss = 0.12394417455175431\n",
            "Cost after 298227 iterations : Training Loss =  0.10151099976372625; Validation Loss = 0.12394417454987988\n",
            "Cost after 298228 iterations : Training Loss =  0.10151099976372609; Validation Loss = 0.12394417454800509\n",
            "Cost after 298229 iterations : Training Loss =  0.10151099976372621; Validation Loss = 0.12394417454613094\n",
            "Cost after 298230 iterations : Training Loss =  0.1015109997637263; Validation Loss = 0.12394417454425663\n",
            "Cost after 298231 iterations : Training Loss =  0.10151099976372613; Validation Loss = 0.1239441745423824\n",
            "Cost after 298232 iterations : Training Loss =  0.10151099976372603; Validation Loss = 0.12394417454050828\n",
            "Cost after 298233 iterations : Training Loss =  0.10151099976372625; Validation Loss = 0.12394417453863427\n",
            "Cost after 298234 iterations : Training Loss =  0.10151099976372606; Validation Loss = 0.12394417453676018\n",
            "Cost after 298235 iterations : Training Loss =  0.1015109997637264; Validation Loss = 0.12394417453488649\n",
            "Cost after 298236 iterations : Training Loss =  0.10151099976372606; Validation Loss = 0.12394417453301255\n",
            "Cost after 298237 iterations : Training Loss =  0.1015109997637263; Validation Loss = 0.123944174531139\n",
            "Cost after 298238 iterations : Training Loss =  0.1015109997637261; Validation Loss = 0.12394417452926565\n",
            "Cost after 298239 iterations : Training Loss =  0.10151099976372618; Validation Loss = 0.12394417452739231\n",
            "Cost after 298240 iterations : Training Loss =  0.10151099976372623; Validation Loss = 0.12394417452551885\n",
            "Cost after 298241 iterations : Training Loss =  0.10151099976372616; Validation Loss = 0.12394417452364533\n",
            "Cost after 298242 iterations : Training Loss =  0.10151099976372606; Validation Loss = 0.12394417452177237\n",
            "Cost after 298243 iterations : Training Loss =  0.10151099976372624; Validation Loss = 0.12394417451989924\n",
            "Cost after 298244 iterations : Training Loss =  0.10151099976372621; Validation Loss = 0.12394417451802625\n",
            "Cost after 298245 iterations : Training Loss =  0.10151099976372605; Validation Loss = 0.12394417451615368\n",
            "Cost after 298246 iterations : Training Loss =  0.10151099976372624; Validation Loss = 0.12394417451428062\n",
            "Cost after 298247 iterations : Training Loss =  0.10151099976372613; Validation Loss = 0.12394417451240802\n",
            "Cost after 298248 iterations : Training Loss =  0.10151099976372602; Validation Loss = 0.12394417451053541\n",
            "Cost after 298249 iterations : Training Loss =  0.10151099976372623; Validation Loss = 0.12394417450866298\n",
            "Cost after 298250 iterations : Training Loss =  0.10151099976372609; Validation Loss = 0.1239441745067906\n",
            "Cost after 298251 iterations : Training Loss =  0.10151099976372616; Validation Loss = 0.12394417450491836\n",
            "Cost after 298252 iterations : Training Loss =  0.10151099976372623; Validation Loss = 0.12394417450304587\n",
            "Cost after 298253 iterations : Training Loss =  0.10151099976372598; Validation Loss = 0.12394417450117393\n",
            "Cost after 298254 iterations : Training Loss =  0.10151099976372625; Validation Loss = 0.12394417449930188\n",
            "Cost after 298255 iterations : Training Loss =  0.10151099976372621; Validation Loss = 0.12394417449742996\n",
            "Cost after 298256 iterations : Training Loss =  0.1015109997637261; Validation Loss = 0.12394417449555796\n",
            "Cost after 298257 iterations : Training Loss =  0.10151099976372623; Validation Loss = 0.12394417449368658\n",
            "Cost after 298258 iterations : Training Loss =  0.10151099976372606; Validation Loss = 0.12394417449181468\n",
            "Cost after 298259 iterations : Training Loss =  0.10151099976372609; Validation Loss = 0.12394417448994333\n",
            "Cost after 298260 iterations : Training Loss =  0.10151099976372605; Validation Loss = 0.1239441744880718\n",
            "Cost after 298261 iterations : Training Loss =  0.10151099976372611; Validation Loss = 0.12394417448620047\n",
            "Cost after 298262 iterations : Training Loss =  0.1015109997637261; Validation Loss = 0.12394417448432922\n",
            "Cost after 298263 iterations : Training Loss =  0.10151099976372598; Validation Loss = 0.12394417448245819\n",
            "Cost after 298264 iterations : Training Loss =  0.10151099976372613; Validation Loss = 0.12394417448058706\n",
            "Cost after 298265 iterations : Training Loss =  0.10151099976372592; Validation Loss = 0.12394417447871593\n",
            "Cost after 298266 iterations : Training Loss =  0.10151099976372598; Validation Loss = 0.12394417447684505\n",
            "Cost after 298267 iterations : Training Loss =  0.10151099976372611; Validation Loss = 0.12394417447497422\n",
            "Cost after 298268 iterations : Training Loss =  0.10151099976372585; Validation Loss = 0.12394417447310362\n",
            "Cost after 298269 iterations : Training Loss =  0.10151099976372603; Validation Loss = 0.12394417447123321\n",
            "Cost after 298270 iterations : Training Loss =  0.10151099976372618; Validation Loss = 0.12394417446936258\n",
            "Cost after 298271 iterations : Training Loss =  0.10151099976372603; Validation Loss = 0.12394417446749202\n",
            "Cost after 298272 iterations : Training Loss =  0.1015109997637261; Validation Loss = 0.12394417446562195\n",
            "Cost after 298273 iterations : Training Loss =  0.10151099976372605; Validation Loss = 0.12394417446375172\n",
            "Cost after 298274 iterations : Training Loss =  0.10151099976372602; Validation Loss = 0.12394417446188165\n",
            "Cost after 298275 iterations : Training Loss =  0.10151099976372593; Validation Loss = 0.12394417446001152\n",
            "Cost after 298276 iterations : Training Loss =  0.1015109997637258; Validation Loss = 0.12394417445814153\n",
            "Cost after 298277 iterations : Training Loss =  0.10151099976372592; Validation Loss = 0.12394417445627183\n",
            "Cost after 298278 iterations : Training Loss =  0.10151099976372602; Validation Loss = 0.1239441744544018\n",
            "Cost after 298279 iterations : Training Loss =  0.10151099976372586; Validation Loss = 0.12394417445253247\n",
            "Cost after 298280 iterations : Training Loss =  0.10151099976372581; Validation Loss = 0.12394417445066289\n",
            "Cost after 298281 iterations : Training Loss =  0.10151099976372592; Validation Loss = 0.12394417444879303\n",
            "Cost after 298282 iterations : Training Loss =  0.10151099976372592; Validation Loss = 0.12394417444692411\n",
            "Cost after 298283 iterations : Training Loss =  0.10151099976372602; Validation Loss = 0.12394417444505484\n",
            "Cost after 298284 iterations : Training Loss =  0.10151099976372609; Validation Loss = 0.12394417444318558\n",
            "Cost after 298285 iterations : Training Loss =  0.10151099976372599; Validation Loss = 0.12394417444131663\n",
            "Cost after 298286 iterations : Training Loss =  0.1015109997637261; Validation Loss = 0.12394417443944751\n",
            "Cost after 298287 iterations : Training Loss =  0.10151099976372596; Validation Loss = 0.12394417443757863\n",
            "Cost after 298288 iterations : Training Loss =  0.10151099976372589; Validation Loss = 0.12394417443570983\n",
            "Cost after 298289 iterations : Training Loss =  0.1015109997637259; Validation Loss = 0.12394417443384122\n",
            "Cost after 298290 iterations : Training Loss =  0.10151099976372596; Validation Loss = 0.12394417443197255\n",
            "Cost after 298291 iterations : Training Loss =  0.1015109997637259; Validation Loss = 0.12394417443010403\n",
            "Cost after 298292 iterations : Training Loss =  0.1015109997637259; Validation Loss = 0.12394417442823581\n",
            "Cost after 298293 iterations : Training Loss =  0.10151099976372598; Validation Loss = 0.12394417442636722\n",
            "Cost after 298294 iterations : Training Loss =  0.10151099976372574; Validation Loss = 0.1239441744244989\n",
            "Cost after 298295 iterations : Training Loss =  0.10151099976372573; Validation Loss = 0.12394417442263095\n",
            "Cost after 298296 iterations : Training Loss =  0.10151099976372602; Validation Loss = 0.12394417442076322\n",
            "Cost after 298297 iterations : Training Loss =  0.10151099976372584; Validation Loss = 0.12394417441889513\n",
            "Cost after 298298 iterations : Training Loss =  0.10151099976372586; Validation Loss = 0.12394417441702744\n",
            "Cost after 298299 iterations : Training Loss =  0.10151099976372578; Validation Loss = 0.12394417441515941\n",
            "Cost after 298300 iterations : Training Loss =  0.10151099976372586; Validation Loss = 0.1239441744132917\n",
            "Cost after 298301 iterations : Training Loss =  0.10151099976372606; Validation Loss = 0.12394417441142407\n",
            "Cost after 298302 iterations : Training Loss =  0.10151099976372584; Validation Loss = 0.12394417440955666\n",
            "Cost after 298303 iterations : Training Loss =  0.10151099976372592; Validation Loss = 0.12394417440768943\n",
            "Cost after 298304 iterations : Training Loss =  0.10151099976372584; Validation Loss = 0.12394417440582232\n",
            "Cost after 298305 iterations : Training Loss =  0.10151099976372568; Validation Loss = 0.12394417440395483\n",
            "Cost after 298306 iterations : Training Loss =  0.10151099976372578; Validation Loss = 0.1239441744020879\n",
            "Cost after 298307 iterations : Training Loss =  0.10151099976372584; Validation Loss = 0.123944174400221\n",
            "Cost after 298308 iterations : Training Loss =  0.10151099976372571; Validation Loss = 0.12394417439835408\n",
            "Cost after 298309 iterations : Training Loss =  0.10151099976372573; Validation Loss = 0.12394417439648721\n",
            "Cost after 298310 iterations : Training Loss =  0.10151099976372574; Validation Loss = 0.12394417439462055\n",
            "Cost after 298311 iterations : Training Loss =  0.10151099976372593; Validation Loss = 0.12394417439275429\n",
            "Cost after 298312 iterations : Training Loss =  0.10151099976372581; Validation Loss = 0.12394417439088735\n",
            "Cost after 298313 iterations : Training Loss =  0.10151099976372581; Validation Loss = 0.12394417438902101\n",
            "Cost after 298314 iterations : Training Loss =  0.10151099976372592; Validation Loss = 0.12394417438715459\n",
            "Cost after 298315 iterations : Training Loss =  0.1015109997637258; Validation Loss = 0.12394417438528849\n",
            "Cost after 298316 iterations : Training Loss =  0.10151099976372573; Validation Loss = 0.12394417438342234\n",
            "Cost after 298317 iterations : Training Loss =  0.10151099976372599; Validation Loss = 0.12394417438155646\n",
            "Cost after 298318 iterations : Training Loss =  0.10151099976372578; Validation Loss = 0.12394417437969021\n",
            "Cost after 298319 iterations : Training Loss =  0.10151099976372585; Validation Loss = 0.12394417437782457\n",
            "Cost after 298320 iterations : Training Loss =  0.10151099976372589; Validation Loss = 0.12394417437595881\n",
            "Cost after 298321 iterations : Training Loss =  0.10151099976372592; Validation Loss = 0.1239441743740931\n",
            "Cost after 298322 iterations : Training Loss =  0.10151099976372584; Validation Loss = 0.12394417437222745\n",
            "Cost after 298323 iterations : Training Loss =  0.10151099976372586; Validation Loss = 0.12394417437036173\n",
            "Cost after 298324 iterations : Training Loss =  0.10151099976372592; Validation Loss = 0.12394417436849686\n",
            "Cost after 298325 iterations : Training Loss =  0.10151099976372584; Validation Loss = 0.1239441743666313\n",
            "Cost after 298326 iterations : Training Loss =  0.10151099976372559; Validation Loss = 0.1239441743647663\n",
            "Cost after 298327 iterations : Training Loss =  0.10151099976372574; Validation Loss = 0.12394417436290128\n",
            "Cost after 298328 iterations : Training Loss =  0.10151099976372568; Validation Loss = 0.12394417436103634\n",
            "Cost after 298329 iterations : Training Loss =  0.10151099976372568; Validation Loss = 0.12394417435917131\n",
            "Cost after 298330 iterations : Training Loss =  0.10151099976372577; Validation Loss = 0.12394417435730638\n",
            "Cost after 298331 iterations : Training Loss =  0.10151099976372577; Validation Loss = 0.12394417435544178\n",
            "Cost after 298332 iterations : Training Loss =  0.1015109997637258; Validation Loss = 0.12394417435357717\n",
            "Cost after 298333 iterations : Training Loss =  0.10151099976372577; Validation Loss = 0.12394417435171237\n",
            "Cost after 298334 iterations : Training Loss =  0.10151099976372567; Validation Loss = 0.12394417434984817\n",
            "Cost after 298335 iterations : Training Loss =  0.10151099976372574; Validation Loss = 0.12394417434798402\n",
            "Cost after 298336 iterations : Training Loss =  0.1015109997637258; Validation Loss = 0.12394417434611957\n",
            "Cost after 298337 iterations : Training Loss =  0.10151099976372585; Validation Loss = 0.1239441743442556\n",
            "Cost after 298338 iterations : Training Loss =  0.10151099976372567; Validation Loss = 0.12394417434239134\n",
            "Cost after 298339 iterations : Training Loss =  0.10151099976372566; Validation Loss = 0.12394417434052729\n",
            "Cost after 298340 iterations : Training Loss =  0.10151099976372578; Validation Loss = 0.12394417433866356\n",
            "Cost after 298341 iterations : Training Loss =  0.1015109997637256; Validation Loss = 0.12394417433680001\n",
            "Cost after 298342 iterations : Training Loss =  0.10151099976372573; Validation Loss = 0.12394417433493611\n",
            "Cost after 298343 iterations : Training Loss =  0.10151099976372578; Validation Loss = 0.1239441743330725\n",
            "Cost after 298344 iterations : Training Loss =  0.10151099976372585; Validation Loss = 0.12394417433120886\n",
            "Cost after 298345 iterations : Training Loss =  0.10151099976372567; Validation Loss = 0.12394417432934536\n",
            "Cost after 298346 iterations : Training Loss =  0.10151099976372578; Validation Loss = 0.12394417432748256\n",
            "Cost after 298347 iterations : Training Loss =  0.10151099976372562; Validation Loss = 0.12394417432561895\n",
            "Cost after 298348 iterations : Training Loss =  0.10151099976372577; Validation Loss = 0.12394417432375583\n",
            "Cost after 298349 iterations : Training Loss =  0.1015109997637258; Validation Loss = 0.12394417432189297\n",
            "Cost after 298350 iterations : Training Loss =  0.10151099976372573; Validation Loss = 0.12394417432002984\n",
            "Cost after 298351 iterations : Training Loss =  0.10151099976372566; Validation Loss = 0.1239441743181672\n",
            "Cost after 298352 iterations : Training Loss =  0.10151099976372573; Validation Loss = 0.1239441743163045\n",
            "Cost after 298353 iterations : Training Loss =  0.10151099976372577; Validation Loss = 0.12394417431444196\n",
            "Cost after 298354 iterations : Training Loss =  0.10151099976372555; Validation Loss = 0.12394417431257931\n",
            "Cost after 298355 iterations : Training Loss =  0.10151099976372571; Validation Loss = 0.12394417431071687\n",
            "Cost after 298356 iterations : Training Loss =  0.10151099976372567; Validation Loss = 0.12394417430885472\n",
            "Cost after 298357 iterations : Training Loss =  0.10151099976372562; Validation Loss = 0.12394417430699232\n",
            "Cost after 298358 iterations : Training Loss =  0.10151099976372559; Validation Loss = 0.12394417430513001\n",
            "Cost after 298359 iterations : Training Loss =  0.10151099976372545; Validation Loss = 0.123944174303268\n",
            "Cost after 298360 iterations : Training Loss =  0.1015109997637258; Validation Loss = 0.12394417430140588\n",
            "Cost after 298361 iterations : Training Loss =  0.10151099976372559; Validation Loss = 0.12394417429954434\n",
            "Cost after 298362 iterations : Training Loss =  0.1015109997637256; Validation Loss = 0.12394417429768256\n",
            "Cost after 298363 iterations : Training Loss =  0.10151099976372553; Validation Loss = 0.12394417429582084\n",
            "Cost after 298364 iterations : Training Loss =  0.10151099976372546; Validation Loss = 0.12394417429395915\n",
            "Cost after 298365 iterations : Training Loss =  0.10151099976372562; Validation Loss = 0.12394417429209771\n",
            "Cost after 298366 iterations : Training Loss =  0.10151099976372546; Validation Loss = 0.12394417429023627\n",
            "Cost after 298367 iterations : Training Loss =  0.10151099976372557; Validation Loss = 0.12394417428837479\n",
            "Cost after 298368 iterations : Training Loss =  0.10151099976372571; Validation Loss = 0.12394417428651376\n",
            "Cost after 298369 iterations : Training Loss =  0.10151099976372555; Validation Loss = 0.1239441742846527\n",
            "Cost after 298370 iterations : Training Loss =  0.10151099976372552; Validation Loss = 0.12394417428279185\n",
            "Cost after 298371 iterations : Training Loss =  0.1015109997637256; Validation Loss = 0.12394417428093096\n",
            "Cost after 298372 iterations : Training Loss =  0.10151099976372549; Validation Loss = 0.12394417427907004\n",
            "Cost after 298373 iterations : Training Loss =  0.10151099976372545; Validation Loss = 0.12394417427720913\n",
            "Cost after 298374 iterations : Training Loss =  0.10151099976372552; Validation Loss = 0.12394417427534865\n",
            "Cost after 298375 iterations : Training Loss =  0.10151099976372548; Validation Loss = 0.12394417427348789\n",
            "Cost after 298376 iterations : Training Loss =  0.1015109997637256; Validation Loss = 0.12394417427162753\n",
            "Cost after 298377 iterations : Training Loss =  0.10151099976372553; Validation Loss = 0.12394417426976709\n",
            "Cost after 298378 iterations : Training Loss =  0.10151099976372548; Validation Loss = 0.12394417426790723\n",
            "Cost after 298379 iterations : Training Loss =  0.10151099976372567; Validation Loss = 0.12394417426604663\n",
            "Cost after 298380 iterations : Training Loss =  0.10151099976372557; Validation Loss = 0.12394417426418679\n",
            "Cost after 298381 iterations : Training Loss =  0.10151099976372566; Validation Loss = 0.12394417426232675\n",
            "Cost after 298382 iterations : Training Loss =  0.10151099976372549; Validation Loss = 0.12394417426046664\n",
            "Cost after 298383 iterations : Training Loss =  0.1015109997637254; Validation Loss = 0.12394417425860692\n",
            "Cost after 298384 iterations : Training Loss =  0.10151099976372559; Validation Loss = 0.12394417425674735\n",
            "Cost after 298385 iterations : Training Loss =  0.10151099976372555; Validation Loss = 0.12394417425488775\n",
            "Cost after 298386 iterations : Training Loss =  0.10151099976372548; Validation Loss = 0.12394417425302838\n",
            "Cost after 298387 iterations : Training Loss =  0.10151099976372539; Validation Loss = 0.12394417425116831\n",
            "Cost after 298388 iterations : Training Loss =  0.10151099976372548; Validation Loss = 0.12394417424930945\n",
            "Cost after 298389 iterations : Training Loss =  0.10151099976372557; Validation Loss = 0.12394417424745001\n",
            "Cost after 298390 iterations : Training Loss =  0.10151099976372553; Validation Loss = 0.1239441742455911\n",
            "Cost after 298391 iterations : Training Loss =  0.10151099976372552; Validation Loss = 0.12394417424373182\n",
            "Cost after 298392 iterations : Training Loss =  0.10151099976372537; Validation Loss = 0.12394417424187293\n",
            "Cost after 298393 iterations : Training Loss =  0.10151099976372549; Validation Loss = 0.12394417424001425\n",
            "Cost after 298394 iterations : Training Loss =  0.10151099976372539; Validation Loss = 0.12394417423815553\n",
            "Cost after 298395 iterations : Training Loss =  0.10151099976372545; Validation Loss = 0.12394417423629676\n",
            "Cost after 298396 iterations : Training Loss =  0.10151099976372542; Validation Loss = 0.12394417423443801\n",
            "Cost after 298397 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.12394417423257968\n",
            "Cost after 298398 iterations : Training Loss =  0.10151099976372553; Validation Loss = 0.12394417423072149\n",
            "Cost after 298399 iterations : Training Loss =  0.10151099976372549; Validation Loss = 0.12394417422886303\n",
            "Cost after 298400 iterations : Training Loss =  0.10151099976372534; Validation Loss = 0.1239441742270047\n",
            "Cost after 298401 iterations : Training Loss =  0.10151099976372546; Validation Loss = 0.12394417422514685\n",
            "Cost after 298402 iterations : Training Loss =  0.10151099976372545; Validation Loss = 0.12394417422328902\n",
            "Cost after 298403 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.12394417422143081\n",
            "Cost after 298404 iterations : Training Loss =  0.10151099976372548; Validation Loss = 0.12394417421957281\n",
            "Cost after 298405 iterations : Training Loss =  0.10151099976372539; Validation Loss = 0.12394417421771532\n",
            "Cost after 298406 iterations : Training Loss =  0.1015109997637254; Validation Loss = 0.12394417421585757\n",
            "Cost after 298407 iterations : Training Loss =  0.10151099976372545; Validation Loss = 0.12394417421400061\n",
            "Cost after 298408 iterations : Training Loss =  0.10151099976372537; Validation Loss = 0.12394417421214261\n",
            "Cost after 298409 iterations : Training Loss =  0.1015109997637254; Validation Loss = 0.12394417421028575\n",
            "Cost after 298410 iterations : Training Loss =  0.10151099976372557; Validation Loss = 0.12394417420842806\n",
            "Cost after 298411 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.12394417420657076\n",
            "Cost after 298412 iterations : Training Loss =  0.10151099976372555; Validation Loss = 0.12394417420471408\n",
            "Cost after 298413 iterations : Training Loss =  0.10151099976372552; Validation Loss = 0.123944174202857\n",
            "Cost after 298414 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.12394417420100047\n",
            "Cost after 298415 iterations : Training Loss =  0.10151099976372546; Validation Loss = 0.12394417419914332\n",
            "Cost after 298416 iterations : Training Loss =  0.10151099976372553; Validation Loss = 0.12394417419728675\n",
            "Cost after 298417 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.1239441741954302\n",
            "Cost after 298418 iterations : Training Loss =  0.10151099976372524; Validation Loss = 0.12394417419357397\n",
            "Cost after 298419 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.12394417419171724\n",
            "Cost after 298420 iterations : Training Loss =  0.10151099976372514; Validation Loss = 0.12394417418986091\n",
            "Cost after 298421 iterations : Training Loss =  0.10151099976372528; Validation Loss = 0.1239441741880048\n",
            "Cost after 298422 iterations : Training Loss =  0.10151099976372523; Validation Loss = 0.12394417418614868\n",
            "Cost after 298423 iterations : Training Loss =  0.10151099976372537; Validation Loss = 0.12394417418429247\n",
            "Cost after 298424 iterations : Training Loss =  0.1015109997637253; Validation Loss = 0.12394417418243645\n",
            "Cost after 298425 iterations : Training Loss =  0.1015109997637254; Validation Loss = 0.12394417418058082\n",
            "Cost after 298426 iterations : Training Loss =  0.10151099976372528; Validation Loss = 0.12394417417872512\n",
            "Cost after 298427 iterations : Training Loss =  0.10151099976372521; Validation Loss = 0.1239441741768695\n",
            "Cost after 298428 iterations : Training Loss =  0.10151099976372528; Validation Loss = 0.12394417417501398\n",
            "Cost after 298429 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.1239441741731585\n",
            "Cost after 298430 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.12394417417130317\n",
            "Cost after 298431 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.12394417416944783\n",
            "Cost after 298432 iterations : Training Loss =  0.10151099976372524; Validation Loss = 0.12394417416759247\n",
            "Cost after 298433 iterations : Training Loss =  0.10151099976372542; Validation Loss = 0.12394417416573741\n",
            "Cost after 298434 iterations : Training Loss =  0.10151099976372534; Validation Loss = 0.1239441741638826\n",
            "Cost after 298435 iterations : Training Loss =  0.10151099976372535; Validation Loss = 0.12394417416202785\n",
            "Cost after 298436 iterations : Training Loss =  0.10151099976372528; Validation Loss = 0.12394417416017302\n",
            "Cost after 298437 iterations : Training Loss =  0.10151099976372527; Validation Loss = 0.123944174158318\n",
            "Cost after 298438 iterations : Training Loss =  0.1015109997637252; Validation Loss = 0.1239441741564637\n",
            "Cost after 298439 iterations : Training Loss =  0.10151099976372534; Validation Loss = 0.12394417415460898\n",
            "Cost after 298440 iterations : Training Loss =  0.10151099976372503; Validation Loss = 0.12394417415275441\n",
            "Cost after 298441 iterations : Training Loss =  0.10151099976372534; Validation Loss = 0.12394417415090049\n",
            "Cost after 298442 iterations : Training Loss =  0.10151099976372524; Validation Loss = 0.1239441741490461\n",
            "Cost after 298443 iterations : Training Loss =  0.1015109997637251; Validation Loss = 0.12394417414719167\n",
            "Cost after 298444 iterations : Training Loss =  0.1015109997637252; Validation Loss = 0.12394417414533791\n",
            "Cost after 298445 iterations : Training Loss =  0.10151099976372513; Validation Loss = 0.12394417414348365\n",
            "Cost after 298446 iterations : Training Loss =  0.1015109997637252; Validation Loss = 0.1239441741416299\n",
            "Cost after 298447 iterations : Training Loss =  0.10151099976372521; Validation Loss = 0.12394417413977643\n",
            "Cost after 298448 iterations : Training Loss =  0.1015109997637253; Validation Loss = 0.12394417413792275\n",
            "Cost after 298449 iterations : Training Loss =  0.10151099976372513; Validation Loss = 0.12394417413606927\n",
            "Cost after 298450 iterations : Training Loss =  0.10151099976372527; Validation Loss = 0.12394417413421555\n",
            "Cost after 298451 iterations : Training Loss =  0.10151099976372521; Validation Loss = 0.12394417413236218\n",
            "Cost after 298452 iterations : Training Loss =  0.10151099976372514; Validation Loss = 0.12394417413050873\n",
            "Cost after 298453 iterations : Training Loss =  0.10151099976372521; Validation Loss = 0.1239441741286554\n",
            "Cost after 298454 iterations : Training Loss =  0.1015109997637253; Validation Loss = 0.12394417412680268\n",
            "Cost after 298455 iterations : Training Loss =  0.10151099976372527; Validation Loss = 0.12394417412494946\n",
            "Cost after 298456 iterations : Training Loss =  0.10151099976372503; Validation Loss = 0.12394417412309644\n",
            "Cost after 298457 iterations : Training Loss =  0.10151099976372513; Validation Loss = 0.12394417412124377\n",
            "Cost after 298458 iterations : Training Loss =  0.10151099976372527; Validation Loss = 0.12394417411939103\n",
            "Cost after 298459 iterations : Training Loss =  0.10151099976372528; Validation Loss = 0.12394417411753844\n",
            "Cost after 298460 iterations : Training Loss =  0.10151099976372507; Validation Loss = 0.12394417411568619\n",
            "Cost after 298461 iterations : Training Loss =  0.10151099976372523; Validation Loss = 0.12394417411383334\n",
            "Cost after 298462 iterations : Training Loss =  0.10151099976372507; Validation Loss = 0.12394417411198116\n",
            "Cost after 298463 iterations : Training Loss =  0.10151099976372505; Validation Loss = 0.12394417411012908\n",
            "Cost after 298464 iterations : Training Loss =  0.101510999763725; Validation Loss = 0.12394417410827664\n",
            "Cost after 298465 iterations : Training Loss =  0.1015109997637253; Validation Loss = 0.12394417410642472\n",
            "Cost after 298466 iterations : Training Loss =  0.10151099976372498; Validation Loss = 0.12394417410457273\n",
            "Cost after 298467 iterations : Training Loss =  0.10151099976372524; Validation Loss = 0.12394417410272106\n",
            "Cost after 298468 iterations : Training Loss =  0.10151099976372516; Validation Loss = 0.1239441741008692\n",
            "Cost after 298469 iterations : Training Loss =  0.10151099976372513; Validation Loss = 0.12394417409901755\n",
            "Cost after 298470 iterations : Training Loss =  0.1015109997637253; Validation Loss = 0.12394417409716596\n",
            "Cost after 298471 iterations : Training Loss =  0.10151099976372509; Validation Loss = 0.12394417409531443\n",
            "Cost after 298472 iterations : Training Loss =  0.10151099976372517; Validation Loss = 0.12394417409346316\n",
            "Cost after 298473 iterations : Training Loss =  0.10151099976372532; Validation Loss = 0.12394417409161175\n",
            "Cost after 298474 iterations : Training Loss =  0.10151099976372532; Validation Loss = 0.1239441740897604\n",
            "Cost after 298475 iterations : Training Loss =  0.10151099976372509; Validation Loss = 0.1239441740879093\n",
            "Cost after 298476 iterations : Training Loss =  0.10151099976372509; Validation Loss = 0.12394417408605804\n",
            "Cost after 298477 iterations : Training Loss =  0.10151099976372505; Validation Loss = 0.1239441740842073\n",
            "Cost after 298478 iterations : Training Loss =  0.10151099976372513; Validation Loss = 0.12394417408235636\n",
            "Cost after 298479 iterations : Training Loss =  0.10151099976372496; Validation Loss = 0.12394417408050597\n",
            "Cost after 298480 iterations : Training Loss =  0.1015109997637251; Validation Loss = 0.12394417407865502\n",
            "Cost after 298481 iterations : Training Loss =  0.10151099976372509; Validation Loss = 0.12394417407680443\n",
            "Cost after 298482 iterations : Training Loss =  0.1015109997637252; Validation Loss = 0.12394417407495402\n",
            "Cost after 298483 iterations : Training Loss =  0.10151099976372509; Validation Loss = 0.1239441740731038\n",
            "Cost after 298484 iterations : Training Loss =  0.10151099976372517; Validation Loss = 0.1239441740712535\n",
            "Cost after 298485 iterations : Training Loss =  0.10151099976372505; Validation Loss = 0.12394417406940327\n",
            "Cost after 298486 iterations : Training Loss =  0.1015109997637251; Validation Loss = 0.12394417406755315\n",
            "Cost after 298487 iterations : Training Loss =  0.10151099976372502; Validation Loss = 0.12394417406570309\n",
            "Cost after 298488 iterations : Training Loss =  0.10151099976372513; Validation Loss = 0.12394417406385326\n",
            "Cost after 298489 iterations : Training Loss =  0.10151099976372514; Validation Loss = 0.12394417406200357\n",
            "Cost after 298490 iterations : Training Loss =  0.10151099976372502; Validation Loss = 0.12394417406015358\n",
            "Cost after 298491 iterations : Training Loss =  0.10151099976372513; Validation Loss = 0.12394417405830405\n",
            "Cost after 298492 iterations : Training Loss =  0.10151099976372513; Validation Loss = 0.12394417405645439\n",
            "Cost after 298493 iterations : Training Loss =  0.10151099976372496; Validation Loss = 0.12394417405460524\n",
            "Cost after 298494 iterations : Training Loss =  0.10151099976372507; Validation Loss = 0.12394417405275572\n",
            "Cost after 298495 iterations : Training Loss =  0.10151099976372509; Validation Loss = 0.12394417405090664\n",
            "Cost after 298496 iterations : Training Loss =  0.10151099976372488; Validation Loss = 0.12394417404905746\n",
            "Cost after 298497 iterations : Training Loss =  0.10151099976372485; Validation Loss = 0.12394417404720838\n",
            "Cost after 298498 iterations : Training Loss =  0.10151099976372498; Validation Loss = 0.12394417404535904\n",
            "Cost after 298499 iterations : Training Loss =  0.10151099976372503; Validation Loss = 0.12394417404351034\n",
            "Cost after 298500 iterations : Training Loss =  0.101510999763725; Validation Loss = 0.12394417404166162\n",
            "Cost after 298501 iterations : Training Loss =  0.10151099976372492; Validation Loss = 0.12394417403981312\n",
            "Cost after 298502 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417403796433\n",
            "Cost after 298503 iterations : Training Loss =  0.10151099976372495; Validation Loss = 0.12394417403611592\n",
            "Cost after 298504 iterations : Training Loss =  0.10151099976372498; Validation Loss = 0.12394417403426765\n",
            "Cost after 298505 iterations : Training Loss =  0.10151099976372492; Validation Loss = 0.12394417403241932\n",
            "Cost after 298506 iterations : Training Loss =  0.10151099976372498; Validation Loss = 0.1239441740305709\n",
            "Cost after 298507 iterations : Training Loss =  0.10151099976372502; Validation Loss = 0.12394417402872274\n",
            "Cost after 298508 iterations : Training Loss =  0.10151099976372507; Validation Loss = 0.12394417402687498\n",
            "Cost after 298509 iterations : Training Loss =  0.1015109997637249; Validation Loss = 0.12394417402502678\n",
            "Cost after 298510 iterations : Training Loss =  0.101510999763725; Validation Loss = 0.12394417402317924\n",
            "Cost after 298511 iterations : Training Loss =  0.10151099976372484; Validation Loss = 0.12394417402133175\n",
            "Cost after 298512 iterations : Training Loss =  0.10151099976372496; Validation Loss = 0.12394417401948345\n",
            "Cost after 298513 iterations : Training Loss =  0.10151099976372488; Validation Loss = 0.12394417401763594\n",
            "Cost after 298514 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417401578862\n",
            "Cost after 298515 iterations : Training Loss =  0.10151099976372496; Validation Loss = 0.12394417401394112\n",
            "Cost after 298516 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417401209401\n",
            "Cost after 298517 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417401024686\n",
            "Cost after 298518 iterations : Training Loss =  0.10151099976372495; Validation Loss = 0.12394417400839974\n",
            "Cost after 298519 iterations : Training Loss =  0.1015109997637249; Validation Loss = 0.12394417400655248\n",
            "Cost after 298520 iterations : Training Loss =  0.10151099976372485; Validation Loss = 0.12394417400470552\n",
            "Cost after 298521 iterations : Training Loss =  0.1015109997637249; Validation Loss = 0.12394417400285906\n",
            "Cost after 298522 iterations : Training Loss =  0.10151099976372488; Validation Loss = 0.12394417400101226\n",
            "Cost after 298523 iterations : Training Loss =  0.10151099976372484; Validation Loss = 0.12394417399916592\n",
            "Cost after 298524 iterations : Training Loss =  0.10151099976372496; Validation Loss = 0.12394417399731907\n",
            "Cost after 298525 iterations : Training Loss =  0.1015109997637247; Validation Loss = 0.1239441739954725\n",
            "Cost after 298526 iterations : Training Loss =  0.1015109997637249; Validation Loss = 0.12394417399362646\n",
            "Cost after 298527 iterations : Training Loss =  0.1015109997637248; Validation Loss = 0.12394417399178\n",
            "Cost after 298528 iterations : Training Loss =  0.10151099976372498; Validation Loss = 0.12394417398993375\n",
            "Cost after 298529 iterations : Training Loss =  0.10151099976372491; Validation Loss = 0.12394417398808784\n",
            "Cost after 298530 iterations : Training Loss =  0.10151099976372484; Validation Loss = 0.12394417398624205\n",
            "Cost after 298531 iterations : Training Loss =  0.10151099976372491; Validation Loss = 0.123944173984396\n",
            "Cost after 298532 iterations : Training Loss =  0.10151099976372491; Validation Loss = 0.12394417398255035\n",
            "Cost after 298533 iterations : Training Loss =  0.10151099976372488; Validation Loss = 0.12394417398070424\n",
            "Cost after 298534 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417397885901\n",
            "Cost after 298535 iterations : Training Loss =  0.10151099976372484; Validation Loss = 0.12394417397701324\n",
            "Cost after 298536 iterations : Training Loss =  0.1015109997637248; Validation Loss = 0.12394417397516812\n",
            "Cost after 298537 iterations : Training Loss =  0.1015109997637248; Validation Loss = 0.1239441739733226\n",
            "Cost after 298538 iterations : Training Loss =  0.10151099976372477; Validation Loss = 0.12394417397147725\n",
            "Cost after 298539 iterations : Training Loss =  0.1015109997637249; Validation Loss = 0.12394417396963249\n",
            "Cost after 298540 iterations : Training Loss =  0.10151099976372457; Validation Loss = 0.12394417396778758\n",
            "Cost after 298541 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417396594247\n",
            "Cost after 298542 iterations : Training Loss =  0.1015109997637249; Validation Loss = 0.12394417396409754\n",
            "Cost after 298543 iterations : Training Loss =  0.10151099976372492; Validation Loss = 0.1239441739622529\n",
            "Cost after 298544 iterations : Training Loss =  0.10151099976372478; Validation Loss = 0.12394417396040833\n",
            "Cost after 298545 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417395856364\n",
            "Cost after 298546 iterations : Training Loss =  0.10151099976372477; Validation Loss = 0.12394417395671938\n",
            "Cost after 298547 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417395487466\n",
            "Cost after 298548 iterations : Training Loss =  0.10151099976372471; Validation Loss = 0.1239441739530304\n",
            "Cost after 298549 iterations : Training Loss =  0.10151099976372478; Validation Loss = 0.12394417395118641\n",
            "Cost after 298550 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.1239441739493423\n",
            "Cost after 298551 iterations : Training Loss =  0.10151099976372475; Validation Loss = 0.12394417394749824\n",
            "Cost after 298552 iterations : Training Loss =  0.10151099976372475; Validation Loss = 0.12394417394565427\n",
            "Cost after 298553 iterations : Training Loss =  0.10151099976372478; Validation Loss = 0.12394417394381044\n",
            "Cost after 298554 iterations : Training Loss =  0.10151099976372496; Validation Loss = 0.12394417394196655\n",
            "Cost after 298555 iterations : Training Loss =  0.10151099976372473; Validation Loss = 0.12394417394012314\n",
            "Cost after 298556 iterations : Training Loss =  0.10151099976372484; Validation Loss = 0.12394417393827972\n",
            "Cost after 298557 iterations : Training Loss =  0.10151099976372485; Validation Loss = 0.12394417393643631\n",
            "Cost after 298558 iterations : Training Loss =  0.10151099976372466; Validation Loss = 0.12394417393459298\n",
            "Cost after 298559 iterations : Training Loss =  0.10151099976372469; Validation Loss = 0.12394417393274938\n",
            "Cost after 298560 iterations : Training Loss =  0.1015109997637247; Validation Loss = 0.1239441739309065\n",
            "Cost after 298561 iterations : Training Loss =  0.10151099976372477; Validation Loss = 0.12394417392906312\n",
            "Cost after 298562 iterations : Training Loss =  0.10151099976372484; Validation Loss = 0.12394417392722053\n",
            "Cost after 298563 iterations : Training Loss =  0.10151099976372464; Validation Loss = 0.12394417392537743\n",
            "Cost after 298564 iterations : Training Loss =  0.10151099976372473; Validation Loss = 0.12394417392353464\n",
            "Cost after 298565 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417392169196\n",
            "Cost after 298566 iterations : Training Loss =  0.1015109997637247; Validation Loss = 0.12394417391984931\n",
            "Cost after 298567 iterations : Training Loss =  0.10151099976372446; Validation Loss = 0.12394417391800694\n",
            "Cost after 298568 iterations : Training Loss =  0.10151099976372482; Validation Loss = 0.12394417391616461\n",
            "Cost after 298569 iterations : Training Loss =  0.1015109997637246; Validation Loss = 0.12394417391432239\n",
            "Cost after 298570 iterations : Training Loss =  0.1015109997637247; Validation Loss = 0.12394417391247982\n",
            "Cost after 298571 iterations : Training Loss =  0.10151099976372473; Validation Loss = 0.12394417391063803\n",
            "Cost after 298572 iterations : Training Loss =  0.10151099976372463; Validation Loss = 0.12394417390879614\n",
            "Cost after 298573 iterations : Training Loss =  0.10151099976372456; Validation Loss = 0.12394417390695382\n",
            "Cost after 298574 iterations : Training Loss =  0.10151099976372464; Validation Loss = 0.12394417390511202\n",
            "Cost after 298575 iterations : Training Loss =  0.10151099976372459; Validation Loss = 0.1239441739032704\n",
            "Cost after 298576 iterations : Training Loss =  0.10151099976372471; Validation Loss = 0.12394417390142866\n",
            "Cost after 298577 iterations : Training Loss =  0.1015109997637247; Validation Loss = 0.1239441738995871\n",
            "Cost after 298578 iterations : Training Loss =  0.10151099976372463; Validation Loss = 0.12394417389774576\n",
            "Cost after 298579 iterations : Training Loss =  0.1015109997637248; Validation Loss = 0.12394417389590422\n",
            "Cost after 298580 iterations : Training Loss =  0.10151099976372456; Validation Loss = 0.12394417389406322\n",
            "Cost after 298581 iterations : Training Loss =  0.10151099976372469; Validation Loss = 0.12394417389222177\n",
            "Cost after 298582 iterations : Training Loss =  0.1015109997637246; Validation Loss = 0.12394417389038088\n",
            "Cost after 298583 iterations : Training Loss =  0.10151099976372469; Validation Loss = 0.12394417388854007\n",
            "Cost after 298584 iterations : Training Loss =  0.10151099976372478; Validation Loss = 0.12394417388669889\n",
            "Cost after 298585 iterations : Training Loss =  0.10151099976372457; Validation Loss = 0.12394417388485807\n",
            "Cost after 298586 iterations : Training Loss =  0.10151099976372464; Validation Loss = 0.1239441738830176\n",
            "Cost after 298587 iterations : Training Loss =  0.10151099976372466; Validation Loss = 0.12394417388117683\n",
            "Cost after 298588 iterations : Training Loss =  0.10151099976372446; Validation Loss = 0.12394417387933622\n",
            "Cost after 298589 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417387749569\n",
            "Cost after 298590 iterations : Training Loss =  0.1015109997637245; Validation Loss = 0.1239441738756555\n",
            "Cost after 298591 iterations : Training Loss =  0.10151099976372448; Validation Loss = 0.1239441738738154\n",
            "Cost after 298592 iterations : Training Loss =  0.10151099976372471; Validation Loss = 0.12394417387197489\n",
            "Cost after 298593 iterations : Training Loss =  0.10151099976372463; Validation Loss = 0.12394417387013458\n",
            "Cost after 298594 iterations : Training Loss =  0.10151099976372466; Validation Loss = 0.12394417386829494\n",
            "Cost after 298595 iterations : Training Loss =  0.10151099976372452; Validation Loss = 0.12394417386645484\n",
            "Cost after 298596 iterations : Training Loss =  0.10151099976372459; Validation Loss = 0.1239441738646152\n",
            "Cost after 298597 iterations : Training Loss =  0.1015109997637247; Validation Loss = 0.12394417386277543\n",
            "Cost after 298598 iterations : Training Loss =  0.10151099976372452; Validation Loss = 0.123944173860936\n",
            "Cost after 298599 iterations : Training Loss =  0.10151099976372452; Validation Loss = 0.12394417385909652\n",
            "Cost after 298600 iterations : Training Loss =  0.10151099976372452; Validation Loss = 0.1239441738572571\n",
            "Cost after 298601 iterations : Training Loss =  0.10151099976372435; Validation Loss = 0.12394417385541744\n",
            "Cost after 298602 iterations : Training Loss =  0.10151099976372456; Validation Loss = 0.12394417385357849\n",
            "Cost after 298603 iterations : Training Loss =  0.1015109997637245; Validation Loss = 0.1239441738517394\n",
            "Cost after 298604 iterations : Training Loss =  0.10151099976372456; Validation Loss = 0.1239441738499004\n",
            "Cost after 298605 iterations : Training Loss =  0.10151099976372448; Validation Loss = 0.12394417384806146\n",
            "Cost after 298606 iterations : Training Loss =  0.10151099976372457; Validation Loss = 0.12394417384622249\n",
            "Cost after 298607 iterations : Training Loss =  0.10151099976372448; Validation Loss = 0.1239441738443839\n",
            "Cost after 298608 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417384254493\n",
            "Cost after 298609 iterations : Training Loss =  0.10151099976372464; Validation Loss = 0.12394417384070654\n",
            "Cost after 298610 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417383886776\n",
            "Cost after 298611 iterations : Training Loss =  0.10151099976372459; Validation Loss = 0.12394417383702962\n",
            "Cost after 298612 iterations : Training Loss =  0.10151099976372463; Validation Loss = 0.12394417383519128\n",
            "Cost after 298613 iterations : Training Loss =  0.1015109997637245; Validation Loss = 0.12394417383335292\n",
            "Cost after 298614 iterations : Training Loss =  0.10151099976372446; Validation Loss = 0.12394417383151493\n",
            "Cost after 298615 iterations : Training Loss =  0.10151099976372456; Validation Loss = 0.12394417382967682\n",
            "Cost after 298616 iterations : Training Loss =  0.10151099976372452; Validation Loss = 0.12394417382783911\n",
            "Cost after 298617 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417382600108\n",
            "Cost after 298618 iterations : Training Loss =  0.10151099976372464; Validation Loss = 0.1239441738241636\n",
            "Cost after 298619 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417382232586\n",
            "Cost after 298620 iterations : Training Loss =  0.10151099976372444; Validation Loss = 0.123944173820488\n",
            "Cost after 298621 iterations : Training Loss =  0.10151099976372448; Validation Loss = 0.12394417381865065\n",
            "Cost after 298622 iterations : Training Loss =  0.10151099976372435; Validation Loss = 0.12394417381681332\n",
            "Cost after 298623 iterations : Training Loss =  0.10151099976372457; Validation Loss = 0.12394417381497601\n",
            "Cost after 298624 iterations : Training Loss =  0.10151099976372435; Validation Loss = 0.12394417381313919\n",
            "Cost after 298625 iterations : Training Loss =  0.10151099976372444; Validation Loss = 0.12394417381130174\n",
            "Cost after 298626 iterations : Training Loss =  0.10151099976372448; Validation Loss = 0.12394417380946465\n",
            "Cost after 298627 iterations : Training Loss =  0.10151099976372432; Validation Loss = 0.12394417380762787\n",
            "Cost after 298628 iterations : Training Loss =  0.10151099976372453; Validation Loss = 0.12394417380579098\n",
            "Cost after 298629 iterations : Training Loss =  0.1015109997637245; Validation Loss = 0.12394417380395448\n",
            "Cost after 298630 iterations : Training Loss =  0.1015109997637245; Validation Loss = 0.12394417380211742\n",
            "Cost after 298631 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417380028115\n",
            "Cost after 298632 iterations : Training Loss =  0.10151099976372432; Validation Loss = 0.12394417379844473\n",
            "Cost after 298633 iterations : Training Loss =  0.10151099976372456; Validation Loss = 0.1239441737966083\n",
            "Cost after 298634 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417379477228\n",
            "Cost after 298635 iterations : Training Loss =  0.10151099976372435; Validation Loss = 0.12394417379293604\n",
            "Cost after 298636 iterations : Training Loss =  0.10151099976372445; Validation Loss = 0.12394417379109995\n",
            "Cost after 298637 iterations : Training Loss =  0.1015109997637245; Validation Loss = 0.12394417378926395\n",
            "Cost after 298638 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417378742822\n",
            "Cost after 298639 iterations : Training Loss =  0.10151099976372426; Validation Loss = 0.12394417378559226\n",
            "Cost after 298640 iterations : Training Loss =  0.10151099976372434; Validation Loss = 0.1239441737837564\n",
            "Cost after 298641 iterations : Training Loss =  0.10151099976372445; Validation Loss = 0.12394417378192131\n",
            "Cost after 298642 iterations : Training Loss =  0.10151099976372444; Validation Loss = 0.12394417378008571\n",
            "Cost after 298643 iterations : Training Loss =  0.10151099976372431; Validation Loss = 0.12394417377824986\n",
            "Cost after 298644 iterations : Training Loss =  0.10151099976372435; Validation Loss = 0.12394417377641474\n",
            "Cost after 298645 iterations : Training Loss =  0.10151099976372441; Validation Loss = 0.12394417377457952\n",
            "Cost after 298646 iterations : Training Loss =  0.10151099976372432; Validation Loss = 0.12394417377274446\n",
            "Cost after 298647 iterations : Training Loss =  0.10151099976372438; Validation Loss = 0.12394417377090944\n",
            "Cost after 298648 iterations : Training Loss =  0.10151099976372438; Validation Loss = 0.12394417376907442\n",
            "Cost after 298649 iterations : Training Loss =  0.10151099976372438; Validation Loss = 0.12394417376723961\n",
            "Cost after 298650 iterations : Training Loss =  0.1015109997637242; Validation Loss = 0.12394417376540495\n",
            "Cost after 298651 iterations : Training Loss =  0.10151099976372421; Validation Loss = 0.12394417376357005\n",
            "Cost after 298652 iterations : Training Loss =  0.10151099976372444; Validation Loss = 0.12394417376173579\n",
            "Cost after 298653 iterations : Training Loss =  0.10151099976372438; Validation Loss = 0.12394417375990123\n",
            "Cost after 298654 iterations : Training Loss =  0.10151099976372444; Validation Loss = 0.12394417375806693\n",
            "Cost after 298655 iterations : Training Loss =  0.10151099976372448; Validation Loss = 0.12394417375623247\n",
            "Cost after 298656 iterations : Training Loss =  0.10151099976372431; Validation Loss = 0.12394417375439833\n",
            "Cost after 298657 iterations : Training Loss =  0.1015109997637242; Validation Loss = 0.12394417375256422\n",
            "Cost after 298658 iterations : Training Loss =  0.10151099976372431; Validation Loss = 0.12394417375073026\n",
            "Cost after 298659 iterations : Training Loss =  0.10151099976372427; Validation Loss = 0.1239441737488959\n",
            "Cost after 298660 iterations : Training Loss =  0.10151099976372438; Validation Loss = 0.12394417374706213\n",
            "Cost after 298661 iterations : Training Loss =  0.10151099976372409; Validation Loss = 0.1239441737452286\n",
            "Cost after 298662 iterations : Training Loss =  0.10151099976372432; Validation Loss = 0.12394417374339511\n",
            "Cost after 298663 iterations : Training Loss =  0.10151099976372428; Validation Loss = 0.12394417374156158\n",
            "Cost after 298664 iterations : Training Loss =  0.10151099976372438; Validation Loss = 0.12394417373972821\n",
            "Cost after 298665 iterations : Training Loss =  0.10151099976372435; Validation Loss = 0.12394417373789471\n",
            "Cost after 298666 iterations : Training Loss =  0.1015109997637242; Validation Loss = 0.12394417373606183\n",
            "Cost after 298667 iterations : Training Loss =  0.10151099976372438; Validation Loss = 0.12394417373422816\n",
            "Cost after 298668 iterations : Training Loss =  0.10151099976372412; Validation Loss = 0.12394417373239555\n",
            "Cost after 298669 iterations : Training Loss =  0.10151099976372427; Validation Loss = 0.12394417373056256\n",
            "Cost after 298670 iterations : Training Loss =  0.10151099976372434; Validation Loss = 0.12394417372872947\n",
            "Cost after 298671 iterations : Training Loss =  0.10151099976372421; Validation Loss = 0.1239441737268967\n",
            "Cost after 298672 iterations : Training Loss =  0.10151099976372416; Validation Loss = 0.12394417372506396\n",
            "Cost after 298673 iterations : Training Loss =  0.10151099976372421; Validation Loss = 0.12394417372323158\n",
            "Cost after 298674 iterations : Training Loss =  0.10151099976372431; Validation Loss = 0.12394417372139893\n",
            "Cost after 298675 iterations : Training Loss =  0.1015109997637242; Validation Loss = 0.12394417371956648\n",
            "Cost after 298676 iterations : Training Loss =  0.10151099976372428; Validation Loss = 0.1239441737177342\n",
            "Cost after 298677 iterations : Training Loss =  0.10151099976372416; Validation Loss = 0.12394417371590179\n",
            "Cost after 298678 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417371406981\n",
            "Cost after 298679 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417371223766\n",
            "Cost after 298680 iterations : Training Loss =  0.10151099976372431; Validation Loss = 0.12394417371040564\n",
            "Cost after 298681 iterations : Training Loss =  0.10151099976372431; Validation Loss = 0.1239441737085739\n",
            "Cost after 298682 iterations : Training Loss =  0.10151099976372424; Validation Loss = 0.12394417370674159\n",
            "Cost after 298683 iterations : Training Loss =  0.10151099976372413; Validation Loss = 0.12394417370491022\n",
            "Cost after 298684 iterations : Training Loss =  0.10151099976372413; Validation Loss = 0.12394417370307892\n",
            "Cost after 298685 iterations : Training Loss =  0.10151099976372409; Validation Loss = 0.12394417370124744\n",
            "Cost after 298686 iterations : Training Loss =  0.10151099976372426; Validation Loss = 0.1239441736994155\n",
            "Cost after 298687 iterations : Training Loss =  0.1015109997637242; Validation Loss = 0.12394417369758465\n",
            "Cost after 298688 iterations : Training Loss =  0.10151099976372432; Validation Loss = 0.12394417369575302\n",
            "Cost after 298689 iterations : Training Loss =  0.10151099976372412; Validation Loss = 0.1239441736939223\n",
            "Cost after 298690 iterations : Training Loss =  0.10151099976372421; Validation Loss = 0.12394417369209126\n",
            "Cost after 298691 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417369025977\n",
            "Cost after 298692 iterations : Training Loss =  0.10151099976372409; Validation Loss = 0.12394417368842905\n",
            "Cost after 298693 iterations : Training Loss =  0.10151099976372426; Validation Loss = 0.12394417368659845\n",
            "Cost after 298694 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417368476796\n",
            "Cost after 298695 iterations : Training Loss =  0.10151099976372407; Validation Loss = 0.12394417368293738\n",
            "Cost after 298696 iterations : Training Loss =  0.10151099976372413; Validation Loss = 0.12394417368110695\n",
            "Cost after 298697 iterations : Training Loss =  0.10151099976372409; Validation Loss = 0.12394417367927649\n",
            "Cost after 298698 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.123944173677446\n",
            "Cost after 298699 iterations : Training Loss =  0.10151099976372421; Validation Loss = 0.12394417367561592\n",
            "Cost after 298700 iterations : Training Loss =  0.10151099976372395; Validation Loss = 0.12394417367378571\n",
            "Cost after 298701 iterations : Training Loss =  0.10151099976372416; Validation Loss = 0.12394417367195576\n",
            "Cost after 298702 iterations : Training Loss =  0.10151099976372413; Validation Loss = 0.12394417367012578\n",
            "Cost after 298703 iterations : Training Loss =  0.10151099976372403; Validation Loss = 0.12394417366829621\n",
            "Cost after 298704 iterations : Training Loss =  0.10151099976372407; Validation Loss = 0.12394417366646622\n",
            "Cost after 298705 iterations : Training Loss =  0.10151099976372407; Validation Loss = 0.12394417366463682\n",
            "Cost after 298706 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417366280731\n",
            "Cost after 298707 iterations : Training Loss =  0.10151099976372413; Validation Loss = 0.1239441736609778\n",
            "Cost after 298708 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417365914809\n",
            "Cost after 298709 iterations : Training Loss =  0.10151099976372426; Validation Loss = 0.12394417365731915\n",
            "Cost after 298710 iterations : Training Loss =  0.10151099976372421; Validation Loss = 0.12394417365548989\n",
            "Cost after 298711 iterations : Training Loss =  0.101510999763724; Validation Loss = 0.12394417365366098\n",
            "Cost after 298712 iterations : Training Loss =  0.10151099976372413; Validation Loss = 0.12394417365183216\n",
            "Cost after 298713 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417365000307\n",
            "Cost after 298714 iterations : Training Loss =  0.10151099976372414; Validation Loss = 0.12394417364817428\n",
            "Cost after 298715 iterations : Training Loss =  0.10151099976372412; Validation Loss = 0.12394417364634532\n",
            "Cost after 298716 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417364451706\n",
            "Cost after 298717 iterations : Training Loss =  0.10151099976372399; Validation Loss = 0.1239441736426885\n",
            "Cost after 298718 iterations : Training Loss =  0.101510999763724; Validation Loss = 0.12394417364086004\n",
            "Cost after 298719 iterations : Training Loss =  0.10151099976372414; Validation Loss = 0.1239441736390315\n",
            "Cost after 298720 iterations : Training Loss =  0.10151099976372396; Validation Loss = 0.12394417363720343\n",
            "Cost after 298721 iterations : Training Loss =  0.1015109997637242; Validation Loss = 0.12394417363537515\n",
            "Cost after 298722 iterations : Training Loss =  0.1015109997637242; Validation Loss = 0.12394417363354698\n",
            "Cost after 298723 iterations : Training Loss =  0.101510999763724; Validation Loss = 0.12394417363171911\n",
            "Cost after 298724 iterations : Training Loss =  0.10151099976372394; Validation Loss = 0.12394417362989137\n",
            "Cost after 298725 iterations : Training Loss =  0.10151099976372407; Validation Loss = 0.12394417362806374\n",
            "Cost after 298726 iterations : Training Loss =  0.10151099976372396; Validation Loss = 0.12394417362623589\n",
            "Cost after 298727 iterations : Training Loss =  0.10151099976372399; Validation Loss = 0.12394417362440845\n",
            "Cost after 298728 iterations : Training Loss =  0.10151099976372395; Validation Loss = 0.1239441736225809\n",
            "Cost after 298729 iterations : Training Loss =  0.10151099976372406; Validation Loss = 0.12394417362075301\n",
            "Cost after 298730 iterations : Training Loss =  0.10151099976372403; Validation Loss = 0.12394417361892579\n",
            "Cost after 298731 iterations : Training Loss =  0.10151099976372403; Validation Loss = 0.12394417361709888\n",
            "Cost after 298732 iterations : Training Loss =  0.10151099976372396; Validation Loss = 0.12394417361527144\n",
            "Cost after 298733 iterations : Training Loss =  0.10151099976372403; Validation Loss = 0.12394417361344495\n",
            "Cost after 298734 iterations : Training Loss =  0.10151099976372409; Validation Loss = 0.12394417361161765\n",
            "Cost after 298735 iterations : Training Loss =  0.10151099976372394; Validation Loss = 0.12394417360979076\n",
            "Cost after 298736 iterations : Training Loss =  0.10151099976372388; Validation Loss = 0.12394417360796407\n",
            "Cost after 298737 iterations : Training Loss =  0.10151099976372407; Validation Loss = 0.12394417360613709\n",
            "Cost after 298738 iterations : Training Loss =  0.10151099976372384; Validation Loss = 0.12394417360431068\n",
            "Cost after 298739 iterations : Training Loss =  0.1015109997637238; Validation Loss = 0.12394417360248419\n",
            "Cost after 298740 iterations : Training Loss =  0.10151099976372396; Validation Loss = 0.12394417360065803\n",
            "Cost after 298741 iterations : Training Loss =  0.10151099976372387; Validation Loss = 0.12394417359883164\n",
            "Cost after 298742 iterations : Training Loss =  0.10151099976372394; Validation Loss = 0.1239441735970055\n",
            "Cost after 298743 iterations : Training Loss =  0.10151099976372396; Validation Loss = 0.12394417359517947\n",
            "Cost after 298744 iterations : Training Loss =  0.10151099976372409; Validation Loss = 0.1239441735933536\n",
            "Cost after 298745 iterations : Training Loss =  0.10151099976372395; Validation Loss = 0.1239441735915274\n",
            "Cost after 298746 iterations : Training Loss =  0.10151099976372387; Validation Loss = 0.12394417358970149\n",
            "Cost after 298747 iterations : Training Loss =  0.10151099976372395; Validation Loss = 0.12394417358787595\n",
            "Cost after 298748 iterations : Training Loss =  0.10151099976372396; Validation Loss = 0.12394417358605006\n",
            "Cost after 298749 iterations : Training Loss =  0.10151099976372374; Validation Loss = 0.12394417358422471\n",
            "Cost after 298750 iterations : Training Loss =  0.10151099976372419; Validation Loss = 0.12394417358239908\n",
            "Cost after 298751 iterations : Training Loss =  0.10151099976372388; Validation Loss = 0.12394417358057361\n",
            "Cost after 298752 iterations : Training Loss =  0.10151099976372396; Validation Loss = 0.12394417357874844\n",
            "Cost after 298753 iterations : Training Loss =  0.10151099976372394; Validation Loss = 0.1239441735769232\n",
            "Cost after 298754 iterations : Training Loss =  0.10151099976372391; Validation Loss = 0.12394417357509835\n",
            "Cost after 298755 iterations : Training Loss =  0.1015109997637242; Validation Loss = 0.12394417357327317\n",
            "Cost after 298756 iterations : Training Loss =  0.10151099976372394; Validation Loss = 0.12394417357144807\n",
            "Cost after 298757 iterations : Training Loss =  0.10151099976372391; Validation Loss = 0.12394417356962338\n",
            "Cost after 298758 iterations : Training Loss =  0.10151099976372376; Validation Loss = 0.12394417356779848\n",
            "Cost after 298759 iterations : Training Loss =  0.1015109997637238; Validation Loss = 0.12394417356597406\n",
            "Cost after 298760 iterations : Training Loss =  0.10151099976372381; Validation Loss = 0.12394417356414962\n",
            "Cost after 298761 iterations : Training Loss =  0.10151099976372395; Validation Loss = 0.12394417356232486\n",
            "Cost after 298762 iterations : Training Loss =  0.10151099976372396; Validation Loss = 0.12394417356050076\n",
            "Cost after 298763 iterations : Training Loss =  0.1015109997637238; Validation Loss = 0.12394417355867637\n",
            "Cost after 298764 iterations : Training Loss =  0.10151099976372387; Validation Loss = 0.1239441735568521\n",
            "Cost after 298765 iterations : Training Loss =  0.10151099976372387; Validation Loss = 0.12394417355502818\n",
            "Cost after 298766 iterations : Training Loss =  0.10151099976372388; Validation Loss = 0.12394417355320435\n",
            "Cost after 298767 iterations : Training Loss =  0.10151099976372382; Validation Loss = 0.12394417355138046\n",
            "Cost after 298768 iterations : Training Loss =  0.10151099976372384; Validation Loss = 0.12394417354955665\n",
            "Cost after 298769 iterations : Training Loss =  0.10151099976372359; Validation Loss = 0.12394417354773296\n",
            "Cost after 298770 iterations : Training Loss =  0.10151099976372382; Validation Loss = 0.12394417354590945\n",
            "Cost after 298771 iterations : Training Loss =  0.10151099976372381; Validation Loss = 0.12394417354408589\n",
            "Cost after 298772 iterations : Training Loss =  0.10151099976372371; Validation Loss = 0.12394417354226227\n",
            "Cost after 298773 iterations : Training Loss =  0.10151099976372391; Validation Loss = 0.12394417354043906\n",
            "Cost after 298774 iterations : Training Loss =  0.10151099976372395; Validation Loss = 0.12394417353861582\n",
            "Cost after 298775 iterations : Training Loss =  0.10151099976372387; Validation Loss = 0.12394417353679257\n",
            "Cost after 298776 iterations : Training Loss =  0.10151099976372384; Validation Loss = 0.12394417353496968\n",
            "Cost after 298777 iterations : Training Loss =  0.10151099976372362; Validation Loss = 0.12394417353314666\n",
            "Cost after 298778 iterations : Training Loss =  0.10151099976372382; Validation Loss = 0.1239441735313239\n",
            "Cost after 298779 iterations : Training Loss =  0.10151099976372376; Validation Loss = 0.12394417352950113\n",
            "Cost after 298780 iterations : Training Loss =  0.10151099976372374; Validation Loss = 0.12394417352767842\n",
            "Cost after 298781 iterations : Training Loss =  0.10151099976372381; Validation Loss = 0.12394417352585588\n",
            "Cost after 298782 iterations : Training Loss =  0.10151099976372388; Validation Loss = 0.12394417352403345\n",
            "Cost after 298783 iterations : Training Loss =  0.10151099976372374; Validation Loss = 0.12394417352221088\n",
            "Cost after 298784 iterations : Training Loss =  0.10151099976372382; Validation Loss = 0.12394417352038843\n",
            "Cost after 298785 iterations : Training Loss =  0.10151099976372374; Validation Loss = 0.12394417351856603\n",
            "Cost after 298786 iterations : Training Loss =  0.10151099976372381; Validation Loss = 0.12394417351674406\n",
            "Cost after 298787 iterations : Training Loss =  0.10151099976372369; Validation Loss = 0.12394417351492192\n",
            "Cost after 298788 iterations : Training Loss =  0.10151099976372358; Validation Loss = 0.12394417351310023\n",
            "Cost after 298789 iterations : Training Loss =  0.10151099976372364; Validation Loss = 0.12394417351127839\n",
            "Cost after 298790 iterations : Training Loss =  0.10151099976372381; Validation Loss = 0.1239441735094567\n",
            "Cost after 298791 iterations : Training Loss =  0.10151099976372371; Validation Loss = 0.12394417350763463\n",
            "Cost after 298792 iterations : Training Loss =  0.10151099976372369; Validation Loss = 0.12394417350581322\n",
            "Cost after 298793 iterations : Training Loss =  0.1015109997637237; Validation Loss = 0.12394417350399184\n",
            "Cost after 298794 iterations : Training Loss =  0.10151099976372364; Validation Loss = 0.12394417350217016\n",
            "Cost after 298795 iterations : Training Loss =  0.10151099976372364; Validation Loss = 0.12394417350034927\n",
            "Cost after 298796 iterations : Training Loss =  0.1015109997637238; Validation Loss = 0.1239441734985277\n",
            "Cost after 298797 iterations : Training Loss =  0.10151099976372359; Validation Loss = 0.12394417349670665\n",
            "Cost after 298798 iterations : Training Loss =  0.10151099976372374; Validation Loss = 0.1239441734948857\n",
            "Cost after 298799 iterations : Training Loss =  0.1015109997637237; Validation Loss = 0.12394417349306473\n",
            "Cost after 298800 iterations : Training Loss =  0.10151099976372367; Validation Loss = 0.1239441734912441\n",
            "Cost after 298801 iterations : Training Loss =  0.10151099976372367; Validation Loss = 0.12394417348942302\n",
            "Cost after 298802 iterations : Training Loss =  0.10151099976372374; Validation Loss = 0.12394417348760253\n",
            "Cost after 298803 iterations : Training Loss =  0.10151099976372369; Validation Loss = 0.12394417348578221\n",
            "Cost after 298804 iterations : Training Loss =  0.10151099976372363; Validation Loss = 0.12394417348396183\n",
            "Cost after 298805 iterations : Training Loss =  0.10151099976372355; Validation Loss = 0.12394417348214115\n",
            "Cost after 298806 iterations : Training Loss =  0.10151099976372363; Validation Loss = 0.12394417348032091\n",
            "Cost after 298807 iterations : Training Loss =  0.10151099976372345; Validation Loss = 0.12394417347850072\n",
            "Cost after 298808 iterations : Training Loss =  0.10151099976372377; Validation Loss = 0.12394417347668066\n",
            "Cost after 298809 iterations : Training Loss =  0.10151099976372359; Validation Loss = 0.1239441734748608\n",
            "Cost after 298810 iterations : Training Loss =  0.10151099976372363; Validation Loss = 0.12394417347304092\n",
            "Cost after 298811 iterations : Training Loss =  0.10151099976372377; Validation Loss = 0.12394417347122087\n",
            "Cost after 298812 iterations : Training Loss =  0.10151099976372363; Validation Loss = 0.12394417346940108\n",
            "Cost after 298813 iterations : Training Loss =  0.10151099976372346; Validation Loss = 0.12394417346758141\n",
            "Cost after 298814 iterations : Training Loss =  0.10151099976372363; Validation Loss = 0.12394417346576221\n",
            "Cost after 298815 iterations : Training Loss =  0.10151099976372363; Validation Loss = 0.12394417346394274\n",
            "Cost after 298816 iterations : Training Loss =  0.1015109997637237; Validation Loss = 0.12394417346212308\n",
            "Cost after 298817 iterations : Training Loss =  0.10151099976372367; Validation Loss = 0.12394417346030417\n",
            "Cost after 298818 iterations : Training Loss =  0.10151099976372364; Validation Loss = 0.12394417345848478\n",
            "Cost after 298819 iterations : Training Loss =  0.10151099976372352; Validation Loss = 0.12394417345666593\n",
            "Cost after 298820 iterations : Training Loss =  0.10151099976372355; Validation Loss = 0.123944173454847\n",
            "Cost after 298821 iterations : Training Loss =  0.10151099976372363; Validation Loss = 0.12394417345302777\n",
            "Cost after 298822 iterations : Training Loss =  0.10151099976372367; Validation Loss = 0.12394417345120914\n",
            "Cost after 298823 iterations : Training Loss =  0.10151099976372355; Validation Loss = 0.1239441734493905\n",
            "Cost after 298824 iterations : Training Loss =  0.10151099976372355; Validation Loss = 0.12394417344757162\n",
            "Cost after 298825 iterations : Training Loss =  0.10151099976372352; Validation Loss = 0.12394417344575338\n",
            "Cost after 298826 iterations : Training Loss =  0.10151099976372346; Validation Loss = 0.1239441734439348\n",
            "Cost after 298827 iterations : Training Loss =  0.10151099976372381; Validation Loss = 0.12394417344211647\n",
            "Cost after 298828 iterations : Training Loss =  0.10151099976372358; Validation Loss = 0.12394417344029818\n",
            "Cost after 298829 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417343847995\n",
            "Cost after 298830 iterations : Training Loss =  0.10151099976372358; Validation Loss = 0.12394417343666221\n",
            "Cost after 298831 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417343484432\n",
            "Cost after 298832 iterations : Training Loss =  0.10151099976372352; Validation Loss = 0.12394417343302631\n",
            "Cost after 298833 iterations : Training Loss =  0.1015109997637235; Validation Loss = 0.12394417343120832\n",
            "Cost after 298834 iterations : Training Loss =  0.10151099976372345; Validation Loss = 0.12394417342939047\n",
            "Cost after 298835 iterations : Training Loss =  0.1015109997637235; Validation Loss = 0.1239441734275729\n",
            "Cost after 298836 iterations : Training Loss =  0.10151099976372364; Validation Loss = 0.12394417342575531\n",
            "Cost after 298837 iterations : Training Loss =  0.10151099976372356; Validation Loss = 0.12394417342393808\n",
            "Cost after 298838 iterations : Training Loss =  0.1015109997637235; Validation Loss = 0.12394417342212084\n",
            "Cost after 298839 iterations : Training Loss =  0.10151099976372359; Validation Loss = 0.12394417342030342\n",
            "Cost after 298840 iterations : Training Loss =  0.10151099976372358; Validation Loss = 0.12394417341848625\n",
            "Cost after 298841 iterations : Training Loss =  0.10151099976372346; Validation Loss = 0.12394417341666931\n",
            "Cost after 298842 iterations : Training Loss =  0.1015109997637233; Validation Loss = 0.12394417341485228\n",
            "Cost after 298843 iterations : Training Loss =  0.10151099976372359; Validation Loss = 0.12394417341303542\n",
            "Cost after 298844 iterations : Training Loss =  0.10151099976372344; Validation Loss = 0.12394417341121866\n",
            "Cost after 298845 iterations : Training Loss =  0.10151099976372346; Validation Loss = 0.12394417340940209\n",
            "Cost after 298846 iterations : Training Loss =  0.10151099976372355; Validation Loss = 0.12394417340758551\n",
            "Cost after 298847 iterations : Training Loss =  0.10151099976372342; Validation Loss = 0.12394417340576894\n",
            "Cost after 298848 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417340395268\n",
            "Cost after 298849 iterations : Training Loss =  0.10151099976372346; Validation Loss = 0.12394417340213633\n",
            "Cost after 298850 iterations : Training Loss =  0.10151099976372349; Validation Loss = 0.12394417340032009\n",
            "Cost after 298851 iterations : Training Loss =  0.10151099976372344; Validation Loss = 0.12394417339850397\n",
            "Cost after 298852 iterations : Training Loss =  0.1015109997637235; Validation Loss = 0.12394417339668815\n",
            "Cost after 298853 iterations : Training Loss =  0.10151099976372346; Validation Loss = 0.12394417339487168\n",
            "Cost after 298854 iterations : Training Loss =  0.10151099976372346; Validation Loss = 0.12394417339305641\n",
            "Cost after 298855 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417339124053\n",
            "Cost after 298856 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417338942465\n",
            "Cost after 298857 iterations : Training Loss =  0.10151099976372337; Validation Loss = 0.12394417338760919\n",
            "Cost after 298858 iterations : Training Loss =  0.10151099976372338; Validation Loss = 0.12394417338579362\n",
            "Cost after 298859 iterations : Training Loss =  0.10151099976372342; Validation Loss = 0.12394417338397822\n",
            "Cost after 298860 iterations : Training Loss =  0.10151099976372349; Validation Loss = 0.12394417338216304\n",
            "Cost after 298861 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417338034774\n",
            "Cost after 298862 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417337853264\n",
            "Cost after 298863 iterations : Training Loss =  0.10151099976372326; Validation Loss = 0.12394417337671769\n",
            "Cost after 298864 iterations : Training Loss =  0.10151099976372344; Validation Loss = 0.12394417337490281\n",
            "Cost after 298865 iterations : Training Loss =  0.10151099976372356; Validation Loss = 0.12394417337308795\n",
            "Cost after 298866 iterations : Training Loss =  0.10151099976372337; Validation Loss = 0.12394417337127318\n",
            "Cost after 298867 iterations : Training Loss =  0.10151099976372338; Validation Loss = 0.12394417336945872\n",
            "Cost after 298868 iterations : Training Loss =  0.10151099976372344; Validation Loss = 0.12394417336764388\n",
            "Cost after 298869 iterations : Training Loss =  0.10151099976372333; Validation Loss = 0.12394417336582951\n",
            "Cost after 298870 iterations : Training Loss =  0.10151099976372338; Validation Loss = 0.12394417336401521\n",
            "Cost after 298871 iterations : Training Loss =  0.10151099976372356; Validation Loss = 0.12394417336220113\n",
            "Cost after 298872 iterations : Training Loss =  0.10151099976372333; Validation Loss = 0.12394417336038663\n",
            "Cost after 298873 iterations : Training Loss =  0.10151099976372338; Validation Loss = 0.1239441733585728\n",
            "Cost after 298874 iterations : Training Loss =  0.10151099976372323; Validation Loss = 0.12394417335675868\n",
            "Cost after 298875 iterations : Training Loss =  0.10151099976372333; Validation Loss = 0.12394417335494459\n",
            "Cost after 298876 iterations : Training Loss =  0.10151099976372333; Validation Loss = 0.12394417335313072\n",
            "Cost after 298877 iterations : Training Loss =  0.10151099976372331; Validation Loss = 0.123944173351317\n",
            "Cost after 298878 iterations : Training Loss =  0.10151099976372344; Validation Loss = 0.12394417334950347\n",
            "Cost after 298879 iterations : Training Loss =  0.10151099976372319; Validation Loss = 0.12394417334768976\n",
            "Cost after 298880 iterations : Training Loss =  0.10151099976372338; Validation Loss = 0.12394417334587637\n",
            "Cost after 298881 iterations : Training Loss =  0.10151099976372324; Validation Loss = 0.12394417334406303\n",
            "Cost after 298882 iterations : Training Loss =  0.10151099976372324; Validation Loss = 0.12394417334224972\n",
            "Cost after 298883 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417334043652\n",
            "Cost after 298884 iterations : Training Loss =  0.1015109997637233; Validation Loss = 0.12394417333862347\n",
            "Cost after 298885 iterations : Training Loss =  0.10151099976372323; Validation Loss = 0.1239441733368107\n",
            "Cost after 298886 iterations : Training Loss =  0.10151099976372337; Validation Loss = 0.12394417333499752\n",
            "Cost after 298887 iterations : Training Loss =  0.10151099976372349; Validation Loss = 0.12394417333318491\n",
            "Cost after 298888 iterations : Training Loss =  0.10151099976372319; Validation Loss = 0.12394417333137212\n",
            "Cost after 298889 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417332955956\n",
            "Cost after 298890 iterations : Training Loss =  0.10151099976372323; Validation Loss = 0.12394417332774715\n",
            "Cost after 298891 iterations : Training Loss =  0.10151099976372337; Validation Loss = 0.1239441733259345\n",
            "Cost after 298892 iterations : Training Loss =  0.10151099976372324; Validation Loss = 0.12394417332412219\n",
            "Cost after 298893 iterations : Training Loss =  0.1015109997637232; Validation Loss = 0.12394417332230973\n",
            "Cost after 298894 iterations : Training Loss =  0.1015109997637234; Validation Loss = 0.12394417332049763\n",
            "Cost after 298895 iterations : Training Loss =  0.10151099976372346; Validation Loss = 0.12394417331868587\n",
            "Cost after 298896 iterations : Training Loss =  0.10151099976372319; Validation Loss = 0.12394417331687375\n",
            "Cost after 298897 iterations : Training Loss =  0.10151099976372335; Validation Loss = 0.1239441733150618\n",
            "Cost after 298898 iterations : Training Loss =  0.10151099976372335; Validation Loss = 0.12394417331325022\n",
            "Cost after 298899 iterations : Training Loss =  0.10151099976372335; Validation Loss = 0.12394417331143846\n",
            "Cost after 298900 iterations : Training Loss =  0.10151099976372319; Validation Loss = 0.12394417330962668\n",
            "Cost after 298901 iterations : Training Loss =  0.1015109997637233; Validation Loss = 0.12394417330781518\n",
            "Cost after 298902 iterations : Training Loss =  0.10151099976372333; Validation Loss = 0.1239441733060038\n",
            "Cost after 298903 iterations : Training Loss =  0.10151099976372331; Validation Loss = 0.1239441733041923\n",
            "Cost after 298904 iterations : Training Loss =  0.10151099976372324; Validation Loss = 0.12394417330238121\n",
            "Cost after 298905 iterations : Training Loss =  0.10151099976372326; Validation Loss = 0.12394417330057028\n",
            "Cost after 298906 iterations : Training Loss =  0.10151099976372338; Validation Loss = 0.12394417329875917\n",
            "Cost after 298907 iterations : Training Loss =  0.10151099976372327; Validation Loss = 0.12394417329694808\n",
            "Cost after 298908 iterations : Training Loss =  0.1015109997637231; Validation Loss = 0.1239441732951373\n",
            "Cost after 298909 iterations : Training Loss =  0.10151099976372327; Validation Loss = 0.12394417329332642\n",
            "Cost after 298910 iterations : Training Loss =  0.10151099976372314; Validation Loss = 0.12394417329151577\n",
            "Cost after 298911 iterations : Training Loss =  0.10151099976372312; Validation Loss = 0.1239441732897053\n",
            "Cost after 298912 iterations : Training Loss =  0.10151099976372314; Validation Loss = 0.12394417328789467\n",
            "Cost after 298913 iterations : Training Loss =  0.10151099976372327; Validation Loss = 0.12394417328608395\n",
            "Cost after 298914 iterations : Training Loss =  0.10151099976372331; Validation Loss = 0.12394417328427405\n",
            "Cost after 298915 iterations : Training Loss =  0.10151099976372326; Validation Loss = 0.1239441732824635\n",
            "Cost after 298916 iterations : Training Loss =  0.10151099976372324; Validation Loss = 0.12394417328065345\n",
            "Cost after 298917 iterations : Training Loss =  0.10151099976372337; Validation Loss = 0.12394417327884372\n",
            "Cost after 298918 iterations : Training Loss =  0.10151099976372323; Validation Loss = 0.12394417327703343\n",
            "Cost after 298919 iterations : Training Loss =  0.10151099976372324; Validation Loss = 0.12394417327522372\n",
            "Cost after 298920 iterations : Training Loss =  0.10151099976372313; Validation Loss = 0.12394417327341373\n",
            "Cost after 298921 iterations : Training Loss =  0.10151099976372313; Validation Loss = 0.12394417327160417\n",
            "Cost after 298922 iterations : Training Loss =  0.10151099976372312; Validation Loss = 0.12394417326979484\n",
            "Cost after 298923 iterations : Training Loss =  0.10151099976372313; Validation Loss = 0.12394417326798512\n",
            "Cost after 298924 iterations : Training Loss =  0.10151099976372308; Validation Loss = 0.12394417326617546\n",
            "Cost after 298925 iterations : Training Loss =  0.1015109997637232; Validation Loss = 0.12394417326436635\n",
            "Cost after 298926 iterations : Training Loss =  0.1015109997637231; Validation Loss = 0.12394417326255688\n",
            "Cost after 298927 iterations : Training Loss =  0.10151099976372314; Validation Loss = 0.12394417326074804\n",
            "Cost after 298928 iterations : Training Loss =  0.10151099976372308; Validation Loss = 0.12394417325893915\n",
            "Cost after 298929 iterations : Training Loss =  0.10151099976372326; Validation Loss = 0.1239441732571299\n",
            "Cost after 298930 iterations : Training Loss =  0.10151099976372312; Validation Loss = 0.12394417325532114\n",
            "Cost after 298931 iterations : Training Loss =  0.10151099976372308; Validation Loss = 0.1239441732535123\n",
            "Cost after 298932 iterations : Training Loss =  0.10151099976372314; Validation Loss = 0.12394417325170365\n",
            "Cost after 298933 iterations : Training Loss =  0.1015109997637232; Validation Loss = 0.12394417324989526\n",
            "Cost after 298934 iterations : Training Loss =  0.10151099976372308; Validation Loss = 0.12394417324808668\n",
            "Cost after 298935 iterations : Training Loss =  0.10151099976372308; Validation Loss = 0.12394417324627827\n",
            "Cost after 298936 iterations : Training Loss =  0.1015109997637233; Validation Loss = 0.12394417324446963\n",
            "Cost after 298937 iterations : Training Loss =  0.10151099976372305; Validation Loss = 0.12394417324266162\n",
            "Cost after 298938 iterations : Training Loss =  0.10151099976372327; Validation Loss = 0.12394417324085354\n",
            "Cost after 298939 iterations : Training Loss =  0.10151099976372302; Validation Loss = 0.1239441732390456\n",
            "Cost after 298940 iterations : Training Loss =  0.10151099976372326; Validation Loss = 0.12394417323723758\n",
            "Cost after 298941 iterations : Training Loss =  0.10151099976372312; Validation Loss = 0.12394417323542953\n",
            "Cost after 298942 iterations : Training Loss =  0.10151099976372294; Validation Loss = 0.12394417323362177\n",
            "Cost after 298943 iterations : Training Loss =  0.10151099976372306; Validation Loss = 0.12394417323181423\n",
            "Cost after 298944 iterations : Training Loss =  0.10151099976372306; Validation Loss = 0.12394417323000696\n",
            "Cost after 298945 iterations : Training Loss =  0.10151099976372319; Validation Loss = 0.12394417322819927\n",
            "Cost after 298946 iterations : Training Loss =  0.10151099976372324; Validation Loss = 0.12394417322639179\n",
            "Cost after 298947 iterations : Training Loss =  0.10151099976372313; Validation Loss = 0.1239441732245847\n",
            "Cost after 298948 iterations : Training Loss =  0.10151099976372292; Validation Loss = 0.12394417322277741\n",
            "Cost after 298949 iterations : Training Loss =  0.10151099976372298; Validation Loss = 0.12394417322096994\n",
            "Cost after 298950 iterations : Training Loss =  0.10151099976372305; Validation Loss = 0.12394417321916334\n",
            "Cost after 298951 iterations : Training Loss =  0.10151099976372302; Validation Loss = 0.12394417321735622\n",
            "Cost after 298952 iterations : Training Loss =  0.101510999763723; Validation Loss = 0.12394417321554965\n",
            "Cost after 298953 iterations : Training Loss =  0.10151099976372314; Validation Loss = 0.12394417321374251\n",
            "Cost after 298954 iterations : Training Loss =  0.10151099976372306; Validation Loss = 0.12394417321193599\n",
            "Cost after 298955 iterations : Training Loss =  0.10151099976372324; Validation Loss = 0.12394417321012945\n",
            "Cost after 298956 iterations : Training Loss =  0.10151099976372326; Validation Loss = 0.12394417320832306\n",
            "Cost after 298957 iterations : Training Loss =  0.10151099976372308; Validation Loss = 0.1239441732065167\n",
            "Cost after 298958 iterations : Training Loss =  0.10151099976372319; Validation Loss = 0.12394417320471035\n",
            "Cost after 298959 iterations : Training Loss =  0.10151099976372287; Validation Loss = 0.12394417320290435\n",
            "Cost after 298960 iterations : Training Loss =  0.10151099976372305; Validation Loss = 0.12394417320109793\n",
            "Cost after 298961 iterations : Training Loss =  0.10151099976372299; Validation Loss = 0.1239441731992919\n",
            "Cost after 298962 iterations : Training Loss =  0.10151099976372314; Validation Loss = 0.12394417319748627\n",
            "Cost after 298963 iterations : Training Loss =  0.10151099976372302; Validation Loss = 0.12394417319568007\n",
            "Cost after 298964 iterations : Training Loss =  0.10151099976372306; Validation Loss = 0.12394417319387432\n",
            "Cost after 298965 iterations : Training Loss =  0.10151099976372323; Validation Loss = 0.12394417319206913\n",
            "Cost after 298966 iterations : Training Loss =  0.10151099976372274; Validation Loss = 0.12394417319026331\n",
            "Cost after 298967 iterations : Training Loss =  0.10151099976372302; Validation Loss = 0.12394417318845793\n",
            "Cost after 298968 iterations : Training Loss =  0.10151099976372305; Validation Loss = 0.12394417318665253\n",
            "Cost after 298969 iterations : Training Loss =  0.10151099976372294; Validation Loss = 0.12394417318484742\n",
            "Cost after 298970 iterations : Training Loss =  0.10151099976372298; Validation Loss = 0.1239441731830421\n",
            "Cost after 298971 iterations : Training Loss =  0.10151099976372294; Validation Loss = 0.12394417318123704\n",
            "Cost after 298972 iterations : Training Loss =  0.10151099976372292; Validation Loss = 0.12394417317943193\n",
            "Cost after 298973 iterations : Training Loss =  0.10151099976372292; Validation Loss = 0.12394417317762688\n",
            "Cost after 298974 iterations : Training Loss =  0.10151099976372294; Validation Loss = 0.12394417317582218\n",
            "Cost after 298975 iterations : Training Loss =  0.10151099976372313; Validation Loss = 0.12394417317401743\n",
            "Cost after 298976 iterations : Training Loss =  0.10151099976372305; Validation Loss = 0.12394417317221314\n",
            "Cost after 298977 iterations : Training Loss =  0.10151099976372281; Validation Loss = 0.12394417317040848\n",
            "Cost after 298978 iterations : Training Loss =  0.1015109997637231; Validation Loss = 0.1239441731686038\n",
            "Cost after 298979 iterations : Training Loss =  0.10151099976372288; Validation Loss = 0.1239441731667994\n",
            "Cost after 298980 iterations : Training Loss =  0.10151099976372302; Validation Loss = 0.12394417316499526\n",
            "Cost after 298981 iterations : Training Loss =  0.10151099976372288; Validation Loss = 0.12394417316319122\n",
            "Cost after 298982 iterations : Training Loss =  0.10151099976372291; Validation Loss = 0.12394417316138676\n",
            "Cost after 298983 iterations : Training Loss =  0.10151099976372305; Validation Loss = 0.12394417315958293\n",
            "Cost after 298984 iterations : Training Loss =  0.10151099976372299; Validation Loss = 0.12394417315777925\n",
            "Cost after 298985 iterations : Training Loss =  0.10151099976372308; Validation Loss = 0.1239441731559751\n",
            "Cost after 298986 iterations : Training Loss =  0.10151099976372288; Validation Loss = 0.12394417315417139\n",
            "Cost after 298987 iterations : Training Loss =  0.10151099976372292; Validation Loss = 0.12394417315236796\n",
            "Cost after 298988 iterations : Training Loss =  0.10151099976372298; Validation Loss = 0.12394417315056462\n",
            "Cost after 298989 iterations : Training Loss =  0.10151099976372292; Validation Loss = 0.12394417314876092\n",
            "Cost after 298990 iterations : Training Loss =  0.10151099976372288; Validation Loss = 0.12394417314695755\n",
            "Cost after 298991 iterations : Training Loss =  0.10151099976372283; Validation Loss = 0.12394417314515421\n",
            "Cost after 298992 iterations : Training Loss =  0.10151099976372285; Validation Loss = 0.12394417314335128\n",
            "Cost after 298993 iterations : Training Loss =  0.10151099976372288; Validation Loss = 0.12394417314154792\n",
            "Cost after 298994 iterations : Training Loss =  0.10151099976372281; Validation Loss = 0.12394417313974505\n",
            "Cost after 298995 iterations : Training Loss =  0.10151099976372273; Validation Loss = 0.12394417313794236\n",
            "Cost after 298996 iterations : Training Loss =  0.1015109997637228; Validation Loss = 0.12394417313613942\n",
            "Cost after 298997 iterations : Training Loss =  0.10151099976372294; Validation Loss = 0.12394417313433655\n",
            "Cost after 298998 iterations : Training Loss =  0.101510999763723; Validation Loss = 0.12394417313253385\n",
            "Cost after 298999 iterations : Training Loss =  0.1015109997637228; Validation Loss = 0.12394417313073153\n",
            "Cost after 299000 iterations : Training Loss =  0.10151099976372285; Validation Loss = 0.12394417312892905\n",
            "Cost after 299001 iterations : Training Loss =  0.10151099976372295; Validation Loss = 0.12394417312712676\n",
            "Cost after 299002 iterations : Training Loss =  0.10151099976372276; Validation Loss = 0.12394417312532433\n",
            "Cost after 299003 iterations : Training Loss =  0.10151099976372305; Validation Loss = 0.12394417312352227\n",
            "Cost after 299004 iterations : Training Loss =  0.10151099976372276; Validation Loss = 0.12394417312172\n",
            "Cost after 299005 iterations : Training Loss =  0.10151099976372287; Validation Loss = 0.12394417311991823\n",
            "Cost after 299006 iterations : Training Loss =  0.10151099976372283; Validation Loss = 0.12394417311811622\n",
            "Cost after 299007 iterations : Training Loss =  0.10151099976372278; Validation Loss = 0.12394417311631446\n",
            "Cost after 299008 iterations : Training Loss =  0.10151099976372291; Validation Loss = 0.12394417311451264\n",
            "Cost after 299009 iterations : Training Loss =  0.10151099976372283; Validation Loss = 0.1239441731127112\n",
            "Cost after 299010 iterations : Training Loss =  0.10151099976372274; Validation Loss = 0.1239441731109098\n",
            "Cost after 299011 iterations : Training Loss =  0.10151099976372287; Validation Loss = 0.12394417310910813\n",
            "Cost after 299012 iterations : Training Loss =  0.10151099976372274; Validation Loss = 0.12394417310730699\n",
            "Cost after 299013 iterations : Training Loss =  0.10151099976372274; Validation Loss = 0.12394417310550568\n",
            "Cost after 299014 iterations : Training Loss =  0.1015109997637228; Validation Loss = 0.12394417310370406\n",
            "Cost after 299015 iterations : Training Loss =  0.10151099976372281; Validation Loss = 0.12394417310190331\n",
            "Cost after 299016 iterations : Training Loss =  0.10151099976372281; Validation Loss = 0.12394417310010251\n",
            "Cost after 299017 iterations : Training Loss =  0.10151099976372281; Validation Loss = 0.1239441730983015\n",
            "Cost after 299018 iterations : Training Loss =  0.10151099976372273; Validation Loss = 0.12394417309650109\n",
            "Cost after 299019 iterations : Training Loss =  0.10151099976372263; Validation Loss = 0.12394417309469999\n",
            "Cost after 299020 iterations : Training Loss =  0.10151099976372278; Validation Loss = 0.12394417309289947\n",
            "Cost after 299021 iterations : Training Loss =  0.1015109997637227; Validation Loss = 0.12394417309109912\n",
            "Cost after 299022 iterations : Training Loss =  0.10151099976372281; Validation Loss = 0.1239441730892984\n",
            "Cost after 299023 iterations : Training Loss =  0.10151099976372294; Validation Loss = 0.12394417308749814\n",
            "Cost after 299024 iterations : Training Loss =  0.10151099976372273; Validation Loss = 0.123944173085698\n",
            "Cost after 299025 iterations : Training Loss =  0.10151099976372285; Validation Loss = 0.12394417308389781\n",
            "Cost after 299026 iterations : Training Loss =  0.10151099976372285; Validation Loss = 0.1239441730820982\n",
            "Cost after 299027 iterations : Training Loss =  0.1015109997637228; Validation Loss = 0.12394417308029762\n",
            "Cost after 299028 iterations : Training Loss =  0.1015109997637228; Validation Loss = 0.12394417307849771\n",
            "Cost after 299029 iterations : Training Loss =  0.1015109997637228; Validation Loss = 0.12394417307669799\n",
            "Cost after 299030 iterations : Training Loss =  0.10151099976372269; Validation Loss = 0.1239441730748985\n",
            "Cost after 299031 iterations : Training Loss =  0.10151099976372273; Validation Loss = 0.12394417307309887\n",
            "Cost after 299032 iterations : Training Loss =  0.1015109997637228; Validation Loss = 0.12394417307129929\n",
            "Cost after 299033 iterations : Training Loss =  0.10151099976372263; Validation Loss = 0.12394417306949987\n",
            "Cost after 299034 iterations : Training Loss =  0.10151099976372263; Validation Loss = 0.12394417306770046\n",
            "Cost after 299035 iterations : Training Loss =  0.10151099976372278; Validation Loss = 0.12394417306590139\n",
            "Cost after 299036 iterations : Training Loss =  0.10151099976372269; Validation Loss = 0.12394417306410205\n",
            "Cost after 299037 iterations : Training Loss =  0.10151099976372255; Validation Loss = 0.12394417306230335\n",
            "Cost after 299038 iterations : Training Loss =  0.10151099976372266; Validation Loss = 0.12394417306050427\n",
            "Cost after 299039 iterations : Training Loss =  0.10151099976372258; Validation Loss = 0.12394417305870527\n",
            "Cost after 299040 iterations : Training Loss =  0.1015109997637226; Validation Loss = 0.12394417305690639\n",
            "Cost after 299041 iterations : Training Loss =  0.10151099976372246; Validation Loss = 0.12394417305510792\n",
            "Cost after 299042 iterations : Training Loss =  0.1015109997637225; Validation Loss = 0.12394417305330904\n",
            "Cost after 299043 iterations : Training Loss =  0.1015109997637228; Validation Loss = 0.1239441730515107\n",
            "Cost after 299044 iterations : Training Loss =  0.10151099976372269; Validation Loss = 0.12394417304971217\n",
            "Cost after 299045 iterations : Training Loss =  0.10151099976372255; Validation Loss = 0.12394417304791402\n",
            "Cost after 299046 iterations : Training Loss =  0.10151099976372258; Validation Loss = 0.1239441730461155\n",
            "Cost after 299047 iterations : Training Loss =  0.10151099976372278; Validation Loss = 0.12394417304431776\n",
            "Cost after 299048 iterations : Training Loss =  0.10151099976372255; Validation Loss = 0.12394417304251963\n",
            "Cost after 299049 iterations : Training Loss =  0.10151099976372263; Validation Loss = 0.12394417304072157\n",
            "Cost after 299050 iterations : Training Loss =  0.10151099976372262; Validation Loss = 0.12394417303892366\n",
            "Cost after 299051 iterations : Training Loss =  0.10151099976372276; Validation Loss = 0.1239441730371259\n",
            "Cost after 299052 iterations : Training Loss =  0.10151099976372276; Validation Loss = 0.1239441730353281\n",
            "Cost after 299053 iterations : Training Loss =  0.10151099976372263; Validation Loss = 0.12394417303353057\n",
            "Cost after 299054 iterations : Training Loss =  0.10151099976372278; Validation Loss = 0.12394417303173282\n",
            "Cost after 299055 iterations : Training Loss =  0.10151099976372269; Validation Loss = 0.1239441730299356\n",
            "Cost after 299056 iterations : Training Loss =  0.10151099976372249; Validation Loss = 0.12394417302813843\n",
            "Cost after 299057 iterations : Training Loss =  0.1015109997637225; Validation Loss = 0.1239441730263412\n",
            "Cost after 299058 iterations : Training Loss =  0.10151099976372262; Validation Loss = 0.12394417302454404\n",
            "Cost after 299059 iterations : Training Loss =  0.10151099976372266; Validation Loss = 0.12394417302274705\n",
            "Cost after 299060 iterations : Training Loss =  0.10151099976372278; Validation Loss = 0.12394417302095014\n",
            "Cost after 299061 iterations : Training Loss =  0.1015109997637227; Validation Loss = 0.1239441730191533\n",
            "Cost after 299062 iterations : Training Loss =  0.10151099976372262; Validation Loss = 0.12394417301735633\n",
            "Cost after 299063 iterations : Training Loss =  0.10151099976372267; Validation Loss = 0.12394417301555986\n",
            "Cost after 299064 iterations : Training Loss =  0.10151099976372238; Validation Loss = 0.12394417301376306\n",
            "Cost after 299065 iterations : Training Loss =  0.1015109997637226; Validation Loss = 0.12394417301196647\n",
            "Cost after 299066 iterations : Training Loss =  0.10151099976372246; Validation Loss = 0.12394417301017022\n",
            "Cost after 299067 iterations : Training Loss =  0.10151099976372266; Validation Loss = 0.12394417300837386\n",
            "Cost after 299068 iterations : Training Loss =  0.1015109997637226; Validation Loss = 0.12394417300657772\n",
            "Cost after 299069 iterations : Training Loss =  0.10151099976372248; Validation Loss = 0.12394417300478154\n",
            "Cost after 299070 iterations : Training Loss =  0.1015109997637225; Validation Loss = 0.12394417300298578\n",
            "Cost after 299071 iterations : Training Loss =  0.1015109997637225; Validation Loss = 0.12394417300118957\n",
            "Cost after 299072 iterations : Training Loss =  0.10151099976372255; Validation Loss = 0.12394417299939361\n",
            "Cost after 299073 iterations : Training Loss =  0.10151099976372256; Validation Loss = 0.12394417299759812\n",
            "Cost after 299074 iterations : Training Loss =  0.10151099976372266; Validation Loss = 0.12394417299580235\n",
            "Cost after 299075 iterations : Training Loss =  0.10151099976372249; Validation Loss = 0.12394417299400658\n",
            "Cost after 299076 iterations : Training Loss =  0.1015109997637227; Validation Loss = 0.12394417299221107\n",
            "Cost after 299077 iterations : Training Loss =  0.10151099976372237; Validation Loss = 0.12394417299041562\n",
            "Cost after 299078 iterations : Training Loss =  0.10151099976372246; Validation Loss = 0.12394417298862039\n",
            "Cost after 299079 iterations : Training Loss =  0.10151099976372255; Validation Loss = 0.12394417298682506\n",
            "Cost after 299080 iterations : Training Loss =  0.10151099976372258; Validation Loss = 0.12394417298503027\n",
            "Cost after 299081 iterations : Training Loss =  0.10151099976372244; Validation Loss = 0.12394417298323523\n",
            "Cost after 299082 iterations : Training Loss =  0.10151099976372256; Validation Loss = 0.1239441729814401\n",
            "Cost after 299083 iterations : Training Loss =  0.10151099976372256; Validation Loss = 0.12394417297964548\n",
            "Cost after 299084 iterations : Training Loss =  0.10151099976372253; Validation Loss = 0.12394417297785046\n",
            "Cost after 299085 iterations : Training Loss =  0.10151099976372253; Validation Loss = 0.12394417297605573\n",
            "Cost after 299086 iterations : Training Loss =  0.1015109997637225; Validation Loss = 0.12394417297426151\n",
            "Cost after 299087 iterations : Training Loss =  0.10151099976372222; Validation Loss = 0.12394417297246675\n",
            "Cost after 299088 iterations : Training Loss =  0.10151099976372255; Validation Loss = 0.12394417297067219\n",
            "Cost after 299089 iterations : Training Loss =  0.10151099976372256; Validation Loss = 0.1239441729688781\n",
            "Cost after 299090 iterations : Training Loss =  0.10151099976372253; Validation Loss = 0.12394417296708367\n",
            "Cost after 299091 iterations : Training Loss =  0.10151099976372253; Validation Loss = 0.12394417296528945\n",
            "Cost after 299092 iterations : Training Loss =  0.10151099976372231; Validation Loss = 0.12394417296349577\n",
            "Cost after 299093 iterations : Training Loss =  0.10151099976372246; Validation Loss = 0.12394417296170153\n",
            "Cost after 299094 iterations : Training Loss =  0.10151099976372246; Validation Loss = 0.12394417295990766\n",
            "Cost after 299095 iterations : Training Loss =  0.10151099976372256; Validation Loss = 0.12394417295811423\n",
            "Cost after 299096 iterations : Training Loss =  0.10151099976372237; Validation Loss = 0.12394417295632035\n",
            "Cost after 299097 iterations : Training Loss =  0.10151099976372249; Validation Loss = 0.1239441729545268\n",
            "Cost after 299098 iterations : Training Loss =  0.10151099976372246; Validation Loss = 0.12394417295273336\n",
            "Cost after 299099 iterations : Training Loss =  0.10151099976372242; Validation Loss = 0.12394417295093992\n",
            "Cost after 299100 iterations : Training Loss =  0.10151099976372249; Validation Loss = 0.12394417294914675\n",
            "Cost after 299101 iterations : Training Loss =  0.10151099976372248; Validation Loss = 0.12394417294735342\n",
            "Cost after 299102 iterations : Training Loss =  0.10151099976372238; Validation Loss = 0.12394417294556043\n",
            "Cost after 299103 iterations : Training Loss =  0.10151099976372242; Validation Loss = 0.1239441729437673\n",
            "Cost after 299104 iterations : Training Loss =  0.10151099976372249; Validation Loss = 0.12394417294197448\n",
            "Cost after 299105 iterations : Training Loss =  0.10151099976372248; Validation Loss = 0.12394417294018151\n",
            "Cost after 299106 iterations : Training Loss =  0.10151099976372237; Validation Loss = 0.1239441729383887\n",
            "Cost after 299107 iterations : Training Loss =  0.10151099976372237; Validation Loss = 0.12394417293659572\n",
            "Cost after 299108 iterations : Training Loss =  0.10151099976372249; Validation Loss = 0.12394417293480349\n",
            "Cost after 299109 iterations : Training Loss =  0.10151099976372241; Validation Loss = 0.123944172933011\n",
            "Cost after 299110 iterations : Training Loss =  0.10151099976372241; Validation Loss = 0.12394417293121841\n",
            "Cost after 299111 iterations : Training Loss =  0.10151099976372238; Validation Loss = 0.12394417292942615\n",
            "Cost after 299112 iterations : Training Loss =  0.10151099976372234; Validation Loss = 0.12394417292763374\n",
            "Cost after 299113 iterations : Training Loss =  0.10151099976372237; Validation Loss = 0.12394417292584212\n",
            "Cost after 299114 iterations : Training Loss =  0.10151099976372244; Validation Loss = 0.12394417292404965\n",
            "Cost after 299115 iterations : Training Loss =  0.10151099976372226; Validation Loss = 0.12394417292225818\n",
            "Cost after 299116 iterations : Training Loss =  0.10151099976372241; Validation Loss = 0.12394417292046603\n",
            "Cost after 299117 iterations : Training Loss =  0.10151099976372238; Validation Loss = 0.1239441729186742\n",
            "Cost after 299118 iterations : Training Loss =  0.10151099976372244; Validation Loss = 0.12394417291688237\n",
            "Cost after 299119 iterations : Training Loss =  0.10151099976372228; Validation Loss = 0.12394417291509088\n",
            "Cost after 299120 iterations : Training Loss =  0.10151099976372213; Validation Loss = 0.12394417291329958\n",
            "Cost after 299121 iterations : Training Loss =  0.10151099976372231; Validation Loss = 0.12394417291150803\n",
            "Cost after 299122 iterations : Training Loss =  0.10151099976372249; Validation Loss = 0.1239441729097167\n",
            "Cost after 299123 iterations : Training Loss =  0.10151099976372231; Validation Loss = 0.12394417290792563\n",
            "Cost after 299124 iterations : Training Loss =  0.10151099976372223; Validation Loss = 0.1239441729061345\n",
            "Cost after 299125 iterations : Training Loss =  0.10151099976372226; Validation Loss = 0.12394417290434338\n",
            "Cost after 299126 iterations : Training Loss =  0.10151099976372244; Validation Loss = 0.1239441729025523\n",
            "Cost after 299127 iterations : Training Loss =  0.10151099976372222; Validation Loss = 0.12394417290076148\n",
            "Cost after 299128 iterations : Training Loss =  0.1015109997637223; Validation Loss = 0.12394417289897092\n",
            "Cost after 299129 iterations : Training Loss =  0.10151099976372224; Validation Loss = 0.12394417289718021\n",
            "Cost after 299130 iterations : Training Loss =  0.10151099976372244; Validation Loss = 0.12394417289538952\n",
            "Cost after 299131 iterations : Training Loss =  0.10151099976372235; Validation Loss = 0.1239441728935988\n",
            "Cost after 299132 iterations : Training Loss =  0.10151099976372222; Validation Loss = 0.12394417289180863\n",
            "Cost after 299133 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.12394417289001823\n",
            "Cost after 299134 iterations : Training Loss =  0.10151099976372242; Validation Loss = 0.12394417288822802\n",
            "Cost after 299135 iterations : Training Loss =  0.10151099976372224; Validation Loss = 0.12394417288643812\n",
            "Cost after 299136 iterations : Training Loss =  0.10151099976372234; Validation Loss = 0.12394417288464805\n",
            "Cost after 299137 iterations : Training Loss =  0.10151099976372219; Validation Loss = 0.12394417288285799\n",
            "Cost after 299138 iterations : Training Loss =  0.10151099976372223; Validation Loss = 0.1239441728810677\n",
            "Cost after 299139 iterations : Training Loss =  0.10151099976372242; Validation Loss = 0.1239441728792785\n",
            "Cost after 299140 iterations : Training Loss =  0.10151099976372244; Validation Loss = 0.12394417287748852\n",
            "Cost after 299141 iterations : Training Loss =  0.1015109997637223; Validation Loss = 0.12394417287569898\n",
            "Cost after 299142 iterations : Training Loss =  0.10151099976372223; Validation Loss = 0.12394417287390946\n",
            "Cost after 299143 iterations : Training Loss =  0.10151099976372235; Validation Loss = 0.12394417287212019\n",
            "Cost after 299144 iterations : Training Loss =  0.10151099976372234; Validation Loss = 0.12394417287033077\n",
            "Cost after 299145 iterations : Training Loss =  0.10151099976372222; Validation Loss = 0.1239441728685416\n",
            "Cost after 299146 iterations : Training Loss =  0.10151099976372219; Validation Loss = 0.12394417286675266\n",
            "Cost after 299147 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.12394417286496356\n",
            "Cost after 299148 iterations : Training Loss =  0.10151099976372223; Validation Loss = 0.12394417286317447\n",
            "Cost after 299149 iterations : Training Loss =  0.10151099976372219; Validation Loss = 0.12394417286138555\n",
            "Cost after 299150 iterations : Training Loss =  0.10151099976372223; Validation Loss = 0.12394417285959712\n",
            "Cost after 299151 iterations : Training Loss =  0.10151099976372209; Validation Loss = 0.12394417285780829\n",
            "Cost after 299152 iterations : Training Loss =  0.10151099976372228; Validation Loss = 0.12394417285601968\n",
            "Cost after 299153 iterations : Training Loss =  0.10151099976372197; Validation Loss = 0.12394417285423114\n",
            "Cost after 299154 iterations : Training Loss =  0.10151099976372228; Validation Loss = 0.12394417285244284\n",
            "Cost after 299155 iterations : Training Loss =  0.10151099976372235; Validation Loss = 0.12394417285065432\n",
            "Cost after 299156 iterations : Training Loss =  0.1015109997637223; Validation Loss = 0.12394417284886626\n",
            "Cost after 299157 iterations : Training Loss =  0.10151099976372217; Validation Loss = 0.12394417284707814\n",
            "Cost after 299158 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.12394417284528995\n",
            "Cost after 299159 iterations : Training Loss =  0.10151099976372217; Validation Loss = 0.12394417284350218\n",
            "Cost after 299160 iterations : Training Loss =  0.10151099976372228; Validation Loss = 0.12394417284171422\n",
            "Cost after 299161 iterations : Training Loss =  0.10151099976372213; Validation Loss = 0.1239441728399263\n",
            "Cost after 299162 iterations : Training Loss =  0.10151099976372209; Validation Loss = 0.12394417283813873\n",
            "Cost after 299163 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.1239441728363511\n",
            "Cost after 299164 iterations : Training Loss =  0.10151099976372203; Validation Loss = 0.12394417283456349\n",
            "Cost after 299165 iterations : Training Loss =  0.10151099976372223; Validation Loss = 0.12394417283277614\n",
            "Cost after 299166 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.12394417283098898\n",
            "Cost after 299167 iterations : Training Loss =  0.10151099976372224; Validation Loss = 0.12394417282920149\n",
            "Cost after 299168 iterations : Training Loss =  0.10151099976372202; Validation Loss = 0.12394417282741475\n",
            "Cost after 299169 iterations : Training Loss =  0.1015109997637221; Validation Loss = 0.12394417282562743\n",
            "Cost after 299170 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.12394417282384063\n",
            "Cost after 299171 iterations : Training Loss =  0.10151099976372203; Validation Loss = 0.12394417282205364\n",
            "Cost after 299172 iterations : Training Loss =  0.10151099976372213; Validation Loss = 0.12394417282026692\n",
            "Cost after 299173 iterations : Training Loss =  0.10151099976372212; Validation Loss = 0.12394417281848036\n",
            "Cost after 299174 iterations : Training Loss =  0.10151099976372242; Validation Loss = 0.12394417281669406\n",
            "Cost after 299175 iterations : Training Loss =  0.10151099976372219; Validation Loss = 0.12394417281490702\n",
            "Cost after 299176 iterations : Training Loss =  0.10151099976372212; Validation Loss = 0.12394417281312073\n",
            "Cost after 299177 iterations : Training Loss =  0.10151099976372237; Validation Loss = 0.1239441728113346\n",
            "Cost after 299178 iterations : Training Loss =  0.10151099976372206; Validation Loss = 0.12394417280954825\n",
            "Cost after 299179 iterations : Training Loss =  0.1015109997637221; Validation Loss = 0.12394417280776214\n",
            "Cost after 299180 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.12394417280597604\n",
            "Cost after 299181 iterations : Training Loss =  0.10151099976372209; Validation Loss = 0.12394417280419026\n",
            "Cost after 299182 iterations : Training Loss =  0.10151099976372203; Validation Loss = 0.12394417280240431\n",
            "Cost after 299183 iterations : Training Loss =  0.10151099976372213; Validation Loss = 0.12394417280061855\n",
            "Cost after 299184 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.12394417279883264\n",
            "Cost after 299185 iterations : Training Loss =  0.10151099976372222; Validation Loss = 0.12394417279704718\n",
            "Cost after 299186 iterations : Training Loss =  0.10151099976372197; Validation Loss = 0.12394417279526185\n",
            "Cost after 299187 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.12394417279347626\n",
            "Cost after 299188 iterations : Training Loss =  0.10151099976372202; Validation Loss = 0.12394417279169107\n",
            "Cost after 299189 iterations : Training Loss =  0.10151099976372213; Validation Loss = 0.12394417278990574\n",
            "Cost after 299190 iterations : Training Loss =  0.10151099976372217; Validation Loss = 0.1239441727881206\n",
            "Cost after 299191 iterations : Training Loss =  0.10151099976372192; Validation Loss = 0.12394417278633556\n",
            "Cost after 299192 iterations : Training Loss =  0.10151099976372216; Validation Loss = 0.1239441727845507\n",
            "Cost after 299193 iterations : Training Loss =  0.10151099976372206; Validation Loss = 0.12394417278276572\n",
            "Cost after 299194 iterations : Training Loss =  0.1015109997637219; Validation Loss = 0.12394417278098101\n",
            "Cost after 299195 iterations : Training Loss =  0.10151099976372213; Validation Loss = 0.12394417277919623\n",
            "Cost after 299196 iterations : Training Loss =  0.10151099976372206; Validation Loss = 0.1239441727774118\n",
            "Cost after 299197 iterations : Training Loss =  0.10151099976372209; Validation Loss = 0.12394417277562726\n",
            "Cost after 299198 iterations : Training Loss =  0.10151099976372197; Validation Loss = 0.12394417277384286\n",
            "Cost after 299199 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.12394417277205862\n",
            "Cost after 299200 iterations : Training Loss =  0.10151099976372191; Validation Loss = 0.12394417277027417\n",
            "Cost after 299201 iterations : Training Loss =  0.10151099976372209; Validation Loss = 0.12394417276849037\n",
            "Cost after 299202 iterations : Training Loss =  0.10151099976372192; Validation Loss = 0.12394417276670627\n",
            "Cost after 299203 iterations : Training Loss =  0.10151099976372199; Validation Loss = 0.12394417276492217\n",
            "Cost after 299204 iterations : Training Loss =  0.10151099976372191; Validation Loss = 0.1239441727631386\n",
            "Cost after 299205 iterations : Training Loss =  0.1015109997637221; Validation Loss = 0.12394417276135451\n",
            "Cost after 299206 iterations : Training Loss =  0.10151099976372198; Validation Loss = 0.12394417275957093\n",
            "Cost after 299207 iterations : Training Loss =  0.10151099976372212; Validation Loss = 0.123944172757787\n",
            "Cost after 299208 iterations : Training Loss =  0.1015109997637219; Validation Loss = 0.12394417275600372\n",
            "Cost after 299209 iterations : Training Loss =  0.10151099976372202; Validation Loss = 0.12394417275422051\n",
            "Cost after 299210 iterations : Training Loss =  0.10151099976372184; Validation Loss = 0.123944172752437\n",
            "Cost after 299211 iterations : Training Loss =  0.10151099976372191; Validation Loss = 0.12394417275065349\n",
            "Cost after 299212 iterations : Training Loss =  0.10151099976372206; Validation Loss = 0.12394417274887061\n",
            "Cost after 299213 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.12394417274708772\n",
            "Cost after 299214 iterations : Training Loss =  0.1015109997637219; Validation Loss = 0.12394417274530456\n",
            "Cost after 299215 iterations : Training Loss =  0.10151099976372209; Validation Loss = 0.12394417274352174\n",
            "Cost after 299216 iterations : Training Loss =  0.10151099976372187; Validation Loss = 0.12394417274173915\n",
            "Cost after 299217 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.1239441727399563\n",
            "Cost after 299218 iterations : Training Loss =  0.1015109997637219; Validation Loss = 0.12394417273817368\n",
            "Cost after 299219 iterations : Training Loss =  0.10151099976372191; Validation Loss = 0.12394417273639115\n",
            "Cost after 299220 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.12394417273460899\n",
            "Cost after 299221 iterations : Training Loss =  0.10151099976372203; Validation Loss = 0.12394417273282642\n",
            "Cost after 299222 iterations : Training Loss =  0.1015109997637221; Validation Loss = 0.12394417273104431\n",
            "Cost after 299223 iterations : Training Loss =  0.10151099976372184; Validation Loss = 0.12394417272926221\n",
            "Cost after 299224 iterations : Training Loss =  0.10151099976372187; Validation Loss = 0.12394417272748015\n",
            "Cost after 299225 iterations : Training Loss =  0.1015109997637219; Validation Loss = 0.12394417272569822\n",
            "Cost after 299226 iterations : Training Loss =  0.10151099976372203; Validation Loss = 0.12394417272391631\n",
            "Cost after 299227 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.1239441727221344\n",
            "Cost after 299228 iterations : Training Loss =  0.10151099976372192; Validation Loss = 0.12394417272035256\n",
            "Cost after 299229 iterations : Training Loss =  0.10151099976372184; Validation Loss = 0.12394417271857118\n",
            "Cost after 299230 iterations : Training Loss =  0.10151099976372177; Validation Loss = 0.12394417271678941\n",
            "Cost after 299231 iterations : Training Loss =  0.10151099976372185; Validation Loss = 0.12394417271500825\n",
            "Cost after 299232 iterations : Training Loss =  0.10151099976372178; Validation Loss = 0.12394417271322673\n",
            "Cost after 299233 iterations : Training Loss =  0.10151099976372206; Validation Loss = 0.12394417271144541\n",
            "Cost after 299234 iterations : Training Loss =  0.10151099976372197; Validation Loss = 0.1239441727096642\n",
            "Cost after 299235 iterations : Training Loss =  0.10151099976372178; Validation Loss = 0.12394417270788329\n",
            "Cost after 299236 iterations : Training Loss =  0.10151099976372187; Validation Loss = 0.12394417270610245\n",
            "Cost after 299237 iterations : Training Loss =  0.1015109997637219; Validation Loss = 0.1239441727043217\n",
            "Cost after 299238 iterations : Training Loss =  0.10151099976372197; Validation Loss = 0.12394417270254074\n",
            "Cost after 299239 iterations : Training Loss =  0.10151099976372185; Validation Loss = 0.12394417270076014\n",
            "Cost after 299240 iterations : Training Loss =  0.10151099976372184; Validation Loss = 0.12394417269897946\n",
            "Cost after 299241 iterations : Training Loss =  0.10151099976372185; Validation Loss = 0.12394417269719882\n",
            "Cost after 299242 iterations : Training Loss =  0.1015109997637218; Validation Loss = 0.12394417269541853\n",
            "Cost after 299243 iterations : Training Loss =  0.1015109997637219; Validation Loss = 0.12394417269363808\n",
            "Cost after 299244 iterations : Training Loss =  0.1015109997637219; Validation Loss = 0.12394417269185791\n",
            "Cost after 299245 iterations : Training Loss =  0.10151099976372185; Validation Loss = 0.1239441726900777\n",
            "Cost after 299246 iterations : Training Loss =  0.10151099976372173; Validation Loss = 0.12394417268829784\n",
            "Cost after 299247 iterations : Training Loss =  0.10151099976372192; Validation Loss = 0.12394417268651758\n",
            "Cost after 299248 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.12394417268473776\n",
            "Cost after 299249 iterations : Training Loss =  0.10151099976372181; Validation Loss = 0.12394417268295772\n",
            "Cost after 299250 iterations : Training Loss =  0.10151099976372181; Validation Loss = 0.12394417268117831\n",
            "Cost after 299251 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.12394417267939867\n",
            "Cost after 299252 iterations : Training Loss =  0.10151099976372185; Validation Loss = 0.12394417267761934\n",
            "Cost after 299253 iterations : Training Loss =  0.10151099976372165; Validation Loss = 0.1239441726758396\n",
            "Cost after 299254 iterations : Training Loss =  0.1015109997637218; Validation Loss = 0.12394417267406016\n",
            "Cost after 299255 iterations : Training Loss =  0.10151099976372198; Validation Loss = 0.12394417267228107\n",
            "Cost after 299256 iterations : Training Loss =  0.10151099976372187; Validation Loss = 0.12394417267050171\n",
            "Cost after 299257 iterations : Training Loss =  0.10151099976372174; Validation Loss = 0.12394417266872303\n",
            "Cost after 299258 iterations : Training Loss =  0.10151099976372194; Validation Loss = 0.12394417266694391\n",
            "Cost after 299259 iterations : Training Loss =  0.10151099976372191; Validation Loss = 0.12394417266516461\n",
            "Cost after 299260 iterations : Training Loss =  0.10151099976372177; Validation Loss = 0.12394417266338614\n",
            "Cost after 299261 iterations : Training Loss =  0.10151099976372181; Validation Loss = 0.12394417266160762\n",
            "Cost after 299262 iterations : Training Loss =  0.1015109997637218; Validation Loss = 0.12394417265982888\n",
            "Cost after 299263 iterations : Training Loss =  0.10151099976372184; Validation Loss = 0.12394417265805037\n",
            "Cost after 299264 iterations : Training Loss =  0.10151099976372174; Validation Loss = 0.12394417265627192\n",
            "Cost after 299265 iterations : Training Loss =  0.10151099976372181; Validation Loss = 0.12394417265449346\n",
            "Cost after 299266 iterations : Training Loss =  0.10151099976372178; Validation Loss = 0.12394417265271501\n",
            "Cost after 299267 iterations : Training Loss =  0.10151099976372166; Validation Loss = 0.12394417265093685\n",
            "Cost after 299268 iterations : Training Loss =  0.10151099976372162; Validation Loss = 0.12394417264915893\n",
            "Cost after 299269 iterations : Training Loss =  0.10151099976372172; Validation Loss = 0.1239441726473811\n",
            "Cost after 299270 iterations : Training Loss =  0.10151099976372169; Validation Loss = 0.12394417264560322\n",
            "Cost after 299271 iterations : Training Loss =  0.10151099976372169; Validation Loss = 0.12394417264382548\n",
            "Cost after 299272 iterations : Training Loss =  0.10151099976372172; Validation Loss = 0.12394417264204761\n",
            "Cost after 299273 iterations : Training Loss =  0.10151099976372162; Validation Loss = 0.12394417264027023\n",
            "Cost after 299274 iterations : Training Loss =  0.10151099976372173; Validation Loss = 0.12394417263849242\n",
            "Cost after 299275 iterations : Training Loss =  0.10151099976372177; Validation Loss = 0.12394417263671502\n",
            "Cost after 299276 iterations : Training Loss =  0.10151099976372167; Validation Loss = 0.12394417263493761\n",
            "Cost after 299277 iterations : Training Loss =  0.10151099976372167; Validation Loss = 0.12394417263316033\n",
            "Cost after 299278 iterations : Training Loss =  0.10151099976372177; Validation Loss = 0.12394417263138294\n",
            "Cost after 299279 iterations : Training Loss =  0.10151099976372177; Validation Loss = 0.12394417262960618\n",
            "Cost after 299280 iterations : Training Loss =  0.10151099976372174; Validation Loss = 0.123944172627829\n",
            "Cost after 299281 iterations : Training Loss =  0.1015109997637216; Validation Loss = 0.1239441726260523\n",
            "Cost after 299282 iterations : Training Loss =  0.10151099976372159; Validation Loss = 0.12394417262427558\n",
            "Cost after 299283 iterations : Training Loss =  0.10151099976372172; Validation Loss = 0.12394417262249846\n",
            "Cost after 299284 iterations : Training Loss =  0.10151099976372169; Validation Loss = 0.12394417262072203\n",
            "Cost after 299285 iterations : Training Loss =  0.10151099976372165; Validation Loss = 0.12394417261894536\n",
            "Cost after 299286 iterations : Training Loss =  0.10151099976372166; Validation Loss = 0.12394417261716888\n",
            "Cost after 299287 iterations : Training Loss =  0.1015109997637216; Validation Loss = 0.12394417261539271\n",
            "Cost after 299288 iterations : Training Loss =  0.10151099976372172; Validation Loss = 0.12394417261361636\n",
            "Cost after 299289 iterations : Training Loss =  0.10151099976372174; Validation Loss = 0.12394417261183996\n",
            "Cost after 299290 iterations : Training Loss =  0.10151099976372153; Validation Loss = 0.1239441726100643\n",
            "Cost after 299291 iterations : Training Loss =  0.10151099976372155; Validation Loss = 0.12394417260828805\n",
            "Cost after 299292 iterations : Training Loss =  0.10151099976372159; Validation Loss = 0.1239441726065121\n",
            "Cost after 299293 iterations : Training Loss =  0.10151099976372173; Validation Loss = 0.1239441726047361\n",
            "Cost after 299294 iterations : Training Loss =  0.1015109997637216; Validation Loss = 0.12394417260296033\n",
            "Cost after 299295 iterations : Training Loss =  0.1015109997637216; Validation Loss = 0.12394417260118515\n",
            "Cost after 299296 iterations : Training Loss =  0.1015109997637216; Validation Loss = 0.12394417259940939\n",
            "Cost after 299297 iterations : Training Loss =  0.10151099976372159; Validation Loss = 0.12394417259763403\n",
            "Cost after 299298 iterations : Training Loss =  0.1015109997637215; Validation Loss = 0.12394417259585845\n",
            "Cost after 299299 iterations : Training Loss =  0.10151099976372145; Validation Loss = 0.12394417259408347\n",
            "Cost after 299300 iterations : Training Loss =  0.10151099976372152; Validation Loss = 0.12394417259230794\n",
            "Cost after 299301 iterations : Training Loss =  0.10151099976372177; Validation Loss = 0.12394417259053299\n",
            "Cost after 299302 iterations : Training Loss =  0.10151099976372155; Validation Loss = 0.123944172588758\n",
            "Cost after 299303 iterations : Training Loss =  0.1015109997637215; Validation Loss = 0.12394417258698302\n",
            "Cost after 299304 iterations : Training Loss =  0.10151099976372155; Validation Loss = 0.123944172585208\n",
            "Cost after 299305 iterations : Training Loss =  0.10151099976372159; Validation Loss = 0.12394417258343332\n",
            "Cost after 299306 iterations : Training Loss =  0.10151099976372166; Validation Loss = 0.12394417258165892\n",
            "Cost after 299307 iterations : Training Loss =  0.10151099976372152; Validation Loss = 0.12394417257988462\n",
            "Cost after 299308 iterations : Training Loss =  0.10151099976372158; Validation Loss = 0.12394417257810984\n",
            "Cost after 299309 iterations : Training Loss =  0.10151099976372159; Validation Loss = 0.12394417257633572\n",
            "Cost after 299310 iterations : Training Loss =  0.10151099976372177; Validation Loss = 0.12394417257456136\n",
            "Cost after 299311 iterations : Training Loss =  0.10151099976372147; Validation Loss = 0.12394417257278693\n",
            "Cost after 299312 iterations : Training Loss =  0.10151099976372153; Validation Loss = 0.12394417257101281\n",
            "Cost after 299313 iterations : Training Loss =  0.10151099976372147; Validation Loss = 0.12394417256923883\n",
            "Cost after 299314 iterations : Training Loss =  0.10151099976372167; Validation Loss = 0.12394417256746484\n",
            "Cost after 299315 iterations : Training Loss =  0.10151099976372159; Validation Loss = 0.12394417256569097\n",
            "Cost after 299316 iterations : Training Loss =  0.1015109997637216; Validation Loss = 0.12394417256391746\n",
            "Cost after 299317 iterations : Training Loss =  0.10151099976372153; Validation Loss = 0.12394417256214386\n",
            "Cost after 299318 iterations : Training Loss =  0.1015109997637215; Validation Loss = 0.12394417256036995\n",
            "Cost after 299319 iterations : Training Loss =  0.1015109997637215; Validation Loss = 0.12394417255859612\n",
            "Cost after 299320 iterations : Training Loss =  0.10151099976372153; Validation Loss = 0.12394417255682334\n",
            "Cost after 299321 iterations : Training Loss =  0.1015109997637215; Validation Loss = 0.12394417255504989\n",
            "Cost after 299322 iterations : Training Loss =  0.10151099976372153; Validation Loss = 0.12394417255327676\n",
            "Cost after 299323 iterations : Training Loss =  0.10151099976372153; Validation Loss = 0.12394417255150345\n",
            "Cost after 299324 iterations : Training Loss =  0.10151099976372147; Validation Loss = 0.1239441725497305\n",
            "Cost after 299325 iterations : Training Loss =  0.10151099976372145; Validation Loss = 0.12394417254795755\n",
            "Cost after 299326 iterations : Training Loss =  0.10151099976372152; Validation Loss = 0.12394417254618445\n",
            "Cost after 299327 iterations : Training Loss =  0.10151099976372155; Validation Loss = 0.12394417254441198\n",
            "Cost after 299328 iterations : Training Loss =  0.10151099976372152; Validation Loss = 0.12394417254263912\n",
            "Cost after 299329 iterations : Training Loss =  0.10151099976372158; Validation Loss = 0.1239441725408669\n",
            "Cost after 299330 iterations : Training Loss =  0.1015109997637216; Validation Loss = 0.12394417253909418\n",
            "Cost after 299331 iterations : Training Loss =  0.10151099976372177; Validation Loss = 0.12394417253732186\n",
            "Cost after 299332 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.12394417253554972\n",
            "Cost after 299333 iterations : Training Loss =  0.10151099976372162; Validation Loss = 0.12394417253377729\n",
            "Cost after 299334 iterations : Training Loss =  0.1015109997637215; Validation Loss = 0.12394417253200497\n",
            "Cost after 299335 iterations : Training Loss =  0.10151099976372159; Validation Loss = 0.1239441725302333\n",
            "Cost after 299336 iterations : Training Loss =  0.10151099976372142; Validation Loss = 0.12394417252846118\n",
            "Cost after 299337 iterations : Training Loss =  0.10151099976372152; Validation Loss = 0.12394417252668906\n",
            "Cost after 299338 iterations : Training Loss =  0.10151099976372155; Validation Loss = 0.12394417252491749\n",
            "Cost after 299339 iterations : Training Loss =  0.1015109997637215; Validation Loss = 0.12394417252314582\n",
            "Cost after 299340 iterations : Training Loss =  0.10151099976372142; Validation Loss = 0.12394417252137416\n",
            "Cost after 299341 iterations : Training Loss =  0.10151099976372145; Validation Loss = 0.12394417251960262\n",
            "Cost after 299342 iterations : Training Loss =  0.10151099976372141; Validation Loss = 0.12394417251783128\n",
            "Cost after 299343 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.12394417251606\n",
            "Cost after 299344 iterations : Training Loss =  0.10151099976372142; Validation Loss = 0.12394417251428873\n",
            "Cost after 299345 iterations : Training Loss =  0.10151099976372135; Validation Loss = 0.12394417251251762\n",
            "Cost after 299346 iterations : Training Loss =  0.10151099976372133; Validation Loss = 0.12394417251074659\n",
            "Cost after 299347 iterations : Training Loss =  0.10151099976372155; Validation Loss = 0.12394417250897583\n",
            "Cost after 299348 iterations : Training Loss =  0.10151099976372134; Validation Loss = 0.12394417250720471\n",
            "Cost after 299349 iterations : Training Loss =  0.10151099976372134; Validation Loss = 0.12394417250543406\n",
            "Cost after 299350 iterations : Training Loss =  0.10151099976372141; Validation Loss = 0.12394417250366331\n",
            "Cost after 299351 iterations : Training Loss =  0.10151099976372133; Validation Loss = 0.12394417250189253\n",
            "Cost after 299352 iterations : Training Loss =  0.10151099976372137; Validation Loss = 0.12394417250012195\n",
            "Cost after 299353 iterations : Training Loss =  0.10151099976372147; Validation Loss = 0.12394417249835166\n",
            "Cost after 299354 iterations : Training Loss =  0.10151099976372137; Validation Loss = 0.12394417249658142\n",
            "Cost after 299355 iterations : Training Loss =  0.10151099976372117; Validation Loss = 0.12394417249481109\n",
            "Cost after 299356 iterations : Training Loss =  0.10151099976372145; Validation Loss = 0.12394417249304102\n",
            "Cost after 299357 iterations : Training Loss =  0.10151099976372148; Validation Loss = 0.1239441724912707\n",
            "Cost after 299358 iterations : Training Loss =  0.10151099976372137; Validation Loss = 0.12394417248950083\n",
            "Cost after 299359 iterations : Training Loss =  0.10151099976372135; Validation Loss = 0.12394417248773092\n",
            "Cost after 299360 iterations : Training Loss =  0.10151099976372133; Validation Loss = 0.1239441724859611\n",
            "Cost after 299361 iterations : Training Loss =  0.10151099976372127; Validation Loss = 0.12394417248419164\n",
            "Cost after 299362 iterations : Training Loss =  0.10151099976372127; Validation Loss = 0.12394417248242177\n",
            "Cost after 299363 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.12394417248065252\n",
            "Cost after 299364 iterations : Training Loss =  0.1015109997637213; Validation Loss = 0.12394417247888288\n",
            "Cost after 299365 iterations : Training Loss =  0.10151099976372127; Validation Loss = 0.12394417247711366\n",
            "Cost after 299366 iterations : Training Loss =  0.10151099976372147; Validation Loss = 0.12394417247534417\n",
            "Cost after 299367 iterations : Training Loss =  0.10151099976372127; Validation Loss = 0.12394417247357549\n",
            "Cost after 299368 iterations : Training Loss =  0.10151099976372113; Validation Loss = 0.12394417247180597\n",
            "Cost after 299369 iterations : Training Loss =  0.10151099976372129; Validation Loss = 0.12394417247003695\n",
            "Cost after 299370 iterations : Training Loss =  0.10151099976372122; Validation Loss = 0.1239441724682682\n",
            "Cost after 299371 iterations : Training Loss =  0.10151099976372134; Validation Loss = 0.12394417246649944\n",
            "Cost after 299372 iterations : Training Loss =  0.10151099976372145; Validation Loss = 0.12394417246473087\n",
            "Cost after 299373 iterations : Training Loss =  0.10151099976372134; Validation Loss = 0.12394417246296201\n",
            "Cost after 299374 iterations : Training Loss =  0.10151099976372134; Validation Loss = 0.12394417246119353\n",
            "Cost after 299375 iterations : Training Loss =  0.10151099976372141; Validation Loss = 0.12394417245942531\n",
            "Cost after 299376 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.12394417245765656\n",
            "Cost after 299377 iterations : Training Loss =  0.10151099976372141; Validation Loss = 0.12394417245588842\n",
            "Cost after 299378 iterations : Training Loss =  0.1015109997637213; Validation Loss = 0.12394417245412026\n",
            "Cost after 299379 iterations : Training Loss =  0.1015109997637213; Validation Loss = 0.1239441724523525\n",
            "Cost after 299380 iterations : Training Loss =  0.10151099976372134; Validation Loss = 0.12394417245058445\n",
            "Cost after 299381 iterations : Training Loss =  0.10151099976372133; Validation Loss = 0.12394417244881632\n",
            "Cost after 299382 iterations : Training Loss =  0.1015109997637212; Validation Loss = 0.12394417244704858\n",
            "Cost after 299383 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.12394417244528073\n",
            "Cost after 299384 iterations : Training Loss =  0.1015109997637213; Validation Loss = 0.123944172443513\n",
            "Cost after 299385 iterations : Training Loss =  0.10151099976372133; Validation Loss = 0.1239441724417457\n",
            "Cost after 299386 iterations : Training Loss =  0.1015109997637214; Validation Loss = 0.12394417243997803\n",
            "Cost after 299387 iterations : Training Loss =  0.10151099976372129; Validation Loss = 0.12394417243821078\n",
            "Cost after 299388 iterations : Training Loss =  0.1015109997637212; Validation Loss = 0.12394417243644357\n",
            "Cost after 299389 iterations : Training Loss =  0.10151099976372129; Validation Loss = 0.12394417243467619\n",
            "Cost after 299390 iterations : Training Loss =  0.10151099976372108; Validation Loss = 0.12394417243290909\n",
            "Cost after 299391 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.12394417243114203\n",
            "Cost after 299392 iterations : Training Loss =  0.10151099976372133; Validation Loss = 0.12394417242937536\n",
            "Cost after 299393 iterations : Training Loss =  0.10151099976372134; Validation Loss = 0.12394417242760832\n",
            "Cost after 299394 iterations : Training Loss =  0.10151099976372123; Validation Loss = 0.12394417242584185\n",
            "Cost after 299395 iterations : Training Loss =  0.10151099976372123; Validation Loss = 0.12394417242407502\n",
            "Cost after 299396 iterations : Training Loss =  0.10151099976372122; Validation Loss = 0.1239441724223088\n",
            "Cost after 299397 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.12394417242054202\n",
            "Cost after 299398 iterations : Training Loss =  0.1015109997637212; Validation Loss = 0.12394417241877571\n",
            "Cost after 299399 iterations : Training Loss =  0.10151099976372104; Validation Loss = 0.1239441724170093\n",
            "Cost after 299400 iterations : Training Loss =  0.10151099976372122; Validation Loss = 0.12394417241524307\n",
            "Cost after 299401 iterations : Training Loss =  0.10151099976372117; Validation Loss = 0.12394417241347692\n",
            "Cost after 299402 iterations : Training Loss =  0.10151099976372113; Validation Loss = 0.1239441724117109\n",
            "Cost after 299403 iterations : Training Loss =  0.10151099976372129; Validation Loss = 0.12394417240994492\n",
            "Cost after 299404 iterations : Training Loss =  0.10151099976372102; Validation Loss = 0.12394417240817925\n",
            "Cost after 299405 iterations : Training Loss =  0.10151099976372117; Validation Loss = 0.12394417240641321\n",
            "Cost after 299406 iterations : Training Loss =  0.10151099976372127; Validation Loss = 0.12394417240464783\n",
            "Cost after 299407 iterations : Training Loss =  0.10151099976372116; Validation Loss = 0.12394417240288202\n",
            "Cost after 299408 iterations : Training Loss =  0.10151099976372116; Validation Loss = 0.12394417240111687\n",
            "Cost after 299409 iterations : Training Loss =  0.10151099976372123; Validation Loss = 0.1239441723993517\n",
            "Cost after 299410 iterations : Training Loss =  0.10151099976372109; Validation Loss = 0.12394417239758596\n",
            "Cost after 299411 iterations : Training Loss =  0.10151099976372122; Validation Loss = 0.12394417239582074\n",
            "Cost after 299412 iterations : Training Loss =  0.10151099976372101; Validation Loss = 0.1239441723940557\n",
            "Cost after 299413 iterations : Training Loss =  0.10151099976372117; Validation Loss = 0.12394417239229079\n",
            "Cost after 299414 iterations : Training Loss =  0.10151099976372122; Validation Loss = 0.12394417239052542\n",
            "Cost after 299415 iterations : Training Loss =  0.10151099976372122; Validation Loss = 0.12394417238876078\n",
            "Cost after 299416 iterations : Training Loss =  0.1015109997637211; Validation Loss = 0.12394417238699619\n",
            "Cost after 299417 iterations : Training Loss =  0.10151099976372133; Validation Loss = 0.12394417238523125\n",
            "Cost after 299418 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.12394417238346661\n",
            "Cost after 299419 iterations : Training Loss =  0.10151099976372117; Validation Loss = 0.12394417238170202\n",
            "Cost after 299420 iterations : Training Loss =  0.10151099976372124; Validation Loss = 0.123944172379938\n",
            "Cost after 299421 iterations : Training Loss =  0.10151099976372115; Validation Loss = 0.12394417237817358\n",
            "Cost after 299422 iterations : Training Loss =  0.10151099976372115; Validation Loss = 0.12394417237640935\n",
            "Cost after 299423 iterations : Training Loss =  0.10151099976372108; Validation Loss = 0.12394417237464517\n",
            "Cost after 299424 iterations : Training Loss =  0.10151099976372105; Validation Loss = 0.12394417237288115\n",
            "Cost after 299425 iterations : Training Loss =  0.10151099976372108; Validation Loss = 0.12394417237111714\n",
            "Cost after 299426 iterations : Training Loss =  0.10151099976372116; Validation Loss = 0.12394417236935333\n",
            "Cost after 299427 iterations : Training Loss =  0.10151099976372092; Validation Loss = 0.12394417236758959\n",
            "Cost after 299428 iterations : Training Loss =  0.10151099976372102; Validation Loss = 0.12394417236582593\n",
            "Cost after 299429 iterations : Training Loss =  0.1015109997637212; Validation Loss = 0.12394417236406195\n",
            "Cost after 299430 iterations : Training Loss =  0.10151099976372105; Validation Loss = 0.12394417236229861\n",
            "Cost after 299431 iterations : Training Loss =  0.10151099976372104; Validation Loss = 0.12394417236053533\n",
            "Cost after 299432 iterations : Training Loss =  0.10151099976372097; Validation Loss = 0.12394417235877202\n",
            "Cost after 299433 iterations : Training Loss =  0.10151099976372115; Validation Loss = 0.12394417235700869\n",
            "Cost after 299434 iterations : Training Loss =  0.10151099976372109; Validation Loss = 0.12394417235524573\n",
            "Cost after 299435 iterations : Training Loss =  0.10151099976372098; Validation Loss = 0.12394417235348254\n",
            "Cost after 299436 iterations : Training Loss =  0.10151099976372108; Validation Loss = 0.1239441723517195\n",
            "Cost after 299437 iterations : Training Loss =  0.10151099976372115; Validation Loss = 0.12394417234995685\n",
            "Cost after 299438 iterations : Training Loss =  0.10151099976372098; Validation Loss = 0.12394417234819421\n",
            "Cost after 299439 iterations : Training Loss =  0.10151099976372098; Validation Loss = 0.12394417234643142\n",
            "Cost after 299440 iterations : Training Loss =  0.10151099976372095; Validation Loss = 0.12394417234466847\n",
            "Cost after 299441 iterations : Training Loss =  0.10151099976372097; Validation Loss = 0.12394417234290582\n",
            "Cost after 299442 iterations : Training Loss =  0.10151099976372102; Validation Loss = 0.12394417234114367\n",
            "Cost after 299443 iterations : Training Loss =  0.10151099976372116; Validation Loss = 0.1239441723393814\n",
            "Cost after 299444 iterations : Training Loss =  0.10151099976372104; Validation Loss = 0.12394417233761862\n",
            "Cost after 299445 iterations : Training Loss =  0.10151099976372102; Validation Loss = 0.12394417233585688\n",
            "Cost after 299446 iterations : Training Loss =  0.10151099976372091; Validation Loss = 0.12394417233409463\n",
            "Cost after 299447 iterations : Training Loss =  0.1015109997637209; Validation Loss = 0.12394417233233272\n",
            "Cost after 299448 iterations : Training Loss =  0.10151099976372097; Validation Loss = 0.12394417233057071\n",
            "Cost after 299449 iterations : Training Loss =  0.10151099976372098; Validation Loss = 0.123944172328809\n",
            "Cost after 299450 iterations : Training Loss =  0.10151099976372091; Validation Loss = 0.1239441723270473\n",
            "Cost after 299451 iterations : Training Loss =  0.10151099976372097; Validation Loss = 0.12394417232528555\n",
            "Cost after 299452 iterations : Training Loss =  0.10151099976372101; Validation Loss = 0.1239441723235239\n",
            "Cost after 299453 iterations : Training Loss =  0.10151099976372117; Validation Loss = 0.12394417232176266\n",
            "Cost after 299454 iterations : Training Loss =  0.10151099976372104; Validation Loss = 0.12394417232000128\n",
            "Cost after 299455 iterations : Training Loss =  0.10151099976372104; Validation Loss = 0.12394417231823988\n",
            "Cost after 299456 iterations : Training Loss =  0.10151099976372088; Validation Loss = 0.12394417231647857\n",
            "Cost after 299457 iterations : Training Loss =  0.10151099976372102; Validation Loss = 0.1239441723147176\n",
            "Cost after 299458 iterations : Training Loss =  0.10151099976372098; Validation Loss = 0.12394417231295655\n",
            "Cost after 299459 iterations : Training Loss =  0.10151099976372102; Validation Loss = 0.12394417231119582\n",
            "Cost after 299460 iterations : Training Loss =  0.1015109997637208; Validation Loss = 0.12394417230943514\n",
            "Cost after 299461 iterations : Training Loss =  0.10151099976372085; Validation Loss = 0.12394417230767424\n",
            "Cost after 299462 iterations : Training Loss =  0.10151099976372083; Validation Loss = 0.1239441723059135\n",
            "Cost after 299463 iterations : Training Loss =  0.10151099976372083; Validation Loss = 0.12394417230415299\n",
            "Cost after 299464 iterations : Training Loss =  0.10151099976372105; Validation Loss = 0.12394417230239244\n",
            "Cost after 299465 iterations : Training Loss =  0.10151099976372097; Validation Loss = 0.12394417230063207\n",
            "Cost after 299466 iterations : Training Loss =  0.10151099976372108; Validation Loss = 0.12394417229887188\n",
            "Cost after 299467 iterations : Training Loss =  0.10151099976372072; Validation Loss = 0.12394417229711163\n",
            "Cost after 299468 iterations : Training Loss =  0.10151099976372097; Validation Loss = 0.1239441722953516\n",
            "Cost after 299469 iterations : Training Loss =  0.10151099976372113; Validation Loss = 0.12394417229359164\n",
            "Cost after 299470 iterations : Training Loss =  0.10151099976372092; Validation Loss = 0.12394417229183137\n",
            "Cost after 299471 iterations : Training Loss =  0.10151099976372098; Validation Loss = 0.12394417229007192\n",
            "Cost after 299472 iterations : Training Loss =  0.10151099976372092; Validation Loss = 0.12394417228831216\n",
            "Cost after 299473 iterations : Training Loss =  0.10151099976372095; Validation Loss = 0.12394417228655204\n",
            "Cost after 299474 iterations : Training Loss =  0.10151099976372076; Validation Loss = 0.12394417228479263\n",
            "Cost after 299475 iterations : Training Loss =  0.10151099976372077; Validation Loss = 0.12394417228303287\n",
            "Cost after 299476 iterations : Training Loss =  0.10151099976372073; Validation Loss = 0.12394417228127379\n",
            "Cost after 299477 iterations : Training Loss =  0.10151099976372076; Validation Loss = 0.12394417227951458\n",
            "Cost after 299478 iterations : Training Loss =  0.1015109997637209; Validation Loss = 0.12394417227775502\n",
            "Cost after 299479 iterations : Training Loss =  0.10151099976372098; Validation Loss = 0.12394417227599613\n",
            "Cost after 299480 iterations : Training Loss =  0.10151099976372083; Validation Loss = 0.12394417227423692\n",
            "Cost after 299481 iterations : Training Loss =  0.10151099976372085; Validation Loss = 0.1239441722724778\n",
            "Cost after 299482 iterations : Training Loss =  0.10151099976372091; Validation Loss = 0.12394417227071934\n",
            "Cost after 299483 iterations : Training Loss =  0.10151099976372101; Validation Loss = 0.12394417226896023\n",
            "Cost after 299484 iterations : Training Loss =  0.10151099976372073; Validation Loss = 0.12394417226720153\n",
            "Cost after 299485 iterations : Training Loss =  0.1015109997637208; Validation Loss = 0.12394417226544292\n",
            "Cost after 299486 iterations : Training Loss =  0.1015109997637209; Validation Loss = 0.12394417226368466\n",
            "Cost after 299487 iterations : Training Loss =  0.10151099976372085; Validation Loss = 0.12394417226192628\n",
            "Cost after 299488 iterations : Training Loss =  0.10151099976372083; Validation Loss = 0.12394417226016799\n",
            "Cost after 299489 iterations : Training Loss =  0.10151099976372091; Validation Loss = 0.12394417225840951\n",
            "Cost after 299490 iterations : Training Loss =  0.10151099976372076; Validation Loss = 0.12394417225665119\n",
            "Cost after 299491 iterations : Training Loss =  0.1015109997637207; Validation Loss = 0.12394417225489317\n",
            "Cost after 299492 iterations : Training Loss =  0.1015109997637208; Validation Loss = 0.12394417225313528\n",
            "Cost after 299493 iterations : Training Loss =  0.10151099976372091; Validation Loss = 0.12394417225137766\n",
            "Cost after 299494 iterations : Training Loss =  0.10151099976372065; Validation Loss = 0.12394417224961979\n",
            "Cost after 299495 iterations : Training Loss =  0.1015109997637209; Validation Loss = 0.12394417224786214\n",
            "Cost after 299496 iterations : Training Loss =  0.1015109997637207; Validation Loss = 0.12394417224610442\n",
            "Cost after 299497 iterations : Training Loss =  0.1015109997637208; Validation Loss = 0.12394417224434708\n",
            "Cost after 299498 iterations : Training Loss =  0.10151099976372065; Validation Loss = 0.1239441722425893\n",
            "Cost after 299499 iterations : Training Loss =  0.10151099976372084; Validation Loss = 0.12394417224083211\n",
            "Cost after 299500 iterations : Training Loss =  0.10151099976372101; Validation Loss = 0.12394417223907465\n",
            "Cost after 299501 iterations : Training Loss =  0.10151099976372088; Validation Loss = 0.12394417223731771\n",
            "Cost after 299502 iterations : Training Loss =  0.10151099976372084; Validation Loss = 0.12394417223556048\n",
            "Cost after 299503 iterations : Training Loss =  0.10151099976372083; Validation Loss = 0.12394417223380368\n",
            "Cost after 299504 iterations : Training Loss =  0.10151099976372084; Validation Loss = 0.12394417223204655\n",
            "Cost after 299505 iterations : Training Loss =  0.10151099976372065; Validation Loss = 0.12394417223028988\n",
            "Cost after 299506 iterations : Training Loss =  0.10151099976372072; Validation Loss = 0.12394417222853306\n",
            "Cost after 299507 iterations : Training Loss =  0.10151099976372072; Validation Loss = 0.12394417222677628\n",
            "Cost after 299508 iterations : Training Loss =  0.10151099976372072; Validation Loss = 0.12394417222502002\n",
            "Cost after 299509 iterations : Training Loss =  0.10151099976372084; Validation Loss = 0.12394417222326339\n",
            "Cost after 299510 iterations : Training Loss =  0.10151099976372065; Validation Loss = 0.12394417222150701\n",
            "Cost after 299511 iterations : Training Loss =  0.10151099976372073; Validation Loss = 0.12394417221975083\n",
            "Cost after 299512 iterations : Training Loss =  0.10151099976372073; Validation Loss = 0.12394417221799447\n",
            "Cost after 299513 iterations : Training Loss =  0.10151099976372077; Validation Loss = 0.12394417221623834\n",
            "Cost after 299514 iterations : Training Loss =  0.10151099976372072; Validation Loss = 0.12394417221448226\n",
            "Cost after 299515 iterations : Training Loss =  0.10151099976372066; Validation Loss = 0.12394417221272624\n",
            "Cost after 299516 iterations : Training Loss =  0.1015109997637207; Validation Loss = 0.12394417221097065\n",
            "Cost after 299517 iterations : Training Loss =  0.10151099976372072; Validation Loss = 0.12394417220921498\n",
            "Cost after 299518 iterations : Training Loss =  0.10151099976372079; Validation Loss = 0.12394417220745933\n",
            "Cost after 299519 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.12394417220570354\n",
            "Cost after 299520 iterations : Training Loss =  0.10151099976372092; Validation Loss = 0.12394417220394825\n",
            "Cost after 299521 iterations : Training Loss =  0.10151099976372059; Validation Loss = 0.12394417220219278\n",
            "Cost after 299522 iterations : Training Loss =  0.10151099976372091; Validation Loss = 0.12394417220043741\n",
            "Cost after 299523 iterations : Training Loss =  0.10151099976372069; Validation Loss = 0.1239441721986822\n",
            "Cost after 299524 iterations : Training Loss =  0.10151099976372058; Validation Loss = 0.12394417219692713\n",
            "Cost after 299525 iterations : Training Loss =  0.10151099976372073; Validation Loss = 0.12394417219517224\n",
            "Cost after 299526 iterations : Training Loss =  0.10151099976372079; Validation Loss = 0.12394417219341698\n",
            "Cost after 299527 iterations : Training Loss =  0.10151099976372059; Validation Loss = 0.12394417219166226\n",
            "Cost after 299528 iterations : Training Loss =  0.10151099976372063; Validation Loss = 0.1239441721899073\n",
            "Cost after 299529 iterations : Training Loss =  0.10151099976372084; Validation Loss = 0.12394417218815294\n",
            "Cost after 299530 iterations : Training Loss =  0.10151099976372058; Validation Loss = 0.12394417218639835\n",
            "Cost after 299531 iterations : Training Loss =  0.10151099976372083; Validation Loss = 0.12394417218464403\n",
            "Cost after 299532 iterations : Training Loss =  0.1015109997637208; Validation Loss = 0.12394417218288938\n",
            "Cost after 299533 iterations : Training Loss =  0.10151099976372066; Validation Loss = 0.12394417218113518\n",
            "Cost after 299534 iterations : Training Loss =  0.10151099976372084; Validation Loss = 0.12394417217938067\n",
            "Cost after 299535 iterations : Training Loss =  0.10151099976372056; Validation Loss = 0.1239441721776269\n",
            "Cost after 299536 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.12394417217587293\n",
            "Cost after 299537 iterations : Training Loss =  0.10151099976372072; Validation Loss = 0.12394417217411872\n",
            "Cost after 299538 iterations : Training Loss =  0.10151099976372084; Validation Loss = 0.1239441721723649\n",
            "Cost after 299539 iterations : Training Loss =  0.10151099976372066; Validation Loss = 0.123944172170611\n",
            "Cost after 299540 iterations : Training Loss =  0.10151099976372051; Validation Loss = 0.12394417216885724\n",
            "Cost after 299541 iterations : Training Loss =  0.10151099976372076; Validation Loss = 0.12394417216710357\n",
            "Cost after 299542 iterations : Training Loss =  0.10151099976372056; Validation Loss = 0.12394417216534982\n",
            "Cost after 299543 iterations : Training Loss =  0.10151099976372054; Validation Loss = 0.12394417216359677\n",
            "Cost after 299544 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.12394417216184374\n",
            "Cost after 299545 iterations : Training Loss =  0.10151099976372069; Validation Loss = 0.12394417216009021\n",
            "Cost after 299546 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.12394417215833693\n",
            "Cost after 299547 iterations : Training Loss =  0.10151099976372059; Validation Loss = 0.12394417215658428\n",
            "Cost after 299548 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.12394417215483103\n",
            "Cost after 299549 iterations : Training Loss =  0.10151099976372048; Validation Loss = 0.12394417215307815\n",
            "Cost after 299550 iterations : Training Loss =  0.10151099976372066; Validation Loss = 0.12394417215132558\n",
            "Cost after 299551 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.12394417214957237\n",
            "Cost after 299552 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.12394417214782019\n",
            "Cost after 299553 iterations : Training Loss =  0.10151099976372056; Validation Loss = 0.12394417214606715\n",
            "Cost after 299554 iterations : Training Loss =  0.10151099976372052; Validation Loss = 0.12394417214431505\n",
            "Cost after 299555 iterations : Training Loss =  0.1015109997637207; Validation Loss = 0.12394417214256272\n",
            "Cost after 299556 iterations : Training Loss =  0.10151099976372066; Validation Loss = 0.12394417214081017\n",
            "Cost after 299557 iterations : Training Loss =  0.10151099976372058; Validation Loss = 0.12394417213905796\n",
            "Cost after 299558 iterations : Training Loss =  0.10151099976372058; Validation Loss = 0.12394417213730609\n",
            "Cost after 299559 iterations : Training Loss =  0.10151099976372048; Validation Loss = 0.12394417213555413\n",
            "Cost after 299560 iterations : Training Loss =  0.10151099976372058; Validation Loss = 0.12394417213380238\n",
            "Cost after 299561 iterations : Training Loss =  0.10151099976372056; Validation Loss = 0.12394417213205047\n",
            "Cost after 299562 iterations : Training Loss =  0.10151099976372054; Validation Loss = 0.12394417213029874\n",
            "Cost after 299563 iterations : Training Loss =  0.10151099976372047; Validation Loss = 0.12394417212854716\n",
            "Cost after 299564 iterations : Training Loss =  0.10151099976372066; Validation Loss = 0.12394417212679529\n",
            "Cost after 299565 iterations : Training Loss =  0.10151099976372041; Validation Loss = 0.12394417212504388\n",
            "Cost after 299566 iterations : Training Loss =  0.10151099976372065; Validation Loss = 0.12394417212329265\n",
            "Cost after 299567 iterations : Training Loss =  0.10151099976372056; Validation Loss = 0.12394417212154143\n",
            "Cost after 299568 iterations : Training Loss =  0.10151099976372065; Validation Loss = 0.12394417211978995\n",
            "Cost after 299569 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.12394417211803901\n",
            "Cost after 299570 iterations : Training Loss =  0.10151099976372066; Validation Loss = 0.1239441721162881\n",
            "Cost after 299571 iterations : Training Loss =  0.10151099976372052; Validation Loss = 0.12394417211453714\n",
            "Cost after 299572 iterations : Training Loss =  0.10151099976372056; Validation Loss = 0.12394417211278633\n",
            "Cost after 299573 iterations : Training Loss =  0.1015109997637207; Validation Loss = 0.12394417211103555\n",
            "Cost after 299574 iterations : Training Loss =  0.10151099976372059; Validation Loss = 0.12394417210928495\n",
            "Cost after 299575 iterations : Training Loss =  0.1015109997637206; Validation Loss = 0.1239441721075342\n",
            "Cost after 299576 iterations : Training Loss =  0.10151099976372059; Validation Loss = 0.12394417210578382\n",
            "Cost after 299577 iterations : Training Loss =  0.10151099976372047; Validation Loss = 0.12394417210403334\n",
            "Cost after 299578 iterations : Training Loss =  0.10151099976372056; Validation Loss = 0.12394417210228323\n",
            "Cost after 299579 iterations : Training Loss =  0.10151099976372063; Validation Loss = 0.12394417210053256\n",
            "Cost after 299580 iterations : Training Loss =  0.1015109997637204; Validation Loss = 0.12394417209878265\n",
            "Cost after 299581 iterations : Training Loss =  0.10151099976372044; Validation Loss = 0.12394417209703244\n",
            "Cost after 299582 iterations : Training Loss =  0.10151099976372047; Validation Loss = 0.1239441720952826\n",
            "Cost after 299583 iterations : Training Loss =  0.10151099976372052; Validation Loss = 0.12394417209353271\n",
            "Cost after 299584 iterations : Training Loss =  0.10151099976372054; Validation Loss = 0.12394417209178278\n",
            "Cost after 299585 iterations : Training Loss =  0.10151099976372044; Validation Loss = 0.12394417209003322\n",
            "Cost after 299586 iterations : Training Loss =  0.10151099976372052; Validation Loss = 0.12394417208828376\n",
            "Cost after 299587 iterations : Training Loss =  0.10151099976372033; Validation Loss = 0.12394417208653426\n",
            "Cost after 299588 iterations : Training Loss =  0.10151099976372056; Validation Loss = 0.12394417208478486\n",
            "Cost after 299589 iterations : Training Loss =  0.10151099976372045; Validation Loss = 0.12394417208303546\n",
            "Cost after 299590 iterations : Training Loss =  0.10151099976372033; Validation Loss = 0.12394417208128594\n",
            "Cost after 299591 iterations : Training Loss =  0.10151099976372044; Validation Loss = 0.12394417207953724\n",
            "Cost after 299592 iterations : Training Loss =  0.10151099976372054; Validation Loss = 0.1239441720777878\n",
            "Cost after 299593 iterations : Training Loss =  0.10151099976372058; Validation Loss = 0.12394417207603882\n",
            "Cost after 299594 iterations : Training Loss =  0.10151099976372052; Validation Loss = 0.12394417207429011\n",
            "Cost after 299595 iterations : Training Loss =  0.10151099976372019; Validation Loss = 0.12394417207254124\n",
            "Cost after 299596 iterations : Training Loss =  0.10151099976372044; Validation Loss = 0.12394417207079254\n",
            "Cost after 299597 iterations : Training Loss =  0.10151099976372022; Validation Loss = 0.12394417206904368\n",
            "Cost after 299598 iterations : Training Loss =  0.10151099976372027; Validation Loss = 0.1239441720672952\n",
            "Cost after 299599 iterations : Training Loss =  0.10151099976372027; Validation Loss = 0.12394417206554667\n",
            "Cost after 299600 iterations : Training Loss =  0.10151099976372052; Validation Loss = 0.12394417206379822\n",
            "Cost after 299601 iterations : Training Loss =  0.10151099976372036; Validation Loss = 0.12394417206205041\n",
            "Cost after 299602 iterations : Training Loss =  0.10151099976372041; Validation Loss = 0.12394417206030223\n",
            "Cost after 299603 iterations : Training Loss =  0.10151099976372038; Validation Loss = 0.12394417205855415\n",
            "Cost after 299604 iterations : Training Loss =  0.10151099976372041; Validation Loss = 0.12394417205680584\n",
            "Cost after 299605 iterations : Training Loss =  0.10151099976372038; Validation Loss = 0.1239441720550581\n",
            "Cost after 299606 iterations : Training Loss =  0.10151099976372044; Validation Loss = 0.12394417205331032\n",
            "Cost after 299607 iterations : Training Loss =  0.1015109997637204; Validation Loss = 0.12394417205156247\n",
            "Cost after 299608 iterations : Training Loss =  0.10151099976372034; Validation Loss = 0.12394417204981496\n",
            "Cost after 299609 iterations : Training Loss =  0.10151099976372031; Validation Loss = 0.1239441720480674\n",
            "Cost after 299610 iterations : Training Loss =  0.10151099976372031; Validation Loss = 0.1239441720463198\n",
            "Cost after 299611 iterations : Training Loss =  0.10151099976372038; Validation Loss = 0.12394417204457195\n",
            "Cost after 299612 iterations : Training Loss =  0.1015109997637204; Validation Loss = 0.12394417204282512\n",
            "Cost after 299613 iterations : Training Loss =  0.10151099976372038; Validation Loss = 0.12394417204107783\n",
            "Cost after 299614 iterations : Training Loss =  0.10151099976372033; Validation Loss = 0.12394417203933093\n",
            "Cost after 299615 iterations : Training Loss =  0.10151099976372054; Validation Loss = 0.12394417203758379\n",
            "Cost after 299616 iterations : Training Loss =  0.10151099976372024; Validation Loss = 0.12394417203583687\n",
            "Cost after 299617 iterations : Training Loss =  0.10151099976372044; Validation Loss = 0.12394417203409006\n",
            "Cost after 299618 iterations : Training Loss =  0.10151099976372024; Validation Loss = 0.12394417203234319\n",
            "Cost after 299619 iterations : Training Loss =  0.10151099976372034; Validation Loss = 0.12394417203059688\n",
            "Cost after 299620 iterations : Training Loss =  0.10151099976372038; Validation Loss = 0.12394417202884993\n",
            "Cost after 299621 iterations : Training Loss =  0.10151099976372047; Validation Loss = 0.12394417202710337\n",
            "Cost after 299622 iterations : Training Loss =  0.10151099976372033; Validation Loss = 0.12394417202535728\n",
            "Cost after 299623 iterations : Training Loss =  0.1015109997637204; Validation Loss = 0.12394417202361092\n",
            "Cost after 299624 iterations : Training Loss =  0.1015109997637204; Validation Loss = 0.12394417202186488\n",
            "Cost after 299625 iterations : Training Loss =  0.10151099976372027; Validation Loss = 0.12394417202011844\n",
            "Cost after 299626 iterations : Training Loss =  0.10151099976372038; Validation Loss = 0.12394417201837249\n",
            "Cost after 299627 iterations : Training Loss =  0.10151099976372031; Validation Loss = 0.12394417201662629\n",
            "Cost after 299628 iterations : Training Loss =  0.10151099976372024; Validation Loss = 0.12394417201488064\n",
            "Cost after 299629 iterations : Training Loss =  0.10151099976372026; Validation Loss = 0.12394417201313475\n",
            "Cost after 299630 iterations : Training Loss =  0.10151099976372041; Validation Loss = 0.12394417201138903\n",
            "Cost after 299631 iterations : Training Loss =  0.10151099976372033; Validation Loss = 0.12394417200964342\n",
            "Cost after 299632 iterations : Training Loss =  0.10151099976372031; Validation Loss = 0.12394417200789783\n",
            "Cost after 299633 iterations : Training Loss =  0.10151099976372034; Validation Loss = 0.12394417200615258\n",
            "Cost after 299634 iterations : Training Loss =  0.10151099976372036; Validation Loss = 0.12394417200440717\n",
            "Cost after 299635 iterations : Training Loss =  0.10151099976372029; Validation Loss = 0.12394417200266185\n",
            "Cost after 299636 iterations : Training Loss =  0.10151099976372024; Validation Loss = 0.12394417200091688\n",
            "Cost after 299637 iterations : Training Loss =  0.10151099976372029; Validation Loss = 0.12394417199917185\n",
            "Cost after 299638 iterations : Training Loss =  0.10151099976372024; Validation Loss = 0.12394417199742691\n",
            "Cost after 299639 iterations : Training Loss =  0.10151099976372027; Validation Loss = 0.12394417199568192\n",
            "Cost after 299640 iterations : Training Loss =  0.10151099976372022; Validation Loss = 0.12394417199393697\n",
            "Cost after 299641 iterations : Training Loss =  0.10151099976372029; Validation Loss = 0.12394417199219224\n",
            "Cost after 299642 iterations : Training Loss =  0.10151099976372012; Validation Loss = 0.12394417199044781\n",
            "Cost after 299643 iterations : Training Loss =  0.1015109997637202; Validation Loss = 0.12394417198870318\n",
            "Cost after 299644 iterations : Training Loss =  0.10151099976372013; Validation Loss = 0.1239441719869588\n",
            "Cost after 299645 iterations : Training Loss =  0.10151099976372015; Validation Loss = 0.12394417198521451\n",
            "Cost after 299646 iterations : Training Loss =  0.1015109997637202; Validation Loss = 0.12394417198346991\n",
            "Cost after 299647 iterations : Training Loss =  0.10151099976372027; Validation Loss = 0.12394417198172578\n",
            "Cost after 299648 iterations : Training Loss =  0.10151099976372015; Validation Loss = 0.12394417197998182\n",
            "Cost after 299649 iterations : Training Loss =  0.10151099976372034; Validation Loss = 0.1239441719782377\n",
            "Cost after 299650 iterations : Training Loss =  0.10151099976372033; Validation Loss = 0.12394417197649375\n",
            "Cost after 299651 iterations : Training Loss =  0.10151099976372019; Validation Loss = 0.12394417197475008\n",
            "Cost after 299652 iterations : Training Loss =  0.10151099976372022; Validation Loss = 0.123944171973006\n",
            "Cost after 299653 iterations : Training Loss =  0.10151099976372031; Validation Loss = 0.12394417197126263\n",
            "Cost after 299654 iterations : Training Loss =  0.10151099976372016; Validation Loss = 0.12394417196951917\n",
            "Cost after 299655 iterations : Training Loss =  0.10151099976372031; Validation Loss = 0.12394417196777555\n",
            "Cost after 299656 iterations : Training Loss =  0.10151099976372016; Validation Loss = 0.12394417196603236\n",
            "Cost after 299657 iterations : Training Loss =  0.10151099976372015; Validation Loss = 0.12394417196428893\n",
            "Cost after 299658 iterations : Training Loss =  0.10151099976371997; Validation Loss = 0.12394417196254551\n",
            "Cost after 299659 iterations : Training Loss =  0.10151099976372026; Validation Loss = 0.12394417196080248\n",
            "Cost after 299660 iterations : Training Loss =  0.10151099976372022; Validation Loss = 0.12394417195905964\n",
            "Cost after 299661 iterations : Training Loss =  0.10151099976372022; Validation Loss = 0.12394417195731627\n",
            "Cost after 299662 iterations : Training Loss =  0.10151099976372006; Validation Loss = 0.12394417195557353\n",
            "Cost after 299663 iterations : Training Loss =  0.1015109997637202; Validation Loss = 0.12394417195383058\n",
            "Cost after 299664 iterations : Training Loss =  0.10151099976372009; Validation Loss = 0.12394417195208828\n",
            "Cost after 299665 iterations : Training Loss =  0.10151099976372031; Validation Loss = 0.1239441719503456\n",
            "Cost after 299666 iterations : Training Loss =  0.10151099976372008; Validation Loss = 0.12394417194860317\n",
            "Cost after 299667 iterations : Training Loss =  0.10151099976372016; Validation Loss = 0.12394417194686076\n",
            "Cost after 299668 iterations : Training Loss =  0.10151099976372008; Validation Loss = 0.12394417194511828\n",
            "Cost after 299669 iterations : Training Loss =  0.1015109997637202; Validation Loss = 0.12394417194337594\n",
            "Cost after 299670 iterations : Training Loss =  0.10151099976372024; Validation Loss = 0.12394417194163407\n",
            "Cost after 299671 iterations : Training Loss =  0.10151099976372004; Validation Loss = 0.12394417193989193\n",
            "Cost after 299672 iterations : Training Loss =  0.10151099976372002; Validation Loss = 0.1239441719381499\n",
            "Cost after 299673 iterations : Training Loss =  0.10151099976372016; Validation Loss = 0.12394417193640796\n",
            "Cost after 299674 iterations : Training Loss =  0.10151099976372009; Validation Loss = 0.12394417193466613\n",
            "Cost after 299675 iterations : Training Loss =  0.10151099976372022; Validation Loss = 0.1239441719329244\n",
            "Cost after 299676 iterations : Training Loss =  0.10151099976372008; Validation Loss = 0.1239441719311829\n",
            "Cost after 299677 iterations : Training Loss =  0.10151099976372013; Validation Loss = 0.12394417192944115\n",
            "Cost after 299678 iterations : Training Loss =  0.10151099976372019; Validation Loss = 0.12394417192769969\n",
            "Cost after 299679 iterations : Training Loss =  0.10151099976372006; Validation Loss = 0.1239441719259582\n",
            "Cost after 299680 iterations : Training Loss =  0.10151099976372019; Validation Loss = 0.12394417192421728\n",
            "Cost after 299681 iterations : Training Loss =  0.10151099976372016; Validation Loss = 0.12394417192247588\n",
            "Cost after 299682 iterations : Training Loss =  0.10151099976372012; Validation Loss = 0.12394417192073481\n",
            "Cost after 299683 iterations : Training Loss =  0.10151099976371997; Validation Loss = 0.1239441719189937\n",
            "Cost after 299684 iterations : Training Loss =  0.10151099976372008; Validation Loss = 0.12394417191725302\n",
            "Cost after 299685 iterations : Training Loss =  0.1015109997637199; Validation Loss = 0.1239441719155122\n",
            "Cost after 299686 iterations : Training Loss =  0.10151099976372009; Validation Loss = 0.12394417191377144\n",
            "Cost after 299687 iterations : Training Loss =  0.10151099976371995; Validation Loss = 0.12394417191203078\n",
            "Cost after 299688 iterations : Training Loss =  0.10151099976372012; Validation Loss = 0.12394417191029018\n",
            "Cost after 299689 iterations : Training Loss =  0.10151099976372006; Validation Loss = 0.12394417190854956\n",
            "Cost after 299690 iterations : Training Loss =  0.10151099976372015; Validation Loss = 0.1239441719068092\n",
            "Cost after 299691 iterations : Training Loss =  0.10151099976371997; Validation Loss = 0.12394417190506914\n",
            "Cost after 299692 iterations : Training Loss =  0.10151099976371994; Validation Loss = 0.12394417190332888\n",
            "Cost after 299693 iterations : Training Loss =  0.10151099976371997; Validation Loss = 0.12394417190158873\n",
            "Cost after 299694 iterations : Training Loss =  0.10151099976372015; Validation Loss = 0.12394417189984853\n",
            "Cost after 299695 iterations : Training Loss =  0.10151099976372004; Validation Loss = 0.12394417189810852\n",
            "Cost after 299696 iterations : Training Loss =  0.10151099976372; Validation Loss = 0.12394417189636878\n",
            "Cost after 299697 iterations : Training Loss =  0.10151099976371997; Validation Loss = 0.12394417189462888\n",
            "Cost after 299698 iterations : Training Loss =  0.10151099976371988; Validation Loss = 0.12394417189288941\n",
            "Cost after 299699 iterations : Training Loss =  0.10151099976371991; Validation Loss = 0.12394417189114978\n",
            "Cost after 299700 iterations : Training Loss =  0.10151099976372001; Validation Loss = 0.12394417188941027\n",
            "Cost after 299701 iterations : Training Loss =  0.10151099976372012; Validation Loss = 0.12394417188767065\n",
            "Cost after 299702 iterations : Training Loss =  0.10151099976371997; Validation Loss = 0.12394417188593153\n",
            "Cost after 299703 iterations : Training Loss =  0.10151099976371995; Validation Loss = 0.12394417188419224\n",
            "Cost after 299704 iterations : Training Loss =  0.10151099976372008; Validation Loss = 0.12394417188245303\n",
            "Cost after 299705 iterations : Training Loss =  0.10151099976371981; Validation Loss = 0.12394417188071416\n",
            "Cost after 299706 iterations : Training Loss =  0.10151099976372006; Validation Loss = 0.123944171878975\n",
            "Cost after 299707 iterations : Training Loss =  0.10151099976372001; Validation Loss = 0.12394417187723641\n",
            "Cost after 299708 iterations : Training Loss =  0.10151099976372019; Validation Loss = 0.12394417187549747\n",
            "Cost after 299709 iterations : Training Loss =  0.10151099976371994; Validation Loss = 0.12394417187375846\n",
            "Cost after 299710 iterations : Training Loss =  0.10151099976372006; Validation Loss = 0.12394417187202024\n",
            "Cost after 299711 iterations : Training Loss =  0.10151099976371984; Validation Loss = 0.12394417187028176\n",
            "Cost after 299712 iterations : Training Loss =  0.10151099976372002; Validation Loss = 0.12394417186854295\n",
            "Cost after 299713 iterations : Training Loss =  0.10151099976371995; Validation Loss = 0.1239441718668046\n",
            "Cost after 299714 iterations : Training Loss =  0.10151099976371974; Validation Loss = 0.12394417186506662\n",
            "Cost after 299715 iterations : Training Loss =  0.10151099976371994; Validation Loss = 0.12394417186332828\n",
            "Cost after 299716 iterations : Training Loss =  0.10151099976372004; Validation Loss = 0.12394417186159024\n",
            "Cost after 299717 iterations : Training Loss =  0.10151099976371984; Validation Loss = 0.1239441718598519\n",
            "Cost after 299718 iterations : Training Loss =  0.10151099976371988; Validation Loss = 0.12394417185811443\n",
            "Cost after 299719 iterations : Training Loss =  0.10151099976371994; Validation Loss = 0.12394417185637661\n",
            "Cost after 299720 iterations : Training Loss =  0.10151099976371995; Validation Loss = 0.12394417185463905\n",
            "Cost after 299721 iterations : Training Loss =  0.10151099976371991; Validation Loss = 0.12394417185290142\n",
            "Cost after 299722 iterations : Training Loss =  0.10151099976371988; Validation Loss = 0.12394417185116358\n",
            "Cost after 299723 iterations : Training Loss =  0.10151099976372; Validation Loss = 0.12394417184942605\n",
            "Cost after 299724 iterations : Training Loss =  0.1015109997637199; Validation Loss = 0.12394417184768874\n",
            "Cost after 299725 iterations : Training Loss =  0.10151099976371991; Validation Loss = 0.12394417184595125\n",
            "Cost after 299726 iterations : Training Loss =  0.10151099976371997; Validation Loss = 0.12394417184421434\n",
            "Cost after 299727 iterations : Training Loss =  0.10151099976371991; Validation Loss = 0.12394417184247702\n",
            "Cost after 299728 iterations : Training Loss =  0.10151099976372001; Validation Loss = 0.12394417184073996\n",
            "Cost after 299729 iterations : Training Loss =  0.1015109997637199; Validation Loss = 0.12394417183900312\n",
            "Cost after 299730 iterations : Training Loss =  0.10151099976371984; Validation Loss = 0.12394417183726628\n",
            "Cost after 299731 iterations : Training Loss =  0.10151099976371995; Validation Loss = 0.1239441718355295\n",
            "Cost after 299732 iterations : Training Loss =  0.10151099976371991; Validation Loss = 0.12394417183379282\n",
            "Cost after 299733 iterations : Training Loss =  0.10151099976371988; Validation Loss = 0.12394417183205635\n",
            "Cost after 299734 iterations : Training Loss =  0.1015109997637199; Validation Loss = 0.12394417183031955\n",
            "Cost after 299735 iterations : Training Loss =  0.1015109997637198; Validation Loss = 0.12394417182858349\n",
            "Cost after 299736 iterations : Training Loss =  0.10151099976371991; Validation Loss = 0.12394417182684672\n",
            "Cost after 299737 iterations : Training Loss =  0.1015109997637199; Validation Loss = 0.12394417182511071\n",
            "Cost after 299738 iterations : Training Loss =  0.10151099976371984; Validation Loss = 0.12394417182337493\n",
            "Cost after 299739 iterations : Training Loss =  0.1015109997637198; Validation Loss = 0.12394417182163853\n",
            "Cost after 299740 iterations : Training Loss =  0.10151099976371987; Validation Loss = 0.12394417181990251\n",
            "Cost after 299741 iterations : Training Loss =  0.10151099976371987; Validation Loss = 0.12394417181816675\n",
            "Cost after 299742 iterations : Training Loss =  0.10151099976371983; Validation Loss = 0.12394417181643082\n",
            "Cost after 299743 iterations : Training Loss =  0.1015109997637197; Validation Loss = 0.12394417181469507\n",
            "Cost after 299744 iterations : Training Loss =  0.10151099976371974; Validation Loss = 0.12394417181295943\n",
            "Cost after 299745 iterations : Training Loss =  0.10151099976371987; Validation Loss = 0.12394417181122389\n",
            "Cost after 299746 iterations : Training Loss =  0.10151099976371984; Validation Loss = 0.12394417180948854\n",
            "Cost after 299747 iterations : Training Loss =  0.10151099976371977; Validation Loss = 0.12394417180775304\n",
            "Cost after 299748 iterations : Training Loss =  0.10151099976371987; Validation Loss = 0.12394417180601837\n",
            "Cost after 299749 iterations : Training Loss =  0.10151099976372; Validation Loss = 0.12394417180428288\n",
            "Cost after 299750 iterations : Training Loss =  0.10151099976371969; Validation Loss = 0.12394417180254792\n",
            "Cost after 299751 iterations : Training Loss =  0.10151099976371977; Validation Loss = 0.12394417180081278\n",
            "Cost after 299752 iterations : Training Loss =  0.10151099976371963; Validation Loss = 0.12394417179907756\n",
            "Cost after 299753 iterations : Training Loss =  0.10151099976371984; Validation Loss = 0.12394417179734309\n",
            "Cost after 299754 iterations : Training Loss =  0.10151099976371997; Validation Loss = 0.12394417179560835\n",
            "Cost after 299755 iterations : Training Loss =  0.10151099976371974; Validation Loss = 0.12394417179387357\n",
            "Cost after 299756 iterations : Training Loss =  0.10151099976371981; Validation Loss = 0.123944171792139\n",
            "Cost after 299757 iterations : Training Loss =  0.10151099976371984; Validation Loss = 0.12394417179040446\n",
            "Cost after 299758 iterations : Training Loss =  0.10151099976371988; Validation Loss = 0.1239441717886703\n",
            "Cost after 299759 iterations : Training Loss =  0.10151099976371988; Validation Loss = 0.12394417178693579\n",
            "Cost after 299760 iterations : Training Loss =  0.10151099976371983; Validation Loss = 0.1239441717852017\n",
            "Cost after 299761 iterations : Training Loss =  0.10151099976371965; Validation Loss = 0.12394417178346759\n",
            "Cost after 299762 iterations : Training Loss =  0.10151099976371969; Validation Loss = 0.12394417178173363\n",
            "Cost after 299763 iterations : Training Loss =  0.10151099976371983; Validation Loss = 0.12394417177999925\n",
            "Cost after 299764 iterations : Training Loss =  0.10151099976371951; Validation Loss = 0.12394417177826578\n",
            "Cost after 299765 iterations : Training Loss =  0.10151099976371988; Validation Loss = 0.12394417177653161\n",
            "Cost after 299766 iterations : Training Loss =  0.10151099976371976; Validation Loss = 0.12394417177479822\n",
            "Cost after 299767 iterations : Training Loss =  0.10151099976371965; Validation Loss = 0.12394417177306451\n",
            "Cost after 299768 iterations : Training Loss =  0.10151099976371994; Validation Loss = 0.12394417177133087\n",
            "Cost after 299769 iterations : Training Loss =  0.10151099976371995; Validation Loss = 0.12394417176959774\n",
            "Cost after 299770 iterations : Training Loss =  0.10151099976371983; Validation Loss = 0.12394417176786443\n",
            "Cost after 299771 iterations : Training Loss =  0.10151099976371965; Validation Loss = 0.12394417176613087\n",
            "Cost after 299772 iterations : Training Loss =  0.10151099976371977; Validation Loss = 0.123944171764398\n",
            "Cost after 299773 iterations : Training Loss =  0.1015109997637197; Validation Loss = 0.12394417176266483\n",
            "Cost after 299774 iterations : Training Loss =  0.1015109997637197; Validation Loss = 0.12394417176093188\n",
            "Cost after 299775 iterations : Training Loss =  0.10151099976371977; Validation Loss = 0.12394417175919901\n",
            "Cost after 299776 iterations : Training Loss =  0.10151099976371958; Validation Loss = 0.1239441717574664\n",
            "Cost after 299777 iterations : Training Loss =  0.10151099976371974; Validation Loss = 0.12394417175573345\n",
            "Cost after 299778 iterations : Training Loss =  0.10151099976371984; Validation Loss = 0.12394417175400058\n",
            "Cost after 299779 iterations : Training Loss =  0.10151099976371968; Validation Loss = 0.12394417175226817\n",
            "Cost after 299780 iterations : Training Loss =  0.10151099976371983; Validation Loss = 0.12394417175053567\n",
            "Cost after 299781 iterations : Training Loss =  0.10151099976371977; Validation Loss = 0.12394417174880336\n",
            "Cost after 299782 iterations : Training Loss =  0.10151099976371958; Validation Loss = 0.12394417174707123\n",
            "Cost after 299783 iterations : Training Loss =  0.10151099976371983; Validation Loss = 0.12394417174533859\n",
            "Cost after 299784 iterations : Training Loss =  0.1015109997637197; Validation Loss = 0.12394417174360679\n",
            "Cost after 299785 iterations : Training Loss =  0.10151099976371972; Validation Loss = 0.12394417174187476\n",
            "Cost after 299786 iterations : Training Loss =  0.10151099976371968; Validation Loss = 0.12394417174014308\n",
            "Cost after 299787 iterations : Training Loss =  0.10151099976371969; Validation Loss = 0.12394417173841095\n",
            "Cost after 299788 iterations : Training Loss =  0.10151099976371974; Validation Loss = 0.12394417173667947\n",
            "Cost after 299789 iterations : Training Loss =  0.10151099976371968; Validation Loss = 0.12394417173494766\n",
            "Cost after 299790 iterations : Training Loss =  0.10151099976371983; Validation Loss = 0.1239441717332163\n",
            "Cost after 299791 iterations : Training Loss =  0.10151099976371955; Validation Loss = 0.12394417173148453\n",
            "Cost after 299792 iterations : Training Loss =  0.10151099976371962; Validation Loss = 0.12394417172975306\n",
            "Cost after 299793 iterations : Training Loss =  0.10151099976371969; Validation Loss = 0.12394417172802215\n",
            "Cost after 299794 iterations : Training Loss =  0.10151099976371958; Validation Loss = 0.12394417172629064\n",
            "Cost after 299795 iterations : Training Loss =  0.10151099976371952; Validation Loss = 0.12394417172455947\n",
            "Cost after 299796 iterations : Training Loss =  0.10151099976371965; Validation Loss = 0.12394417172282833\n",
            "Cost after 299797 iterations : Training Loss =  0.10151099976371962; Validation Loss = 0.12394417172109734\n",
            "Cost after 299798 iterations : Training Loss =  0.10151099976371977; Validation Loss = 0.12394417171936643\n",
            "Cost after 299799 iterations : Training Loss =  0.10151099976371977; Validation Loss = 0.12394417171763576\n",
            "Cost after 299800 iterations : Training Loss =  0.10151099976371976; Validation Loss = 0.12394417171590513\n",
            "Cost after 299801 iterations : Training Loss =  0.10151099976371959; Validation Loss = 0.12394417171417453\n",
            "Cost after 299802 iterations : Training Loss =  0.10151099976371968; Validation Loss = 0.12394417171244397\n",
            "Cost after 299803 iterations : Training Loss =  0.10151099976371974; Validation Loss = 0.12394417171071392\n",
            "Cost after 299804 iterations : Training Loss =  0.10151099976371974; Validation Loss = 0.1239441717089835\n",
            "Cost after 299805 iterations : Training Loss =  0.10151099976371956; Validation Loss = 0.1239441717072532\n",
            "Cost after 299806 iterations : Training Loss =  0.10151099976371955; Validation Loss = 0.123944171705523\n",
            "Cost after 299807 iterations : Training Loss =  0.10151099976371962; Validation Loss = 0.12394417170379289\n",
            "Cost after 299808 iterations : Training Loss =  0.10151099976371977; Validation Loss = 0.12394417170206293\n",
            "Cost after 299809 iterations : Training Loss =  0.10151099976371958; Validation Loss = 0.12394417170033295\n",
            "Cost after 299810 iterations : Training Loss =  0.10151099976371968; Validation Loss = 0.12394417169860297\n",
            "Cost after 299811 iterations : Training Loss =  0.10151099976371968; Validation Loss = 0.12394417169687334\n",
            "Cost after 299812 iterations : Training Loss =  0.10151099976371959; Validation Loss = 0.12394417169514334\n",
            "Cost after 299813 iterations : Training Loss =  0.10151099976371955; Validation Loss = 0.12394417169341429\n",
            "Cost after 299814 iterations : Training Loss =  0.10151099976371963; Validation Loss = 0.1239441716916846\n",
            "Cost after 299815 iterations : Training Loss =  0.10151099976371955; Validation Loss = 0.12394417168995528\n",
            "Cost after 299816 iterations : Training Loss =  0.10151099976371965; Validation Loss = 0.12394417168822586\n",
            "Cost after 299817 iterations : Training Loss =  0.10151099976371951; Validation Loss = 0.12394417168649671\n",
            "Cost after 299818 iterations : Training Loss =  0.10151099976371952; Validation Loss = 0.1239441716847677\n",
            "Cost after 299819 iterations : Training Loss =  0.10151099976371956; Validation Loss = 0.12394417168303858\n",
            "Cost after 299820 iterations : Training Loss =  0.10151099976371933; Validation Loss = 0.12394417168130988\n",
            "Cost after 299821 iterations : Training Loss =  0.10151099976371945; Validation Loss = 0.12394417167958112\n",
            "Cost after 299822 iterations : Training Loss =  0.10151099976371963; Validation Loss = 0.12394417167785211\n",
            "Cost after 299823 iterations : Training Loss =  0.10151099976371951; Validation Loss = 0.12394417167612336\n",
            "Cost after 299824 iterations : Training Loss =  0.1015109997637195; Validation Loss = 0.12394417167439471\n",
            "Cost after 299825 iterations : Training Loss =  0.10151099976371968; Validation Loss = 0.12394417167266657\n",
            "Cost after 299826 iterations : Training Loss =  0.10151099976371952; Validation Loss = 0.12394417167093771\n",
            "Cost after 299827 iterations : Training Loss =  0.1015109997637195; Validation Loss = 0.12394417166920946\n",
            "Cost after 299828 iterations : Training Loss =  0.10151099976371958; Validation Loss = 0.12394417166748133\n",
            "Cost after 299829 iterations : Training Loss =  0.10151099976371945; Validation Loss = 0.12394417166575292\n",
            "Cost after 299830 iterations : Training Loss =  0.10151099976371943; Validation Loss = 0.1239441716640254\n",
            "Cost after 299831 iterations : Training Loss =  0.10151099976371951; Validation Loss = 0.12394417166229695\n",
            "Cost after 299832 iterations : Training Loss =  0.10151099976371981; Validation Loss = 0.12394417166056947\n",
            "Cost after 299833 iterations : Training Loss =  0.10151099976371955; Validation Loss = 0.12394417165884143\n",
            "Cost after 299834 iterations : Training Loss =  0.10151099976371938; Validation Loss = 0.12394417165711366\n",
            "Cost after 299835 iterations : Training Loss =  0.10151099976371945; Validation Loss = 0.12394417165538597\n",
            "Cost after 299836 iterations : Training Loss =  0.10151099976371955; Validation Loss = 0.12394417165365837\n",
            "Cost after 299837 iterations : Training Loss =  0.10151099976371956; Validation Loss = 0.12394417165193129\n",
            "Cost after 299838 iterations : Training Loss =  0.10151099976371937; Validation Loss = 0.12394417165020344\n",
            "Cost after 299839 iterations : Training Loss =  0.10151099976371951; Validation Loss = 0.12394417164847621\n",
            "Cost after 299840 iterations : Training Loss =  0.10151099976371969; Validation Loss = 0.12394417164674916\n",
            "Cost after 299841 iterations : Training Loss =  0.10151099976371952; Validation Loss = 0.12394417164502212\n",
            "Cost after 299842 iterations : Training Loss =  0.1015109997637195; Validation Loss = 0.12394417164329488\n",
            "Cost after 299843 iterations : Training Loss =  0.10151099976371938; Validation Loss = 0.12394417164156848\n",
            "Cost after 299844 iterations : Training Loss =  0.10151099976371951; Validation Loss = 0.12394417163984155\n",
            "Cost after 299845 iterations : Training Loss =  0.1015109997637194; Validation Loss = 0.12394417163811461\n",
            "Cost after 299846 iterations : Training Loss =  0.10151099976371944; Validation Loss = 0.12394417163638805\n",
            "Cost after 299847 iterations : Training Loss =  0.10151099976371952; Validation Loss = 0.12394417163466141\n",
            "Cost after 299848 iterations : Training Loss =  0.10151099976371951; Validation Loss = 0.12394417163293486\n",
            "Cost after 299849 iterations : Training Loss =  0.10151099976371951; Validation Loss = 0.12394417163120865\n",
            "Cost after 299850 iterations : Training Loss =  0.10151099976371937; Validation Loss = 0.12394417162948233\n",
            "Cost after 299851 iterations : Training Loss =  0.10151099976371944; Validation Loss = 0.12394417162775614\n",
            "Cost after 299852 iterations : Training Loss =  0.10151099976371937; Validation Loss = 0.12394417162602989\n",
            "Cost after 299853 iterations : Training Loss =  0.10151099976371955; Validation Loss = 0.12394417162430411\n",
            "Cost after 299854 iterations : Training Loss =  0.10151099976371962; Validation Loss = 0.12394417162257775\n",
            "Cost after 299855 iterations : Training Loss =  0.10151099976371936; Validation Loss = 0.123944171620852\n",
            "Cost after 299856 iterations : Training Loss =  0.10151099976371962; Validation Loss = 0.12394417161912631\n",
            "Cost after 299857 iterations : Training Loss =  0.10151099976371937; Validation Loss = 0.12394417161740073\n",
            "Cost after 299858 iterations : Training Loss =  0.1015109997637193; Validation Loss = 0.12394417161567503\n",
            "Cost after 299859 iterations : Training Loss =  0.10151099976371936; Validation Loss = 0.12394417161394937\n",
            "Cost after 299860 iterations : Training Loss =  0.10151099976371926; Validation Loss = 0.12394417161222418\n",
            "Cost after 299861 iterations : Training Loss =  0.10151099976371945; Validation Loss = 0.12394417161049873\n",
            "Cost after 299862 iterations : Training Loss =  0.10151099976371947; Validation Loss = 0.12394417160877366\n",
            "Cost after 299863 iterations : Training Loss =  0.10151099976371918; Validation Loss = 0.1239441716070484\n",
            "Cost after 299864 iterations : Training Loss =  0.10151099976371938; Validation Loss = 0.12394417160532346\n",
            "Cost after 299865 iterations : Training Loss =  0.1015109997637194; Validation Loss = 0.12394417160359841\n",
            "Cost after 299866 iterations : Training Loss =  0.10151099976371923; Validation Loss = 0.12394417160187331\n",
            "Cost after 299867 iterations : Training Loss =  0.10151099976371955; Validation Loss = 0.12394417160014863\n",
            "Cost after 299868 iterations : Training Loss =  0.10151099976371937; Validation Loss = 0.12394417159842398\n",
            "Cost after 299869 iterations : Training Loss =  0.10151099976371945; Validation Loss = 0.12394417159669921\n",
            "Cost after 299870 iterations : Training Loss =  0.10151099976371938; Validation Loss = 0.12394417159497448\n",
            "Cost after 299871 iterations : Training Loss =  0.1015109997637193; Validation Loss = 0.12394417159324993\n",
            "Cost after 299872 iterations : Training Loss =  0.10151099976371943; Validation Loss = 0.12394417159152575\n",
            "Cost after 299873 iterations : Training Loss =  0.10151099976371943; Validation Loss = 0.12394417158980148\n",
            "Cost after 299874 iterations : Training Loss =  0.1015109997637193; Validation Loss = 0.1239441715880767\n",
            "Cost after 299875 iterations : Training Loss =  0.10151099976371938; Validation Loss = 0.12394417158635303\n",
            "Cost after 299876 iterations : Training Loss =  0.10151099976371944; Validation Loss = 0.123944171584629\n",
            "Cost after 299877 iterations : Training Loss =  0.1015109997637194; Validation Loss = 0.12394417158290504\n",
            "Cost after 299878 iterations : Training Loss =  0.1015109997637193; Validation Loss = 0.1239441715811812\n",
            "Cost after 299879 iterations : Training Loss =  0.10151099976371926; Validation Loss = 0.12394417157945732\n",
            "Cost after 299880 iterations : Training Loss =  0.10151099976371937; Validation Loss = 0.1239441715777338\n",
            "Cost after 299881 iterations : Training Loss =  0.1015109997637193; Validation Loss = 0.12394417157601056\n",
            "Cost after 299882 iterations : Training Loss =  0.10151099976371925; Validation Loss = 0.12394417157428661\n",
            "Cost after 299883 iterations : Training Loss =  0.10151099976371938; Validation Loss = 0.12394417157256345\n",
            "Cost after 299884 iterations : Training Loss =  0.10151099976371931; Validation Loss = 0.1239441715708402\n",
            "Cost after 299885 iterations : Training Loss =  0.10151099976371947; Validation Loss = 0.12394417156911669\n",
            "Cost after 299886 iterations : Training Loss =  0.1015109997637193; Validation Loss = 0.12394417156739368\n",
            "Cost after 299887 iterations : Training Loss =  0.10151099976371918; Validation Loss = 0.12394417156567065\n",
            "Cost after 299888 iterations : Training Loss =  0.10151099976371915; Validation Loss = 0.1239441715639474\n",
            "Cost after 299889 iterations : Training Loss =  0.10151099976371923; Validation Loss = 0.1239441715622247\n",
            "Cost after 299890 iterations : Training Loss =  0.10151099976371919; Validation Loss = 0.12394417156050182\n",
            "Cost after 299891 iterations : Training Loss =  0.10151099976371943; Validation Loss = 0.12394417155877903\n",
            "Cost after 299892 iterations : Training Loss =  0.10151099976371926; Validation Loss = 0.12394417155705614\n",
            "Cost after 299893 iterations : Training Loss =  0.10151099976371926; Validation Loss = 0.12394417155533397\n",
            "Cost after 299894 iterations : Training Loss =  0.10151099976371926; Validation Loss = 0.12394417155361151\n",
            "Cost after 299895 iterations : Training Loss =  0.10151099976371938; Validation Loss = 0.12394417155188872\n",
            "Cost after 299896 iterations : Training Loss =  0.10151099976371931; Validation Loss = 0.12394417155016653\n",
            "Cost after 299897 iterations : Training Loss =  0.10151099976371925; Validation Loss = 0.12394417154844456\n",
            "Cost after 299898 iterations : Training Loss =  0.1015109997637191; Validation Loss = 0.12394417154672262\n",
            "Cost after 299899 iterations : Training Loss =  0.10151099976371926; Validation Loss = 0.12394417154500051\n",
            "Cost after 299900 iterations : Training Loss =  0.10151099976371933; Validation Loss = 0.12394417154327854\n",
            "Cost after 299901 iterations : Training Loss =  0.10151099976371925; Validation Loss = 0.12394417154155665\n",
            "Cost after 299902 iterations : Training Loss =  0.1015109997637192; Validation Loss = 0.12394417153983488\n",
            "Cost after 299903 iterations : Training Loss =  0.10151099976371915; Validation Loss = 0.12394417153811332\n",
            "Cost after 299904 iterations : Training Loss =  0.1015109997637192; Validation Loss = 0.1239441715363919\n",
            "Cost after 299905 iterations : Training Loss =  0.10151099976371925; Validation Loss = 0.12394417153467015\n",
            "Cost after 299906 iterations : Training Loss =  0.10151099976371936; Validation Loss = 0.12394417153294882\n",
            "Cost after 299907 iterations : Training Loss =  0.10151099976371926; Validation Loss = 0.12394417153122746\n",
            "Cost after 299908 iterations : Training Loss =  0.10151099976371915; Validation Loss = 0.1239441715295063\n",
            "Cost after 299909 iterations : Training Loss =  0.10151099976371933; Validation Loss = 0.12394417152778495\n",
            "Cost after 299910 iterations : Training Loss =  0.10151099976371944; Validation Loss = 0.12394417152606385\n",
            "Cost after 299911 iterations : Training Loss =  0.10151099976371925; Validation Loss = 0.12394417152434306\n",
            "Cost after 299912 iterations : Training Loss =  0.10151099976371912; Validation Loss = 0.12394417152262203\n",
            "Cost after 299913 iterations : Training Loss =  0.10151099976371915; Validation Loss = 0.12394417152090144\n",
            "Cost after 299914 iterations : Training Loss =  0.10151099976371938; Validation Loss = 0.12394417151918051\n",
            "Cost after 299915 iterations : Training Loss =  0.10151099976371919; Validation Loss = 0.12394417151745994\n",
            "Cost after 299916 iterations : Training Loss =  0.1015109997637192; Validation Loss = 0.12394417151573926\n",
            "Cost after 299917 iterations : Training Loss =  0.10151099976371923; Validation Loss = 0.12394417151401911\n",
            "Cost after 299918 iterations : Training Loss =  0.10151099976371925; Validation Loss = 0.12394417151229835\n",
            "Cost after 299919 iterations : Training Loss =  0.10151099976371919; Validation Loss = 0.12394417151057843\n",
            "Cost after 299920 iterations : Training Loss =  0.1015109997637191; Validation Loss = 0.12394417150885811\n",
            "Cost after 299921 iterations : Training Loss =  0.10151099976371927; Validation Loss = 0.1239441715071379\n",
            "Cost after 299922 iterations : Training Loss =  0.10151099976371919; Validation Loss = 0.12394417150541795\n",
            "Cost after 299923 iterations : Training Loss =  0.10151099976371908; Validation Loss = 0.12394417150369824\n",
            "Cost after 299924 iterations : Training Loss =  0.10151099976371915; Validation Loss = 0.1239441715019784\n",
            "Cost after 299925 iterations : Training Loss =  0.10151099976371923; Validation Loss = 0.12394417150025878\n",
            "Cost after 299926 iterations : Training Loss =  0.10151099976371891; Validation Loss = 0.12394417149853887\n",
            "Cost after 299927 iterations : Training Loss =  0.10151099976371923; Validation Loss = 0.12394417149681931\n",
            "Cost after 299928 iterations : Training Loss =  0.10151099976371926; Validation Loss = 0.12394417149509977\n",
            "Cost after 299929 iterations : Training Loss =  0.101510999763719; Validation Loss = 0.12394417149338045\n",
            "Cost after 299930 iterations : Training Loss =  0.10151099976371902; Validation Loss = 0.1239441714916612\n",
            "Cost after 299931 iterations : Training Loss =  0.10151099976371915; Validation Loss = 0.12394417148994186\n",
            "Cost after 299932 iterations : Training Loss =  0.10151099976371908; Validation Loss = 0.12394417148822288\n",
            "Cost after 299933 iterations : Training Loss =  0.10151099976371915; Validation Loss = 0.12394417148650361\n",
            "Cost after 299934 iterations : Training Loss =  0.10151099976371906; Validation Loss = 0.12394417148478494\n",
            "Cost after 299935 iterations : Training Loss =  0.10151099976371931; Validation Loss = 0.1239441714830658\n",
            "Cost after 299936 iterations : Training Loss =  0.10151099976371918; Validation Loss = 0.12394417148134686\n",
            "Cost after 299937 iterations : Training Loss =  0.10151099976371913; Validation Loss = 0.12394417147962843\n",
            "Cost after 299938 iterations : Training Loss =  0.10151099976371919; Validation Loss = 0.12394417147790977\n",
            "Cost after 299939 iterations : Training Loss =  0.10151099976371913; Validation Loss = 0.12394417147619112\n",
            "Cost after 299940 iterations : Training Loss =  0.10151099976371912; Validation Loss = 0.1239441714744728\n",
            "Cost after 299941 iterations : Training Loss =  0.10151099976371893; Validation Loss = 0.12394417147275469\n",
            "Cost after 299942 iterations : Training Loss =  0.10151099976371908; Validation Loss = 0.12394417147103635\n",
            "Cost after 299943 iterations : Training Loss =  0.10151099976371905; Validation Loss = 0.1239441714693182\n",
            "Cost after 299944 iterations : Training Loss =  0.10151099976371905; Validation Loss = 0.12394417146760012\n",
            "Cost after 299945 iterations : Training Loss =  0.10151099976371908; Validation Loss = 0.12394417146588187\n",
            "Cost after 299946 iterations : Training Loss =  0.10151099976371913; Validation Loss = 0.1239441714641642\n",
            "Cost after 299947 iterations : Training Loss =  0.10151099976371895; Validation Loss = 0.12394417146244646\n",
            "Cost after 299948 iterations : Training Loss =  0.10151099976371912; Validation Loss = 0.12394417146072848\n",
            "Cost after 299949 iterations : Training Loss =  0.10151099976371905; Validation Loss = 0.12394417145901081\n",
            "Cost after 299950 iterations : Training Loss =  0.10151099976371893; Validation Loss = 0.12394417145729325\n",
            "Cost after 299951 iterations : Training Loss =  0.10151099976371902; Validation Loss = 0.12394417145557578\n",
            "Cost after 299952 iterations : Training Loss =  0.10151099976371886; Validation Loss = 0.12394417145385847\n",
            "Cost after 299953 iterations : Training Loss =  0.10151099976371898; Validation Loss = 0.12394417145214093\n",
            "Cost after 299954 iterations : Training Loss =  0.101510999763719; Validation Loss = 0.12394417145042373\n",
            "Cost after 299955 iterations : Training Loss =  0.10151099976371893; Validation Loss = 0.12394417144870672\n",
            "Cost after 299956 iterations : Training Loss =  0.10151099976371905; Validation Loss = 0.12394417144698956\n",
            "Cost after 299957 iterations : Training Loss =  0.1015109997637191; Validation Loss = 0.12394417144527277\n",
            "Cost after 299958 iterations : Training Loss =  0.101510999763719; Validation Loss = 0.123944171443556\n",
            "Cost after 299959 iterations : Training Loss =  0.10151099976371894; Validation Loss = 0.12394417144183913\n",
            "Cost after 299960 iterations : Training Loss =  0.101510999763719; Validation Loss = 0.12394417144012236\n",
            "Cost after 299961 iterations : Training Loss =  0.10151099976371913; Validation Loss = 0.12394417143840582\n",
            "Cost after 299962 iterations : Training Loss =  0.10151099976371898; Validation Loss = 0.12394417143668938\n",
            "Cost after 299963 iterations : Training Loss =  0.10151099976371901; Validation Loss = 0.12394417143497315\n",
            "Cost after 299964 iterations : Training Loss =  0.10151099976371898; Validation Loss = 0.12394417143325688\n",
            "Cost after 299965 iterations : Training Loss =  0.10151099976371901; Validation Loss = 0.1239441714315403\n",
            "Cost after 299966 iterations : Training Loss =  0.10151099976371891; Validation Loss = 0.12394417142982408\n",
            "Cost after 299967 iterations : Training Loss =  0.10151099976371891; Validation Loss = 0.12394417142810812\n",
            "Cost after 299968 iterations : Training Loss =  0.10151099976371906; Validation Loss = 0.12394417142639202\n",
            "Cost after 299969 iterations : Training Loss =  0.10151099976371895; Validation Loss = 0.12394417142467606\n",
            "Cost after 299970 iterations : Training Loss =  0.101510999763719; Validation Loss = 0.12394417142296024\n",
            "Cost after 299971 iterations : Training Loss =  0.10151099976371883; Validation Loss = 0.12394417142124471\n",
            "Cost after 299972 iterations : Training Loss =  0.10151099976371886; Validation Loss = 0.12394417141952918\n",
            "Cost after 299973 iterations : Training Loss =  0.10151099976371895; Validation Loss = 0.12394417141781354\n",
            "Cost after 299974 iterations : Training Loss =  0.10151099976371901; Validation Loss = 0.12394417141609783\n",
            "Cost after 299975 iterations : Training Loss =  0.10151099976371894; Validation Loss = 0.12394417141438252\n",
            "Cost after 299976 iterations : Training Loss =  0.10151099976371891; Validation Loss = 0.12394417141266711\n",
            "Cost after 299977 iterations : Training Loss =  0.101510999763719; Validation Loss = 0.12394417141095207\n",
            "Cost after 299978 iterations : Training Loss =  0.10151099976371883; Validation Loss = 0.1239441714092367\n",
            "Cost after 299979 iterations : Training Loss =  0.10151099976371898; Validation Loss = 0.1239441714075217\n",
            "Cost after 299980 iterations : Training Loss =  0.10151099976371901; Validation Loss = 0.12394417140580691\n",
            "Cost after 299981 iterations : Training Loss =  0.10151099976371887; Validation Loss = 0.1239441714040919\n",
            "Cost after 299982 iterations : Training Loss =  0.10151099976371891; Validation Loss = 0.12394417140237718\n",
            "Cost after 299983 iterations : Training Loss =  0.10151099976371887; Validation Loss = 0.1239441714006625\n",
            "Cost after 299984 iterations : Training Loss =  0.10151099976371883; Validation Loss = 0.12394417139894798\n",
            "Cost after 299985 iterations : Training Loss =  0.10151099976371902; Validation Loss = 0.12394417139723336\n",
            "Cost after 299986 iterations : Training Loss =  0.10151099976371895; Validation Loss = 0.12394417139551908\n",
            "Cost after 299987 iterations : Training Loss =  0.10151099976371873; Validation Loss = 0.12394417139380477\n",
            "Cost after 299988 iterations : Training Loss =  0.10151099976371894; Validation Loss = 0.12394417139209025\n",
            "Cost after 299989 iterations : Training Loss =  0.10151099976371888; Validation Loss = 0.12394417139037626\n",
            "Cost after 299990 iterations : Training Loss =  0.10151099976371902; Validation Loss = 0.12394417138866225\n",
            "Cost after 299991 iterations : Training Loss =  0.10151099976371886; Validation Loss = 0.12394417138694822\n",
            "Cost after 299992 iterations : Training Loss =  0.10151099976371894; Validation Loss = 0.12394417138523414\n",
            "Cost after 299993 iterations : Training Loss =  0.10151099976371891; Validation Loss = 0.12394417138352051\n",
            "Cost after 299994 iterations : Training Loss =  0.10151099976371888; Validation Loss = 0.12394417138180687\n",
            "Cost after 299995 iterations : Training Loss =  0.10151099976371888; Validation Loss = 0.12394417138009296\n",
            "Cost after 299996 iterations : Training Loss =  0.10151099976371887; Validation Loss = 0.12394417137837957\n",
            "Cost after 299997 iterations : Training Loss =  0.10151099976371887; Validation Loss = 0.123944171376666\n",
            "Cost after 299998 iterations : Training Loss =  0.10151099976371894; Validation Loss = 0.12394417137495277\n",
            "Cost after 299999 iterations : Training Loss =  0.10151099976371898; Validation Loss = 0.12394417137323928\n",
            "Cost after 300000 iterations : Training Loss =  0.101510999763719; Validation Loss = 0.12394417137152605\n",
            "Training Complete : min_loss_achieved = 0.12394417137152605; W,B = (4.019748081756162, 11.919076021621109)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8SCBA2jp4Qe",
        "outputId": "25f3a01e-783c-4e51-f686-6144ac9568a5"
      },
      "source": [
        "# Model-3 Training\n",
        "quad_results.append(quad_train(quad_data,Q2,Q1,Q0,4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Quadratic Regression Model using COST = (y-z)**4\n",
            "Cost after 1 iterations : Training Loss =  419740.13584265724; Validation Loss = 343321.8832003988\n",
            "Cost after 2 iterations : Training Loss =  3879205157070.3047; Validation Loss = 3214407905191.0527\n",
            "Cost after 3 iterations : Training Loss =  3.293143817952754e+33; Validation Loss = 2.728144829418682e+33\n",
            "Cost after 4 iterations : Training Loss =  2.0147670734330614e+96; Validation Loss = 1.669096935176711e+96\n",
            "Cost after 5 iterations : Training Loss =  4.613890529654465e+284; Validation Loss = 3.822293228531463e+284\n",
            "Cost after 6 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 7 iterations : Training Loss =  159559.2863715371; Validation Loss = 129123.11954558575\n",
            "Cost after 8 iterations : Training Loss =  202757309808.73825; Validation Loss = 168103127748.14554\n",
            "Cost after 9 iterations : Training Loss =  4.701925686833383e+29; Validation Loss = 3.895227288763655e+29\n",
            "Cost after 10 iterations : Training Loss =  5.864352056866047e+84; Validation Loss = 4.858215220889333e+84\n",
            "Cost after 11 iterations : Training Loss =  1.137765748789938e+250; Validation Loss = 9.425612266530701e+249\n",
            "Cost after 12 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 13 iterations : Training Loss =  12932.860975917678; Validation Loss = 10318.452389805198\n",
            "Cost after 14 iterations : Training Loss =  76571416.57470612; Validation Loss = 63961051.45830842\n",
            "Cost after 15 iterations : Training Loss =  2.517439393428916e+19; Validation Loss = 2.085528695982938e+19\n",
            "Cost after 16 iterations : Training Loss =  9.000565960545339e+53; Validation Loss = 7.456354363018515e+53\n",
            "Cost after 17 iterations : Training Loss =  4.1134119824804513e+157; Validation Loss = 3.407680929196336e+157\n",
            "Cost after 18 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 19 iterations : Training Loss =  111.5021452088902; Validation Loss = 134.89344140035834\n",
            "Cost after 20 iterations : Training Loss =  15.62299739116684; Validation Loss = 19.20293950500318\n",
            "Cost after 21 iterations : Training Loss =  12.52352977928947; Validation Loss = 15.21078487468639\n",
            "Cost after 22 iterations : Training Loss =  10.727965781549488; Validation Loss = 12.94544773750885\n",
            "Cost after 23 iterations : Training Loss =  9.66822694205821; Validation Loss = 11.622581233532706\n",
            "Cost after 24 iterations : Training Loss =  9.059638454532541; Validation Loss = 10.858946722473977\n",
            "Cost after 25 iterations : Training Loss =  8.722783806196567; Validation Loss = 10.426002999384952\n",
            "Cost after 26 iterations : Training Loss =  8.539917436098737; Validation Loss = 10.180832849454942\n",
            "Cost after 27 iterations : Training Loss =  8.438322521228596; Validation Loss = 10.037371903940482\n",
            "Cost after 28 iterations : Training Loss =  8.376950290512914; Validation Loss = 9.9470370284202\n",
            "Cost after 29 iterations : Training Loss =  8.334511534526108; Validation Loss = 9.883978176414248\n",
            "Cost after 30 iterations : Training Loss =  8.300691944522798; Validation Loss = 9.834980316143517\n",
            "Cost after 31 iterations : Training Loss =  8.270739017564013; Validation Loss = 9.793403182892183\n",
            "Cost after 32 iterations : Training Loss =  8.242538050299249; Validation Loss = 9.755896500303969\n",
            "Cost after 33 iterations : Training Loss =  8.215169539562138; Validation Loss = 9.720733528532204\n",
            "Cost after 34 iterations : Training Loss =  8.188239326761131; Validation Loss = 9.686994709875979\n",
            "Cost after 35 iterations : Training Loss =  8.16157967308986; Validation Loss = 9.65417152270193\n",
            "Cost after 36 iterations : Training Loss =  8.135119229372712; Validation Loss = 9.621972123897942\n",
            "Cost after 37 iterations : Training Loss =  8.108827377700129; Validation Loss = 9.59022343879057\n",
            "Cost after 38 iterations : Training Loss =  8.082690641669512; Validation Loss = 9.558820009515822\n",
            "Cost after 39 iterations : Training Loss =  8.056702746305811; Validation Loss = 9.527696167829689\n",
            "Cost after 40 iterations : Training Loss =  8.03086044284194; Validation Loss = 9.496810291251219\n",
            "Cost after 41 iterations : Training Loss =  8.005161757500666; Validation Loss = 9.46613558528684\n",
            "Cost after 42 iterations : Training Loss =  7.979605257209963; Validation Loss = 9.435654535479895\n",
            "Cost after 43 iterations : Training Loss =  7.954189741575831; Validation Loss = 9.405355497410326\n",
            "Cost after 44 iterations : Training Loss =  7.928914113528845; Validation Loss = 9.37523056839871\n",
            "Cost after 45 iterations : Training Loss =  7.903777324909749; Validation Loss = 9.345274244574014\n",
            "Cost after 46 iterations : Training Loss =  7.87877835351666; Validation Loss = 9.315482566938536\n",
            "Cost after 47 iterations : Training Loss =  7.85391619337657; Validation Loss = 9.285852575400511\n",
            "Cost after 48 iterations : Training Loss =  7.829189850583434; Validation Loss = 9.256381958333874\n",
            "Cost after 49 iterations : Training Loss =  7.8045983414830555; Validation Loss = 9.227068826986057\n",
            "Cost after 50 iterations : Training Loss =  7.780140691849207; Validation Loss = 9.197911569929007\n",
            "Cost after 51 iterations : Training Loss =  7.755815936479316; Validation Loss = 9.1689087589801\n",
            "Cost after 52 iterations : Training Loss =  7.731623118968561; Validation Loss = 9.140059088294514\n",
            "Cost after 53 iterations : Training Loss =  7.707561291560155; Validation Loss = 9.111361334873036\n",
            "Cost after 54 iterations : Training Loss =  7.683629515028671; Validation Loss = 9.082814332916048\n",
            "Cost after 55 iterations : Training Loss =  7.659826858578108; Validation Loss = 9.05441695714054\n",
            "Cost after 56 iterations : Training Loss =  7.636152399746751; Validation Loss = 9.026168111905658\n",
            "Cost after 57 iterations : Training Loss =  7.6126052243157325; Validation Loss = 8.998066724106504\n",
            "Cost after 58 iterations : Training Loss =  7.589184426219687; Validation Loss = 8.970111738514673\n",
            "Cost after 59 iterations : Training Loss =  7.565889107458974; Validation Loss = 8.942302114709335\n",
            "Cost after 60 iterations : Training Loss =  7.542718378013161; Validation Loss = 8.91463682504305\n",
            "Cost after 61 iterations : Training Loss =  7.519671355755714; Validation Loss = 8.887114853281554\n",
            "Cost after 62 iterations : Training Loss =  7.496747166369736; Validation Loss = 8.859735193683074\n",
            "Cost after 63 iterations : Training Loss =  7.473944943264768; Validation Loss = 8.832496850364493\n",
            "Cost after 64 iterations : Training Loss =  7.45126382749468; Validation Loss = 8.805398836855177\n",
            "Cost after 65 iterations : Training Loss =  7.428702967676541; Validation Loss = 8.778440175773712\n",
            "Cost after 66 iterations : Training Loss =  7.406261519910487; Validation Loss = 8.7516198985853\n",
            "Cost after 67 iterations : Training Loss =  7.383938647700615; Validation Loss = 8.724937045412378\n",
            "Cost after 68 iterations : Training Loss =  7.361733521876849; Validation Loss = 8.698390664880383\n",
            "Cost after 69 iterations : Training Loss =  7.339645320517738; Validation Loss = 8.671979813986939\n",
            "Cost after 70 iterations : Training Loss =  7.317673228874227; Validation Loss = 8.645703557986732\n",
            "Cost after 71 iterations : Training Loss =  7.295816439294381; Validation Loss = 8.619560970287083\n",
            "Cost after 72 iterations : Training Loss =  7.274074151149023; Validation Loss = 8.593551132350894\n",
            "Cost after 73 iterations : Training Loss =  7.252445570758223; Validation Loss = 8.567673133604657\n",
            "Cost after 74 iterations : Training Loss =  7.23092991131878; Validation Loss = 8.541926071350348\n",
            "Cost after 75 iterations : Training Loss =  7.209526392832565; Validation Loss = 8.5163090506801\n",
            "Cost after 76 iterations : Training Loss =  7.188234242035648; Validation Loss = 8.49082118439293\n",
            "Cost after 77 iterations : Training Loss =  7.167052692328426; Validation Loss = 8.465461592913403\n",
            "Cost after 78 iterations : Training Loss =  7.145980983706481; Validation Loss = 8.440229404211632\n",
            "Cost after 79 iterations : Training Loss =  7.125018362692371; Validation Loss = 8.415123753724682\n",
            "Cost after 80 iterations : Training Loss =  7.104164082268154; Validation Loss = 8.390143784279047\n",
            "Cost after 81 iterations : Training Loss =  7.083417401808787; Validation Loss = 8.36528864601428\n",
            "Cost after 82 iterations : Training Loss =  7.062777587016338; Validation Loss = 8.340557496307579\n",
            "Cost after 83 iterations : Training Loss =  7.0422439098549106; Validation Loss = 8.315949499699386\n",
            "Cost after 84 iterations : Training Loss =  7.021815648486458; Validation Loss = 8.291463827819907\n",
            "Cost after 85 iterations : Training Loss =  7.001492087207289; Validation Loss = 8.267099659316537\n",
            "Cost after 86 iterations : Training Loss =  6.981272516385344; Validation Loss = 8.242856179782137\n",
            "Cost after 87 iterations : Training Loss =  6.961156232398253; Validation Loss = 8.21873258168424\n",
            "Cost after 88 iterations : Training Loss =  6.941142537572112; Validation Loss = 8.1947280642951\n",
            "Cost after 89 iterations : Training Loss =  6.921230740120972; Validation Loss = 8.170841833622516\n",
            "Cost after 90 iterations : Training Loss =  6.901420154087077; Validation Loss = 8.147073102341562\n",
            "Cost after 91 iterations : Training Loss =  6.881710099281813; Validation Loss = 8.123421089727083\n",
            "Cost after 92 iterations : Training Loss =  6.862099901227299; Validation Loss = 8.099885021586976\n",
            "Cost after 93 iterations : Training Loss =  6.842588891098757; Validation Loss = 8.076464130196332\n",
            "Cost after 94 iterations : Training Loss =  6.82317640566749; Validation Loss = 8.053157654232269\n",
            "Cost after 95 iterations : Training Loss =  6.803861787244568; Validation Loss = 8.029964838709597\n",
            "Cost after 96 iterations : Training Loss =  6.78464438362516; Validation Loss = 8.0068849349172\n",
            "Cost after 97 iterations : Training Loss =  6.7655235480335305; Validation Loss = 7.983917200355189\n",
            "Cost after 98 iterations : Training Loss =  6.7464986390686885; Validation Loss = 7.961060898672788\n",
            "Cost after 99 iterations : Training Loss =  6.727569020650627; Validation Loss = 7.938315299606933\n",
            "Cost after 100 iterations : Training Loss =  6.708734061967269; Validation Loss = 7.915679678921597\n",
            "Cost after 101 iterations : Training Loss =  6.6899931374219594; Validation Loss = 7.893153318347834\n",
            "Cost after 102 iterations : Training Loss =  6.67134562658159; Validation Loss = 7.870735505524479\n",
            "Cost after 103 iterations : Training Loss =  6.6527909141253625; Validation Loss = 7.8484255339396185\n",
            "Cost after 104 iterations : Training Loss =  6.634328389794097; Validation Loss = 7.826222702872627\n",
            "Cost after 105 iterations : Training Loss =  6.615957448340148; Validation Loss = 7.804126317336988\n",
            "Cost after 106 iterations : Training Loss =  6.597677489477899; Validation Loss = 7.782135688023669\n",
            "Cost after 107 iterations : Training Loss =  6.57948791783487; Validation Loss = 7.760250131245291\n",
            "Cost after 108 iterations : Training Loss =  6.561388142903255; Validation Loss = 7.73846896888073\n",
            "Cost after 109 iterations : Training Loss =  6.543377578992216; Validation Loss = 7.716791528320599\n",
            "Cost after 110 iterations : Training Loss =  6.525455645180565; Validation Loss = 7.695217142413185\n",
            "Cost after 111 iterations : Training Loss =  6.507621765270028; Validation Loss = 7.673745149411026\n",
            "Cost after 112 iterations : Training Loss =  6.489875367739099; Validation Loss = 7.652374892918192\n",
            "Cost after 113 iterations : Training Loss =  6.472215885697353; Validation Loss = 7.631105721838077\n",
            "Cost after 114 iterations : Training Loss =  6.454642756840327; Validation Loss = 7.609936990321809\n",
            "Cost after 115 iterations : Training Loss =  6.437155423404882; Validation Loss = 7.58886805771726\n",
            "Cost after 116 iterations : Training Loss =  6.419753332125089; Validation Loss = 7.567898288518645\n",
            "Cost after 117 iterations : Training Loss =  6.402435934188623; Validation Loss = 7.547027052316671\n",
            "Cost after 118 iterations : Training Loss =  6.3852026851936445; Validation Loss = 7.5262537237492415\n",
            "Cost after 119 iterations : Training Loss =  6.368053045106148; Validation Loss = 7.505577682452748\n",
            "Cost after 120 iterations : Training Loss =  6.350986478217837; Validation Loss = 7.484998313013911\n",
            "Cost after 121 iterations : Training Loss =  6.334002453104423; Validation Loss = 7.4645150049221325\n",
            "Cost after 122 iterations : Training Loss =  6.317100442584423; Validation Loss = 7.44412715252238\n",
            "Cost after 123 iterations : Training Loss =  6.300279923678464; Validation Loss = 7.423834154968712\n",
            "Cost after 124 iterations : Training Loss =  6.283540377568913; Validation Loss = 7.403635416178125\n",
            "Cost after 125 iterations : Training Loss =  6.266881289560114; Validation Loss = 7.383530344785097\n",
            "Cost after 126 iterations : Training Loss =  6.250302149038959; Validation Loss = 7.363518354096541\n",
            "Cost after 127 iterations : Training Loss =  6.233802449435981; Validation Loss = 7.343598862047337\n",
            "Cost after 128 iterations : Training Loss =  6.217381688186769; Validation Loss = 7.323771291156239\n",
            "Cost after 129 iterations : Training Loss =  6.201039366693972; Validation Loss = 7.304035068482409\n",
            "Cost after 130 iterations : Training Loss =  6.184774990289569; Validation Loss = 7.284389625582362\n",
            "Cost after 131 iterations : Training Loss =  6.168588068197638; Validation Loss = 7.264834398467347\n",
            "Cost after 132 iterations : Training Loss =  6.152478113497551; Validation Loss = 7.245368827561324\n",
            "Cost after 133 iterations : Training Loss =  6.136444643087534; Validation Loss = 7.225992357659305\n",
            "Cost after 134 iterations : Training Loss =  6.120487177648627; Validation Loss = 7.206704437886152\n",
            "Cost after 135 iterations : Training Loss =  6.104605241609087; Validation Loss = 7.187504521655891\n",
            "Cost after 136 iterations : Training Loss =  6.088798363109134; Validation Loss = 7.168392066631448\n",
            "Cost after 137 iterations : Training Loss =  6.073066073966142; Validation Loss = 7.149366534684811\n",
            "Cost after 138 iterations : Training Loss =  6.057407909640119; Validation Loss = 7.130427391857611\n",
            "Cost after 139 iterations : Training Loss =  6.0418234091996545; Validation Loss = 7.11157410832221\n",
            "Cost after 140 iterations : Training Loss =  6.026312115288225; Validation Loss = 7.092806158343143\n",
            "Cost after 141 iterations : Training Loss =  6.010873574090811; Validation Loss = 7.074123020239004\n",
            "Cost after 142 iterations : Training Loss =  5.995507335300946; Validation Loss = 7.055524176344753\n",
            "Cost after 143 iterations : Training Loss =  5.98021295208807; Validation Loss = 7.037009112974424\n",
            "Cost after 144 iterations : Training Loss =  5.964989981065278; Validation Loss = 7.018577320384245\n",
            "Cost after 145 iterations : Training Loss =  5.949837982257405; Validation Loss = 7.000228292736154\n",
            "Cost after 146 iterations : Training Loss =  5.9347565190694445; Validation Loss = 6.981961528061717\n",
            "Cost after 147 iterations : Training Loss =  5.91974515825532; Validation Loss = 6.963776528226406\n",
            "Cost after 148 iterations : Training Loss =  5.9048034698869944; Validation Loss = 6.945672798894326\n",
            "Cost after 149 iterations : Training Loss =  5.889931027323924; Validation Loss = 6.927649849493256\n",
            "Cost after 150 iterations : Training Loss =  5.8751274071827835; Validation Loss = 6.909707193180079\n",
            "Cost after 151 iterations : Training Loss =  5.860392189307621; Validation Loss = 6.891844346806631\n",
            "Cost after 152 iterations : Training Loss =  5.8457249567402405; Validation Loss = 6.874060830885884\n",
            "Cost after 153 iterations : Training Loss =  5.831125295690915; Validation Loss = 6.856356169558438\n",
            "Cost after 154 iterations : Training Loss =  5.816592795509485; Validation Loss = 6.838729890559478\n",
            "Cost after 155 iterations : Training Loss =  5.802127048656654; Validation Loss = 6.821181525186001\n",
            "Cost after 156 iterations : Training Loss =  5.787727650675694; Validation Loss = 6.80371060826442\n",
            "Cost after 157 iterations : Training Loss =  5.7733942001643825; Validation Loss = 6.78631667811853\n",
            "Cost after 158 iterations : Training Loss =  5.759126298747271; Validation Loss = 6.768999276537771\n",
            "Cost after 159 iterations : Training Loss =  5.744923551048262; Validation Loss = 6.751757948745893\n",
            "Cost after 160 iterations : Training Loss =  5.730785564663389; Validation Loss = 6.734592243369853\n",
            "Cost after 161 iterations : Training Loss =  5.716711950134027; Validation Loss = 6.717501712409152\n",
            "Cost after 162 iterations : Training Loss =  5.702702320920259; Validation Loss = 6.70048591120543\n",
            "Cost after 163 iterations : Training Loss =  5.688756293374593; Validation Loss = 6.683544398412392\n",
            "Cost after 164 iterations : Training Loss =  5.6748734867159305; Validation Loss = 6.666676735966056\n",
            "Cost after 165 iterations : Training Loss =  5.6610535230038215; Validation Loss = 6.649882489055315\n",
            "Cost after 166 iterations : Training Loss =  5.647296027112971; Validation Loss = 6.633161226092804\n",
            "Cost after 167 iterations : Training Loss =  5.63360062670803; Validation Loss = 6.616512518686078\n",
            "Cost after 168 iterations : Training Loss =  5.619966952218659; Validation Loss = 6.599935941609085\n",
            "Cost after 169 iterations : Training Loss =  5.606394636814806; Validation Loss = 6.583431072773934\n",
            "Cost after 170 iterations : Training Loss =  5.592883316382314; Validation Loss = 6.566997493202965\n",
            "Cost after 171 iterations : Training Loss =  5.579432629498715; Validation Loss = 6.5506347870011234\n",
            "Cost after 172 iterations : Training Loss =  5.566042217409322; Validation Loss = 6.534342541328593\n",
            "Cost after 173 iterations : Training Loss =  5.552711724003526; Validation Loss = 6.518120346373698\n",
            "Cost after 174 iterations : Training Loss =  5.539440795791404; Validation Loss = 6.501967795326181\n",
            "Cost after 175 iterations : Training Loss =  5.5262290818805075; Validation Loss = 6.485884484350628\n",
            "Cost after 176 iterations : Training Loss =  5.5130762339529; Validation Loss = 6.469870012560266\n",
            "Cost after 177 iterations : Training Loss =  5.499981906242477; Validation Loss = 6.45392398199097\n",
            "Cost after 178 iterations : Training Loss =  5.486945755512472; Validation Loss = 6.438045997575605\n",
            "Cost after 179 iterations : Training Loss =  5.473967441033197; Validation Loss = 6.4222356671185326\n",
            "Cost after 180 iterations : Training Loss =  5.461046624560058; Validation Loss = 6.406492601270497\n",
            "Cost after 181 iterations : Training Loss =  5.448182970311725; Validation Loss = 6.39081641350365\n",
            "Cost after 182 iterations : Training Loss =  5.435376144948607; Validation Loss = 6.375206720086956\n",
            "Cost after 183 iterations : Training Loss =  5.422625817551473; Validation Loss = 6.359663140061732\n",
            "Cost after 184 iterations : Training Loss =  5.409931659600325; Validation Loss = 6.344185295217504\n",
            "Cost after 185 iterations : Training Loss =  5.397293344953503; Validation Loss = 6.328772810068103\n",
            "Cost after 186 iterations : Training Loss =  5.384710549826961; Validation Loss = 6.313425311827988\n",
            "Cost after 187 iterations : Training Loss =  5.372182952773811; Validation Loss = 6.298142430388812\n",
            "Cost after 188 iterations : Training Loss =  5.3597102346640035; Validation Loss = 6.2829237982962445\n",
            "Cost after 189 iterations : Training Loss =  5.34729207866427; Validation Loss = 6.26776905072703\n",
            "Cost after 190 iterations : Training Loss =  5.334928170218265; Validation Loss = 6.252677825466203\n",
            "Cost after 191 iterations : Training Loss =  5.322618197026858; Validation Loss = 6.2376497628846685\n",
            "Cost after 192 iterations : Training Loss =  5.310361849028704; Validation Loss = 6.222684505916891\n",
            "Cost after 193 iterations : Training Loss =  5.298158818380926; Validation Loss = 6.2077817000388675\n",
            "Cost after 194 iterations : Training Loss =  5.286008799440075; Validation Loss = 6.192940993246303\n",
            "Cost after 195 iterations : Training Loss =  5.273911488743192; Validation Loss = 6.1781620360330125\n",
            "Cost after 196 iterations : Training Loss =  5.261866584989156; Validation Loss = 6.163444481369519\n",
            "Cost after 197 iterations : Training Loss =  5.249873789020133; Validation Loss = 6.148787984681926\n",
            "Cost after 198 iterations : Training Loss =  5.237932803803269; Validation Loss = 6.134192203830906\n",
            "Cost after 199 iterations : Training Loss =  5.226043334412533; Validation Loss = 6.119656799090991\n",
            "Cost after 200 iterations : Training Loss =  5.214205088010774; Validation Loss = 6.105181433130014\n",
            "Cost after 201 iterations : Training Loss =  5.202417773831907; Validation Loss = 6.0907657709887575\n",
            "Cost after 202 iterations : Training Loss =  5.190681103163331; Validation Loss = 6.076409480060861\n",
            "Cost after 203 iterations : Training Loss =  5.178994789328489; Validation Loss = 6.062112230072845\n",
            "Cost after 204 iterations : Training Loss =  5.167358547669592; Validation Loss = 6.047873693064372\n",
            "Cost after 205 iterations : Training Loss =  5.155772095530558; Validation Loss = 6.033693543368742\n",
            "Cost after 206 iterations : Training Loss =  5.1442351522400775; Validation Loss = 6.019571457593501\n",
            "Cost after 207 iterations : Training Loss =  5.13274743909487; Validation Loss = 6.005507114601325\n",
            "Cost after 208 iterations : Training Loss =  5.121308679343072; Validation Loss = 5.9915001954909926\n",
            "Cost after 209 iterations : Training Loss =  5.10991859816786; Validation Loss = 5.977550383578671\n",
            "Cost after 210 iterations : Training Loss =  5.098576922671139; Validation Loss = 5.963657364379266\n",
            "Cost after 211 iterations : Training Loss =  5.087283381857474; Validation Loss = 5.949820825588035\n",
            "Cost after 212 iterations : Training Loss =  5.076037706618131; Validation Loss = 5.936040457062327\n",
            "Cost after 213 iterations : Training Loss =  5.064839629715298; Validation Loss = 5.922315950803557\n",
            "Cost after 214 iterations : Training Loss =  5.053688885766446; Validation Loss = 5.908647000939298\n",
            "Cost after 215 iterations : Training Loss =  5.042585211228844; Validation Loss = 5.8950333037056\n",
            "Cost after 216 iterations : Training Loss =  5.031528344384243; Validation Loss = 5.8814745574294385\n",
            "Cost after 217 iterations : Training Loss =  5.020518025323687; Validation Loss = 5.8679704625113605\n",
            "Cost after 218 iterations : Training Loss =  5.00955399593247; Validation Loss = 5.854520721408285\n",
            "Cost after 219 iterations : Training Loss =  4.9986359998752725; Validation Loss = 5.841125038616491\n",
            "Cost after 220 iterations : Training Loss =  4.987763782581407; Validation Loss = 5.827783120654736\n",
            "Cost after 221 iterations : Training Loss =  4.976937091230205; Validation Loss = 5.814494676047559\n",
            "Cost after 222 iterations : Training Loss =  4.966155674736592; Validation Loss = 5.801259415308784\n",
            "Cost after 223 iterations : Training Loss =  4.955419283736734; Validation Loss = 5.788077050925079\n",
            "Cost after 224 iterations : Training Loss =  4.944727670573879; Validation Loss = 5.7749472973397715\n",
            "Cost after 225 iterations : Training Loss =  4.934080589284309; Validation Loss = 5.761869870936799\n",
            "Cost after 226 iterations : Training Loss =  4.923477795583424; Validation Loss = 5.748844490024762\n",
            "Cost after 227 iterations : Training Loss =  4.912919046851967; Validation Loss = 5.735870874821188\n",
            "Cost after 228 iterations : Training Loss =  4.902404102122402; Validation Loss = 5.722948747436947\n",
            "Cost after 229 iterations : Training Loss =  4.891932722065358; Validation Loss = 5.710077831860737\n",
            "Cost after 230 iterations : Training Loss =  4.881504668976311; Validation Loss = 5.697257853943851\n",
            "Cost after 231 iterations : Training Loss =  4.871119706762246; Validation Loss = 5.684488541384933\n",
            "Cost after 232 iterations : Training Loss =  4.860777600928613; Validation Loss = 5.67176962371503\n",
            "Cost after 233 iterations : Training Loss =  4.850478118566252; Validation Loss = 5.659100832282686\n",
            "Cost after 234 iterations : Training Loss =  4.840221028338562; Validation Loss = 5.6464819002391975\n",
            "Cost after 235 iterations : Training Loss =  4.830006100468715; Validation Loss = 5.633912562524044\n",
            "Cost after 236 iterations : Training Loss =  4.819833106727034; Validation Loss = 5.621392555850415\n",
            "Cost after 237 iterations : Training Loss =  4.80970182041845; Validation Loss = 5.608921618690877\n",
            "Cost after 238 iterations : Training Loss =  4.7996120163701645; Validation Loss = 5.596499491263257\n",
            "Cost after 239 iterations : Training Loss =  4.789563470919294; Validation Loss = 5.584125915516502\n",
            "Cost after 240 iterations : Training Loss =  4.779555961900735; Validation Loss = 5.571800635116814\n",
            "Cost after 241 iterations : Training Loss =  4.769589268635141; Validation Loss = 5.559523395433874\n",
            "Cost after 242 iterations : Training Loss =  4.759663171916925; Validation Loss = 5.547293943527126\n",
            "Cost after 243 iterations : Training Loss =  4.749777454002499; Validation Loss = 5.535112028132331\n",
            "Cost after 244 iterations : Training Loss =  4.739931898598501; Validation Loss = 5.522977399648092\n",
            "Cost after 245 iterations : Training Loss =  4.73012629085023; Validation Loss = 5.510889810122607\n",
            "Cost after 246 iterations : Training Loss =  4.720360417330136; Validation Loss = 5.498849013240531\n",
            "Cost after 247 iterations : Training Loss =  4.710634066026418; Validation Loss = 5.486854764309916\n",
            "Cost after 248 iterations : Training Loss =  4.700947026331766; Validation Loss = 5.47490682024933\n",
            "Cost after 249 iterations : Training Loss =  4.691299089032167; Validation Loss = 5.463004939575048\n",
            "Cost after 250 iterations : Training Loss =  4.681690046295838; Validation Loss = 5.451148882388391\n",
            "Cost after 251 iterations : Training Loss =  4.672119691662245; Validation Loss = 5.439338410363185\n",
            "Cost after 252 iterations : Training Loss =  4.662587820031245; Validation Loss = 5.427573286733301\n",
            "Cost after 253 iterations : Training Loss =  4.653094227652313; Validation Loss = 5.415853276280354\n",
            "Cost after 254 iterations : Training Loss =  4.643638712113878; Validation Loss = 5.404178145321496\n",
            "Cost after 255 iterations : Training Loss =  4.634221072332743; Validation Loss = 5.392547661697311\n",
            "Cost after 256 iterations : Training Loss =  4.6248411085436265; Validation Loss = 5.380961594759846\n",
            "Cost after 257 iterations : Training Loss =  4.615498622288773; Validation Loss = 5.369419715360728\n",
            "Cost after 258 iterations : Training Loss =  4.606193416407674; Validation Loss = 5.357921795839404\n",
            "Cost after 259 iterations : Training Loss =  4.596925295026895; Validation Loss = 5.346467610011496\n",
            "Cost after 260 iterations : Training Loss =  4.587694063549973; Validation Loss = 5.335056933157234\n",
            "Cost after 261 iterations : Training Loss =  4.578499528647417; Validation Loss = 5.323689542010039\n",
            "Cost after 262 iterations : Training Loss =  4.5693414982467955; Validation Loss = 5.312365214745167\n",
            "Cost after 263 iterations : Training Loss =  4.560219781522931; Validation Loss = 5.3010837309684735\n",
            "Cost after 264 iterations : Training Loss =  4.551134188888154; Validation Loss = 5.2898448717052835\n",
            "Cost after 265 iterations : Training Loss =  4.542084531982687; Validation Loss = 5.278648419389397\n",
            "Cost after 266 iterations : Training Loss =  4.533070623665075; Validation Loss = 5.267494157852093\n",
            "Cost after 267 iterations : Training Loss =  4.524092278002721; Validation Loss = 5.256381872311356\n",
            "Cost after 268 iterations : Training Loss =  4.515149310262512; Validation Loss = 5.2453113493611045\n",
            "Cost after 269 iterations : Training Loss =  4.506241536901532; Validation Loss = 5.234282376960606\n",
            "Cost after 270 iterations : Training Loss =  4.497368775557839; Validation Loss = 5.223294744423875\n",
            "Cost after 271 iterations : Training Loss =  4.4885308450413515; Validation Loss = 5.2123482424092735\n",
            "Cost after 272 iterations : Training Loss =  4.47972756532479; Validation Loss = 5.201442662909146\n",
            "Cost after 273 iterations : Training Loss =  4.470958757534729; Validation Loss = 5.190577799239583\n",
            "Cost after 274 iterations : Training Loss =  4.462224243942696; Validation Loss = 5.179753446030204\n",
            "Cost after 275 iterations : Training Loss =  4.453523847956414; Validation Loss = 5.168969399214172\n",
            "Cost after 276 iterations : Training Loss =  4.444857394111017; Validation Loss = 5.158225456018125\n",
            "Cost after 277 iterations : Training Loss =  4.436224708060457; Validation Loss = 5.147521414952338\n",
            "Cost after 278 iterations : Training Loss =  4.427625616568906; Validation Loss = 5.1368570758008945\n",
            "Cost after 279 iterations : Training Loss =  4.419059947502281; Validation Loss = 5.126232239611975\n",
            "Cost after 280 iterations : Training Loss =  4.410527529819829; Validation Loss = 5.115646708688239\n",
            "Cost after 281 iterations : Training Loss =  4.402028193565789; Validation Loss = 5.105100286577275\n",
            "Cost after 282 iterations : Training Loss =  4.3935617698611145; Validation Loss = 5.094592778062125\n",
            "Cost after 283 iterations : Training Loss =  4.385128090895301; Validation Loss = 5.084123989151944\n",
            "Cost after 284 iterations : Training Loss =  4.376726989918255; Validation Loss = 5.073693727072678\n",
            "Cost after 285 iterations : Training Loss =  4.36835830123225; Validation Loss = 5.063301800257872\n",
            "Cost after 286 iterations : Training Loss =  4.360021860183953; Validation Loss = 5.052948018339521\n",
            "Cost after 287 iterations : Training Loss =  4.3517175031565305; Validation Loss = 5.042632192139074\n",
            "Cost after 288 iterations : Training Loss =  4.3434450675617935; Validation Loss = 5.032354133658411\n",
            "Cost after 289 iterations : Training Loss =  4.3352043918324545; Validation Loss = 5.022113656070986\n",
            "Cost after 290 iterations : Training Loss =  4.3269953154144085; Validation Loss = 5.011910573713009\n",
            "Cost after 291 iterations : Training Loss =  4.318817678759131; Validation Loss = 5.001744702074718\n",
            "Cost after 292 iterations : Training Loss =  4.3106713233160905; Validation Loss = 4.991615857791722\n",
            "Cost after 293 iterations : Training Loss =  4.302556091525273; Validation Loss = 4.981523858636432\n",
            "Cost after 294 iterations : Training Loss =  4.294471826809744; Validation Loss = 4.971468523509542\n",
            "Cost after 295 iterations : Training Loss =  4.286418373568278; Validation Loss = 4.961449672431604\n",
            "Cost after 296 iterations : Training Loss =  4.278395577168083; Validation Loss = 4.951467126534681\n",
            "Cost after 297 iterations : Training Loss =  4.270403283937533; Validation Loss = 4.94152070805407\n",
            "Cost after 298 iterations : Training Loss =  4.262441341159022; Validation Loss = 4.931610240320061\n",
            "Cost after 299 iterations : Training Loss =  4.254509597061841; Validation Loss = 4.9217355477498375\n",
            "Cost after 300 iterations : Training Loss =  4.246607900815132; Validation Loss = 4.911896455839385\n",
            "Cost after 301 iterations : Training Loss =  4.238736102520897; Validation Loss = 4.902092791155497\n",
            "Cost after 302 iterations : Training Loss =  4.230894053207091; Validation Loss = 4.89232438132786\n",
            "Cost after 303 iterations : Training Loss =  4.223081604820726; Validation Loss = 4.882591055041172\n",
            "Cost after 304 iterations : Training Loss =  4.215298610221096; Validation Loss = 4.872892642027365\n",
            "Cost after 305 iterations : Training Loss =  4.207544923173006; Validation Loss = 4.86322897305789\n",
            "Cost after 306 iterations : Training Loss =  4.19982039834009; Validation Loss = 4.853599879936032\n",
            "Cost after 307 iterations : Training Loss =  4.192124891278197; Validation Loss = 4.84400519548935\n",
            "Cost after 308 iterations : Training Loss =  4.184458258428783; Validation Loss = 4.834444753562135\n",
            "Cost after 309 iterations : Training Loss =  4.176820357112426; Validation Loss = 4.824918389007943\n",
            "Cost after 310 iterations : Training Loss =  4.169211045522342; Validation Loss = 4.815425937682206\n",
            "Cost after 311 iterations : Training Loss =  4.161630182717997; Validation Loss = 4.805967236434913\n",
            "Cost after 312 iterations : Training Loss =  4.154077628618744; Validation Loss = 4.796542123103315\n",
            "Cost after 313 iterations : Training Loss =  4.146553243997525; Validation Loss = 4.787150436504722\n",
            "Cost after 314 iterations : Training Loss =  4.1390568904746425; Validation Loss = 4.777792016429372\n",
            "Cost after 315 iterations : Training Loss =  4.131588430511558; Validation Loss = 4.768466703633324\n",
            "Cost after 316 iterations : Training Loss =  4.124147727404756; Validation Loss = 4.7591743398314446\n",
            "Cost after 317 iterations : Training Loss =  4.116734645279671; Validation Loss = 4.749914767690455\n",
            "Cost after 318 iterations : Training Loss =  4.109349049084646; Validation Loss = 4.740687830821995\n",
            "Cost after 319 iterations : Training Loss =  4.101990804584944; Validation Loss = 4.73149337377579\n",
            "Cost after 320 iterations : Training Loss =  4.09465977835684; Validation Loss = 4.7223312420328645\n",
            "Cost after 321 iterations : Training Loss =  4.087355837781717; Validation Loss = 4.713201281998821\n",
            "Cost after 322 iterations : Training Loss =  4.080078851040266; Validation Loss = 4.7041033409971496\n",
            "Cost after 323 iterations : Training Loss =  4.072828687106661; Validation Loss = 4.695037267262585\n",
            "Cost after 324 iterations : Training Loss =  4.065605215742886; Validation Loss = 4.686002909934614\n",
            "Cost after 325 iterations : Training Loss =  4.0584083074929955; Validation Loss = 4.677000119050897\n",
            "Cost after 326 iterations : Training Loss =  4.051237833677513; Validation Loss = 4.668028745540843\n",
            "Cost after 327 iterations : Training Loss =  4.044093666387826; Validation Loss = 4.6590886412192285\n",
            "Cost after 328 iterations : Training Loss =  4.036975678480671; Validation Loss = 4.650179658779831\n",
            "Cost after 329 iterations : Training Loss =  4.0298837435726; Validation Loss = 4.6413016517891394\n",
            "Cost after 330 iterations : Training Loss =  4.0228177360345665; Validation Loss = 4.632454474680126\n",
            "Cost after 331 iterations : Training Loss =  4.015777530986508; Validation Loss = 4.62363798274605\n",
            "Cost after 332 iterations : Training Loss =  4.008763004291984; Validation Loss = 4.614852032134328\n",
            "Cost after 333 iterations : Training Loss =  4.001774032552882; Validation Loss = 4.606096479840452\n",
            "Cost after 334 iterations : Training Loss =  3.9948104931041293; Validation Loss = 4.5973711837019495\n",
            "Cost after 335 iterations : Training Loss =  3.987872264008475; Validation Loss = 4.588676002392413\n",
            "Cost after 336 iterations : Training Loss =  3.980959224051324; Validation Loss = 4.5800107954155616\n",
            "Cost after 337 iterations : Training Loss =  3.974071252735582; Validation Loss = 4.571375423099357\n",
            "Cost after 338 iterations : Training Loss =  3.9672082302765546; Validation Loss = 4.562769746590162\n",
            "Cost after 339 iterations : Training Loss =  3.960370037596918; Validation Loss = 4.554193627846961\n",
            "Cost after 340 iterations : Training Loss =  3.953556556321702; Validation Loss = 4.545646929635638\n",
            "Cost after 341 iterations : Training Loss =  3.9467676687732967; Validation Loss = 4.53712951552326\n",
            "Cost after 342 iterations : Training Loss =  3.940003257966558; Validation Loss = 4.528641249872434\n",
            "Cost after 343 iterations : Training Loss =  3.933263207603902; Validation Loss = 4.520181997835744\n",
            "Cost after 344 iterations : Training Loss =  3.9265474020704687; Validation Loss = 4.511751625350156\n",
            "Cost after 345 iterations : Training Loss =  3.9198557264292897; Validation Loss = 4.50334999913154\n",
            "Cost after 346 iterations : Training Loss =  3.9131880664165464; Validation Loss = 4.49497698666919\n",
            "Cost after 347 iterations : Training Loss =  3.9065443084368447; Validation Loss = 4.486632456220443\n",
            "Cost after 348 iterations : Training Loss =  3.8999243395585053; Validation Loss = 4.478316276805267\n",
            "Cost after 349 iterations : Training Loss =  3.8933280475089194; Validation Loss = 4.470028318200967\n",
            "Cost after 350 iterations : Training Loss =  3.8867553206699417; Validation Loss = 4.461768450936871\n",
            "Cost after 351 iterations : Training Loss =  3.8802060480733225; Validation Loss = 4.453536546289128\n",
            "Cost after 352 iterations : Training Loss =  3.8736801193961448; Validation Loss = 4.445332476275476\n",
            "Cost after 353 iterations : Training Loss =  3.8671774249563384; Validation Loss = 4.437156113650092\n",
            "Cost after 354 iterations : Training Loss =  3.8606978557082283; Validation Loss = 4.4290073318985055\n",
            "Cost after 355 iterations : Training Loss =  3.8542413032380773; Validation Loss = 4.420886005232494\n",
            "Cost after 356 iterations : Training Loss =  3.8478076597597375; Validation Loss = 4.4127920085850905\n",
            "Cost after 357 iterations : Training Loss =  3.841396818110238; Validation Loss = 4.404725217605537\n",
            "Cost after 358 iterations : Training Loss =  3.835008671745518; Validation Loss = 4.396685508654394\n",
            "Cost after 359 iterations : Training Loss =  3.828643114736098; Validation Loss = 4.3886727587986\n",
            "Cost after 360 iterations : Training Loss =  3.8223000417628623; Validation Loss = 4.380686845806602\n",
            "Cost after 361 iterations : Training Loss =  3.8159793481128323; Validation Loss = 4.37272764814355\n",
            "Cost after 362 iterations : Training Loss =  3.809680929674962; Validation Loss = 4.364795044966463\n",
            "Cost after 363 iterations : Training Loss =  3.8034046829360277; Validation Loss = 4.356888916119531\n",
            "Cost after 364 iterations : Training Loss =  3.797150504976469; Validation Loss = 4.3490091421293435\n",
            "Cost after 365 iterations : Training Loss =  3.7909182934663557; Validation Loss = 4.341155604200262\n",
            "Cost after 366 iterations : Training Loss =  3.784707946661295; Validation Loss = 4.333328184209749\n",
            "Cost after 367 iterations : Training Loss =  3.7785193633984377; Validation Loss = 4.32552676470378\n",
            "Cost after 368 iterations : Training Loss =  3.772352443092485; Validation Loss = 4.317751228892278\n",
            "Cost after 369 iterations : Training Loss =  3.766207085731744; Validation Loss = 4.310001460644582\n",
            "Cost after 370 iterations : Training Loss =  3.7600831918741973; Validation Loss = 4.302277344484957\n",
            "Cost after 371 iterations : Training Loss =  3.7539806626436127; Validation Loss = 4.294578765588134\n",
            "Cost after 372 iterations : Training Loss =  3.7478993997256986; Validation Loss = 4.2869056097749\n",
            "Cost after 373 iterations : Training Loss =  3.741839305364258; Validation Loss = 4.279257763507692\n",
            "Cost after 374 iterations : Training Loss =  3.7358002823574017; Validation Loss = 4.271635113886275\n",
            "Cost after 375 iterations : Training Loss =  3.7297822340537694; Validation Loss = 4.264037548643389\n",
            "Cost after 376 iterations : Training Loss =  3.723785064348819; Validation Loss = 4.256464956140508\n",
            "Cost after 377 iterations : Training Loss =  3.7178086776810875; Validation Loss = 4.24891722536356\n",
            "Cost after 378 iterations : Training Loss =  3.711852979028526; Validation Loss = 4.241394245918725\n",
            "Cost after 379 iterations : Training Loss =  3.7059178739048564; Validation Loss = 4.233895908028254\n",
            "Cost after 380 iterations : Training Loss =  3.700003268355939; Validation Loss = 4.226422102526325\n",
            "Cost after 381 iterations : Training Loss =  3.694109068956192; Validation Loss = 4.2189727208549135\n",
            "Cost after 382 iterations : Training Loss =  3.6882351828050144; Validation Loss = 4.211547655059729\n",
            "Cost after 383 iterations : Training Loss =  3.6823815175232593; Validation Loss = 4.2041467977861355\n",
            "Cost after 384 iterations : Training Loss =  3.6765479812497426; Validation Loss = 4.196770042275171\n",
            "Cost after 385 iterations : Training Loss =  3.6707344826377297; Validation Loss = 4.18941728235954\n",
            "Cost after 386 iterations : Training Loss =  3.664940930851497; Validation Loss = 4.182088412459636\n",
            "Cost after 387 iterations : Training Loss =  3.659167235562925; Validation Loss = 4.174783327579659\n",
            "Cost after 388 iterations : Training Loss =  3.6534133069480608; Validation Loss = 4.167501923303681\n",
            "Cost after 389 iterations : Training Loss =  3.647679055683793; Validation Loss = 4.160244095791821\n",
            "Cost after 390 iterations : Training Loss =  3.6419643929444607; Validation Loss = 4.153009741776367\n",
            "Cost after 391 iterations : Training Loss =  3.6362692303985624; Validation Loss = 4.1457987585580165\n",
            "Cost after 392 iterations : Training Loss =  3.6305934802054622; Validation Loss = 4.138611044002075\n",
            "Cost after 393 iterations : Training Loss =  3.6249370550120985; Validation Loss = 4.131446496534699\n",
            "Cost after 394 iterations : Training Loss =  3.619299867949763; Validation Loss = 4.124305015139221\n",
            "Cost after 395 iterations : Training Loss =  3.6136818326308884; Validation Loss = 4.117186499352438\n",
            "Cost after 396 iterations : Training Loss =  3.60808286314582; Validation Loss = 4.1100908492609305\n",
            "Cost after 397 iterations : Training Loss =  3.6025028740596965; Validation Loss = 4.103017965497481\n",
            "Cost after 398 iterations : Training Loss =  3.5969417804092685; Validation Loss = 4.095967749237442\n",
            "Cost after 399 iterations : Training Loss =  3.5913994976997983; Validation Loss = 4.088940102195154\n",
            "Cost after 400 iterations : Training Loss =  3.585875941901959; Validation Loss = 4.081934926620426\n",
            "Cost after 401 iterations : Training Loss =  3.5803710294487665; Validation Loss = 4.074952125294998\n",
            "Cost after 402 iterations : Training Loss =  3.5748846772325296; Validation Loss = 4.067991601529039\n",
            "Cost after 403 iterations : Training Loss =  3.5694168026018334; Validation Loss = 4.061053259157716\n",
            "Cost after 404 iterations : Training Loss =  3.563967323358526; Validation Loss = 4.054137002537715\n",
            "Cost after 405 iterations : Training Loss =  3.55853615775475; Validation Loss = 4.0472427365438595\n",
            "Cost after 406 iterations : Training Loss =  3.553123224489986; Validation Loss = 4.040370366565705\n",
            "Cost after 407 iterations : Training Loss =  3.5477284427081304; Validation Loss = 4.033519798504186\n",
            "Cost after 408 iterations : Training Loss =  3.542351731994561; Validation Loss = 4.026690938768296\n",
            "Cost after 409 iterations : Training Loss =  3.5369930123732742; Validation Loss = 4.019883694271733\n",
            "Cost after 410 iterations : Training Loss =  3.53165220430401; Validation Loss = 4.0130979724296765\n",
            "Cost after 411 iterations : Training Loss =  3.5263292286794137; Validation Loss = 4.006333681155483\n",
            "Cost after 412 iterations : Training Loss =  3.5210240068222034; Validation Loss = 3.999590728857463\n",
            "Cost after 413 iterations : Training Loss =  3.515736460482384; Validation Loss = 3.9928690244356853\n",
            "Cost after 414 iterations : Training Loss =  3.510466511834457; Validation Loss = 3.9861684772787807\n",
            "Cost after 415 iterations : Training Loss =  3.5052140834746672; Validation Loss = 3.9794889972607725\n",
            "Cost after 416 iterations : Training Loss =  3.499979098418271; Validation Loss = 3.9728304947379622\n",
            "Cost after 417 iterations : Training Loss =  3.494761480096818; Validation Loss = 3.9661928805457967\n",
            "Cost after 418 iterations : Training Loss =  3.4895611523554386; Validation Loss = 3.9595760659957815\n",
            "Cost after 419 iterations : Training Loss =  3.484378039450202; Validation Loss = 3.9529799628724263\n",
            "Cost after 420 iterations : Training Loss =  3.4792120660454326; Validation Loss = 3.946404483430176\n",
            "Cost after 421 iterations : Training Loss =  3.4740631572110856; Validation Loss = 3.939849540390432\n",
            "Cost after 422 iterations : Training Loss =  3.4689312384201463; Validation Loss = 3.933315046938521\n",
            "Cost after 423 iterations : Training Loss =  3.4638162355460036; Validation Loss = 3.9268009167207207\n",
            "Cost after 424 iterations : Training Loss =  3.4587180748599; Validation Loss = 3.920307063841319\n",
            "Cost after 425 iterations : Training Loss =  3.453636683028371; Validation Loss = 3.913833402859696\n",
            "Cost after 426 iterations : Training Loss =  3.448571987110703; Validation Loss = 3.907379848787369\n",
            "Cost after 427 iterations : Training Loss =  3.44352391455641; Validation Loss = 3.900946317085163\n",
            "Cost after 428 iterations : Training Loss =  3.438492393202748; Validation Loss = 3.894532723660308\n",
            "Cost after 429 iterations : Training Loss =  3.4334773512722325; Validation Loss = 3.888138984863613\n",
            "Cost after 430 iterations : Training Loss =  3.4284787173701545; Validation Loss = 3.88176501748662\n",
            "Cost after 431 iterations : Training Loss =  3.423496420482175; Validation Loss = 3.8754107387588346\n",
            "Cost after 432 iterations : Training Loss =  3.418530389971869; Validation Loss = 3.8690760663449253\n",
            "Cost after 433 iterations : Training Loss =  3.4135805555783283; Validation Loss = 3.8627609183419604\n",
            "Cost after 434 iterations : Training Loss =  3.4086468474137757; Validation Loss = 3.8564652132766923\n",
            "Cost after 435 iterations : Training Loss =  3.4037291959611857; Validation Loss = 3.850188870102801\n",
            "Cost after 436 iterations : Training Loss =  3.398827532071941; Validation Loss = 3.843931808198234\n",
            "Cost after 437 iterations : Training Loss =  3.393941786963484; Validation Loss = 3.8376939473624994\n",
            "Cost after 438 iterations : Training Loss =  3.389071892217008; Validation Loss = 3.8314752078140133\n",
            "Cost after 439 iterations : Training Loss =  3.384217779775152; Validation Loss = 3.8252755101874776\n",
            "Cost after 440 iterations : Training Loss =  3.3793793819397075; Validation Loss = 3.8190947755312283\n",
            "Cost after 441 iterations : Training Loss =  3.3745566313693693; Validation Loss = 3.812932925304669\n",
            "Cost after 442 iterations : Training Loss =  3.369749461077461; Validation Loss = 3.806789881375658\n",
            "Cost after 443 iterations : Training Loss =  3.3649578044297197; Validation Loss = 3.8006655660179707\n",
            "Cost after 444 iterations : Training Loss =  3.3601815951420666; Validation Loss = 3.7945599019087415\n",
            "Cost after 445 iterations : Training Loss =  3.355420767278409; Validation Loss = 3.788472812125943\n",
            "Cost after 446 iterations : Training Loss =  3.3506752552484516; Validation Loss = 3.7824042201458745\n",
            "Cost after 447 iterations : Training Loss =  3.345944993805535; Validation Loss = 3.7763540498406845\n",
            "Cost after 448 iterations : Training Loss =  3.341229918044472; Validation Loss = 3.77032222547589\n",
            "Cost after 449 iterations : Training Loss =  3.3365299633994043; Validation Loss = 3.7643086717079335\n",
            "Cost after 450 iterations : Training Loss =  3.33184506564171; Validation Loss = 3.758313313581739\n",
            "Cost after 451 iterations : Training Loss =  3.3271751608778577; Validation Loss = 3.752336076528311\n",
            "Cost after 452 iterations : Training Loss =  3.322520185547339; Validation Loss = 3.7463768863623073\n",
            "Cost after 453 iterations : Training Loss =  3.3178800764205896; Validation Loss = 3.7404356692797034\n",
            "Cost after 454 iterations : Training Loss =  3.3132547705969224; Validation Loss = 3.734512351855374\n",
            "Cost after 455 iterations : Training Loss =  3.308644205502478; Validation Loss = 3.728606861040788\n",
            "Cost after 456 iterations : Training Loss =  3.3040483188882157; Validation Loss = 3.722719124161674\n",
            "Cost after 457 iterations : Training Loss =  3.2994670488278577; Validation Loss = 3.716849068915674\n",
            "Cost after 458 iterations : Training Loss =  3.2949003337159297; Validation Loss = 3.7109966233700886\n",
            "Cost after 459 iterations : Training Loss =  3.290348112265743; Validation Loss = 3.7051617159595787\n",
            "Cost after 460 iterations : Training Loss =  3.285810323507439; Validation Loss = 3.6993442754838974\n",
            "Cost after 461 iterations : Training Loss =  3.2812869067860193; Validation Loss = 3.693544231105664\n",
            "Cost after 462 iterations : Training Loss =  3.2767778017594047; Validation Loss = 3.687761512348098\n",
            "Cost after 463 iterations : Training Loss =  3.2722829483965117; Validation Loss = 3.681996049092847\n",
            "Cost after 464 iterations : Training Loss =  3.2678022869753223; Validation Loss = 3.6762477715777506\n",
            "Cost after 465 iterations : Training Loss =  3.2633357580809967; Validation Loss = 3.6705166103946922\n",
            "Cost after 466 iterations : Training Loss =  3.2588833026039703; Validation Loss = 3.6648024964873973\n",
            "Cost after 467 iterations : Training Loss =  3.2544448617380817; Validation Loss = 3.6591053611493103\n",
            "Cost after 468 iterations : Training Loss =  3.25002037697872; Validation Loss = 3.653425136021439\n",
            "Cost after 469 iterations : Training Loss =  3.2456097901209664; Validation Loss = 3.64776175309025\n",
            "Cost after 470 iterations : Training Loss =  3.24121304325776; Validation Loss = 3.642115144685553\n",
            "Cost after 471 iterations : Training Loss =  3.236830078778071; Validation Loss = 3.6364852434784023\n",
            "Cost after 472 iterations : Training Loss =  3.2324608393651095; Validation Loss = 3.6308719824790483\n",
            "Cost after 473 iterations : Training Loss =  3.228105267994506; Validation Loss = 3.625275295034844\n",
            "Cost after 474 iterations : Training Loss =  3.2237633079325456; Validation Loss = 3.6196951148282235\n",
            "Cost after 475 iterations : Training Loss =  3.21943490273438; Validation Loss = 3.6141313758746545\n",
            "Cost after 476 iterations : Training Loss =  3.215119996242288; Validation Loss = 3.6085840125206334\n",
            "Cost after 477 iterations : Training Loss =  3.210818532583919; Validation Loss = 3.603052959441673\n",
            "Cost after 478 iterations : Training Loss =  3.2065304561705545; Validation Loss = 3.597538151640316\n",
            "Cost after 479 iterations : Training Loss =  3.202255711695406; Validation Loss = 3.59203952444417\n",
            "Cost after 480 iterations : Training Loss =  3.1979942441318823; Validation Loss = 3.5865570135039277\n",
            "Cost after 481 iterations : Training Loss =  3.193745998731907; Validation Loss = 3.5810905547914302\n",
            "Cost after 482 iterations : Training Loss =  3.1895109210242385; Validation Loss = 3.575640084597749\n",
            "Cost after 483 iterations : Training Loss =  3.1852889568127787; Validation Loss = 3.5702055395312438\n",
            "Cost after 484 iterations : Training Loss =  3.1810800521749263; Validation Loss = 3.56478685651565\n",
            "Cost after 485 iterations : Training Loss =  3.1768841534599255; Validation Loss = 3.5593839727882233\n",
            "Cost after 486 iterations : Training Loss =  3.17270120728723; Validation Loss = 3.5539968258978343\n",
            "Cost after 487 iterations : Training Loss =  3.1685311605448665; Validation Loss = 3.5486253537030916\n",
            "Cost after 488 iterations : Training Loss =  3.164373960387832; Validation Loss = 3.5432694943705263\n",
            "Cost after 489 iterations : Training Loss =  3.160229554236478; Validation Loss = 3.537929186372704\n",
            "Cost after 490 iterations : Training Loss =  3.1560978897749385; Validation Loss = 3.5326043684864477\n",
            "Cost after 491 iterations : Training Loss =  3.15197891494952; Validation Loss = 3.5272949797909843\n",
            "Cost after 492 iterations : Training Loss =  3.14787257796716; Validation Loss = 3.5220009596661574\n",
            "Cost after 493 iterations : Training Loss =  3.1437788272938487; Validation Loss = 3.5167222477906415\n",
            "Cost after 494 iterations : Training Loss =  3.139697611653089; Validation Loss = 3.5114587841401734\n",
            "Cost after 495 iterations : Training Loss =  3.1356288800243624; Validation Loss = 3.5062105089857614\n",
            "Cost after 496 iterations : Training Loss =  3.1315725816415916; Validation Loss = 3.500977362891951\n",
            "Cost after 497 iterations : Training Loss =  3.127528665991633; Validation Loss = 3.4957592867150997\n",
            "Cost after 498 iterations : Training Loss =  3.1234970828127695; Validation Loss = 3.4905562216016093\n",
            "Cost after 499 iterations : Training Loss =  3.1194777820932176; Validation Loss = 3.485368108986252\n",
            "Cost after 500 iterations : Training Loss =  3.1154707140696356; Validation Loss = 3.480194890590442\n",
            "Cost after 501 iterations : Training Loss =  3.1114758292256592; Validation Loss = 3.4750365084205495\n",
            "Cost after 502 iterations : Training Loss =  3.10749307829043; Validation Loss = 3.4698929047662217\n",
            "Cost after 503 iterations : Training Loss =  3.1035224122371448; Validation Loss = 3.4647640221987146\n",
            "Cost after 504 iterations : Training Loss =  3.0995637822816122; Validation Loss = 3.4596498035692353\n",
            "Cost after 505 iterations : Training Loss =  3.0956171398808117; Validation Loss = 3.4545501920072956\n",
            "Cost after 506 iterations : Training Loss =  3.0916824367314892; Validation Loss = 3.449465130919075\n",
            "Cost after 507 iterations : Training Loss =  3.0877596247687125; Validation Loss = 3.4443945639858033\n",
            "Cost after 508 iterations : Training Loss =  3.0838486561644975; Validation Loss = 3.4393384351621457\n",
            "Cost after 509 iterations : Training Loss =  3.0799494833263936; Validation Loss = 3.434296688674595\n",
            "Cost after 510 iterations : Training Loss =  3.076062058896106; Validation Loss = 3.4292692690199\n",
            "Cost after 511 iterations : Training Loss =  3.0721863357481185; Validation Loss = 3.4242561209634688\n",
            "Cost after 512 iterations : Training Loss =  3.068322266988331; Validation Loss = 3.4192571895378077\n",
            "Cost after 513 iterations : Training Loss =  3.0644698059526907; Validation Loss = 3.4142724200409567\n",
            "Cost after 514 iterations : Training Loss =  3.0606289062058636; Validation Loss = 3.409301758034959\n",
            "Cost after 515 iterations : Training Loss =  3.056799521539881; Validation Loss = 3.404345149344312\n",
            "Cost after 516 iterations : Training Loss =  3.0529816059728114; Validation Loss = 3.3994025400544308\n",
            "Cost after 517 iterations : Training Loss =  3.0491751137474528; Validation Loss = 3.3944738765101636\n",
            "Cost after 518 iterations : Training Loss =  3.045379999330011; Validation Loss = 3.3895591053142646\n",
            "Cost after 519 iterations : Training Loss =  3.041596217408787; Validation Loss = 3.384658173325889\n",
            "Cost after 520 iterations : Training Loss =  3.037823722892918; Validation Loss = 3.3797710276591495\n",
            "Cost after 521 iterations : Training Loss =  3.034062470911052; Validation Loss = 3.374897615681584\n",
            "Cost after 522 iterations : Training Loss =  3.030312416810101; Validation Loss = 3.370037885012743\n",
            "Cost after 523 iterations : Training Loss =  3.0265735161539618; Validation Loss = 3.365191783522702\n",
            "Cost after 524 iterations : Training Loss =  3.022845724722255; Validation Loss = 3.3603592593306333\n",
            "Cost after 525 iterations : Training Loss =  3.019128998509088; Validation Loss = 3.3555402608033744\n",
            "Cost after 526 iterations : Training Loss =  3.015423293721803; Validation Loss = 3.350734736553972\n",
            "Cost after 527 iterations : Training Loss =  3.011728566779741; Validation Loss = 3.345942635440303\n",
            "Cost after 528 iterations : Training Loss =  3.0080447743130265; Validation Loss = 3.3411639065636525\n",
            "Cost after 529 iterations : Training Loss =  3.004371873161344; Validation Loss = 3.3363984992673092\n",
            "Cost after 530 iterations : Training Loss =  3.000709820372733; Validation Loss = 3.3316463631351927\n",
            "Cost after 531 iterations : Training Loss =  2.997058573202389; Validation Loss = 3.3269074479904717\n",
            "Cost after 532 iterations : Training Loss =  2.993418089111468; Validation Loss = 3.322181703894186\n",
            "Cost after 533 iterations : Training Loss =  2.9897883257659035; Validation Loss = 3.3174690811439014\n",
            "Cost after 534 iterations : Training Loss =  2.9861692410352227; Validation Loss = 3.312769530272333\n",
            "Cost after 535 iterations : Training Loss =  2.982560792991405; Validation Loss = 3.308083002046039\n",
            "Cost after 536 iterations : Training Loss =  2.978962939907681; Validation Loss = 3.3034094474640607\n",
            "Cost after 537 iterations : Training Loss =  2.975375640257417; Validation Loss = 3.298748817756622\n",
            "Cost after 538 iterations : Training Loss =  2.9717988527129444; Validation Loss = 3.294101064383785\n",
            "Cost after 539 iterations : Training Loss =  2.9682325361444377; Validation Loss = 3.2894661390341584\n",
            "Cost after 540 iterations : Training Loss =  2.964676649618775; Validation Loss = 3.284843993623619\n",
            "Cost after 541 iterations : Training Loss =  2.9611311523984263; Validation Loss = 3.280234580293993\n",
            "Cost after 542 iterations : Training Loss =  2.957596003940326; Validation Loss = 3.2756378514117928\n",
            "Cost after 543 iterations : Training Loss =  2.9540711638947807; Validation Loss = 3.2710537595669416\n",
            "Cost after 544 iterations : Training Loss =  2.9505565921043484; Validation Loss = 3.266482257571501\n",
            "Cost after 545 iterations : Training Loss =  2.947052248602769; Validation Loss = 3.2619232984584325\n",
            "Cost after 546 iterations : Training Loss =  2.9435580936138606; Validation Loss = 3.2573768354803407\n",
            "Cost after 547 iterations : Training Loss =  2.9400740875504527; Validation Loss = 3.252842822108238\n",
            "Cost after 548 iterations : Training Loss =  2.93660019101331; Validation Loss = 3.2483212120303095\n",
            "Cost after 549 iterations : Training Loss =  2.9331363647900695; Validation Loss = 3.243811959150698\n",
            "Cost after 550 iterations : Training Loss =  2.9296825698541835; Validation Loss = 3.2393150175882868\n",
            "Cost after 551 iterations : Training Loss =  2.926238767363869; Validation Loss = 3.234830341675494\n",
            "Cost after 552 iterations : Training Loss =  2.9228049186610714; Validation Loss = 3.230357885957076\n",
            "Cost after 553 iterations : Training Loss =  2.919380985270408; Validation Loss = 3.2258976051889263\n",
            "Cost after 554 iterations : Training Loss =  2.915966928898172; Validation Loss = 3.2214494543369194\n",
            "Cost after 555 iterations : Training Loss =  2.912562711431272; Validation Loss = 3.217013388575694\n",
            "Cost after 556 iterations : Training Loss =  2.9091682949362467; Validation Loss = 3.2125893632875346\n",
            "Cost after 557 iterations : Training Loss =  2.905783641658245; Validation Loss = 3.2081773340611783\n",
            "Cost after 558 iterations : Training Loss =  2.902408714020016; Validation Loss = 3.203777256690674\n",
            "Cost after 559 iterations : Training Loss =  2.8990434746209184; Validation Loss = 3.199389087174235\n",
            "Cost after 560 iterations : Training Loss =  2.895687886235935; Validation Loss = 3.1950127817131073\n",
            "Cost after 561 iterations : Training Loss =  2.8923419118146922; Validation Loss = 3.190648296710443\n",
            "Cost after 562 iterations : Training Loss =  2.889005514480471; Validation Loss = 3.186295588770168\n",
            "Cost after 563 iterations : Training Loss =  2.885678657529249; Validation Loss = 3.181954614695877\n",
            "Cost after 564 iterations : Training Loss =  2.8823613044287213; Validation Loss = 3.177625331489712\n",
            "Cost after 565 iterations : Training Loss =  2.8790534188173678; Validation Loss = 3.1733076963512854\n",
            "Cost after 566 iterations : Training Loss =  2.8757549645034675; Validation Loss = 3.1690016666765537\n",
            "Cost after 567 iterations : Training Loss =  2.8724659054641943; Validation Loss = 3.164707200056783\n",
            "Cost after 568 iterations : Training Loss =  2.86918620584464; Validation Loss = 3.1604242542774057\n",
            "Cost after 569 iterations : Training Loss =  2.8659158299569043; Validation Loss = 3.1561527873169957\n",
            "Cost after 570 iterations : Training Loss =  2.8626547422791675; Validation Loss = 3.1518927573462\n",
            "Cost after 571 iterations : Training Loss =  2.859402907454746; Validation Loss = 3.1476441227266467\n",
            "Cost after 572 iterations : Training Loss =  2.85616029029121; Validation Loss = 3.143406842009932\n",
            "Cost after 573 iterations : Training Loss =  2.852926855759456; Validation Loss = 3.1391808739365583\n",
            "Cost after 574 iterations : Training Loss =  2.8497025689928064; Validation Loss = 3.1349661774348925\n",
            "Cost after 575 iterations : Training Loss =  2.84648739528611; Validation Loss = 3.13076271162015\n",
            "Cost after 576 iterations : Training Loss =  2.8432813000948522; Validation Loss = 3.1265704357933526\n",
            "Cost after 577 iterations : Training Loss =  2.840084249034278; Validation Loss = 3.122389309440324\n",
            "Cost after 578 iterations : Training Loss =  2.836896207878498; Validation Loss = 3.11821929223067\n",
            "Cost after 579 iterations : Training Loss =  2.8337171425596193; Validation Loss = 3.114060344016781\n",
            "Cost after 580 iterations : Training Loss =  2.83054701916688; Validation Loss = 3.109912424832824\n",
            "Cost after 581 iterations : Training Loss =  2.8273858039457895; Validation Loss = 3.105775494893772\n",
            "Cost after 582 iterations : Training Loss =  2.8242334632972494; Validation Loss = 3.101649514594377\n",
            "Cost after 583 iterations : Training Loss =  2.82108996377673; Validation Loss = 3.097534444508238\n",
            "Cost after 584 iterations : Training Loss =  2.817955272093401; Validation Loss = 3.0934302453867937\n",
            "Cost after 585 iterations : Training Loss =  2.8148293551093126; Validation Loss = 3.089336878158377\n",
            "Cost after 586 iterations : Training Loss =  2.8117121798385343; Validation Loss = 3.0852543039272398\n",
            "Cost after 587 iterations : Training Loss =  2.8086037134463355; Validation Loss = 3.0811824839726034\n",
            "Cost after 588 iterations : Training Loss =  2.8055039232483723; Validation Loss = 3.0771213797477106\n",
            "Cost after 589 iterations : Training Loss =  2.8024127767098412; Validation Loss = 3.0730709528788807\n",
            "Cost after 590 iterations : Training Loss =  2.7993302414446934; Validation Loss = 3.0690311651645867\n",
            "Cost after 591 iterations : Training Loss =  2.7962562852148034; Validation Loss = 3.0650019785744944\n",
            "Cost after 592 iterations : Training Loss =  2.7931908759291715; Validation Loss = 3.060983355248571\n",
            "Cost after 593 iterations : Training Loss =  2.7901339816431268; Validation Loss = 3.056975257496145\n",
            "Cost after 594 iterations : Training Loss =  2.7870855705575286; Validation Loss = 3.052977647795\n",
            "Cost after 595 iterations : Training Loss =  2.7840456110179814; Validation Loss = 3.048990488790464\n",
            "Cost after 596 iterations : Training Loss =  2.781014071514054; Validation Loss = 3.0450137432945215\n",
            "Cost after 597 iterations : Training Loss =  2.7779909206784827; Validation Loss = 3.041047374284894\n",
            "Cost after 598 iterations : Training Loss =  2.774976127286418; Validation Loss = 3.037091344904166\n",
            "Cost after 599 iterations : Training Loss =  2.7719696602546424; Validation Loss = 3.0331456184589043\n",
            "Cost after 600 iterations : Training Loss =  2.768971488640803; Validation Loss = 3.0292101584187563\n",
            "Cost after 601 iterations : Training Loss =  2.7659815816426763; Validation Loss = 3.0252849284156094\n",
            "Cost after 602 iterations : Training Loss =  2.762999908597373; Validation Loss = 3.02136989224269\n",
            "Cost after 603 iterations : Training Loss =  2.7600264389806184; Validation Loss = 3.0174650138537196\n",
            "Cost after 604 iterations : Training Loss =  2.7570611424060036; Validation Loss = 3.0135702573620673\n",
            "Cost after 605 iterations : Training Loss =  2.7541039886242307; Validation Loss = 3.0096855870398644\n",
            "Cost after 606 iterations : Training Loss =  2.7511549475223824; Validation Loss = 3.005810967317188\n",
            "Cost after 607 iterations : Training Loss =  2.7482139891232005; Validation Loss = 3.001946362781208\n",
            "Cost after 608 iterations : Training Loss =  2.745281083584343; Validation Loss = 2.9980917381753525\n",
            "Cost after 609 iterations : Training Loss =  2.7423562011976723; Validation Loss = 2.9942470583984795\n",
            "Cost after 610 iterations : Training Loss =  2.7394393123885346; Validation Loss = 2.9904122885040416\n",
            "Cost after 611 iterations : Training Loss =  2.7365303877150438; Validation Loss = 2.9865873936992853\n",
            "Cost after 612 iterations : Training Loss =  2.7336293978673734; Validation Loss = 2.9827723393444097\n",
            "Cost after 613 iterations : Training Loss =  2.7307363136670553; Validation Loss = 2.9789670909517816\n",
            "Cost after 614 iterations : Training Loss =  2.72785110606627; Validation Loss = 2.9751716141850997\n",
            "Cost after 615 iterations : Training Loss =  2.72497374614717; Validation Loss = 2.971385874858642\n",
            "Cost after 616 iterations : Training Loss =  2.7221042051211612; Validation Loss = 2.967609838936411\n",
            "Cost after 617 iterations : Training Loss =  2.71924245432824; Validation Loss = 2.9638434725313947\n",
            "Cost after 618 iterations : Training Loss =  2.716388465236297; Validation Loss = 2.960086741904752\n",
            "Cost after 619 iterations : Training Loss =  2.7135422094404333; Validation Loss = 2.956339613465031\n",
            "Cost after 620 iterations : Training Loss =  2.7107036586623123; Validation Loss = 2.952602053767423\n",
            "Cost after 621 iterations : Training Loss =  2.707872784749449; Validation Loss = 2.9488740295129525\n",
            "Cost after 622 iterations : Training Loss =  2.7050495596745807; Validation Loss = 2.945155507547731\n",
            "Cost after 623 iterations : Training Loss =  2.702233955534983; Validation Loss = 2.9414464548622057\n",
            "Cost after 624 iterations : Training Loss =  2.699425944551818; Validation Loss = 2.937746838590373\n",
            "Cost after 625 iterations : Training Loss =  2.696625499069485; Validation Loss = 2.9340566260090566\n",
            "Cost after 626 iterations : Training Loss =  2.6938325915549575; Validation Loss = 2.9303757845371394\n",
            "Cost after 627 iterations : Training Loss =  2.6910471945971564; Validation Loss = 2.9267042817348345\n",
            "Cost after 628 iterations : Training Loss =  2.6882692809062863; Validation Loss = 2.92304208530293\n",
            "Cost after 629 iterations : Training Loss =  2.685498823313211; Validation Loss = 2.919389163082073\n",
            "Cost after 630 iterations : Training Loss =  2.6827357947688224; Validation Loss = 2.915745483052025\n",
            "Cost after 631 iterations : Training Loss =  2.6799801683433953; Validation Loss = 2.9121110133309527\n",
            "Cost after 632 iterations : Training Loss =  2.677231917225979; Validation Loss = 2.908485722174694\n",
            "Cost after 633 iterations : Training Loss =  2.6744910147237553; Validation Loss = 2.904869577976037\n",
            "Cost after 634 iterations : Training Loss =  2.6717574342614347; Validation Loss = 2.901262549264037\n",
            "Cost after 635 iterations : Training Loss =  2.6690311493806376; Validation Loss = 2.89766460470327\n",
            "Cost after 636 iterations : Training Loss =  2.666312133739284; Validation Loss = 2.894075713093161\n",
            "Cost after 637 iterations : Training Loss =  2.6636003611109733; Validation Loss = 2.8904958433672676\n",
            "Cost after 638 iterations : Training Loss =  2.6608958053844014; Validation Loss = 2.886924964592598\n",
            "Cost after 639 iterations : Training Loss =  2.658198440562749; Validation Loss = 2.8833630459689004\n",
            "Cost after 640 iterations : Training Loss =  2.6555082407630772; Validation Loss = 2.879810056827996\n",
            "Cost after 641 iterations : Training Loss =  2.6528251802157508; Validation Loss = 2.876265966633097\n",
            "Cost after 642 iterations : Training Loss =  2.6501492332638454; Validation Loss = 2.872730744978109\n",
            "Cost after 643 iterations : Training Loss =  2.6474803743625483; Validation Loss = 2.8692043615869807\n",
            "Cost after 644 iterations : Training Loss =  2.6448185780785924; Validation Loss = 2.865686786313018\n",
            "Cost after 645 iterations : Training Loss =  2.642163819089664; Validation Loss = 2.8621779891382233\n",
            "Cost after 646 iterations : Training Loss =  2.639516072183852; Validation Loss = 2.8586779401726408\n",
            "Cost after 647 iterations : Training Loss =  2.6368753122590336; Validation Loss = 2.855186609653678\n",
            "Cost after 648 iterations : Training Loss =  2.634241514322347; Validation Loss = 2.8517039679454803\n",
            "Cost after 649 iterations : Training Loss =  2.631614653489616; Validation Loss = 2.848229985538259\n",
            "Cost after 650 iterations : Training Loss =  2.628994704984767; Validation Loss = 2.8447646330476566\n",
            "Cost after 651 iterations : Training Loss =  2.626381644139298; Validation Loss = 2.8413078812140964\n",
            "Cost after 652 iterations : Training Loss =  2.6237754463917176; Validation Loss = 2.8378597009021536\n",
            "Cost after 653 iterations : Training Loss =  2.621176087286979; Validation Loss = 2.8344200630999103\n",
            "Cost after 654 iterations : Training Loss =  2.6185835424759434; Validation Loss = 2.8309889389183325\n",
            "Cost after 655 iterations : Training Loss =  2.6159977877148477; Validation Loss = 2.8275662995906528\n",
            "Cost after 656 iterations : Training Loss =  2.613418798864733; Validation Loss = 2.824152116471715\n",
            "Cost after 657 iterations : Training Loss =  2.610846551890934; Validation Loss = 2.820746361037393\n",
            "Cost after 658 iterations : Training Loss =  2.608281022862523; Validation Loss = 2.8173490048839454\n",
            "Cost after 659 iterations : Training Loss =  2.6057221879518058; Validation Loss = 2.813960019727431\n",
            "Cost after 660 iterations : Training Loss =  2.603170023433756; Validation Loss = 2.810579377403063\n",
            "Cost after 661 iterations : Training Loss =  2.6006245056855177; Validation Loss = 2.807207049864646\n",
            "Cost after 662 iterations : Training Loss =  2.598085611185881; Validation Loss = 2.8038430091839497\n",
            "Cost after 663 iterations : Training Loss =  2.595553316514745; Validation Loss = 2.800487227550114\n",
            "Cost after 664 iterations : Training Loss =  2.5930275983526143; Validation Loss = 2.7971396772690507\n",
            "Cost after 665 iterations : Training Loss =  2.590508433480094; Validation Loss = 2.7938003307628767\n",
            "Cost after 666 iterations : Training Loss =  2.5879957987773605; Validation Loss = 2.790469160569299\n",
            "Cost after 667 iterations : Training Loss =  2.585489671223674; Validation Loss = 2.787146139341047\n",
            "Cost after 668 iterations : Training Loss =  2.582990027896853; Validation Loss = 2.783831239845284\n",
            "Cost after 669 iterations : Training Loss =  2.5804968459727933; Validation Loss = 2.7805244349630307\n",
            "Cost after 670 iterations : Training Loss =  2.5780101027249622; Validation Loss = 2.7772256976886105\n",
            "Cost after 671 iterations : Training Loss =  2.575529775523902; Validation Loss = 2.7739350011290504\n",
            "Cost after 672 iterations : Training Loss =  2.573055841836732; Validation Loss = 2.770652318503533\n",
            "Cost after 673 iterations : Training Loss =  2.570588279226677; Validation Loss = 2.767377623142823\n",
            "Cost after 674 iterations : Training Loss =  2.5681270653525696; Validation Loss = 2.7641108884887338\n",
            "Cost after 675 iterations : Training Loss =  2.5656721779683567; Validation Loss = 2.760852088093526\n",
            "Cost after 676 iterations : Training Loss =  2.563223594922642; Validation Loss = 2.7576011956193907\n",
            "Cost after 677 iterations : Training Loss =  2.5607812941581876; Validation Loss = 2.754358184837887\n",
            "Cost after 678 iterations : Training Loss =  2.5583452537114515; Validation Loss = 2.75112302962939\n",
            "Cost after 679 iterations : Training Loss =  2.555915451712109; Validation Loss = 2.7478957039825587\n",
            "Cost after 680 iterations : Training Loss =  2.5534918663825876; Validation Loss = 2.7446761819937864\n",
            "Cost after 681 iterations : Training Loss =  2.5510744760375985; Validation Loss = 2.7414644378666693\n",
            "Cost after 682 iterations : Training Loss =  2.5486632590836673; Validation Loss = 2.7382604459114606\n",
            "Cost after 683 iterations : Training Loss =  2.546258194018683; Validation Loss = 2.735064180544559\n",
            "Cost after 684 iterations : Training Loss =  2.543859259431438; Validation Loss = 2.731875616287969\n",
            "Cost after 685 iterations : Training Loss =  2.5414664340011552; Validation Loss = 2.7286947277687648\n",
            "Cost after 686 iterations : Training Loss =  2.5390796964970614; Validation Loss = 2.7255214897185924\n",
            "Cost after 687 iterations : Training Loss =  2.536699025777924; Validation Loss = 2.7223558769731437\n",
            "Cost after 688 iterations : Training Loss =  2.53432440079159; Validation Loss = 2.719197864471621\n",
            "Cost after 689 iterations : Training Loss =  2.5319558005745693; Validation Loss = 2.71604742725625\n",
            "Cost after 690 iterations : Training Loss =  2.529593204251561; Validation Loss = 2.7129045404717522\n",
            "Cost after 691 iterations : Training Loss =  2.5272365910350416; Validation Loss = 2.7097691793648533\n",
            "Cost after 692 iterations : Training Loss =  2.524885940224807; Validation Loss = 2.7066413192837673\n",
            "Cost after 693 iterations : Training Loss =  2.5225412312075517; Validation Loss = 2.7035209356777017\n",
            "Cost after 694 iterations : Training Loss =  2.520202443456419; Validation Loss = 2.70040800409635\n",
            "Cost after 695 iterations : Training Loss =  2.517869556530593; Validation Loss = 2.6973025001894118\n",
            "Cost after 696 iterations : Training Loss =  2.5155425500748523; Validation Loss = 2.694204399706093\n",
            "Cost after 697 iterations : Training Loss =  2.5132214038191534; Validation Loss = 2.6911136784946077\n",
            "Cost after 698 iterations : Training Loss =  2.5109060975782103; Validation Loss = 2.6880303125017035\n",
            "Cost after 699 iterations : Training Loss =  2.508596611251069; Validation Loss = 2.6849542777721753\n",
            "Cost after 700 iterations : Training Loss =  2.5062929248206958; Validation Loss = 2.68188555044838\n",
            "Cost after 701 iterations : Training Loss =  2.5039950183535495; Validation Loss = 2.678824106769761\n",
            "Cost after 702 iterations : Training Loss =  2.501702871999185; Validation Loss = 2.675769923072374\n",
            "Cost after 703 iterations : Training Loss =  2.4994164659898397; Validation Loss = 2.6727229757884134\n",
            "Cost after 704 iterations : Training Loss =  2.497135780640007; Validation Loss = 2.6696832414457385\n",
            "Cost after 705 iterations : Training Loss =  2.4948607963460563; Validation Loss = 2.666650696667407\n",
            "Cost after 706 iterations : Training Loss =  2.492591493585811; Validation Loss = 2.663625318171213\n",
            "Cost after 707 iterations : Training Loss =  2.490327852918153; Validation Loss = 2.6606070827692223\n",
            "Cost after 708 iterations : Training Loss =  2.4880698549826348; Validation Loss = 2.657595967367324\n",
            "Cost after 709 iterations : Training Loss =  2.485817480499066; Validation Loss = 2.6545919489647565\n",
            "Cost after 710 iterations : Training Loss =  2.483570710267123; Validation Loss = 2.651595004653655\n",
            "Cost after 711 iterations : Training Loss =  2.481329525165962; Validation Loss = 2.6486051116186133\n",
            "Cost after 712 iterations : Training Loss =  2.4790939061538357; Validation Loss = 2.6456222471362327\n",
            "Cost after 713 iterations : Training Loss =  2.4768638342676823; Validation Loss = 2.642646388574651\n",
            "Cost after 714 iterations : Training Loss =  2.474639290622769; Validation Loss = 2.6396775133931385\n",
            "Cost after 715 iterations : Training Loss =  2.4724202564122852; Validation Loss = 2.636715599141618\n",
            "Cost after 716 iterations : Training Loss =  2.4702067129069794; Validation Loss = 2.6337606234602537\n",
            "Cost after 717 iterations : Training Loss =  2.4679986414547646; Validation Loss = 2.630812564079006\n",
            "Cost after 718 iterations : Training Loss =  2.4657960234803538; Validation Loss = 2.627871398817186\n",
            "Cost after 719 iterations : Training Loss =  2.463598840484882; Validation Loss = 2.6249371055830415\n",
            "Cost after 720 iterations : Training Loss =  2.4614070740455274; Validation Loss = 2.6220096623733116\n",
            "Cost after 721 iterations : Training Loss =  2.459220705815158; Validation Loss = 2.6190890472728228\n",
            "Cost after 722 iterations : Training Loss =  2.457039717521941; Validation Loss = 2.6161752384540398\n",
            "Cost after 723 iterations : Training Loss =  2.4548640909689836; Validation Loss = 2.6132682141766463\n",
            "Cost after 724 iterations : Training Loss =  2.452693808033995; Validation Loss = 2.610367952787156\n",
            "Cost after 725 iterations : Training Loss =  2.4505288506688774; Validation Loss = 2.6074744327184494\n",
            "Cost after 726 iterations : Training Loss =  2.448369200899413; Validation Loss = 2.604587632489404\n",
            "Cost after 727 iterations : Training Loss =  2.446214840824864; Validation Loss = 2.6017075307044375\n",
            "Cost after 728 iterations : Training Loss =  2.4440657526176564; Validation Loss = 2.5988341060531464\n",
            "Cost after 729 iterations : Training Loss =  2.4419219185229943; Validation Loss = 2.59596733730985\n",
            "Cost after 730 iterations : Training Loss =  2.4397833208585262; Validation Loss = 2.5931072033332176\n",
            "Cost after 731 iterations : Training Loss =  2.437649942013987; Validation Loss = 2.590253683065851\n",
            "Cost after 732 iterations : Training Loss =  2.4355217644508587; Validation Loss = 2.587406755533895\n",
            "Cost after 733 iterations : Training Loss =  2.433398770702009; Validation Loss = 2.5845663998466204\n",
            "Cost after 734 iterations : Training Loss =  2.431280943371364; Validation Loss = 2.5817325951960415\n",
            "Cost after 735 iterations : Training Loss =  2.4291682651335598; Validation Loss = 2.578905320856525\n",
            "Cost after 736 iterations : Training Loss =  2.4270607187335944; Validation Loss = 2.576084556184381\n",
            "Cost after 737 iterations : Training Loss =  2.4249582869865063; Validation Loss = 2.573270280617493\n",
            "Cost after 738 iterations : Training Loss =  2.422860952777021; Validation Loss = 2.57046247367491\n",
            "Cost after 739 iterations : Training Loss =  2.4207686990592268; Validation Loss = 2.5676611149564783\n",
            "Cost after 740 iterations : Training Loss =  2.418681508856236; Validation Loss = 2.564866184142452\n",
            "Cost after 741 iterations : Training Loss =  2.4165993652598696; Validation Loss = 2.5620776609931126\n",
            "Cost after 742 iterations : Training Loss =  2.4145222514303; Validation Loss = 2.5592955253483685\n",
            "Cost after 743 iterations : Training Loss =  2.4124501505957476; Validation Loss = 2.556519757127427\n",
            "Cost after 744 iterations : Training Loss =  2.410383046052151; Validation Loss = 2.553750336328363\n",
            "Cost after 745 iterations : Training Loss =  2.408320921162838; Validation Loss = 2.550987243027797\n",
            "Cost after 746 iterations : Training Loss =  2.406263759358208; Validation Loss = 2.5482304573804804\n",
            "Cost after 747 iterations : Training Loss =  2.404211544135405; Validation Loss = 2.545479959618953\n",
            "Cost after 748 iterations : Training Loss =  2.4021642590580154; Validation Loss = 2.542735730053173\n",
            "Cost after 749 iterations : Training Loss =  2.400121887755735; Validation Loss = 2.5399977490701424\n",
            "Cost after 750 iterations : Training Loss =  2.3980844139240594; Validation Loss = 2.537265997133552\n",
            "Cost after 751 iterations : Training Loss =  2.3960518213239808; Validation Loss = 2.534540454783427\n",
            "Cost after 752 iterations : Training Loss =  2.3940240937816615; Validation Loss = 2.531821102635752\n",
            "Cost after 753 iterations : Training Loss =  2.3920012151881362; Validation Loss = 2.529107921382121\n",
            "Cost after 754 iterations : Training Loss =  2.3899831694990024; Validation Loss = 2.5264008917893968\n",
            "Cost after 755 iterations : Training Loss =  2.3879699407341013; Validation Loss = 2.52369999469933\n",
            "Cost after 756 iterations : Training Loss =  2.385961512977239; Validation Loss = 2.521005211028234\n",
            "Cost after 757 iterations : Training Loss =  2.383957870375858; Validation Loss = 2.5183165217666224\n",
            "Cost after 758 iterations : Training Loss =  2.381958997140753; Validation Loss = 2.5156339079788674\n",
            "Cost after 759 iterations : Training Loss =  2.379964877545765; Validation Loss = 2.512957350802846\n",
            "Cost after 760 iterations : Training Loss =  2.377975495927488; Validation Loss = 2.5102868314496143\n",
            "Cost after 761 iterations : Training Loss =  2.3759908366849603; Validation Loss = 2.5076223312030423\n",
            "Cost after 762 iterations : Training Loss =  2.3740108842793943; Validation Loss = 2.5049638314194995\n",
            "Cost after 763 iterations : Training Loss =  2.3720356232338577; Validation Loss = 2.502311313527498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in power\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in power\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Cost after 295002 iterations : Training Loss =  0.028112945764689635; Validation Loss = 0.034731800398502946\n",
            "Cost after 295003 iterations : Training Loss =  0.028112935579224105; Validation Loss = 0.03473179169926208\n",
            "Cost after 295004 iterations : Training Loss =  0.028112925393876283; Validation Loss = 0.03473178300010996\n",
            "Cost after 295005 iterations : Training Loss =  0.028112915208645967; Validation Loss = 0.034731774301046285\n",
            "Cost after 295006 iterations : Training Loss =  0.028112905023533346; Validation Loss = 0.034731765602070516\n",
            "Cost after 295007 iterations : Training Loss =  0.0281128948385382; Validation Loss = 0.03473175690318339\n",
            "Cost after 295008 iterations : Training Loss =  0.028112884653660748; Validation Loss = 0.034731748204384766\n",
            "Cost after 295009 iterations : Training Loss =  0.028112874468900786; Validation Loss = 0.03473173950567439\n",
            "Cost after 295010 iterations : Training Loss =  0.028112864284258507; Validation Loss = 0.03473173080705272\n",
            "Cost after 295011 iterations : Training Loss =  0.02811285409973372; Validation Loss = 0.03473172210851924\n",
            "Cost after 295012 iterations : Training Loss =  0.02811284391532658; Validation Loss = 0.034731713410074264\n",
            "Cost after 295013 iterations : Training Loss =  0.02811283373103695; Validation Loss = 0.034731704711717985\n",
            "Cost after 295014 iterations : Training Loss =  0.028112823546864906; Validation Loss = 0.03473169601344976\n",
            "Cost after 295015 iterations : Training Loss =  0.028112813362810504; Validation Loss = 0.03473168731526982\n",
            "Cost after 295016 iterations : Training Loss =  0.02811280317887363; Validation Loss = 0.03473167861717848\n",
            "Cost after 295017 iterations : Training Loss =  0.02811279299505433; Validation Loss = 0.03473166991917532\n",
            "Cost after 295018 iterations : Training Loss =  0.028112782811352672; Validation Loss = 0.034731661221260576\n",
            "Cost after 295019 iterations : Training Loss =  0.02811277262776849; Validation Loss = 0.03473165252343453\n",
            "Cost after 295020 iterations : Training Loss =  0.0281127624443018; Validation Loss = 0.03473164382569656\n",
            "Cost after 295021 iterations : Training Loss =  0.02811275226095276; Validation Loss = 0.034731635128046834\n",
            "Cost after 295022 iterations : Training Loss =  0.028112742077721353; Validation Loss = 0.0347316264304857\n",
            "Cost after 295023 iterations : Training Loss =  0.028112731894607435; Validation Loss = 0.03473161773301299\n",
            "Cost after 295024 iterations : Training Loss =  0.028112721711611232; Validation Loss = 0.034731609035628684\n",
            "Cost after 295025 iterations : Training Loss =  0.028112711528732334; Validation Loss = 0.034731600338332703\n",
            "Cost after 295026 iterations : Training Loss =  0.028112701345971228; Validation Loss = 0.034731591641125055\n",
            "Cost after 295027 iterations : Training Loss =  0.028112691163327458; Validation Loss = 0.03473158294400598\n",
            "Cost after 295028 iterations : Training Loss =  0.028112680980801438; Validation Loss = 0.034731574246975025\n",
            "Cost after 295029 iterations : Training Loss =  0.0281126707983929; Validation Loss = 0.03473156555003272\n",
            "Cost after 295030 iterations : Training Loss =  0.02811266061610183; Validation Loss = 0.03473155685317863\n",
            "Cost after 295031 iterations : Training Loss =  0.028112650433928466; Validation Loss = 0.03473154815641284\n",
            "Cost after 295032 iterations : Training Loss =  0.028112640251872465; Validation Loss = 0.034731539459735714\n",
            "Cost after 295033 iterations : Training Loss =  0.028112630069934127; Validation Loss = 0.03473153076314689\n",
            "Cost after 295034 iterations : Training Loss =  0.028112619888113338; Validation Loss = 0.03473152206664625\n",
            "Cost after 295035 iterations : Training Loss =  0.028112609706410115; Validation Loss = 0.03473151337023399\n",
            "Cost after 295036 iterations : Training Loss =  0.02811259952482431; Validation Loss = 0.0347315046739101\n",
            "Cost after 295037 iterations : Training Loss =  0.028112589343356094; Validation Loss = 0.034731495977674734\n",
            "Cost after 295038 iterations : Training Loss =  0.028112579162005352; Validation Loss = 0.0347314872815276\n",
            "Cost after 295039 iterations : Training Loss =  0.02811256898077228; Validation Loss = 0.03473147858546857\n",
            "Cost after 295040 iterations : Training Loss =  0.02811255879965672; Validation Loss = 0.034731469889498226\n",
            "Cost after 295041 iterations : Training Loss =  0.028112548618658605; Validation Loss = 0.03473146119361618\n",
            "Cost after 295042 iterations : Training Loss =  0.028112538437778088; Validation Loss = 0.03473145249782282\n",
            "Cost after 295043 iterations : Training Loss =  0.028112528257015143; Validation Loss = 0.03473144380211731\n",
            "Cost after 295044 iterations : Training Loss =  0.028112518076369716; Validation Loss = 0.03473143510650076\n",
            "Cost after 295045 iterations : Training Loss =  0.028112507895841663; Validation Loss = 0.03473142641097197\n",
            "Cost after 295046 iterations : Training Loss =  0.028112497715431308; Validation Loss = 0.03473141771553178\n",
            "Cost after 295047 iterations : Training Loss =  0.0281124875351383; Validation Loss = 0.034731409020180015\n",
            "Cost after 295048 iterations : Training Loss =  0.028112477354962993; Validation Loss = 0.03473140032491644\n",
            "Cost after 295049 iterations : Training Loss =  0.028112467174905058; Validation Loss = 0.034731391629741266\n",
            "Cost after 295050 iterations : Training Loss =  0.028112456994964844; Validation Loss = 0.034731382934654595\n",
            "Cost after 295051 iterations : Training Loss =  0.028112446815141943; Validation Loss = 0.034731374239656235\n",
            "Cost after 295052 iterations : Training Loss =  0.028112436635436562; Validation Loss = 0.034731365544746075\n",
            "Cost after 295053 iterations : Training Loss =  0.028112426455848737; Validation Loss = 0.03473135684992427\n",
            "Cost after 295054 iterations : Training Loss =  0.02811241627637847; Validation Loss = 0.03473134815519095\n",
            "Cost after 295055 iterations : Training Loss =  0.0281124060970257; Validation Loss = 0.03473133946054595\n",
            "Cost after 295056 iterations : Training Loss =  0.028112395917790415; Validation Loss = 0.03473133076598921\n",
            "Cost after 295057 iterations : Training Loss =  0.0281123857386726; Validation Loss = 0.0347313220715206\n",
            "Cost after 295058 iterations : Training Loss =  0.028112375559672404; Validation Loss = 0.034731313377140825\n",
            "Cost after 295059 iterations : Training Loss =  0.028112365380789584; Validation Loss = 0.034731304682849115\n",
            "Cost after 295060 iterations : Training Loss =  0.02811235520202422; Validation Loss = 0.0347312959886456\n",
            "Cost after 295061 iterations : Training Loss =  0.02811234502337653; Validation Loss = 0.0347312872945307\n",
            "Cost after 295062 iterations : Training Loss =  0.028112334844846203; Validation Loss = 0.0347312786005038\n",
            "Cost after 295063 iterations : Training Loss =  0.02811232466643336; Validation Loss = 0.03473126990656533\n",
            "Cost after 295064 iterations : Training Loss =  0.028112314488138074; Validation Loss = 0.0347312612127154\n",
            "Cost after 295065 iterations : Training Loss =  0.02811230430996033; Validation Loss = 0.034731252518953676\n",
            "Cost after 295066 iterations : Training Loss =  0.028112294131899977; Validation Loss = 0.03473124382528042\n",
            "Cost after 295067 iterations : Training Loss =  0.028112283953957217; Validation Loss = 0.034731235131695286\n",
            "Cost after 295068 iterations : Training Loss =  0.028112273776131874; Validation Loss = 0.034731226438198294\n",
            "Cost after 295069 iterations : Training Loss =  0.028112263598424017; Validation Loss = 0.03473121774478998\n",
            "Cost after 295070 iterations : Training Loss =  0.028112253420833722; Validation Loss = 0.034731209051470126\n",
            "Cost after 295071 iterations : Training Loss =  0.0281122432433608; Validation Loss = 0.03473120035823821\n",
            "Cost after 295072 iterations : Training Loss =  0.028112233066005424; Validation Loss = 0.034731191665095106\n",
            "Cost after 295073 iterations : Training Loss =  0.028112222888767496; Validation Loss = 0.034731182972039955\n",
            "Cost after 295074 iterations : Training Loss =  0.028112212711646913; Validation Loss = 0.034731174279073074\n",
            "Cost after 295075 iterations : Training Loss =  0.028112202534644; Validation Loss = 0.03473116558619431\n",
            "Cost after 295076 iterations : Training Loss =  0.02811219235775848; Validation Loss = 0.03473115689340406\n",
            "Cost after 295077 iterations : Training Loss =  0.028112182180990486; Validation Loss = 0.034731148200702305\n",
            "Cost after 295078 iterations : Training Loss =  0.02811217200433987; Validation Loss = 0.03473113950808895\n",
            "Cost after 295079 iterations : Training Loss =  0.028112161827806827; Validation Loss = 0.03473113081556396\n",
            "Cost after 295080 iterations : Training Loss =  0.028112151651391193; Validation Loss = 0.03473112212312704\n",
            "Cost after 295081 iterations : Training Loss =  0.028112141475092948; Validation Loss = 0.03473111343077854\n",
            "Cost after 295082 iterations : Training Loss =  0.02811213129891223; Validation Loss = 0.03473110473851838\n",
            "Cost after 295083 iterations : Training Loss =  0.028112121122848947; Validation Loss = 0.03473109604634664\n",
            "Cost after 295084 iterations : Training Loss =  0.028112110946903277; Validation Loss = 0.034731087354262814\n",
            "Cost after 295085 iterations : Training Loss =  0.028112100771074917; Validation Loss = 0.034731078662267593\n",
            "Cost after 295086 iterations : Training Loss =  0.028112090595364105; Validation Loss = 0.034731069970360864\n",
            "Cost after 295087 iterations : Training Loss =  0.02811208041977061; Validation Loss = 0.03473106127854202\n",
            "Cost after 295088 iterations : Training Loss =  0.028112070244294705; Validation Loss = 0.034731052586811514\n",
            "Cost after 295089 iterations : Training Loss =  0.0281120600689361; Validation Loss = 0.03473104389516935\n",
            "Cost after 295090 iterations : Training Loss =  0.02811204989369504; Validation Loss = 0.034731035203615784\n",
            "Cost after 295091 iterations : Training Loss =  0.02811203971857152; Validation Loss = 0.034731026512150466\n",
            "Cost after 295092 iterations : Training Loss =  0.028112029543565287; Validation Loss = 0.034731017820773466\n",
            "Cost after 295093 iterations : Training Loss =  0.028112019368676563; Validation Loss = 0.034731009129484576\n",
            "Cost after 295094 iterations : Training Loss =  0.028112009193905302; Validation Loss = 0.03473100043828411\n",
            "Cost after 295095 iterations : Training Loss =  0.02811199901925135; Validation Loss = 0.034730991747171765\n",
            "Cost after 295096 iterations : Training Loss =  0.0281119888447151; Validation Loss = 0.03473098305614775\n",
            "Cost after 295097 iterations : Training Loss =  0.028111978670296096; Validation Loss = 0.03473097436521197\n",
            "Cost after 295098 iterations : Training Loss =  0.028111968495994538; Validation Loss = 0.03473096567436461\n",
            "Cost after 295099 iterations : Training Loss =  0.02811195832181049; Validation Loss = 0.03473095698360574\n",
            "Cost after 295100 iterations : Training Loss =  0.028111948147743857; Validation Loss = 0.03473094829293484\n",
            "Cost after 295101 iterations : Training Loss =  0.028111937973794535; Validation Loss = 0.034730939602352205\n",
            "Cost after 295102 iterations : Training Loss =  0.028111927799962747; Validation Loss = 0.03473093091185797\n",
            "Cost after 295103 iterations : Training Loss =  0.028111917626248442; Validation Loss = 0.03473092222145186\n",
            "Cost after 295104 iterations : Training Loss =  0.028111907452651394; Validation Loss = 0.03473091353113414\n",
            "Cost after 295105 iterations : Training Loss =  0.028111897279171915; Validation Loss = 0.03473090484090466\n",
            "Cost after 295106 iterations : Training Loss =  0.02811188710580982; Validation Loss = 0.03473089615076367\n",
            "Cost after 295107 iterations : Training Loss =  0.028111876932565144; Validation Loss = 0.0347308874607108\n",
            "Cost after 295108 iterations : Training Loss =  0.028111866759437824; Validation Loss = 0.03473087877074648\n",
            "Cost after 295109 iterations : Training Loss =  0.028111856586428; Validation Loss = 0.03473087008087005\n",
            "Cost after 295110 iterations : Training Loss =  0.028111846413535573; Validation Loss = 0.03473086139108233\n",
            "Cost after 295111 iterations : Training Loss =  0.028111836240760548; Validation Loss = 0.034730852701382244\n",
            "Cost after 295112 iterations : Training Loss =  0.02811182606810298; Validation Loss = 0.03473084401177077\n",
            "Cost after 295113 iterations : Training Loss =  0.02811181589556277; Validation Loss = 0.034730835322247695\n",
            "Cost after 295114 iterations : Training Loss =  0.028111805723139986; Validation Loss = 0.034730826632812746\n",
            "Cost after 295115 iterations : Training Loss =  0.0281117955508346; Validation Loss = 0.03473081794346624\n",
            "Cost after 295116 iterations : Training Loss =  0.02811178537864662; Validation Loss = 0.034730809254208135\n",
            "Cost after 295117 iterations : Training Loss =  0.028111775206576033; Validation Loss = 0.03473080056503794\n",
            "Cost after 295118 iterations : Training Loss =  0.028111765034622895; Validation Loss = 0.034730791875956124\n",
            "Cost after 295119 iterations : Training Loss =  0.02811175486278721; Validation Loss = 0.03473078318696256\n",
            "Cost after 295120 iterations : Training Loss =  0.028111744691068762; Validation Loss = 0.03473077449805733\n",
            "Cost after 295121 iterations : Training Loss =  0.028111734519467912; Validation Loss = 0.03473076580924037\n",
            "Cost after 295122 iterations : Training Loss =  0.028111724347984157; Validation Loss = 0.0347307571205117\n",
            "Cost after 295123 iterations : Training Loss =  0.028111714176617984; Validation Loss = 0.034730748431871156\n",
            "Cost after 295124 iterations : Training Loss =  0.02811170400536928; Validation Loss = 0.03473073974331908\n",
            "Cost after 295125 iterations : Training Loss =  0.028111693834237835; Validation Loss = 0.03473073105485522\n",
            "Cost after 295126 iterations : Training Loss =  0.02811168366322379; Validation Loss = 0.03473072236647944\n",
            "Cost after 295127 iterations : Training Loss =  0.02811167349232721; Validation Loss = 0.034730713678191825\n",
            "Cost after 295128 iterations : Training Loss =  0.02811166332154799; Validation Loss = 0.03473070498999256\n",
            "Cost after 295129 iterations : Training Loss =  0.02811165315088616; Validation Loss = 0.034730696301881483\n",
            "Cost after 295130 iterations : Training Loss =  0.02811164298034163; Validation Loss = 0.03473068761385884\n",
            "Cost after 295131 iterations : Training Loss =  0.02811163280991448; Validation Loss = 0.03473067892592419\n",
            "Cost after 295132 iterations : Training Loss =  0.028111622639604802; Validation Loss = 0.03473067023807835\n",
            "Cost after 295133 iterations : Training Loss =  0.028111612469412527; Validation Loss = 0.03473066155032028\n",
            "Cost after 295134 iterations : Training Loss =  0.028111602299337513; Validation Loss = 0.034730652862650556\n",
            "Cost after 295135 iterations : Training Loss =  0.028111592129379862; Validation Loss = 0.034730644175068937\n",
            "Cost after 295136 iterations : Training Loss =  0.02811158195953971; Validation Loss = 0.03473063548757562\n",
            "Cost after 295137 iterations : Training Loss =  0.02811157178981693; Validation Loss = 0.0347306268001708\n",
            "Cost after 295138 iterations : Training Loss =  0.02811156162021135; Validation Loss = 0.034730618112854036\n",
            "Cost after 295139 iterations : Training Loss =  0.028111551450723266; Validation Loss = 0.03473060942562572\n",
            "Cost after 295140 iterations : Training Loss =  0.02811154128135248; Validation Loss = 0.03473060073848573\n",
            "Cost after 295141 iterations : Training Loss =  0.028111531112099134; Validation Loss = 0.03473059205143359\n",
            "Cost after 295142 iterations : Training Loss =  0.028111520942963163; Validation Loss = 0.03473058336446949\n",
            "Cost after 295143 iterations : Training Loss =  0.0281115107739444; Validation Loss = 0.03473057467759414\n",
            "Cost after 295144 iterations : Training Loss =  0.02811150060504316; Validation Loss = 0.034730565990806865\n",
            "Cost after 295145 iterations : Training Loss =  0.028111490436259278; Validation Loss = 0.03473055730410792\n",
            "Cost after 295146 iterations : Training Loss =  0.02811148026759268; Validation Loss = 0.03473054861749726\n",
            "Cost after 295147 iterations : Training Loss =  0.02811147009904328; Validation Loss = 0.03473053993097431\n",
            "Cost after 295148 iterations : Training Loss =  0.02811145993061143; Validation Loss = 0.03473053124454\n",
            "Cost after 295149 iterations : Training Loss =  0.028111449762296938; Validation Loss = 0.03473052255819395\n",
            "Cost after 295150 iterations : Training Loss =  0.02811143959409972; Validation Loss = 0.03473051387193608\n",
            "Cost after 295151 iterations : Training Loss =  0.028111429426019883; Validation Loss = 0.03473050518576642\n",
            "Cost after 295152 iterations : Training Loss =  0.028111419258057442; Validation Loss = 0.03473049649968478\n",
            "Cost after 295153 iterations : Training Loss =  0.028111409090212362; Validation Loss = 0.03473048781369139\n",
            "Cost after 295154 iterations : Training Loss =  0.028111398922484546; Validation Loss = 0.03473047912778664\n",
            "Cost after 295155 iterations : Training Loss =  0.028111388754874046; Validation Loss = 0.03473047044196978\n",
            "Cost after 295156 iterations : Training Loss =  0.028111378587380945; Validation Loss = 0.03473046175624111\n",
            "Cost after 295157 iterations : Training Loss =  0.02811136842000502; Validation Loss = 0.0347304530706008\n",
            "Cost after 295158 iterations : Training Loss =  0.028111358252746483; Validation Loss = 0.03473044438504876\n",
            "Cost after 295159 iterations : Training Loss =  0.02811134808560549; Validation Loss = 0.03473043569958473\n",
            "Cost after 295160 iterations : Training Loss =  0.028111337918581634; Validation Loss = 0.03473042701420913\n",
            "Cost after 295161 iterations : Training Loss =  0.028111327751675116; Validation Loss = 0.03473041832892188\n",
            "Cost after 295162 iterations : Training Loss =  0.028111317584885984; Validation Loss = 0.03473040964372264\n",
            "Cost after 295163 iterations : Training Loss =  0.028111307418214247; Validation Loss = 0.034730400958611525\n",
            "Cost after 295164 iterations : Training Loss =  0.02811129725165973; Validation Loss = 0.034730392273588644\n",
            "Cost after 295165 iterations : Training Loss =  0.028111287085222637; Validation Loss = 0.034730383588653824\n",
            "Cost after 295166 iterations : Training Loss =  0.028111276918902667; Validation Loss = 0.03473037490380755\n",
            "Cost after 295167 iterations : Training Loss =  0.028111266752700228; Validation Loss = 0.03473036621904926\n",
            "Cost after 295168 iterations : Training Loss =  0.02811125658661506; Validation Loss = 0.03473035753437937\n",
            "Cost after 295169 iterations : Training Loss =  0.028111246420647138; Validation Loss = 0.034730348849797636\n",
            "Cost after 295170 iterations : Training Loss =  0.028111236254796577; Validation Loss = 0.03473034016530386\n",
            "Cost after 295171 iterations : Training Loss =  0.028111226089063242; Validation Loss = 0.03473033148089844\n",
            "Cost after 295172 iterations : Training Loss =  0.028111215923447303; Validation Loss = 0.034730322796581274\n",
            "Cost after 295173 iterations : Training Loss =  0.02811120575794863; Validation Loss = 0.03473031411235227\n",
            "Cost after 295174 iterations : Training Loss =  0.02811119559256741; Validation Loss = 0.034730305428211296\n",
            "Cost after 295175 iterations : Training Loss =  0.02811118542730344; Validation Loss = 0.03473029674415886\n",
            "Cost after 295176 iterations : Training Loss =  0.028111175262156556; Validation Loss = 0.03473028806019455\n",
            "Cost after 295177 iterations : Training Loss =  0.02811116509712711; Validation Loss = 0.03473027937631868\n",
            "Cost after 295178 iterations : Training Loss =  0.028111154932215027; Validation Loss = 0.03473027069253058\n",
            "Cost after 295179 iterations : Training Loss =  0.02811114476742025; Validation Loss = 0.0347302620088308\n",
            "Cost after 295180 iterations : Training Loss =  0.02811113460274274; Validation Loss = 0.034730253325218896\n",
            "Cost after 295181 iterations : Training Loss =  0.028111124438182516; Validation Loss = 0.03473024464169539\n",
            "Cost after 295182 iterations : Training Loss =  0.028111114273739565; Validation Loss = 0.034730235958259977\n",
            "Cost after 295183 iterations : Training Loss =  0.028111104109413927; Validation Loss = 0.0347302272749129\n",
            "Cost after 295184 iterations : Training Loss =  0.028111093945205536; Validation Loss = 0.03473021859165428\n",
            "Cost after 295185 iterations : Training Loss =  0.0281110837811145; Validation Loss = 0.03473020990848367\n",
            "Cost after 295186 iterations : Training Loss =  0.028111073617140717; Validation Loss = 0.0347302012254014\n",
            "Cost after 295187 iterations : Training Loss =  0.028111063453284246; Validation Loss = 0.034730192542407105\n",
            "Cost after 295188 iterations : Training Loss =  0.028111053289544958; Validation Loss = 0.03473018385950107\n",
            "Cost after 295189 iterations : Training Loss =  0.02811104312592307; Validation Loss = 0.03473017517668328\n",
            "Cost after 295190 iterations : Training Loss =  0.028111032962418483; Validation Loss = 0.03473016649395343\n",
            "Cost after 295191 iterations : Training Loss =  0.028111022799031136; Validation Loss = 0.03473015781131185\n",
            "Cost after 295192 iterations : Training Loss =  0.02811101263576094; Validation Loss = 0.03473014912875853\n",
            "Cost after 295193 iterations : Training Loss =  0.028111002472608178; Validation Loss = 0.03473014044629356\n",
            "Cost after 295194 iterations : Training Loss =  0.028110992309572667; Validation Loss = 0.03473013176391631\n",
            "Cost after 295195 iterations : Training Loss =  0.02811098214665441; Validation Loss = 0.03473012308162782\n",
            "Cost after 295196 iterations : Training Loss =  0.02811097198385343; Validation Loss = 0.03473011439942708\n",
            "Cost after 295197 iterations : Training Loss =  0.028110961821169768; Validation Loss = 0.03473010571731485\n",
            "Cost after 295198 iterations : Training Loss =  0.0281109516586033; Validation Loss = 0.034730097035290626\n",
            "Cost after 295199 iterations : Training Loss =  0.028110941496154097; Validation Loss = 0.03473008835335417\n",
            "Cost after 295200 iterations : Training Loss =  0.028110931333822125; Validation Loss = 0.0347300796715064\n",
            "Cost after 295201 iterations : Training Loss =  0.02811092117160748; Validation Loss = 0.03473007098974652\n",
            "Cost after 295202 iterations : Training Loss =  0.028110911009509985; Validation Loss = 0.034730062308074905\n",
            "Cost after 295203 iterations : Training Loss =  0.028110900847529968; Validation Loss = 0.03473005362649129\n",
            "Cost after 295204 iterations : Training Loss =  0.028110890685667102; Validation Loss = 0.03473004494499602\n",
            "Cost after 295205 iterations : Training Loss =  0.028110880523921467; Validation Loss = 0.03473003626358897\n",
            "Cost after 295206 iterations : Training Loss =  0.028110870362293077; Validation Loss = 0.03473002758226982\n",
            "Cost after 295207 iterations : Training Loss =  0.02811086020078195; Validation Loss = 0.03473001890103894\n",
            "Cost after 295208 iterations : Training Loss =  0.028110850039388017; Validation Loss = 0.034730010219895975\n",
            "Cost after 295209 iterations : Training Loss =  0.028110839878111513; Validation Loss = 0.03473000153884131\n",
            "Cost after 295210 iterations : Training Loss =  0.028110829716952068; Validation Loss = 0.03472999285787517\n",
            "Cost after 295211 iterations : Training Loss =  0.028110819555909997; Validation Loss = 0.034729984176996874\n",
            "Cost after 295212 iterations : Training Loss =  0.02811080939498507; Validation Loss = 0.03472997549620667\n",
            "Cost after 295213 iterations : Training Loss =  0.02811079923417744; Validation Loss = 0.0347299668155048\n",
            "Cost after 295214 iterations : Training Loss =  0.028110789073486978; Validation Loss = 0.034729958134891145\n",
            "Cost after 295215 iterations : Training Loss =  0.028110778912913862; Validation Loss = 0.03472994945436541\n",
            "Cost after 295216 iterations : Training Loss =  0.028110768752457948; Validation Loss = 0.03472994077392792\n",
            "Cost after 295217 iterations : Training Loss =  0.028110758592119187; Validation Loss = 0.03472993209357872\n",
            "Cost after 295218 iterations : Training Loss =  0.028110748431897842; Validation Loss = 0.03472992341331752\n",
            "Cost after 295219 iterations : Training Loss =  0.028110738271793573; Validation Loss = 0.034729914733144296\n",
            "Cost after 295220 iterations : Training Loss =  0.028110728111806555; Validation Loss = 0.034729906053059686\n",
            "Cost after 295221 iterations : Training Loss =  0.028110717951936783; Validation Loss = 0.034729897373062715\n",
            "Cost after 295222 iterations : Training Loss =  0.02811070779218432; Validation Loss = 0.034729888693154146\n",
            "Cost after 295223 iterations : Training Loss =  0.02811069763254897; Validation Loss = 0.03472988001333357\n",
            "Cost after 295224 iterations : Training Loss =  0.028110687473030894; Validation Loss = 0.03472987133360116\n",
            "Cost after 295225 iterations : Training Loss =  0.028110677313630032; Validation Loss = 0.03472986265395696\n",
            "Cost after 295226 iterations : Training Loss =  0.02811066715434635; Validation Loss = 0.03472985397440115\n",
            "Cost after 295227 iterations : Training Loss =  0.02811065699517997; Validation Loss = 0.034729845294932914\n",
            "Cost after 295228 iterations : Training Loss =  0.028110646836130754; Validation Loss = 0.03472983661555306\n",
            "Cost after 295229 iterations : Training Loss =  0.02811063667719877; Validation Loss = 0.03472982793626179\n",
            "Cost after 295230 iterations : Training Loss =  0.028110626518383974; Validation Loss = 0.03472981925705819\n",
            "Cost after 295231 iterations : Training Loss =  0.02811061635968634; Validation Loss = 0.0347298105779428\n",
            "Cost after 295232 iterations : Training Loss =  0.02811060620110606; Validation Loss = 0.03472980189891565\n",
            "Cost after 295233 iterations : Training Loss =  0.02811059604264295; Validation Loss = 0.0347297932199765\n",
            "Cost after 295234 iterations : Training Loss =  0.028110585884296962; Validation Loss = 0.034729784541125616\n",
            "Cost after 295235 iterations : Training Loss =  0.028110575726068215; Validation Loss = 0.03472977586236276\n",
            "Cost after 295236 iterations : Training Loss =  0.02811056556795665; Validation Loss = 0.03472976718368808\n",
            "Cost after 295237 iterations : Training Loss =  0.028110555409962407; Validation Loss = 0.03472975850510146\n",
            "Cost after 295238 iterations : Training Loss =  0.02811054525208532; Validation Loss = 0.03472974982660272\n",
            "Cost after 295239 iterations : Training Loss =  0.028110535094325255; Validation Loss = 0.03472974114819238\n",
            "Cost after 295240 iterations : Training Loss =  0.02811052493668257; Validation Loss = 0.03472973246987025\n",
            "Cost after 295241 iterations : Training Loss =  0.02811051477915704; Validation Loss = 0.034729723791636025\n",
            "Cost after 295242 iterations : Training Loss =  0.02811050462174872; Validation Loss = 0.03472971511349008\n",
            "Cost after 295243 iterations : Training Loss =  0.02811049446445754; Validation Loss = 0.03472970643543185\n",
            "Cost after 295244 iterations : Training Loss =  0.028110484307283558; Validation Loss = 0.03472969775746208\n",
            "Cost after 295245 iterations : Training Loss =  0.02811047415022673; Validation Loss = 0.034729689079580385\n",
            "Cost after 295246 iterations : Training Loss =  0.028110463993287205; Validation Loss = 0.0347296804017867\n",
            "Cost after 295247 iterations : Training Loss =  0.028110453836464804; Validation Loss = 0.03472967172408119\n",
            "Cost after 295248 iterations : Training Loss =  0.028110443679759615; Validation Loss = 0.03472966304646413\n",
            "Cost after 295249 iterations : Training Loss =  0.02811043352317152; Validation Loss = 0.034729654368934944\n",
            "Cost after 295250 iterations : Training Loss =  0.028110423366700707; Validation Loss = 0.03472964569149369\n",
            "Cost after 295251 iterations : Training Loss =  0.028110413210346966; Validation Loss = 0.0347296370141406\n",
            "Cost after 295252 iterations : Training Loss =  0.028110403054110455; Validation Loss = 0.034729628336875734\n",
            "Cost after 295253 iterations : Training Loss =  0.02811039289799115; Validation Loss = 0.034729619659698854\n",
            "Cost after 295254 iterations : Training Loss =  0.028110382741988987; Validation Loss = 0.034729610982610196\n",
            "Cost after 295255 iterations : Training Loss =  0.028110372586103993; Validation Loss = 0.03472960230560958\n",
            "Cost after 295256 iterations : Training Loss =  0.02811036243033615; Validation Loss = 0.03472959362869667\n",
            "Cost after 295257 iterations : Training Loss =  0.02811035227468558; Validation Loss = 0.034729584951872165\n",
            "Cost after 295258 iterations : Training Loss =  0.02811034211915209; Validation Loss = 0.0347295762751359\n",
            "Cost after 295259 iterations : Training Loss =  0.028110331963735736; Validation Loss = 0.0347295675984877\n",
            "Cost after 295260 iterations : Training Loss =  0.02811032180843658; Validation Loss = 0.03472955892192761\n",
            "Cost after 295261 iterations : Training Loss =  0.028110311653254585; Validation Loss = 0.03472955024545569\n",
            "Cost after 295262 iterations : Training Loss =  0.02811030149818974; Validation Loss = 0.03472954156907185\n",
            "Cost after 295263 iterations : Training Loss =  0.02811029134324205; Validation Loss = 0.034729532892775895\n",
            "Cost after 295264 iterations : Training Loss =  0.028110281188411586; Validation Loss = 0.0347295242165682\n",
            "Cost after 295265 iterations : Training Loss =  0.028110271033698286; Validation Loss = 0.03472951554044839\n",
            "Cost after 295266 iterations : Training Loss =  0.02811026087910198; Validation Loss = 0.034729506864416754\n",
            "Cost after 295267 iterations : Training Loss =  0.028110250724622945; Validation Loss = 0.03472949818847314\n",
            "Cost after 295268 iterations : Training Loss =  0.02811024057026112; Validation Loss = 0.03472948951261784\n",
            "Cost after 295269 iterations : Training Loss =  0.028110230416016308; Validation Loss = 0.03472948083685038\n",
            "Cost after 295270 iterations : Training Loss =  0.028110220261888776; Validation Loss = 0.03472947216117121\n",
            "Cost after 295271 iterations : Training Loss =  0.028110210107878248; Validation Loss = 0.03472946348557993\n",
            "Cost after 295272 iterations : Training Loss =  0.028110199953984776; Validation Loss = 0.03472945481007662\n",
            "Cost after 295273 iterations : Training Loss =  0.028110189800208706; Validation Loss = 0.03472944613466136\n",
            "Cost after 295274 iterations : Training Loss =  0.02811017964654954; Validation Loss = 0.03472943745933451\n",
            "Cost after 295275 iterations : Training Loss =  0.028110169493007823; Validation Loss = 0.03472942878409548\n",
            "Cost after 295276 iterations : Training Loss =  0.028110159339582982; Validation Loss = 0.03472942010894481\n",
            "Cost after 295277 iterations : Training Loss =  0.02811014918627532; Validation Loss = 0.03472941143388222\n",
            "Cost after 295278 iterations : Training Loss =  0.028110139033084833; Validation Loss = 0.03472940275890748\n",
            "Cost after 295279 iterations : Training Loss =  0.028110128880011548; Validation Loss = 0.03472939408402096\n",
            "Cost after 295280 iterations : Training Loss =  0.02811011872705521; Validation Loss = 0.03472938540922253\n",
            "Cost after 295281 iterations : Training Loss =  0.02811010857421608; Validation Loss = 0.034729376734511916\n",
            "Cost after 295282 iterations : Training Loss =  0.028110098421494204; Validation Loss = 0.034729368059889826\n",
            "Cost after 295283 iterations : Training Loss =  0.028110088268889326; Validation Loss = 0.03472935938535544\n",
            "Cost after 295284 iterations : Training Loss =  0.028110078116401614; Validation Loss = 0.03472935071090898\n",
            "Cost after 295285 iterations : Training Loss =  0.02811006796403091; Validation Loss = 0.034729342036550766\n",
            "Cost after 295286 iterations : Training Loss =  0.028110057811777413; Validation Loss = 0.034729333362280845\n",
            "Cost after 295287 iterations : Training Loss =  0.028110047659641077; Validation Loss = 0.034729324688098896\n",
            "Cost after 295288 iterations : Training Loss =  0.028110037507621772; Validation Loss = 0.03472931601400492\n",
            "Cost after 295289 iterations : Training Loss =  0.02811002735571967; Validation Loss = 0.034729307339998926\n",
            "Cost after 295290 iterations : Training Loss =  0.02811001720393472; Validation Loss = 0.03472929866608116\n",
            "Cost after 295291 iterations : Training Loss =  0.028110007052266742; Validation Loss = 0.03472928999225119\n",
            "Cost after 295292 iterations : Training Loss =  0.02810999690071585; Validation Loss = 0.03472928131850954\n",
            "Cost after 295293 iterations : Training Loss =  0.028109986749282254; Validation Loss = 0.03472927264485585\n",
            "Cost after 295294 iterations : Training Loss =  0.028109976597965575; Validation Loss = 0.0347292639712904\n",
            "Cost after 295295 iterations : Training Loss =  0.028109966446766173; Validation Loss = 0.03472925529781267\n",
            "Cost after 295296 iterations : Training Loss =  0.028109956295683754; Validation Loss = 0.034729246624423234\n",
            "Cost after 295297 iterations : Training Loss =  0.028109946144718394; Validation Loss = 0.034729237951121567\n",
            "Cost after 295298 iterations : Training Loss =  0.02810993599387025; Validation Loss = 0.034729229277908176\n",
            "Cost after 295299 iterations : Training Loss =  0.028109925843139236; Validation Loss = 0.03472922060478254\n",
            "Cost after 295300 iterations : Training Loss =  0.02810991569252506; Validation Loss = 0.03472921193174523\n",
            "Cost after 295301 iterations : Training Loss =  0.028109905542028222; Validation Loss = 0.03472920325879601\n",
            "Cost after 295302 iterations : Training Loss =  0.028109895391648435; Validation Loss = 0.034729194585934695\n",
            "Cost after 295303 iterations : Training Loss =  0.028109885241385735; Validation Loss = 0.03472918591316119\n",
            "Cost after 295304 iterations : Training Loss =  0.02810987509124014; Validation Loss = 0.03472917724047607\n",
            "Cost after 295305 iterations : Training Loss =  0.028109864941211447; Validation Loss = 0.03472916856787898\n",
            "Cost after 295306 iterations : Training Loss =  0.02810985479130012; Validation Loss = 0.03472915989536975\n",
            "Cost after 295307 iterations : Training Loss =  0.0281098446415058; Validation Loss = 0.03472915122294865\n",
            "Cost after 295308 iterations : Training Loss =  0.028109834491828325; Validation Loss = 0.03472914255061576\n",
            "Cost after 295309 iterations : Training Loss =  0.028109824342268247; Validation Loss = 0.034729133878370774\n",
            "Cost after 295310 iterations : Training Loss =  0.028109814192825083; Validation Loss = 0.03472912520621339\n",
            "Cost after 295311 iterations : Training Loss =  0.02810980404349909; Validation Loss = 0.03472911653414451\n",
            "Cost after 295312 iterations : Training Loss =  0.028109793894290042; Validation Loss = 0.03472910786216361\n",
            "Cost after 295313 iterations : Training Loss =  0.028109783745198125; Validation Loss = 0.0347290991902708\n",
            "Cost after 295314 iterations : Training Loss =  0.028109773596223322; Validation Loss = 0.03472909051846575\n",
            "Cost after 295315 iterations : Training Loss =  0.028109763447365536; Validation Loss = 0.03472908184674891\n",
            "Cost after 295316 iterations : Training Loss =  0.028109753298624855; Validation Loss = 0.03472907317512035\n",
            "Cost after 295317 iterations : Training Loss =  0.02810974315000128; Validation Loss = 0.034729064503579286\n",
            "Cost after 295318 iterations : Training Loss =  0.02810973300149466; Validation Loss = 0.034729055832126626\n",
            "Cost after 295319 iterations : Training Loss =  0.028109722853105295; Validation Loss = 0.03472904716076199\n",
            "Cost after 295320 iterations : Training Loss =  0.028109712704832788; Validation Loss = 0.03472903848948528\n",
            "Cost after 295321 iterations : Training Loss =  0.028109702556677402; Validation Loss = 0.03472902981829655\n",
            "Cost after 295322 iterations : Training Loss =  0.028109692408639027; Validation Loss = 0.03472902114719599\n",
            "Cost after 295323 iterations : Training Loss =  0.02810968226071781; Validation Loss = 0.03472901247618331\n",
            "Cost after 295324 iterations : Training Loss =  0.02810967211291366; Validation Loss = 0.03472900380525876\n",
            "Cost after 295325 iterations : Training Loss =  0.028109661965226497; Validation Loss = 0.034728995134422125\n",
            "Cost after 295326 iterations : Training Loss =  0.028109651817656393; Validation Loss = 0.03472898646367376\n",
            "Cost after 295327 iterations : Training Loss =  0.0281096416702034; Validation Loss = 0.03472897779301324\n",
            "Cost after 295328 iterations : Training Loss =  0.028109631522867388; Validation Loss = 0.03472896912244099\n",
            "Cost after 295329 iterations : Training Loss =  0.028109621375648448; Validation Loss = 0.03472896045195644\n",
            "Cost after 295330 iterations : Training Loss =  0.028109611228546577; Validation Loss = 0.03472895178155977\n",
            "Cost after 295331 iterations : Training Loss =  0.02810960108156174; Validation Loss = 0.0347289431112511\n",
            "Cost after 295332 iterations : Training Loss =  0.028109590934693868; Validation Loss = 0.03472893444103041\n",
            "Cost after 295333 iterations : Training Loss =  0.028109580787943206; Validation Loss = 0.03472892577089806\n",
            "Cost after 295334 iterations : Training Loss =  0.028109570641309402; Validation Loss = 0.034728917100853435\n",
            "Cost after 295335 iterations : Training Loss =  0.028109560494792702; Validation Loss = 0.034728908430896585\n",
            "Cost after 295336 iterations : Training Loss =  0.02810955034839305; Validation Loss = 0.03472889976102832\n",
            "Cost after 295337 iterations : Training Loss =  0.0281095402021104; Validation Loss = 0.03472889109124773\n",
            "Cost after 295338 iterations : Training Loss =  0.028109530055944822; Validation Loss = 0.03472888242155498\n",
            "Cost after 295339 iterations : Training Loss =  0.02810951990989621; Validation Loss = 0.034728873751950495\n",
            "Cost after 295340 iterations : Training Loss =  0.02810950976396488; Validation Loss = 0.03472886508243397\n",
            "Cost after 295341 iterations : Training Loss =  0.028109499618150208; Validation Loss = 0.03472885641300548\n",
            "Cost after 295342 iterations : Training Loss =  0.028109489472452728; Validation Loss = 0.03472884774366491\n",
            "Cost after 295343 iterations : Training Loss =  0.028109479326872397; Validation Loss = 0.034728839074412335\n",
            "Cost after 295344 iterations : Training Loss =  0.0281094691814089; Validation Loss = 0.03472883040524771\n",
            "Cost after 295345 iterations : Training Loss =  0.028109459036062497; Validation Loss = 0.034728821736171145\n",
            "Cost after 295346 iterations : Training Loss =  0.028109448890833066; Validation Loss = 0.03472881306718258\n",
            "Cost after 295347 iterations : Training Loss =  0.028109438745720767; Validation Loss = 0.03472880439828197\n",
            "Cost after 295348 iterations : Training Loss =  0.028109428600725354; Validation Loss = 0.03472879572946927\n",
            "Cost after 295349 iterations : Training Loss =  0.028109418455847038; Validation Loss = 0.03472878706074496\n",
            "Cost after 295350 iterations : Training Loss =  0.02810940831108561; Validation Loss = 0.034728778392108424\n",
            "Cost after 295351 iterations : Training Loss =  0.028109398166441393; Validation Loss = 0.0347287697235595\n",
            "Cost after 295352 iterations : Training Loss =  0.028109388021914032; Validation Loss = 0.034728761055099006\n",
            "Cost after 295353 iterations : Training Loss =  0.0281093778775038; Validation Loss = 0.03472875238672623\n",
            "Cost after 295354 iterations : Training Loss =  0.02810936773321039; Validation Loss = 0.0347287437184416\n",
            "Cost after 295355 iterations : Training Loss =  0.02810935758903415; Validation Loss = 0.03472873505024472\n",
            "Cost after 295356 iterations : Training Loss =  0.028109347444974803; Validation Loss = 0.0347287263821359\n",
            "Cost after 295357 iterations : Training Loss =  0.028109337301032586; Validation Loss = 0.034728717714115155\n",
            "Cost after 295358 iterations : Training Loss =  0.028109327157207185; Validation Loss = 0.03472870904618251\n",
            "Cost after 295359 iterations : Training Loss =  0.028109317013498898; Validation Loss = 0.034728700378337626\n",
            "Cost after 295360 iterations : Training Loss =  0.02810930686990766; Validation Loss = 0.034728691710580906\n",
            "Cost after 295361 iterations : Training Loss =  0.028109296726433308; Validation Loss = 0.034728683042912094\n",
            "Cost after 295362 iterations : Training Loss =  0.02810928658307597; Validation Loss = 0.03472867437533121\n",
            "Cost after 295363 iterations : Training Loss =  0.028109276439835695; Validation Loss = 0.03472866570783811\n",
            "Cost after 295364 iterations : Training Loss =  0.02810926629671226; Validation Loss = 0.03472865704043293\n",
            "Cost after 295365 iterations : Training Loss =  0.028109256153705964; Validation Loss = 0.03472864837311605\n",
            "Cost after 295366 iterations : Training Loss =  0.02810924601081663; Validation Loss = 0.034728639705886945\n",
            "Cost after 295367 iterations : Training Loss =  0.02810923586804423; Validation Loss = 0.03472863103874596\n",
            "Cost after 295368 iterations : Training Loss =  0.028109225725388872; Validation Loss = 0.03472862237169284\n",
            "Cost after 295369 iterations : Training Loss =  0.028109215582850386; Validation Loss = 0.03472861370472769\n",
            "Cost after 295370 iterations : Training Loss =  0.028109205440428938; Validation Loss = 0.034728605037850295\n",
            "Cost after 295371 iterations : Training Loss =  0.02810919529812448; Validation Loss = 0.03472859637106126\n",
            "Cost after 295372 iterations : Training Loss =  0.028109185155936994; Validation Loss = 0.03472858770436013\n",
            "Cost after 295373 iterations : Training Loss =  0.028109175013866478; Validation Loss = 0.03472857903774664\n",
            "Cost after 295374 iterations : Training Loss =  0.02810916487191296; Validation Loss = 0.03472857037122118\n",
            "Cost after 295375 iterations : Training Loss =  0.0281091547300764; Validation Loss = 0.03472856170478401\n",
            "Cost after 295376 iterations : Training Loss =  0.028109144588356793; Validation Loss = 0.03472855303843445\n",
            "Cost after 295377 iterations : Training Loss =  0.028109134446754117; Validation Loss = 0.03472854437217309\n",
            "Cost after 295378 iterations : Training Loss =  0.028109124305268466; Validation Loss = 0.034728535705999555\n",
            "Cost after 295379 iterations : Training Loss =  0.028109114163899742; Validation Loss = 0.03472852703991404\n",
            "Cost after 295380 iterations : Training Loss =  0.02810910402264799; Validation Loss = 0.03472851837391656\n",
            "Cost after 295381 iterations : Training Loss =  0.028109093881513263; Validation Loss = 0.03472850970800705\n",
            "Cost after 295382 iterations : Training Loss =  0.02810908374049538; Validation Loss = 0.03472850104218546\n",
            "Cost after 295383 iterations : Training Loss =  0.028109073599594497; Validation Loss = 0.03472849237645174\n",
            "Cost after 295384 iterations : Training Loss =  0.028109063458810608; Validation Loss = 0.034728483710806016\n",
            "Cost after 295385 iterations : Training Loss =  0.02810905331814358; Validation Loss = 0.034728475045247995\n",
            "Cost after 295386 iterations : Training Loss =  0.0281090431775936; Validation Loss = 0.03472846637977801\n",
            "Cost after 295387 iterations : Training Loss =  0.028109033037160565; Validation Loss = 0.034728457714395804\n",
            "Cost after 295388 iterations : Training Loss =  0.028109022896844485; Validation Loss = 0.034728449049101696\n",
            "Cost after 295389 iterations : Training Loss =  0.028109012756645357; Validation Loss = 0.034728440383895734\n",
            "Cost after 295390 iterations : Training Loss =  0.02810900261656313; Validation Loss = 0.034728431718777285\n",
            "Cost after 295391 iterations : Training Loss =  0.028108992476597847; Validation Loss = 0.03472842305374698\n",
            "Cost after 295392 iterations : Training Loss =  0.02810898233674952; Validation Loss = 0.034728414388804744\n",
            "Cost after 295393 iterations : Training Loss =  0.028108972197018033; Validation Loss = 0.03472840572395034\n",
            "Cost after 295394 iterations : Training Loss =  0.0281089620574036; Validation Loss = 0.03472839705918391\n",
            "Cost after 295395 iterations : Training Loss =  0.028108951917906123; Validation Loss = 0.03472838839450522\n",
            "Cost after 295396 iterations : Training Loss =  0.02810894177852549; Validation Loss = 0.03472837972991467\n",
            "Cost after 295397 iterations : Training Loss =  0.02810893163926182; Validation Loss = 0.03472837106541193\n",
            "Cost after 295398 iterations : Training Loss =  0.02810892150011506; Validation Loss = 0.03472836240099704\n",
            "Cost after 295399 iterations : Training Loss =  0.02810891136108528; Validation Loss = 0.03472835373667031\n",
            "Cost after 295400 iterations : Training Loss =  0.02810890122217247; Validation Loss = 0.03472834507243104\n",
            "Cost after 295401 iterations : Training Loss =  0.028108891083376514; Validation Loss = 0.03472833640828035\n",
            "Cost after 295402 iterations : Training Loss =  0.02810888094469751; Validation Loss = 0.03472832774421718\n",
            "Cost after 295403 iterations : Training Loss =  0.028108870806135413; Validation Loss = 0.03472831908024202\n",
            "Cost after 295404 iterations : Training Loss =  0.02810886066769029; Validation Loss = 0.034728310416354345\n",
            "Cost after 295405 iterations : Training Loss =  0.028108850529361976; Validation Loss = 0.034728301752555306\n",
            "Cost after 295406 iterations : Training Loss =  0.028108840391150726; Validation Loss = 0.03472829308884392\n",
            "Cost after 295407 iterations : Training Loss =  0.028108830253056275; Validation Loss = 0.03472828442522043\n",
            "Cost after 295408 iterations : Training Loss =  0.028108820115078786; Validation Loss = 0.034728275761684885\n",
            "Cost after 295409 iterations : Training Loss =  0.028108809977218083; Validation Loss = 0.03472826709823718\n",
            "Cost after 295410 iterations : Training Loss =  0.028108799839474532; Validation Loss = 0.03472825843487737\n",
            "Cost after 295411 iterations : Training Loss =  0.028108789701847804; Validation Loss = 0.03472824977160583\n",
            "Cost after 295412 iterations : Training Loss =  0.028108779564337917; Validation Loss = 0.03472824110842195\n",
            "Cost after 295413 iterations : Training Loss =  0.02810876942694499; Validation Loss = 0.03472823244532579\n",
            "Cost after 295414 iterations : Training Loss =  0.028108759289668883; Validation Loss = 0.03472822378231762\n",
            "Cost after 295415 iterations : Training Loss =  0.028108749152509823; Validation Loss = 0.03472821511939741\n",
            "Cost after 295416 iterations : Training Loss =  0.028108739015467527; Validation Loss = 0.03472820645656489\n",
            "Cost after 295417 iterations : Training Loss =  0.02810872887854214; Validation Loss = 0.03472819779382064\n",
            "Cost after 295418 iterations : Training Loss =  0.02810871874173379; Validation Loss = 0.034728189131164065\n",
            "Cost after 295419 iterations : Training Loss =  0.028108708605042193; Validation Loss = 0.034728180468595826\n",
            "Cost after 295420 iterations : Training Loss =  0.02810869846846765; Validation Loss = 0.03472817180611516\n",
            "Cost after 295421 iterations : Training Loss =  0.028108688332009808; Validation Loss = 0.03472816314372258\n",
            "Cost after 295422 iterations : Training Loss =  0.02810867819566898; Validation Loss = 0.03472815448141756\n",
            "Cost after 295423 iterations : Training Loss =  0.02810866805944501; Validation Loss = 0.03472814581920084\n",
            "Cost after 295424 iterations : Training Loss =  0.02810865792333793; Validation Loss = 0.03472813715707141\n",
            "Cost after 295425 iterations : Training Loss =  0.02810864778734775; Validation Loss = 0.03472812849503037\n",
            "Cost after 295426 iterations : Training Loss =  0.028108637651474314; Validation Loss = 0.03472811983307703\n",
            "Cost after 295427 iterations : Training Loss =  0.02810862751571802; Validation Loss = 0.03472811117121149\n",
            "Cost after 295428 iterations : Training Loss =  0.028108617380078433; Validation Loss = 0.03472810250943393\n",
            "Cost after 295429 iterations : Training Loss =  0.028108607244555744; Validation Loss = 0.03472809384774446\n",
            "Cost after 295430 iterations : Training Loss =  0.028108597109149938; Validation Loss = 0.03472808518614262\n",
            "Cost after 295431 iterations : Training Loss =  0.028108586973861087; Validation Loss = 0.034728076524628476\n",
            "Cost after 295432 iterations : Training Loss =  0.028108576838689024; Validation Loss = 0.034728067863202546\n",
            "Cost after 295433 iterations : Training Loss =  0.028108566703633892; Validation Loss = 0.03472805920186412\n",
            "Cost after 295434 iterations : Training Loss =  0.02810855656869549; Validation Loss = 0.03472805054061401\n",
            "Cost after 295435 iterations : Training Loss =  0.028108546433874126; Validation Loss = 0.034728041879451735\n",
            "Cost after 295436 iterations : Training Loss =  0.02810853629916952; Validation Loss = 0.034728033218377016\n",
            "Cost after 295437 iterations : Training Loss =  0.028108526164581957; Validation Loss = 0.034728024557390595\n",
            "Cost after 295438 iterations : Training Loss =  0.028108516030111063; Validation Loss = 0.03472801589649187\n",
            "Cost after 295439 iterations : Training Loss =  0.02810850589575711; Validation Loss = 0.03472800723568079\n",
            "Cost after 295440 iterations : Training Loss =  0.02810849576152003; Validation Loss = 0.03472799857495793\n",
            "Cost after 295441 iterations : Training Loss =  0.028108485627399708; Validation Loss = 0.03472798991432253\n",
            "Cost after 295442 iterations : Training Loss =  0.028108475493396262; Validation Loss = 0.03472798125377549\n",
            "Cost after 295443 iterations : Training Loss =  0.0281084653595098; Validation Loss = 0.034727972593316336\n",
            "Cost after 295444 iterations : Training Loss =  0.028108455225740184; Validation Loss = 0.03472796393294491\n",
            "Cost after 295445 iterations : Training Loss =  0.028108445092087292; Validation Loss = 0.034727955272661176\n",
            "Cost after 295446 iterations : Training Loss =  0.028108434958551264; Validation Loss = 0.0347279466124656\n",
            "Cost after 295447 iterations : Training Loss =  0.02810842482513226; Validation Loss = 0.0347279379523575\n",
            "Cost after 295448 iterations : Training Loss =  0.028108414691830032; Validation Loss = 0.03472792929233723\n",
            "Cost after 295449 iterations : Training Loss =  0.028108404558644568; Validation Loss = 0.034727920632405256\n",
            "Cost after 295450 iterations : Training Loss =  0.028108394425576028; Validation Loss = 0.034727911972561316\n",
            "Cost after 295451 iterations : Training Loss =  0.0281083842926242; Validation Loss = 0.03472790331280463\n",
            "Cost after 295452 iterations : Training Loss =  0.02810837415978936; Validation Loss = 0.03472789465313613\n",
            "Cost after 295453 iterations : Training Loss =  0.028108364027071252; Validation Loss = 0.03472788599355546\n",
            "Cost after 295454 iterations : Training Loss =  0.02810835389447005; Validation Loss = 0.034727877334062594\n",
            "Cost after 295455 iterations : Training Loss =  0.02810834376198564; Validation Loss = 0.03472786867465766\n",
            "Cost after 295456 iterations : Training Loss =  0.02810833362961815; Validation Loss = 0.03472786001534071\n",
            "Cost after 295457 iterations : Training Loss =  0.028108323497367405; Validation Loss = 0.03472785135611129\n",
            "Cost after 295458 iterations : Training Loss =  0.02810831336523346; Validation Loss = 0.03472784269696967\n",
            "Cost after 295459 iterations : Training Loss =  0.028108303233216465; Validation Loss = 0.0347278340379161\n",
            "Cost after 295460 iterations : Training Loss =  0.02810829310131622; Validation Loss = 0.034727825378950465\n",
            "Cost after 295461 iterations : Training Loss =  0.02810828296953279; Validation Loss = 0.034727816720072434\n",
            "Cost after 295462 iterations : Training Loss =  0.028108272837866266; Validation Loss = 0.03472780806128229\n",
            "Cost after 295463 iterations : Training Loss =  0.028108262706316525; Validation Loss = 0.03472779940258052\n",
            "Cost after 295464 iterations : Training Loss =  0.0281082525748836; Validation Loss = 0.034727790743966154\n",
            "Cost after 295465 iterations : Training Loss =  0.028108242443567466; Validation Loss = 0.03472778208543975\n",
            "Cost after 295466 iterations : Training Loss =  0.028108232312368153; Validation Loss = 0.03472777342700116\n",
            "Cost after 295467 iterations : Training Loss =  0.028108222181285662; Validation Loss = 0.03472776476865021\n",
            "Cost after 295468 iterations : Training Loss =  0.02810821205031993; Validation Loss = 0.03472775611038744\n",
            "Cost after 295469 iterations : Training Loss =  0.028108201919471137; Validation Loss = 0.03472774745221217\n",
            "Cost after 295470 iterations : Training Loss =  0.02810819178873909; Validation Loss = 0.03472773879412498\n",
            "Cost after 295471 iterations : Training Loss =  0.028108181658123958; Validation Loss = 0.03472773013612571\n",
            "Cost after 295472 iterations : Training Loss =  0.028108171527625493; Validation Loss = 0.03472772147821405\n",
            "Cost after 295473 iterations : Training Loss =  0.02810816139724388; Validation Loss = 0.03472771282039051\n",
            "Cost after 295474 iterations : Training Loss =  0.028108151266978947; Validation Loss = 0.034727704162654555\n",
            "Cost after 295475 iterations : Training Loss =  0.028108141136831036; Validation Loss = 0.034727695505006546\n",
            "Cost after 295476 iterations : Training Loss =  0.02810813100679975; Validation Loss = 0.03472768684744626\n",
            "Cost after 295477 iterations : Training Loss =  0.028108120876885405; Validation Loss = 0.03472767818997376\n",
            "Cost after 295478 iterations : Training Loss =  0.028108110747087803; Validation Loss = 0.03472766953258923\n",
            "Cost after 295479 iterations : Training Loss =  0.028108100617406998; Validation Loss = 0.03472766087529245\n",
            "Cost after 295480 iterations : Training Loss =  0.028108090487842855; Validation Loss = 0.034727652218083875\n",
            "Cost after 295481 iterations : Training Loss =  0.02810808035839573; Validation Loss = 0.03472764356096262\n",
            "Cost after 295482 iterations : Training Loss =  0.028108070229065252; Validation Loss = 0.03472763490392931\n",
            "Cost after 295483 iterations : Training Loss =  0.028108060099851565; Validation Loss = 0.034727626246984104\n",
            "Cost after 295484 iterations : Training Loss =  0.02810804997075479; Validation Loss = 0.034727617590126145\n",
            "Cost after 295485 iterations : Training Loss =  0.028108039841774658; Validation Loss = 0.03472760893335644\n",
            "Cost after 295486 iterations : Training Loss =  0.02810802971291139; Validation Loss = 0.03472760027667413\n",
            "Cost after 295487 iterations : Training Loss =  0.02810801958416491; Validation Loss = 0.03472759162008006\n",
            "Cost after 295488 iterations : Training Loss =  0.028108009455535234; Validation Loss = 0.034727582963573596\n",
            "Cost after 295489 iterations : Training Loss =  0.028107999327022248; Validation Loss = 0.03472757430715488\n",
            "Cost after 295490 iterations : Training Loss =  0.028107989198625956; Validation Loss = 0.03472756565082435\n",
            "Cost after 295491 iterations : Training Loss =  0.02810797907034666; Validation Loss = 0.0347275569945814\n",
            "Cost after 295492 iterations : Training Loss =  0.02810796894218408; Validation Loss = 0.03472754833842638\n",
            "Cost after 295493 iterations : Training Loss =  0.028107958814138138; Validation Loss = 0.03472753968235905\n",
            "Cost after 295494 iterations : Training Loss =  0.028107948686209118; Validation Loss = 0.03472753102637956\n",
            "Cost after 295495 iterations : Training Loss =  0.028107938558396862; Validation Loss = 0.034727522370487804\n",
            "Cost after 295496 iterations : Training Loss =  0.028107928430701284; Validation Loss = 0.034727513714683945\n",
            "Cost after 295497 iterations : Training Loss =  0.02810791830312238; Validation Loss = 0.034727505058967836\n",
            "Cost after 295498 iterations : Training Loss =  0.02810790817566049; Validation Loss = 0.03472749640333927\n",
            "Cost after 295499 iterations : Training Loss =  0.02810789804831521; Validation Loss = 0.03472748774779878\n",
            "Cost after 295500 iterations : Training Loss =  0.028107887921086715; Validation Loss = 0.03472747909234613\n",
            "Cost after 295501 iterations : Training Loss =  0.028107877793974938; Validation Loss = 0.03472747043698131\n",
            "Cost after 295502 iterations : Training Loss =  0.02810786766698006; Validation Loss = 0.03472746178170392\n",
            "Cost after 295503 iterations : Training Loss =  0.028107857540101783; Validation Loss = 0.03472745312651499\n",
            "Cost after 295504 iterations : Training Loss =  0.028107847413340395; Validation Loss = 0.034727444471413446\n",
            "Cost after 295505 iterations : Training Loss =  0.0281078372866956; Validation Loss = 0.034727435816399796\n",
            "Cost after 295506 iterations : Training Loss =  0.028107827160167638; Validation Loss = 0.03472742716147394\n",
            "Cost after 295507 iterations : Training Loss =  0.02810781703375645; Validation Loss = 0.03472741850663585\n",
            "Cost after 295508 iterations : Training Loss =  0.028107806907462042; Validation Loss = 0.0347274098518858\n",
            "Cost after 295509 iterations : Training Loss =  0.02810779678128429; Validation Loss = 0.034727401197223284\n",
            "Cost after 295510 iterations : Training Loss =  0.028107786655223285; Validation Loss = 0.03472739254264869\n",
            "Cost after 295511 iterations : Training Loss =  0.02810777652927913; Validation Loss = 0.03472738388816164\n",
            "Cost after 295512 iterations : Training Loss =  0.028107766403451695; Validation Loss = 0.034727375233762736\n",
            "Cost after 295513 iterations : Training Loss =  0.028107756277740897; Validation Loss = 0.03472736657945155\n",
            "Cost after 295514 iterations : Training Loss =  0.028107746152146874; Validation Loss = 0.03472735792522801\n",
            "Cost after 295515 iterations : Training Loss =  0.028107736026669584; Validation Loss = 0.0347273492710921\n",
            "Cost after 295516 iterations : Training Loss =  0.028107725901309114; Validation Loss = 0.03472734061704399\n",
            "Cost after 295517 iterations : Training Loss =  0.028107715776065172; Validation Loss = 0.03472733196308419\n",
            "Cost after 295518 iterations : Training Loss =  0.028107705650938147; Validation Loss = 0.03472732330921182\n",
            "Cost after 295519 iterations : Training Loss =  0.028107695525927744; Validation Loss = 0.034727314655427034\n",
            "Cost after 295520 iterations : Training Loss =  0.02810768540103424; Validation Loss = 0.034727306001730275\n",
            "Cost after 295521 iterations : Training Loss =  0.028107675276257273; Validation Loss = 0.03472729734812123\n",
            "Cost after 295522 iterations : Training Loss =  0.028107665151597114; Validation Loss = 0.034727288694599893\n",
            "Cost after 295523 iterations : Training Loss =  0.028107655027053757; Validation Loss = 0.034727280041166354\n",
            "Cost after 295524 iterations : Training Loss =  0.028107644902627044; Validation Loss = 0.03472727138782071\n",
            "Cost after 295525 iterations : Training Loss =  0.028107634778317046; Validation Loss = 0.03472726273456283\n",
            "Cost after 295526 iterations : Training Loss =  0.028107624654123725; Validation Loss = 0.034727254081392524\n",
            "Cost after 295527 iterations : Training Loss =  0.028107614530047155; Validation Loss = 0.034727245428310016\n",
            "Cost after 295528 iterations : Training Loss =  0.028107604406087217; Validation Loss = 0.034727236775315674\n",
            "Cost after 295529 iterations : Training Loss =  0.02810759428224401; Validation Loss = 0.03472722812240889\n",
            "Cost after 295530 iterations : Training Loss =  0.028107584158517607; Validation Loss = 0.03472721946958998\n",
            "Cost after 295531 iterations : Training Loss =  0.028107574034907917; Validation Loss = 0.03472721081685849\n",
            "Cost after 295532 iterations : Training Loss =  0.02810756391141494; Validation Loss = 0.03472720216421498\n",
            "Cost after 295533 iterations : Training Loss =  0.028107553788038562; Validation Loss = 0.03472719351165908\n",
            "Cost after 295534 iterations : Training Loss =  0.028107543664779015; Validation Loss = 0.03472718485919137\n",
            "Cost after 295535 iterations : Training Loss =  0.028107533541636042; Validation Loss = 0.03472717620681091\n",
            "Cost after 295536 iterations : Training Loss =  0.028107523418609847; Validation Loss = 0.03472716755451864\n",
            "Cost after 295537 iterations : Training Loss =  0.028107513295700316; Validation Loss = 0.034727158902313635\n",
            "Cost after 295538 iterations : Training Loss =  0.02810750317290762; Validation Loss = 0.0347271502501968\n",
            "Cost after 295539 iterations : Training Loss =  0.028107493050231424; Validation Loss = 0.034727141598167595\n",
            "Cost after 295540 iterations : Training Loss =  0.028107482927672; Validation Loss = 0.03472713294622607\n",
            "Cost after 295541 iterations : Training Loss =  0.028107472805229305; Validation Loss = 0.03472712429437254\n",
            "Cost after 295542 iterations : Training Loss =  0.02810746268290325; Validation Loss = 0.03472711564260659\n",
            "Cost after 295543 iterations : Training Loss =  0.028107452560693795; Validation Loss = 0.03472710699092844\n",
            "Cost after 295544 iterations : Training Loss =  0.028107442438601197; Validation Loss = 0.034727098339337834\n",
            "Cost after 295545 iterations : Training Loss =  0.028107432316625266; Validation Loss = 0.034727089687834944\n",
            "Cost after 295546 iterations : Training Loss =  0.02810742219476596; Validation Loss = 0.034727081036420034\n",
            "Cost after 295547 iterations : Training Loss =  0.028107412073023388; Validation Loss = 0.034727072385093004\n",
            "Cost after 295548 iterations : Training Loss =  0.02810740195139741; Validation Loss = 0.03472706373385333\n",
            "Cost after 295549 iterations : Training Loss =  0.028107391829888206; Validation Loss = 0.03472705508270148\n",
            "Cost after 295550 iterations : Training Loss =  0.028107381708495575; Validation Loss = 0.03472704643163751\n",
            "Cost after 295551 iterations : Training Loss =  0.028107371587219664; Validation Loss = 0.03472703778066097\n",
            "Cost after 295552 iterations : Training Loss =  0.028107361466060382; Validation Loss = 0.03472702912977257\n",
            "Cost after 295553 iterations : Training Loss =  0.028107351345017975; Validation Loss = 0.03472702047897163\n",
            "Cost after 295554 iterations : Training Loss =  0.028107341224092086; Validation Loss = 0.03472701182825834\n",
            "Cost after 295555 iterations : Training Loss =  0.028107331103282826; Validation Loss = 0.03472700317763325\n",
            "Cost after 295556 iterations : Training Loss =  0.02810732098259025; Validation Loss = 0.03472699452709571\n",
            "Cost after 295557 iterations : Training Loss =  0.02810731086201434; Validation Loss = 0.03472698587664566\n",
            "Cost after 295558 iterations : Training Loss =  0.028107300741555173; Validation Loss = 0.0347269772262835\n",
            "Cost after 295559 iterations : Training Loss =  0.02810729062121258; Validation Loss = 0.0347269685760093\n",
            "Cost after 295560 iterations : Training Loss =  0.028107280500986747; Validation Loss = 0.0347269599258225\n",
            "Cost after 295561 iterations : Training Loss =  0.02810727038087747; Validation Loss = 0.03472695127572359\n",
            "Cost after 295562 iterations : Training Loss =  0.028107260260884837; Validation Loss = 0.03472694262571273\n",
            "Cost after 295563 iterations : Training Loss =  0.028107250141008976; Validation Loss = 0.034726933975789\n",
            "Cost after 295564 iterations : Training Loss =  0.02810724002124971; Validation Loss = 0.03472692532595325\n",
            "Cost after 295565 iterations : Training Loss =  0.028107229901607092; Validation Loss = 0.0347269166762053\n",
            "Cost after 295566 iterations : Training Loss =  0.02810721978208116; Validation Loss = 0.03472690802654481\n",
            "Cost after 295567 iterations : Training Loss =  0.02810720966267169; Validation Loss = 0.034726899376972424\n",
            "Cost after 295568 iterations : Training Loss =  0.028107199543379206; Validation Loss = 0.034726890727487456\n",
            "Cost after 295569 iterations : Training Loss =  0.028107189424203215; Validation Loss = 0.034726882078090236\n",
            "Cost after 295570 iterations : Training Loss =  0.028107179305143816; Validation Loss = 0.034726873428780544\n",
            "Cost after 295571 iterations : Training Loss =  0.028107169186201042; Validation Loss = 0.034726864779559156\n",
            "Cost after 295572 iterations : Training Loss =  0.02810715906737495; Validation Loss = 0.03472685613042502\n",
            "Cost after 295573 iterations : Training Loss =  0.028107148948665478; Validation Loss = 0.034726847481378734\n",
            "Cost after 295574 iterations : Training Loss =  0.028107138830072556; Validation Loss = 0.0347268388324202\n",
            "Cost after 295575 iterations : Training Loss =  0.028107128711596478; Validation Loss = 0.03472683018354909\n",
            "Cost after 295576 iterations : Training Loss =  0.028107118593236852; Validation Loss = 0.0347268215347661\n",
            "Cost after 295577 iterations : Training Loss =  0.028107108474993966; Validation Loss = 0.034726812886070636\n",
            "Cost after 295578 iterations : Training Loss =  0.028107098356867675; Validation Loss = 0.034726804237463056\n",
            "Cost after 295579 iterations : Training Loss =  0.028107088238858075; Validation Loss = 0.03472679558894278\n",
            "Cost after 295580 iterations : Training Loss =  0.028107078120965006; Validation Loss = 0.03472678694051048\n",
            "Cost after 295581 iterations : Training Loss =  0.02810706800318863; Validation Loss = 0.034726778292165916\n",
            "Cost after 295582 iterations : Training Loss =  0.02810705788552892; Validation Loss = 0.03472676964390899\n",
            "Cost after 295583 iterations : Training Loss =  0.028107047767985754; Validation Loss = 0.034726760995739764\n",
            "Cost after 295584 iterations : Training Loss =  0.028107037650559188; Validation Loss = 0.03472675234765816\n",
            "Cost after 295585 iterations : Training Loss =  0.02810702753324911; Validation Loss = 0.034726743699664143\n",
            "Cost after 295586 iterations : Training Loss =  0.028107017416055917; Validation Loss = 0.03472673505175832\n",
            "Cost after 295587 iterations : Training Loss =  0.02810700729897916; Validation Loss = 0.03472672640393994\n",
            "Cost after 295588 iterations : Training Loss =  0.028106997182019137; Validation Loss = 0.03472671775620922\n",
            "Cost after 295589 iterations : Training Loss =  0.02810698706517562; Validation Loss = 0.03472670910856628\n",
            "Cost after 295590 iterations : Training Loss =  0.02810697694844886; Validation Loss = 0.03472670046101094\n",
            "Cost after 295591 iterations : Training Loss =  0.028106966831838533; Validation Loss = 0.03472669181354311\n",
            "Cost after 295592 iterations : Training Loss =  0.028106956715344907; Validation Loss = 0.03472668316616314\n",
            "Cost after 295593 iterations : Training Loss =  0.028106946598967886; Validation Loss = 0.03472667451887096\n",
            "Cost after 295594 iterations : Training Loss =  0.02810693648270754; Validation Loss = 0.03472666587166627\n",
            "Cost after 295595 iterations : Training Loss =  0.028106926366563596; Validation Loss = 0.03472665722454922\n",
            "Cost after 295596 iterations : Training Loss =  0.028106916250536546; Validation Loss = 0.03472664857751998\n",
            "Cost after 295597 iterations : Training Loss =  0.028106906134625846; Validation Loss = 0.03472663993057872\n",
            "Cost after 295598 iterations : Training Loss =  0.02810689601883174; Validation Loss = 0.03472663128372473\n",
            "Cost after 295599 iterations : Training Loss =  0.028106885903154334; Validation Loss = 0.03472662263695857\n",
            "Cost after 295600 iterations : Training Loss =  0.02810687578759357; Validation Loss = 0.03472661399027993\n",
            "Cost after 295601 iterations : Training Loss =  0.02810686567214923; Validation Loss = 0.03472660534368927\n",
            "Cost after 295602 iterations : Training Loss =  0.028106855556821568; Validation Loss = 0.034726596697186016\n",
            "Cost after 295603 iterations : Training Loss =  0.028106845441610556; Validation Loss = 0.03472658805077075\n",
            "Cost after 295604 iterations : Training Loss =  0.02810683532651608; Validation Loss = 0.03472657940444297\n",
            "Cost after 295605 iterations : Training Loss =  0.028106825211538157; Validation Loss = 0.03472657075820311\n",
            "Cost after 295606 iterations : Training Loss =  0.02810681509667691; Validation Loss = 0.03472656211205079\n",
            "Cost after 295607 iterations : Training Loss =  0.028106804981932118; Validation Loss = 0.03472655346598626\n",
            "Cost after 295608 iterations : Training Loss =  0.028106794867303952; Validation Loss = 0.03472654482000915\n",
            "Cost after 295609 iterations : Training Loss =  0.028106784752792373; Validation Loss = 0.034726536174119825\n",
            "Cost after 295610 iterations : Training Loss =  0.028106774638397395; Validation Loss = 0.03472652752831798\n",
            "Cost after 295611 iterations : Training Loss =  0.02810676452411893; Validation Loss = 0.03472651888260389\n",
            "Cost after 295612 iterations : Training Loss =  0.02810675440995716; Validation Loss = 0.03472651023697755\n",
            "Cost after 295613 iterations : Training Loss =  0.028106744295911788; Validation Loss = 0.03472650159143854\n",
            "Cost after 295614 iterations : Training Loss =  0.028106734181983156; Validation Loss = 0.03472649294598721\n",
            "Cost after 295615 iterations : Training Loss =  0.028106724068170965; Validation Loss = 0.03472648430062412\n",
            "Cost after 295616 iterations : Training Loss =  0.0281067139544755; Validation Loss = 0.03472647565534825\n",
            "Cost after 295617 iterations : Training Loss =  0.02810670384089639; Validation Loss = 0.03472646701016012\n",
            "Cost after 295618 iterations : Training Loss =  0.028106693727433987; Validation Loss = 0.03472645836505966\n",
            "Cost after 295619 iterations : Training Loss =  0.028106683614088114; Validation Loss = 0.034726449720047\n",
            "Cost after 295620 iterations : Training Loss =  0.02810667350085883; Validation Loss = 0.03472644107512181\n",
            "Cost after 295621 iterations : Training Loss =  0.02810666338774611; Validation Loss = 0.03472643243028433\n",
            "Cost after 295622 iterations : Training Loss =  0.028106653274749878; Validation Loss = 0.034726423785534595\n",
            "Cost after 295623 iterations : Training Loss =  0.028106643161870226; Validation Loss = 0.0347264151408722\n",
            "Cost after 295624 iterations : Training Loss =  0.028106633049107092; Validation Loss = 0.03472640649629774\n",
            "Cost after 295625 iterations : Training Loss =  0.02810662293646068; Validation Loss = 0.03472639785181072\n",
            "Cost after 295626 iterations : Training Loss =  0.028106612823930614; Validation Loss = 0.034726389207411695\n",
            "Cost after 295627 iterations : Training Loss =  0.02810660271151713; Validation Loss = 0.0347263805631001\n",
            "Cost after 295628 iterations : Training Loss =  0.02810659259922021; Validation Loss = 0.034726371918876356\n",
            "Cost after 295629 iterations : Training Loss =  0.028106582487039903; Validation Loss = 0.03472636327474009\n",
            "Cost after 295630 iterations : Training Loss =  0.02810657237497614; Validation Loss = 0.03472635463069139\n",
            "Cost after 295631 iterations : Training Loss =  0.02810656226302881; Validation Loss = 0.03472634598673046\n",
            "Cost after 295632 iterations : Training Loss =  0.02810655215119809; Validation Loss = 0.034726337342857246\n",
            "Cost after 295633 iterations : Training Loss =  0.028106542039483977; Validation Loss = 0.034726328699071406\n",
            "Cost after 295634 iterations : Training Loss =  0.02810653192788627; Validation Loss = 0.034726320055373676\n",
            "Cost after 295635 iterations : Training Loss =  0.028106521816405234; Validation Loss = 0.03472631141176331\n",
            "Cost after 295636 iterations : Training Loss =  0.028106511705040593; Validation Loss = 0.034726302768240534\n",
            "Cost after 295637 iterations : Training Loss =  0.028106501593792594; Validation Loss = 0.03472629412480554\n",
            "Cost after 295638 iterations : Training Loss =  0.02810649148266105; Validation Loss = 0.034726285481457814\n",
            "Cost after 295639 iterations : Training Loss =  0.028106481371646074; Validation Loss = 0.03472627683819805\n",
            "Cost after 295640 iterations : Training Loss =  0.028106471260747556; Validation Loss = 0.03472626819502576\n",
            "Cost after 295641 iterations : Training Loss =  0.028106461149965713; Validation Loss = 0.034726259551940995\n",
            "Cost after 295642 iterations : Training Loss =  0.028106451039300255; Validation Loss = 0.034726250908943886\n",
            "Cost after 295643 iterations : Training Loss =  0.028106440928751364; Validation Loss = 0.034726242266034796\n",
            "Cost after 295644 iterations : Training Loss =  0.028106430818318904; Validation Loss = 0.03472623362321317\n",
            "Cost after 295645 iterations : Training Loss =  0.028106420708003156; Validation Loss = 0.03472622498047915\n",
            "Cost after 295646 iterations : Training Loss =  0.028106410597803756; Validation Loss = 0.034726216337832926\n",
            "Cost after 295647 iterations : Training Loss =  0.02810640048772098; Validation Loss = 0.03472620769527427\n",
            "Cost after 295648 iterations : Training Loss =  0.028106390377754652; Validation Loss = 0.03472619905280325\n",
            "Cost after 295649 iterations : Training Loss =  0.028106380267904962; Validation Loss = 0.03472619041041974\n",
            "Cost after 295650 iterations : Training Loss =  0.0281063701581716; Validation Loss = 0.034726181768123844\n",
            "Cost after 295651 iterations : Training Loss =  0.028106360048554793; Validation Loss = 0.0347261731259152\n",
            "Cost after 295652 iterations : Training Loss =  0.028106349939054604; Validation Loss = 0.03472616448379471\n",
            "Cost after 295653 iterations : Training Loss =  0.028106339829670847; Validation Loss = 0.03472615584176148\n",
            "Cost after 295654 iterations : Training Loss =  0.028106329720403524; Validation Loss = 0.03472614719981596\n",
            "Cost after 295655 iterations : Training Loss =  0.028106319611252855; Validation Loss = 0.03472613855795829\n",
            "Cost after 295656 iterations : Training Loss =  0.028106309502218613; Validation Loss = 0.03472612991618787\n",
            "Cost after 295657 iterations : Training Loss =  0.0281062993933008; Validation Loss = 0.034726121274505174\n",
            "Cost after 295658 iterations : Training Loss =  0.0281062892844996; Validation Loss = 0.03472611263291018\n",
            "Cost after 295659 iterations : Training Loss =  0.028106279175814798; Validation Loss = 0.03472610399140283\n",
            "Cost after 295660 iterations : Training Loss =  0.02810626906724648; Validation Loss = 0.03472609534998313\n",
            "Cost after 295661 iterations : Training Loss =  0.02810625895879474; Validation Loss = 0.03472608670865103\n",
            "Cost after 295662 iterations : Training Loss =  0.028106248850459453; Validation Loss = 0.03472607806740635\n",
            "Cost after 295663 iterations : Training Loss =  0.028106238742240688; Validation Loss = 0.034726069426249166\n",
            "Cost after 295664 iterations : Training Loss =  0.028106228634138326; Validation Loss = 0.03472606078518\n",
            "Cost after 295665 iterations : Training Loss =  0.02810621852615253; Validation Loss = 0.03472605214419812\n",
            "Cost after 295666 iterations : Training Loss =  0.028106208418283152; Validation Loss = 0.03472604350330419\n",
            "Cost after 295667 iterations : Training Loss =  0.02810619831053035; Validation Loss = 0.03472603486249758\n",
            "Cost after 295668 iterations : Training Loss =  0.0281061882028939; Validation Loss = 0.034726026221778825\n",
            "Cost after 295669 iterations : Training Loss =  0.028106178095374; Validation Loss = 0.03472601758114731\n",
            "Cost after 295670 iterations : Training Loss =  0.028106167987970592; Validation Loss = 0.03472600894060334\n",
            "Cost after 295671 iterations : Training Loss =  0.0281061578806836; Validation Loss = 0.034726000300147095\n",
            "Cost after 295672 iterations : Training Loss =  0.02810614777351314; Validation Loss = 0.03472599165977851\n",
            "Cost after 295673 iterations : Training Loss =  0.02810613766645916; Validation Loss = 0.03472598301949756\n",
            "Cost after 295674 iterations : Training Loss =  0.028106127559521705; Validation Loss = 0.03472597437930405\n",
            "Cost after 295675 iterations : Training Loss =  0.02810611745270058; Validation Loss = 0.03472596573919833\n",
            "Cost after 295676 iterations : Training Loss =  0.028106107345995928; Validation Loss = 0.03472595709917991\n",
            "Cost after 295677 iterations : Training Loss =  0.028106097239407806; Validation Loss = 0.03472594845924943\n",
            "Cost after 295678 iterations : Training Loss =  0.028106087132936095; Validation Loss = 0.034725939819406304\n",
            "Cost after 295679 iterations : Training Loss =  0.028106077026580915; Validation Loss = 0.034725931179650685\n",
            "Cost after 295680 iterations : Training Loss =  0.028106066920342218; Validation Loss = 0.03472592253998291\n",
            "Cost after 295681 iterations : Training Loss =  0.028106056814219883; Validation Loss = 0.03472591390040279\n",
            "Cost after 295682 iterations : Training Loss =  0.02810604670821404; Validation Loss = 0.034725905260909885\n",
            "Cost after 295683 iterations : Training Loss =  0.02810603660232467; Validation Loss = 0.03472589662150498\n",
            "Cost after 295684 iterations : Training Loss =  0.028106026496551802; Validation Loss = 0.034725887982187384\n",
            "Cost after 295685 iterations : Training Loss =  0.02810601639089533; Validation Loss = 0.03472587934295738\n",
            "Cost after 295686 iterations : Training Loss =  0.028106006285355256; Validation Loss = 0.03472587070381511\n",
            "Cost after 295687 iterations : Training Loss =  0.028105996179931553; Validation Loss = 0.03472586206476028\n",
            "Cost after 295688 iterations : Training Loss =  0.028105986074624504; Validation Loss = 0.03472585342579321\n",
            "Cost after 295689 iterations : Training Loss =  0.028105975969433858; Validation Loss = 0.03472584478691358\n",
            "Cost after 295690 iterations : Training Loss =  0.028105965864359667; Validation Loss = 0.034725836148121574\n",
            "Cost after 295691 iterations : Training Loss =  0.028105955759401793; Validation Loss = 0.03472582750941684\n",
            "Cost after 295692 iterations : Training Loss =  0.028105945654560496; Validation Loss = 0.034725818870799986\n",
            "Cost after 295693 iterations : Training Loss =  0.02810593554983563; Validation Loss = 0.03472581023227062\n",
            "Cost after 295694 iterations : Training Loss =  0.028105925445227053; Validation Loss = 0.03472580159382876\n",
            "Cost after 295695 iterations : Training Loss =  0.028105915340735088; Validation Loss = 0.0347257929554745\n",
            "Cost after 295696 iterations : Training Loss =  0.02810590523635936; Validation Loss = 0.03472578431720792\n",
            "Cost after 295697 iterations : Training Loss =  0.02810589513210021; Validation Loss = 0.03472577567902855\n",
            "Cost after 295698 iterations : Training Loss =  0.028105885027957505; Validation Loss = 0.03472576704093725\n",
            "Cost after 295699 iterations : Training Loss =  0.02810587492393113; Validation Loss = 0.034725758402933085\n",
            "Cost after 295700 iterations : Training Loss =  0.02810586482002128; Validation Loss = 0.0347257497650167\n",
            "Cost after 295701 iterations : Training Loss =  0.028105854716227806; Validation Loss = 0.034725741127187786\n",
            "Cost after 295702 iterations : Training Loss =  0.028105844612550782; Validation Loss = 0.034725732489446486\n",
            "Cost after 295703 iterations : Training Loss =  0.028105834508990193; Validation Loss = 0.0347257238517925\n",
            "Cost after 295704 iterations : Training Loss =  0.028105824405545935; Validation Loss = 0.034725715214226294\n",
            "Cost after 295705 iterations : Training Loss =  0.028105814302218108; Validation Loss = 0.03472570657674773\n",
            "Cost after 295706 iterations : Training Loss =  0.02810580419900677; Validation Loss = 0.03472569793935679\n",
            "Cost after 295707 iterations : Training Loss =  0.028105794095911806; Validation Loss = 0.034725689302053206\n",
            "Cost after 295708 iterations : Training Loss =  0.028105783992933283; Validation Loss = 0.03472568066483708\n",
            "Cost after 295709 iterations : Training Loss =  0.028105773890071132; Validation Loss = 0.03472567202770844\n",
            "Cost after 295710 iterations : Training Loss =  0.02810576378732546; Validation Loss = 0.0347256633906676\n",
            "Cost after 295711 iterations : Training Loss =  0.028105753684696116; Validation Loss = 0.03472565475371441\n",
            "Cost after 295712 iterations : Training Loss =  0.028105743582183186; Validation Loss = 0.03472564611684842\n",
            "Cost after 295713 iterations : Training Loss =  0.028105733479786687; Validation Loss = 0.034725637480070085\n",
            "Cost after 295714 iterations : Training Loss =  0.02810572337750659; Validation Loss = 0.0347256288433794\n",
            "Cost after 295715 iterations : Training Loss =  0.028105713275342916; Validation Loss = 0.03472562020677625\n",
            "Cost after 295716 iterations : Training Loss =  0.028105703173295603; Validation Loss = 0.03472561157026036\n",
            "Cost after 295717 iterations : Training Loss =  0.02810569307136463; Validation Loss = 0.034725602933832526\n",
            "Cost after 295718 iterations : Training Loss =  0.028105682969550187; Validation Loss = 0.034725594297492016\n",
            "Cost after 295719 iterations : Training Loss =  0.028105672867852136; Validation Loss = 0.034725585661238914\n",
            "Cost after 295720 iterations : Training Loss =  0.028105662766270467; Validation Loss = 0.034725577025073236\n",
            "Cost after 295721 iterations : Training Loss =  0.028105652664805116; Validation Loss = 0.034725568388995334\n",
            "Cost after 295722 iterations : Training Loss =  0.028105642563456126; Validation Loss = 0.034725559753005\n",
            "Cost after 295723 iterations : Training Loss =  0.028105632462223622; Validation Loss = 0.034725551117101995\n",
            "Cost after 295724 iterations : Training Loss =  0.028105622361107568; Validation Loss = 0.03472554248128673\n",
            "Cost after 295725 iterations : Training Loss =  0.028105612260107753; Validation Loss = 0.03472553384555897\n",
            "Cost after 295726 iterations : Training Loss =  0.028105602159224374; Validation Loss = 0.03472552520991868\n",
            "Cost after 295727 iterations : Training Loss =  0.028105592058457356; Validation Loss = 0.03472551657436609\n",
            "Cost after 295728 iterations : Training Loss =  0.028105581957806852; Validation Loss = 0.03472550793890102\n",
            "Cost after 295729 iterations : Training Loss =  0.028105571857272495; Validation Loss = 0.03472549930352327\n",
            "Cost after 295730 iterations : Training Loss =  0.028105561756854677; Validation Loss = 0.03472549066823299\n",
            "Cost after 295731 iterations : Training Loss =  0.028105551656553117; Validation Loss = 0.03472548203303036\n",
            "Cost after 295732 iterations : Training Loss =  0.028105541556368123; Validation Loss = 0.03472547339791512\n",
            "Cost after 295733 iterations : Training Loss =  0.02810553145629928; Validation Loss = 0.03472546476288762\n",
            "Cost after 295734 iterations : Training Loss =  0.028105521356346842; Validation Loss = 0.034725456127947564\n",
            "Cost after 295735 iterations : Training Loss =  0.028105511256510913; Validation Loss = 0.034725447493095085\n",
            "Cost after 295736 iterations : Training Loss =  0.028105501156791248; Validation Loss = 0.03472543885832989\n",
            "Cost after 295737 iterations : Training Loss =  0.028105491057188008; Validation Loss = 0.03472543022365227\n",
            "Cost after 295738 iterations : Training Loss =  0.028105480957701073; Validation Loss = 0.03472542158906224\n",
            "Cost after 295739 iterations : Training Loss =  0.02810547085833056; Validation Loss = 0.034725412954559734\n",
            "Cost after 295740 iterations : Training Loss =  0.028105460759076363; Validation Loss = 0.0347254043201445\n",
            "Cost after 295741 iterations : Training Loss =  0.028105450659938597; Validation Loss = 0.03472539568581715\n",
            "Cost after 295742 iterations : Training Loss =  0.028105440560916982; Validation Loss = 0.034725387051576934\n",
            "Cost after 295743 iterations : Training Loss =  0.028105430462011943; Validation Loss = 0.03472537841742452\n",
            "Cost after 295744 iterations : Training Loss =  0.028105420363223128; Validation Loss = 0.03472536978335943\n",
            "Cost after 295745 iterations : Training Loss =  0.02810541026455087; Validation Loss = 0.034725361149381934\n",
            "Cost after 295746 iterations : Training Loss =  0.028105400165994655; Validation Loss = 0.0347253525154919\n",
            "Cost after 295747 iterations : Training Loss =  0.02810539006755503; Validation Loss = 0.03472534388168957\n",
            "Cost after 295748 iterations : Training Loss =  0.0281053799692316; Validation Loss = 0.03472533524797453\n",
            "Cost after 295749 iterations : Training Loss =  0.028105369871024587; Validation Loss = 0.03472532661434752\n",
            "Cost after 295750 iterations : Training Loss =  0.02810535977293389; Validation Loss = 0.03472531798080722\n",
            "Cost after 295751 iterations : Training Loss =  0.0281053496749596; Validation Loss = 0.03472530934735475\n",
            "Cost after 295752 iterations : Training Loss =  0.028105339577101596; Validation Loss = 0.034725300713989625\n",
            "Cost after 295753 iterations : Training Loss =  0.028105329479359925; Validation Loss = 0.03472529208071241\n",
            "Cost after 295754 iterations : Training Loss =  0.028105319381734543; Validation Loss = 0.03472528344752236\n",
            "Cost after 295755 iterations : Training Loss =  0.028105309284225638; Validation Loss = 0.03472527481441972\n",
            "Cost after 295756 iterations : Training Loss =  0.028105299186833004; Validation Loss = 0.03472526618140478\n",
            "Cost after 295757 iterations : Training Loss =  0.02810528908955666; Validation Loss = 0.03472525754847746\n",
            "Cost after 295758 iterations : Training Loss =  0.02810527899239669; Validation Loss = 0.03472524891563768\n",
            "Cost after 295759 iterations : Training Loss =  0.028105268895352968; Validation Loss = 0.03472524028288497\n",
            "Cost after 295760 iterations : Training Loss =  0.02810525879842566; Validation Loss = 0.03472523165022011\n",
            "Cost after 295761 iterations : Training Loss =  0.02810524870161462; Validation Loss = 0.03472522301764252\n",
            "Cost after 295762 iterations : Training Loss =  0.02810523860492004; Validation Loss = 0.03472521438515238\n",
            "Cost after 295763 iterations : Training Loss =  0.028105228508341674; Validation Loss = 0.03472520575274978\n",
            "Cost after 295764 iterations : Training Loss =  0.02810521841187963; Validation Loss = 0.034725197120434856\n",
            "Cost after 295765 iterations : Training Loss =  0.02810520831553385; Validation Loss = 0.034725188488207256\n",
            "Cost after 295766 iterations : Training Loss =  0.028105198219304454; Validation Loss = 0.03472517985606724\n",
            "Cost after 295767 iterations : Training Loss =  0.028105188123191436; Validation Loss = 0.03472517122401463\n",
            "Cost after 295768 iterations : Training Loss =  0.02810517802719454; Validation Loss = 0.03472516259204949\n",
            "Cost after 295769 iterations : Training Loss =  0.028105167931314165; Validation Loss = 0.03472515396017169\n",
            "Cost after 295770 iterations : Training Loss =  0.02810515783554998; Validation Loss = 0.03472514532838143\n",
            "Cost after 295771 iterations : Training Loss =  0.028105147739902168; Validation Loss = 0.034725136696679\n",
            "Cost after 295772 iterations : Training Loss =  0.028105137644370528; Validation Loss = 0.03472512806506346\n",
            "Cost after 295773 iterations : Training Loss =  0.028105127548955326; Validation Loss = 0.03472511943353599\n",
            "Cost after 295774 iterations : Training Loss =  0.0281051174536564; Validation Loss = 0.034725110802095664\n",
            "Cost after 295775 iterations : Training Loss =  0.028105107358473772; Validation Loss = 0.034725102170742914\n",
            "Cost after 295776 iterations : Training Loss =  0.02810509726340747; Validation Loss = 0.034725093539477156\n",
            "Cost after 295777 iterations : Training Loss =  0.0281050871684575; Validation Loss = 0.03472508490829926\n",
            "Cost after 295778 iterations : Training Loss =  0.02810507707362362; Validation Loss = 0.03472507627720872\n",
            "Cost after 295779 iterations : Training Loss =  0.02810506697890618; Validation Loss = 0.03472506764620576\n",
            "Cost after 295780 iterations : Training Loss =  0.02810505688430514; Validation Loss = 0.034725059015290334\n",
            "Cost after 295781 iterations : Training Loss =  0.028105046789820256; Validation Loss = 0.034725050384462226\n",
            "Cost after 295782 iterations : Training Loss =  0.028105036695451727; Validation Loss = 0.03472504175372149\n",
            "Cost after 295783 iterations : Training Loss =  0.028105026601199505; Validation Loss = 0.03472503312306845\n",
            "Cost after 295784 iterations : Training Loss =  0.028105016507063457; Validation Loss = 0.03472502449250299\n",
            "Cost after 295785 iterations : Training Loss =  0.028105006413043795; Validation Loss = 0.034725015862024596\n",
            "Cost after 295786 iterations : Training Loss =  0.02810499631914033; Validation Loss = 0.03472500723163384\n",
            "Cost after 295787 iterations : Training Loss =  0.028104986225353158; Validation Loss = 0.03472499860133051\n",
            "Cost after 295788 iterations : Training Loss =  0.028104976131682315; Validation Loss = 0.03472498997111466\n",
            "Cost after 295789 iterations : Training Loss =  0.02810496603812778; Validation Loss = 0.034724981340986456\n",
            "Cost after 295790 iterations : Training Loss =  0.028104955944689473; Validation Loss = 0.03472497271094522\n",
            "Cost after 295791 iterations : Training Loss =  0.02810494585136745; Validation Loss = 0.03472496408099173\n",
            "Cost after 295792 iterations : Training Loss =  0.028104935758161635; Validation Loss = 0.03472495545112554\n",
            "Cost after 295793 iterations : Training Loss =  0.028104925665072167; Validation Loss = 0.03472494682134702\n",
            "Cost after 295794 iterations : Training Loss =  0.028104915572099026; Validation Loss = 0.034724938191655584\n",
            "Cost after 295795 iterations : Training Loss =  0.028104905479242034; Validation Loss = 0.03472492956205182\n",
            "Cost after 295796 iterations : Training Loss =  0.028104895386501436; Validation Loss = 0.034724920932535694\n",
            "Cost after 295797 iterations : Training Loss =  0.02810488529387691; Validation Loss = 0.03472491230310698\n",
            "Cost after 295798 iterations : Training Loss =  0.028104875201368843; Validation Loss = 0.03472490367376547\n",
            "Cost after 295799 iterations : Training Loss =  0.028104865108977; Validation Loss = 0.03472489504451175\n",
            "Cost after 295800 iterations : Training Loss =  0.02810485501670137; Validation Loss = 0.03472488641534516\n",
            "Cost after 295801 iterations : Training Loss =  0.02810484492454196; Validation Loss = 0.034724877786265936\n",
            "Cost after 295802 iterations : Training Loss =  0.02810483483249881; Validation Loss = 0.03472486915727436\n",
            "Cost after 295803 iterations : Training Loss =  0.028104824740571997; Validation Loss = 0.03472486052836982\n",
            "Cost after 295804 iterations : Training Loss =  0.028104814648761415; Validation Loss = 0.03472485189955335\n",
            "Cost after 295805 iterations : Training Loss =  0.028104804557067115; Validation Loss = 0.03472484327082407\n",
            "Cost after 295806 iterations : Training Loss =  0.028104794465488973; Validation Loss = 0.03472483464218223\n",
            "Cost after 295807 iterations : Training Loss =  0.028104784374027154; Validation Loss = 0.0347248260136277\n",
            "Cost after 295808 iterations : Training Loss =  0.02810477428268151; Validation Loss = 0.03472481738516079\n",
            "Cost after 295809 iterations : Training Loss =  0.028104764191452213; Validation Loss = 0.03472480875678123\n",
            "Cost after 295810 iterations : Training Loss =  0.028104754100339083; Validation Loss = 0.03472480012848889\n",
            "Cost after 295811 iterations : Training Loss =  0.028104744009342215; Validation Loss = 0.03472479150028423\n",
            "Cost after 295812 iterations : Training Loss =  0.028104733918461543; Validation Loss = 0.03472478287216684\n",
            "Cost after 295813 iterations : Training Loss =  0.028104723827697194; Validation Loss = 0.03472477424413694\n",
            "Cost after 295814 iterations : Training Loss =  0.028104713737049047; Validation Loss = 0.03472476561619459\n",
            "Cost after 295815 iterations : Training Loss =  0.028104703646517102; Validation Loss = 0.03472475698833964\n",
            "Cost after 295816 iterations : Training Loss =  0.028104693556101394; Validation Loss = 0.03472474836057225\n",
            "Cost after 295817 iterations : Training Loss =  0.028104683465801878; Validation Loss = 0.03472473973289181\n",
            "Cost after 295818 iterations : Training Loss =  0.02810467337561864; Validation Loss = 0.034724731105298855\n",
            "Cost after 295819 iterations : Training Loss =  0.028104663285551754; Validation Loss = 0.03472472247779343\n",
            "Cost after 295820 iterations : Training Loss =  0.02810465319560099; Validation Loss = 0.03472471385037533\n",
            "Cost after 295821 iterations : Training Loss =  0.028104643105766526; Validation Loss = 0.034724705223044806\n",
            "Cost after 295822 iterations : Training Loss =  0.028104633016048118; Validation Loss = 0.03472469659580163\n",
            "Cost after 295823 iterations : Training Loss =  0.028104622926446034; Validation Loss = 0.034724687968645966\n",
            "Cost after 295824 iterations : Training Loss =  0.02810461283696022; Validation Loss = 0.03472467934157763\n",
            "Cost after 295825 iterations : Training Loss =  0.02810460274759049; Validation Loss = 0.03472467071459677\n",
            "Cost after 295826 iterations : Training Loss =  0.02810459265833702; Validation Loss = 0.03472466208770343\n",
            "Cost after 295827 iterations : Training Loss =  0.028104582569199843; Validation Loss = 0.034724653460897204\n",
            "Cost after 295828 iterations : Training Loss =  0.028104572480178853; Validation Loss = 0.034724644834178545\n",
            "Cost after 295829 iterations : Training Loss =  0.02810456239127403; Validation Loss = 0.034724636207547364\n",
            "Cost after 295830 iterations : Training Loss =  0.028104552302485444; Validation Loss = 0.03472462758100324\n",
            "Cost after 295831 iterations : Training Loss =  0.028104542213813144; Validation Loss = 0.034724618954546836\n",
            "Cost after 295832 iterations : Training Loss =  0.028104532125256917; Validation Loss = 0.03472461032817769\n",
            "Cost after 295833 iterations : Training Loss =  0.02810452203681685; Validation Loss = 0.034724601701896125\n",
            "Cost after 295834 iterations : Training Loss =  0.028104511948493144; Validation Loss = 0.03472459307570184\n",
            "Cost after 295835 iterations : Training Loss =  0.028104501860285534; Validation Loss = 0.034724584449594906\n",
            "Cost after 295836 iterations : Training Loss =  0.028104491772194335; Validation Loss = 0.03472457582357515\n",
            "Cost after 295837 iterations : Training Loss =  0.028104481684219126; Validation Loss = 0.03472456719764319\n",
            "Cost after 295838 iterations : Training Loss =  0.028104471596360137; Validation Loss = 0.034724558571798445\n",
            "Cost after 295839 iterations : Training Loss =  0.028104461508617323; Validation Loss = 0.034724549946041405\n",
            "Cost after 295840 iterations : Training Loss =  0.02810445142099074; Validation Loss = 0.03472454132037154\n",
            "Cost after 295841 iterations : Training Loss =  0.028104441333480376; Validation Loss = 0.034724532694789034\n",
            "Cost after 295842 iterations : Training Loss =  0.028104431246086192; Validation Loss = 0.03472452406929382\n",
            "Cost after 295843 iterations : Training Loss =  0.028104421158808228; Validation Loss = 0.034724515443886154\n",
            "Cost after 295844 iterations : Training Loss =  0.02810441107164644; Validation Loss = 0.03472450681856589\n",
            "Cost after 295845 iterations : Training Loss =  0.028104400984600823; Validation Loss = 0.03472449819333311\n",
            "Cost after 295846 iterations : Training Loss =  0.028104390897671267; Validation Loss = 0.034724489568187474\n",
            "Cost after 295847 iterations : Training Loss =  0.028104380810858066; Validation Loss = 0.0347244809431294\n",
            "Cost after 295848 iterations : Training Loss =  0.028104370724160898; Validation Loss = 0.03472447231815866\n",
            "Cost after 295849 iterations : Training Loss =  0.028104360637580036; Validation Loss = 0.03472446369327517\n",
            "Cost after 295850 iterations : Training Loss =  0.02810435055111531; Validation Loss = 0.03472445506847911\n",
            "Cost after 295851 iterations : Training Loss =  0.028104340464766772; Validation Loss = 0.03472444644377054\n",
            "Cost after 295852 iterations : Training Loss =  0.02810433037853442; Validation Loss = 0.03472443781914918\n",
            "Cost after 295853 iterations : Training Loss =  0.028104320292418244; Validation Loss = 0.034724429194615096\n",
            "Cost after 295854 iterations : Training Loss =  0.028104310206418046; Validation Loss = 0.0347244205701686\n",
            "Cost after 295855 iterations : Training Loss =  0.028104300120534248; Validation Loss = 0.03472441194580934\n",
            "Cost after 295856 iterations : Training Loss =  0.028104290034766617; Validation Loss = 0.0347244033215374\n",
            "Cost after 295857 iterations : Training Loss =  0.02810427994911511; Validation Loss = 0.03472439469735299\n",
            "Cost after 295858 iterations : Training Loss =  0.028104269863579688; Validation Loss = 0.03472438607325598\n",
            "Cost after 295859 iterations : Training Loss =  0.02810425977816054; Validation Loss = 0.03472437744924613\n",
            "Cost after 295860 iterations : Training Loss =  0.028104249692857474; Validation Loss = 0.034724368825323884\n",
            "Cost after 295861 iterations : Training Loss =  0.028104239607670525; Validation Loss = 0.03472436020148881\n",
            "Cost after 295862 iterations : Training Loss =  0.028104229522599816; Validation Loss = 0.03472435157774133\n",
            "Cost after 295863 iterations : Training Loss =  0.028104219437645215; Validation Loss = 0.034724342954080986\n",
            "Cost after 295864 iterations : Training Loss =  0.028104209352806726; Validation Loss = 0.03472433433050824\n",
            "Cost after 295865 iterations : Training Loss =  0.02810419926808443; Validation Loss = 0.03472432570702261\n",
            "Cost after 295866 iterations : Training Loss =  0.02810418918347842; Validation Loss = 0.03472431708362446\n",
            "Cost after 295867 iterations : Training Loss =  0.028104179098988432; Validation Loss = 0.03472430846031365\n",
            "Cost after 295868 iterations : Training Loss =  0.028104169014614495; Validation Loss = 0.03472429983709019\n",
            "Cost after 295869 iterations : Training Loss =  0.028104158930356917; Validation Loss = 0.0347242912139542\n",
            "Cost after 295870 iterations : Training Loss =  0.02810414884621542; Validation Loss = 0.034724282590905216\n",
            "Cost after 295871 iterations : Training Loss =  0.028104138762189944; Validation Loss = 0.034724273967944023\n",
            "Cost after 295872 iterations : Training Loss =  0.028104128678280698; Validation Loss = 0.034724265345070116\n",
            "Cost after 295873 iterations : Training Loss =  0.028104118594487595; Validation Loss = 0.03472425672228359\n",
            "Cost after 295874 iterations : Training Loss =  0.02810410851081067; Validation Loss = 0.0347242480995843\n",
            "Cost after 295875 iterations : Training Loss =  0.028104098427249695; Validation Loss = 0.03472423947697233\n",
            "Cost after 295876 iterations : Training Loss =  0.028104088343805123; Validation Loss = 0.034724230854447646\n",
            "Cost after 295877 iterations : Training Loss =  0.028104078260476603; Validation Loss = 0.03472422223201008\n",
            "Cost after 295878 iterations : Training Loss =  0.028104068177264057; Validation Loss = 0.034724213609660264\n",
            "Cost after 295879 iterations : Training Loss =  0.028104058094167793; Validation Loss = 0.034724204987397554\n",
            "Cost after 295880 iterations : Training Loss =  0.02810404801118748; Validation Loss = 0.03472419636522248\n",
            "Cost after 295881 iterations : Training Loss =  0.028104037928323534; Validation Loss = 0.034724187743134724\n",
            "Cost after 295882 iterations : Training Loss =  0.028104027845575547; Validation Loss = 0.03472417912113416\n",
            "Cost after 295883 iterations : Training Loss =  0.028104017762943787; Validation Loss = 0.03472417049922103\n",
            "Cost after 295884 iterations : Training Loss =  0.028104007680428054; Validation Loss = 0.03472416187739527\n",
            "Cost after 295885 iterations : Training Loss =  0.02810399759802846; Validation Loss = 0.03472415325565645\n",
            "Cost after 295886 iterations : Training Loss =  0.028103987515745017; Validation Loss = 0.03472414463400532\n",
            "Cost after 295887 iterations : Training Loss =  0.028103977433577697; Validation Loss = 0.03472413601244131\n",
            "Cost after 295888 iterations : Training Loss =  0.02810396735152649; Validation Loss = 0.034724127390964525\n",
            "Cost after 295889 iterations : Training Loss =  0.028103957269591372; Validation Loss = 0.03472411876957544\n",
            "Cost after 295890 iterations : Training Loss =  0.028103947187772396; Validation Loss = 0.03472411014827381\n",
            "Cost after 295891 iterations : Training Loss =  0.02810393710606944; Validation Loss = 0.034724101527058994\n",
            "Cost after 295892 iterations : Training Loss =  0.02810392702448261; Validation Loss = 0.03472409290593203\n",
            "Cost after 295893 iterations : Training Loss =  0.028103916943011887; Validation Loss = 0.03472408428489211\n",
            "Cost after 295894 iterations : Training Loss =  0.02810390686165737; Validation Loss = 0.03472407566393975\n",
            "Cost after 295895 iterations : Training Loss =  0.0281038967804189; Validation Loss = 0.03472406704307451\n",
            "Cost after 295896 iterations : Training Loss =  0.028103886699296525; Validation Loss = 0.03472405842229659\n",
            "Cost after 295897 iterations : Training Loss =  0.028103876618290197; Validation Loss = 0.03472404980160609\n",
            "Cost after 295898 iterations : Training Loss =  0.028103866537400143; Validation Loss = 0.03472404118100296\n",
            "Cost after 295899 iterations : Training Loss =  0.028103856456625876; Validation Loss = 0.034724032560487283\n",
            "Cost after 295900 iterations : Training Loss =  0.028103846375968012; Validation Loss = 0.034724023940058475\n",
            "Cost after 295901 iterations : Training Loss =  0.028103836295426093; Validation Loss = 0.034724015319717465\n",
            "Cost after 295902 iterations : Training Loss =  0.028103826215000197; Validation Loss = 0.034724006699463454\n",
            "Cost after 295903 iterations : Training Loss =  0.028103816134690478; Validation Loss = 0.03472399807929678\n",
            "Cost after 295904 iterations : Training Loss =  0.028103806054496888; Validation Loss = 0.03472398945921742\n",
            "Cost after 295905 iterations : Training Loss =  0.028103795974419338; Validation Loss = 0.03472398083922544\n",
            "Cost after 295906 iterations : Training Loss =  0.0281037858944579; Validation Loss = 0.034723972219320685\n",
            "Cost after 295907 iterations : Training Loss =  0.02810377581461252; Validation Loss = 0.03472396359950348\n",
            "Cost after 295908 iterations : Training Loss =  0.028103765734883216; Validation Loss = 0.03472395497977308\n",
            "Cost after 295909 iterations : Training Loss =  0.028103755655269985; Validation Loss = 0.03472394636013057\n",
            "Cost after 295910 iterations : Training Loss =  0.028103745575772953; Validation Loss = 0.03472393774057507\n",
            "Cost after 295911 iterations : Training Loss =  0.028103735496391696; Validation Loss = 0.034723929121106875\n",
            "Cost after 295912 iterations : Training Loss =  0.028103725417126715; Validation Loss = 0.03472392050172584\n",
            "Cost after 295913 iterations : Training Loss =  0.028103715337977845; Validation Loss = 0.034723911882432214\n",
            "Cost after 295914 iterations : Training Loss =  0.02810370525894504; Validation Loss = 0.034723903263225886\n",
            "Cost after 295915 iterations : Training Loss =  0.028103695180028254; Validation Loss = 0.03472389464410666\n",
            "Cost after 295916 iterations : Training Loss =  0.028103685101227578; Validation Loss = 0.034723886025075194\n",
            "Cost after 295917 iterations : Training Loss =  0.02810367502254285; Validation Loss = 0.034723877406130844\n",
            "Cost after 295918 iterations : Training Loss =  0.02810366494397431; Validation Loss = 0.03472386878727362\n",
            "Cost after 295919 iterations : Training Loss =  0.028103654865521847; Validation Loss = 0.03472386016850392\n",
            "Cost after 295920 iterations : Training Loss =  0.028103644787185383; Validation Loss = 0.034723851549821515\n",
            "Cost after 295921 iterations : Training Loss =  0.028103634708964976; Validation Loss = 0.03472384293122608\n",
            "Cost after 295922 iterations : Training Loss =  0.028103624630860528; Validation Loss = 0.03472383431271819\n",
            "Cost after 295923 iterations : Training Loss =  0.02810361455287223; Validation Loss = 0.03472382569429746\n",
            "Cost after 295924 iterations : Training Loss =  0.028103604474999995; Validation Loss = 0.034723817075964336\n",
            "Cost after 295925 iterations : Training Loss =  0.02810359439724384; Validation Loss = 0.034723808457718185\n",
            "Cost after 295926 iterations : Training Loss =  0.02810358431960369; Validation Loss = 0.03472379983955941\n",
            "Cost after 295927 iterations : Training Loss =  0.028103574242079616; Validation Loss = 0.034723791221488005\n",
            "Cost after 295928 iterations : Training Loss =  0.02810356416467168; Validation Loss = 0.03472378260350407\n",
            "Cost after 295929 iterations : Training Loss =  0.02810355408737966; Validation Loss = 0.03472377398560689\n",
            "Cost after 295930 iterations : Training Loss =  0.028103544010203707; Validation Loss = 0.03472376536779732\n",
            "Cost after 295931 iterations : Training Loss =  0.028103533933143812; Validation Loss = 0.0347237567500751\n",
            "Cost after 295932 iterations : Training Loss =  0.028103523856199956; Validation Loss = 0.03472374813244014\n",
            "Cost after 295933 iterations : Training Loss =  0.028103513779372116; Validation Loss = 0.034723739514892195\n",
            "Cost after 295934 iterations : Training Loss =  0.028103503702660345; Validation Loss = 0.034723730897431984\n",
            "Cost after 295935 iterations : Training Loss =  0.028103493626064596; Validation Loss = 0.034723722280058815\n",
            "Cost after 295936 iterations : Training Loss =  0.028103483549584772; Validation Loss = 0.03472371366277244\n",
            "Cost after 295937 iterations : Training Loss =  0.028103473473221254; Validation Loss = 0.0347237050455739\n",
            "Cost after 295938 iterations : Training Loss =  0.028103463396973512; Validation Loss = 0.03472369642846268\n",
            "Cost after 295939 iterations : Training Loss =  0.028103453320842006; Validation Loss = 0.03472368781143843\n",
            "Cost after 295940 iterations : Training Loss =  0.02810344324482622; Validation Loss = 0.03472367919450167\n",
            "Cost after 295941 iterations : Training Loss =  0.028103433168926696; Validation Loss = 0.03472367057765215\n",
            "Cost after 295942 iterations : Training Loss =  0.028103423093143207; Validation Loss = 0.034723661960889766\n",
            "Cost after 295943 iterations : Training Loss =  0.028103413017475622; Validation Loss = 0.034723653344214615\n",
            "Cost after 295944 iterations : Training Loss =  0.028103402941924194; Validation Loss = 0.0347236447276267\n",
            "Cost after 295945 iterations : Training Loss =  0.028103392866488767; Validation Loss = 0.034723636111126166\n",
            "Cost after 295946 iterations : Training Loss =  0.028103382791169182; Validation Loss = 0.03472362749471298\n",
            "Cost after 295947 iterations : Training Loss =  0.028103372715965837; Validation Loss = 0.03472361887838698\n",
            "Cost after 295948 iterations : Training Loss =  0.028103362640878288; Validation Loss = 0.034723610262148145\n",
            "Cost after 295949 iterations : Training Loss =  0.028103352565906965; Validation Loss = 0.03472360164599668\n",
            "Cost after 295950 iterations : Training Loss =  0.028103342491051553; Validation Loss = 0.03472359302993249\n",
            "Cost after 295951 iterations : Training Loss =  0.028103332416312212; Validation Loss = 0.034723584413955615\n",
            "Cost after 295952 iterations : Training Loss =  0.028103322341688677; Validation Loss = 0.03472357579806599\n",
            "Cost after 295953 iterations : Training Loss =  0.028103312267181334; Validation Loss = 0.034723567182263396\n",
            "Cost after 295954 iterations : Training Loss =  0.028103302192789954; Validation Loss = 0.03472355856654838\n",
            "Cost after 295955 iterations : Training Loss =  0.028103292118514592; Validation Loss = 0.034723549950920474\n",
            "Cost after 295956 iterations : Training Loss =  0.02810328204435521; Validation Loss = 0.03472354133537973\n",
            "Cost after 295957 iterations : Training Loss =  0.028103271970311858; Validation Loss = 0.03472353271992636\n",
            "Cost after 295958 iterations : Training Loss =  0.02810326189638449; Validation Loss = 0.0347235241045601\n",
            "Cost after 295959 iterations : Training Loss =  0.02810325182257305; Validation Loss = 0.03472351548928129\n",
            "Cost after 295960 iterations : Training Loss =  0.02810324174887763; Validation Loss = 0.034723506874089385\n",
            "Cost after 295961 iterations : Training Loss =  0.02810323167529836; Validation Loss = 0.034723498258985006\n",
            "Cost after 295962 iterations : Training Loss =  0.028103221601834896; Validation Loss = 0.03472348964396806\n",
            "Cost after 295963 iterations : Training Loss =  0.028103211528487458; Validation Loss = 0.034723481029038\n",
            "Cost after 295964 iterations : Training Loss =  0.028103201455255994; Validation Loss = 0.0347234724141952\n",
            "Cost after 295965 iterations : Training Loss =  0.02810319138214052; Validation Loss = 0.03472346379943994\n",
            "Cost after 295966 iterations : Training Loss =  0.028103181309141058; Validation Loss = 0.034723455184771566\n",
            "Cost after 295967 iterations : Training Loss =  0.02810317123625768; Validation Loss = 0.034723446570190955\n",
            "Cost after 295968 iterations : Training Loss =  0.028103161163490233; Validation Loss = 0.03472343795569698\n",
            "Cost after 295969 iterations : Training Loss =  0.02810315109083862; Validation Loss = 0.03472342934129055\n",
            "Cost after 295970 iterations : Training Loss =  0.028103141018303114; Validation Loss = 0.03472342072697104\n",
            "Cost after 295971 iterations : Training Loss =  0.028103130945883563; Validation Loss = 0.03472341211273919\n",
            "Cost after 295972 iterations : Training Loss =  0.028103120873579992; Validation Loss = 0.034723403498594464\n",
            "Cost after 295973 iterations : Training Loss =  0.028103110801392245; Validation Loss = 0.03472339488453678\n",
            "Cost after 295974 iterations : Training Loss =  0.028103100729320696; Validation Loss = 0.034723386270566106\n",
            "Cost after 295975 iterations : Training Loss =  0.02810309065736501; Validation Loss = 0.03472337765668318\n",
            "Cost after 295976 iterations : Training Loss =  0.028103080585525252; Validation Loss = 0.03472336904288717\n",
            "Cost after 295977 iterations : Training Loss =  0.028103070513801624; Validation Loss = 0.03472336042917855\n",
            "Cost after 295978 iterations : Training Loss =  0.028103060442193746; Validation Loss = 0.03472335181555709\n",
            "Cost after 295979 iterations : Training Loss =  0.028103050370702026; Validation Loss = 0.03472334320202283\n",
            "Cost after 295980 iterations : Training Loss =  0.02810304029932605; Validation Loss = 0.03472333458857606\n",
            "Cost after 295981 iterations : Training Loss =  0.02810303022806629; Validation Loss = 0.03472332597521623\n",
            "Cost after 295982 iterations : Training Loss =  0.028103020156922336; Validation Loss = 0.03472331736194382\n",
            "Cost after 295983 iterations : Training Loss =  0.0281030100858943; Validation Loss = 0.03472330874875848\n",
            "Cost after 295984 iterations : Training Loss =  0.02810300001498223; Validation Loss = 0.0347233001356601\n",
            "Cost after 295985 iterations : Training Loss =  0.028102989944186182; Validation Loss = 0.034723291522649254\n",
            "Cost after 295986 iterations : Training Loss =  0.02810297987350613; Validation Loss = 0.034723282909725504\n",
            "Cost after 295987 iterations : Training Loss =  0.028102969802941788; Validation Loss = 0.0347232742968892\n",
            "Cost after 295988 iterations : Training Loss =  0.02810295973249367; Validation Loss = 0.0347232656841396\n",
            "Cost after 295989 iterations : Training Loss =  0.028102949662161397; Validation Loss = 0.034723257071477734\n",
            "Cost after 295990 iterations : Training Loss =  0.028102939591945066; Validation Loss = 0.034723248458903046\n",
            "Cost after 295991 iterations : Training Loss =  0.028102929521844705; Validation Loss = 0.03472323984641529\n",
            "Cost after 295992 iterations : Training Loss =  0.028102919451860317; Validation Loss = 0.034723231234014766\n",
            "Cost after 295993 iterations : Training Loss =  0.028102909381991777; Validation Loss = 0.03472322262170138\n",
            "Cost after 295994 iterations : Training Loss =  0.028102899312239166; Validation Loss = 0.03472321400947522\n",
            "Cost after 295995 iterations : Training Loss =  0.0281028892426025; Validation Loss = 0.03472320539733644\n",
            "Cost after 295996 iterations : Training Loss =  0.028102879173081914; Validation Loss = 0.034723196785284925\n",
            "Cost after 295997 iterations : Training Loss =  0.028102869103677045; Validation Loss = 0.03472318817332047\n",
            "Cost after 295998 iterations : Training Loss =  0.028102859034388278; Validation Loss = 0.03472317956144317\n",
            "Cost after 295999 iterations : Training Loss =  0.028102848965215275; Validation Loss = 0.03472317094965339\n",
            "Cost after 296000 iterations : Training Loss =  0.02810283889615834; Validation Loss = 0.03472316233795058\n",
            "Cost after 296001 iterations : Training Loss =  0.028102828827217326; Validation Loss = 0.03472315372633505\n",
            "Cost after 296002 iterations : Training Loss =  0.028102818758392222; Validation Loss = 0.03472314511480654\n",
            "Cost after 296003 iterations : Training Loss =  0.02810280868968307; Validation Loss = 0.03472313650336532\n",
            "Cost after 296004 iterations : Training Loss =  0.02810279862108974; Validation Loss = 0.03472312789201147\n",
            "Cost after 296005 iterations : Training Loss =  0.028102788552612376; Validation Loss = 0.0347231192807447\n",
            "Cost after 296006 iterations : Training Loss =  0.028102778484250898; Validation Loss = 0.03472311066956483\n",
            "Cost after 296007 iterations : Training Loss =  0.02810276841600539; Validation Loss = 0.034723102058472434\n",
            "Cost after 296008 iterations : Training Loss =  0.02810275834787579; Validation Loss = 0.03472309344746737\n",
            "Cost after 296009 iterations : Training Loss =  0.0281027482798621; Validation Loss = 0.03472308483654945\n",
            "Cost after 296010 iterations : Training Loss =  0.028102738211964323; Validation Loss = 0.03472307622571841\n",
            "Cost after 296011 iterations : Training Loss =  0.028102728144182382; Validation Loss = 0.034723067614974554\n",
            "Cost after 296012 iterations : Training Loss =  0.0281027180765164; Validation Loss = 0.03472305900431819\n",
            "Cost after 296013 iterations : Training Loss =  0.02810270800896637; Validation Loss = 0.0347230503937487\n",
            "Cost after 296014 iterations : Training Loss =  0.028102697941532223; Validation Loss = 0.03472304178326691\n",
            "Cost after 296015 iterations : Training Loss =  0.02810268787421398; Validation Loss = 0.03472303317287181\n",
            "Cost after 296016 iterations : Training Loss =  0.028102677807011592; Validation Loss = 0.034723024562564\n",
            "Cost after 296017 iterations : Training Loss =  0.028102667739925088; Validation Loss = 0.03472301595234354\n",
            "Cost after 296018 iterations : Training Loss =  0.02810265767295458; Validation Loss = 0.03472300734221017\n",
            "Cost after 296019 iterations : Training Loss =  0.02810264760609994; Validation Loss = 0.03472299873216366\n",
            "Cost after 296020 iterations : Training Loss =  0.028102637539361177; Validation Loss = 0.034722990122204526\n",
            "Cost after 296021 iterations : Training Loss =  0.028102627472738263; Validation Loss = 0.034722981512332315\n",
            "Cost after 296022 iterations : Training Loss =  0.028102617406231347; Validation Loss = 0.03472297290254743\n",
            "Cost after 296023 iterations : Training Loss =  0.02810260733984028; Validation Loss = 0.0347229642928502\n",
            "Cost after 296024 iterations : Training Loss =  0.028102597273565094; Validation Loss = 0.03472295568323974\n",
            "Cost after 296025 iterations : Training Loss =  0.02810258720740571; Validation Loss = 0.034722947073716495\n",
            "Cost after 296026 iterations : Training Loss =  0.02810257714136234; Validation Loss = 0.03472293846428047\n",
            "Cost after 296027 iterations : Training Loss =  0.028102567075434712; Validation Loss = 0.034722929854931445\n",
            "Cost after 296028 iterations : Training Loss =  0.028102557009623094; Validation Loss = 0.03472292124566979\n",
            "Cost after 296029 iterations : Training Loss =  0.028102546943927276; Validation Loss = 0.03472291263649523\n",
            "Cost after 296030 iterations : Training Loss =  0.02810253687834736; Validation Loss = 0.034722904027407735\n",
            "Cost after 296031 iterations : Training Loss =  0.028102526812883385; Validation Loss = 0.03472289541840758\n",
            "Cost after 296032 iterations : Training Loss =  0.028102516747535247; Validation Loss = 0.03472288680949449\n",
            "Cost after 296033 iterations : Training Loss =  0.028102506682302946; Validation Loss = 0.034722878200668536\n",
            "Cost after 296034 iterations : Training Loss =  0.02810249661718644; Validation Loss = 0.03472286959192987\n",
            "Cost after 296035 iterations : Training Loss =  0.02810248655218597; Validation Loss = 0.03472286098327838\n",
            "Cost after 296036 iterations : Training Loss =  0.02810247648730127; Validation Loss = 0.034722852374714\n",
            "Cost after 296037 iterations : Training Loss =  0.02810246642253247; Validation Loss = 0.0347228437662367\n",
            "Cost after 296038 iterations : Training Loss =  0.028102456357879606; Validation Loss = 0.03472283515784656\n",
            "Cost after 296039 iterations : Training Loss =  0.028102446293342388; Validation Loss = 0.034722826549543485\n",
            "Cost after 296040 iterations : Training Loss =  0.028102436228921265; Validation Loss = 0.03472281794132772\n",
            "Cost after 296041 iterations : Training Loss =  0.028102426164615918; Validation Loss = 0.034722809333198955\n",
            "Cost after 296042 iterations : Training Loss =  0.02810241610042636; Validation Loss = 0.03472280072515768\n",
            "Cost after 296043 iterations : Training Loss =  0.028102406036352796; Validation Loss = 0.034722792117203055\n",
            "Cost after 296044 iterations : Training Loss =  0.028102395972394972; Validation Loss = 0.03472278350933583\n",
            "Cost after 296045 iterations : Training Loss =  0.028102385908553004; Validation Loss = 0.03472277490155575\n",
            "Cost after 296046 iterations : Training Loss =  0.028102375844826964; Validation Loss = 0.034722766293862874\n",
            "Cost after 296047 iterations : Training Loss =  0.028102365781216786; Validation Loss = 0.03472275768625717\n",
            "Cost after 296048 iterations : Training Loss =  0.028102355717722367; Validation Loss = 0.03472274907873856\n",
            "Cost after 296049 iterations : Training Loss =  0.028102345654343792; Validation Loss = 0.03472274047130678\n",
            "Cost after 296050 iterations : Training Loss =  0.02810233559108109; Validation Loss = 0.03472273186396234\n",
            "Cost after 296051 iterations : Training Loss =  0.028102325527934298; Validation Loss = 0.03472272325670501\n",
            "Cost after 296052 iterations : Training Loss =  0.028102315464903327; Validation Loss = 0.03472271464953503\n",
            "Cost after 296053 iterations : Training Loss =  0.028102305401988104; Validation Loss = 0.03472270604245202\n",
            "Cost after 296054 iterations : Training Loss =  0.028102295339188812; Validation Loss = 0.03472269743545623\n",
            "Cost after 296055 iterations : Training Loss =  0.028102285276505316; Validation Loss = 0.034722688828547114\n",
            "Cost after 296056 iterations : Training Loss =  0.028102275213937593; Validation Loss = 0.03472268022172554\n",
            "Cost after 296057 iterations : Training Loss =  0.028102265151485818; Validation Loss = 0.03472267161499124\n",
            "Cost after 296058 iterations : Training Loss =  0.028102255089149843; Validation Loss = 0.03472266300834376\n",
            "Cost after 296059 iterations : Training Loss =  0.02810224502692972; Validation Loss = 0.03472265440178344\n",
            "Cost after 296060 iterations : Training Loss =  0.028102234964825338; Validation Loss = 0.034722645795310085\n",
            "Cost after 296061 iterations : Training Loss =  0.028102224902836814; Validation Loss = 0.0347226371889244\n",
            "Cost after 296062 iterations : Training Loss =  0.028102214840964073; Validation Loss = 0.03472262858262557\n",
            "Cost after 296063 iterations : Training Loss =  0.028102204779207295; Validation Loss = 0.03472261997641385\n",
            "Cost after 296064 iterations : Training Loss =  0.0281021947175663; Validation Loss = 0.03472261137028941\n",
            "Cost after 296065 iterations : Training Loss =  0.02810218465604108; Validation Loss = 0.034722602764251966\n",
            "Cost after 296066 iterations : Training Loss =  0.028102174594631626; Validation Loss = 0.034722594158301784\n",
            "Cost after 296067 iterations : Training Loss =  0.02810216453333813; Validation Loss = 0.03472258555243838\n",
            "Cost after 296068 iterations : Training Loss =  0.028102154472160353; Validation Loss = 0.034722576946662385\n",
            "Cost after 296069 iterations : Training Loss =  0.028102144411098352; Validation Loss = 0.03472256834097335\n",
            "Cost after 296070 iterations : Training Loss =  0.028102134350152297; Validation Loss = 0.03472255973537144\n",
            "Cost after 296071 iterations : Training Loss =  0.028102124289321878; Validation Loss = 0.034722551129856796\n",
            "Cost after 296072 iterations : Training Loss =  0.028102114228607384; Validation Loss = 0.03472254252442921\n",
            "Cost after 296073 iterations : Training Loss =  0.02810210416800862; Validation Loss = 0.03472253391908883\n",
            "Cost after 296074 iterations : Training Loss =  0.02810209410752579; Validation Loss = 0.03472252531383564\n",
            "Cost after 296075 iterations : Training Loss =  0.028102084047158808; Validation Loss = 0.034722516708669326\n",
            "Cost after 296076 iterations : Training Loss =  0.02810207398690748; Validation Loss = 0.0347225081035902\n",
            "Cost after 296077 iterations : Training Loss =  0.02810206392677193; Validation Loss = 0.03472249949859804\n",
            "Cost after 296078 iterations : Training Loss =  0.028102053866752186; Validation Loss = 0.03472249089369306\n",
            "Cost after 296079 iterations : Training Loss =  0.028102043806848262; Validation Loss = 0.034722482288875076\n",
            "Cost after 296080 iterations : Training Loss =  0.028102033747060235; Validation Loss = 0.034722473684144245\n",
            "Cost after 296081 iterations : Training Loss =  0.028102023687387862; Validation Loss = 0.03472246507950049\n",
            "Cost after 296082 iterations : Training Loss =  0.02810201362783136; Validation Loss = 0.03472245647494391\n",
            "Cost after 296083 iterations : Training Loss =  0.02810200356839059; Validation Loss = 0.0347224478704744\n",
            "Cost after 296084 iterations : Training Loss =  0.028101993509065594; Validation Loss = 0.03472243926609215\n",
            "Cost after 296085 iterations : Training Loss =  0.028101983449856517; Validation Loss = 0.034722430661796526\n",
            "Cost after 296086 iterations : Training Loss =  0.02810197339076317; Validation Loss = 0.03472242205758822\n",
            "Cost after 296087 iterations : Training Loss =  0.02810196333178557; Validation Loss = 0.034722413453467146\n",
            "Cost after 296088 iterations : Training Loss =  0.0281019532729238; Validation Loss = 0.03472240484943287\n",
            "Cost after 296089 iterations : Training Loss =  0.028101943214177708; Validation Loss = 0.03472239624548595\n",
            "Cost after 296090 iterations : Training Loss =  0.028101933155547543; Validation Loss = 0.03472238764162601\n",
            "Cost after 296091 iterations : Training Loss =  0.028101923097033032; Validation Loss = 0.03472237903785337\n",
            "Cost after 296092 iterations : Training Loss =  0.0281019130386344; Validation Loss = 0.034722370434167746\n",
            "Cost after 296093 iterations : Training Loss =  0.028101902980351482; Validation Loss = 0.034722361830568906\n",
            "Cost after 296094 iterations : Training Loss =  0.02810189292218431; Validation Loss = 0.034722353227057405\n",
            "Cost after 296095 iterations : Training Loss =  0.02810188286413297; Validation Loss = 0.0347223446236331\n",
            "Cost after 296096 iterations : Training Loss =  0.028101872806197352; Validation Loss = 0.034722336020296\n",
            "Cost after 296097 iterations : Training Loss =  0.028101862748377508; Validation Loss = 0.03472232741704554\n",
            "Cost after 296098 iterations : Training Loss =  0.028101852690673366; Validation Loss = 0.0347223188138824\n",
            "Cost after 296099 iterations : Training Loss =  0.028101842633085083; Validation Loss = 0.03472231021080612\n",
            "Cost after 296100 iterations : Training Loss =  0.02810183257561252; Validation Loss = 0.03472230160781729\n",
            "Cost after 296101 iterations : Training Loss =  0.028101822518255774; Validation Loss = 0.03472229300491535\n",
            "Cost after 296102 iterations : Training Loss =  0.028101812461014748; Validation Loss = 0.03472228440210048\n",
            "Cost after 296103 iterations : Training Loss =  0.0281018024038895; Validation Loss = 0.03472227579937282\n",
            "Cost after 296104 iterations : Training Loss =  0.028101792346879956; Validation Loss = 0.03472226719673205\n",
            "Cost after 296105 iterations : Training Loss =  0.02810178228998623; Validation Loss = 0.034722258594178504\n",
            "Cost after 296106 iterations : Training Loss =  0.028101772233208214; Validation Loss = 0.03472224999171183\n",
            "Cost after 296107 iterations : Training Loss =  0.028101762176545923; Validation Loss = 0.034722241389332394\n",
            "Cost after 296108 iterations : Training Loss =  0.028101752119999452; Validation Loss = 0.03472223278703992\n",
            "Cost after 296109 iterations : Training Loss =  0.028101742063568646; Validation Loss = 0.03472222418483491\n",
            "Cost after 296110 iterations : Training Loss =  0.02810173200725363; Validation Loss = 0.034722215582716556\n",
            "Cost after 296111 iterations : Training Loss =  0.028101721951054422; Validation Loss = 0.03472220698068514\n",
            "Cost after 296112 iterations : Training Loss =  0.028101711894970825; Validation Loss = 0.034722198378740965\n",
            "Cost after 296113 iterations : Training Loss =  0.028101701839003068; Validation Loss = 0.0347221897768838\n",
            "Cost after 296114 iterations : Training Loss =  0.028101691783151053; Validation Loss = 0.03472218117511393\n",
            "Cost after 296115 iterations : Training Loss =  0.02810168172741477; Validation Loss = 0.03472217257343082\n",
            "Cost after 296116 iterations : Training Loss =  0.028101671671794223; Validation Loss = 0.034722163971834925\n",
            "Cost after 296117 iterations : Training Loss =  0.028101661616289346; Validation Loss = 0.034722155370325726\n",
            "Cost after 296118 iterations : Training Loss =  0.028101651560900193; Validation Loss = 0.03472214676890427\n",
            "Cost after 296119 iterations : Training Loss =  0.028101641505626784; Validation Loss = 0.03472213816756938\n",
            "Cost after 296120 iterations : Training Loss =  0.028101631450469133; Validation Loss = 0.03472212956632176\n",
            "Cost after 296121 iterations : Training Loss =  0.028101621395427254; Validation Loss = 0.034722120965161016\n",
            "Cost after 296122 iterations : Training Loss =  0.028101611340501098; Validation Loss = 0.03472211236408727\n",
            "Cost after 296123 iterations : Training Loss =  0.028101601285690687; Validation Loss = 0.03472210376310057\n",
            "Cost after 296124 iterations : Training Loss =  0.028101591230995912; Validation Loss = 0.03472209516220124\n",
            "Cost after 296125 iterations : Training Loss =  0.02810158117641687; Validation Loss = 0.03472208656138889\n",
            "Cost after 296126 iterations : Training Loss =  0.028101571121953654; Validation Loss = 0.034722077960663525\n",
            "Cost after 296127 iterations : Training Loss =  0.02810156106760593; Validation Loss = 0.03472206936002527\n",
            "Cost after 296128 iterations : Training Loss =  0.028101551013374075; Validation Loss = 0.034722060759474\n",
            "Cost after 296129 iterations : Training Loss =  0.02810154095925791; Validation Loss = 0.03472205215900981\n",
            "Cost after 296130 iterations : Training Loss =  0.02810153090525745; Validation Loss = 0.03472204355863282\n",
            "Cost after 296131 iterations : Training Loss =  0.028101520851372706; Validation Loss = 0.03472203495834267\n",
            "Cost after 296132 iterations : Training Loss =  0.028101510797603674; Validation Loss = 0.034722026358139486\n",
            "Cost after 296133 iterations : Training Loss =  0.02810150074395035; Validation Loss = 0.03472201775802341\n",
            "Cost after 296134 iterations : Training Loss =  0.028101490690412775; Validation Loss = 0.03472200915799418\n",
            "Cost after 296135 iterations : Training Loss =  0.028101480636990913; Validation Loss = 0.034722000558052434\n",
            "Cost after 296136 iterations : Training Loss =  0.028101470583684753; Validation Loss = 0.03472199195819729\n",
            "Cost after 296137 iterations : Training Loss =  0.02810146053049417; Validation Loss = 0.03472198335842924\n",
            "Cost after 296138 iterations : Training Loss =  0.02810145047741931; Validation Loss = 0.03472197475874826\n",
            "Cost after 296139 iterations : Training Loss =  0.0281014404244602; Validation Loss = 0.03472196615915462\n",
            "Cost after 296140 iterations : Training Loss =  0.02810143037161678; Validation Loss = 0.03472195755964762\n",
            "Cost after 296141 iterations : Training Loss =  0.028101420318889094; Validation Loss = 0.03472194896022783\n",
            "Cost after 296142 iterations : Training Loss =  0.028101410266277167; Validation Loss = 0.03472194036089494\n",
            "Cost after 296143 iterations : Training Loss =  0.028101400213780724; Validation Loss = 0.03472193176164932\n",
            "Cost after 296144 iterations : Training Loss =  0.028101390161400137; Validation Loss = 0.03472192316249066\n",
            "Cost after 296145 iterations : Training Loss =  0.028101380109135116; Validation Loss = 0.0347219145634189\n",
            "Cost after 296146 iterations : Training Loss =  0.028101370056985812; Validation Loss = 0.0347219059644345\n",
            "Cost after 296147 iterations : Training Loss =  0.028101360004952267; Validation Loss = 0.03472189736553648\n",
            "Cost after 296148 iterations : Training Loss =  0.028101349953034364; Validation Loss = 0.03472188876672579\n",
            "Cost after 296149 iterations : Training Loss =  0.02810133990123219; Validation Loss = 0.03472188016800206\n",
            "Cost after 296150 iterations : Training Loss =  0.028101329849545643; Validation Loss = 0.0347218715693653\n",
            "Cost after 296151 iterations : Training Loss =  0.02810131979797482; Validation Loss = 0.034721862970815795\n",
            "Cost after 296152 iterations : Training Loss =  0.028101309746519566; Validation Loss = 0.03472185437235346\n",
            "Cost after 296153 iterations : Training Loss =  0.02810129969518004; Validation Loss = 0.03472184577397785\n",
            "Cost after 296154 iterations : Training Loss =  0.028101289643956147; Validation Loss = 0.0347218371756896\n",
            "Cost after 296155 iterations : Training Loss =  0.028101279592848104; Validation Loss = 0.034721828577488026\n",
            "Cost after 296156 iterations : Training Loss =  0.02810126954185559; Validation Loss = 0.034721819979373326\n",
            "Cost after 296157 iterations : Training Loss =  0.02810125949097876; Validation Loss = 0.03472181138134604\n",
            "Cost after 296158 iterations : Training Loss =  0.028101249440217517; Validation Loss = 0.03472180278340568\n",
            "Cost after 296159 iterations : Training Loss =  0.028101239389571986; Validation Loss = 0.0347217941855518\n",
            "Cost after 296160 iterations : Training Loss =  0.028101229339042286; Validation Loss = 0.034721785587785284\n",
            "Cost after 296161 iterations : Training Loss =  0.02810121928862805; Validation Loss = 0.034721776990105625\n",
            "Cost after 296162 iterations : Training Loss =  0.028101209238329546; Validation Loss = 0.03472176839251317\n",
            "Cost after 296163 iterations : Training Loss =  0.02810119918814669; Validation Loss = 0.03472175979500799\n",
            "Cost after 296164 iterations : Training Loss =  0.028101189138079574; Validation Loss = 0.03472175119758916\n",
            "Cost after 296165 iterations : Training Loss =  0.028101179088128016; Validation Loss = 0.03472174260025745\n",
            "Cost after 296166 iterations : Training Loss =  0.02810116903829193; Validation Loss = 0.03472173400301316\n",
            "Cost after 296167 iterations : Training Loss =  0.028101158988571868; Validation Loss = 0.034721725405855636\n",
            "Cost after 296168 iterations : Training Loss =  0.0281011489389672; Validation Loss = 0.03472171680878518\n",
            "Cost after 296169 iterations : Training Loss =  0.028101138889478287; Validation Loss = 0.03472170821180167\n",
            "Cost after 296170 iterations : Training Loss =  0.028101128840105023; Validation Loss = 0.03472169961490513\n",
            "Cost after 296171 iterations : Training Loss =  0.028101118790847323; Validation Loss = 0.034721691018095455\n",
            "Cost after 296172 iterations : Training Loss =  0.028101108741705298; Validation Loss = 0.03472168242137273\n",
            "Cost after 296173 iterations : Training Loss =  0.028101098692678995; Validation Loss = 0.03472167382473729\n",
            "Cost after 296174 iterations : Training Loss =  0.02810108864376824; Validation Loss = 0.034721665228188904\n",
            "Cost after 296175 iterations : Training Loss =  0.0281010785949731; Validation Loss = 0.03472165663172721\n",
            "Cost after 296176 iterations : Training Loss =  0.028101068546293732; Validation Loss = 0.0347216480353527\n",
            "Cost after 296177 iterations : Training Loss =  0.028101058497729856; Validation Loss = 0.03472163943906486\n",
            "Cost after 296178 iterations : Training Loss =  0.028101048449281678; Validation Loss = 0.034721630842864254\n",
            "Cost after 296179 iterations : Training Loss =  0.028101038400949123; Validation Loss = 0.03472162224675071\n",
            "Cost after 296180 iterations : Training Loss =  0.02810102835273216; Validation Loss = 0.03472161365072404\n",
            "Cost after 296181 iterations : Training Loss =  0.028101018304630927; Validation Loss = 0.034721605054784284\n",
            "Cost after 296182 iterations : Training Loss =  0.028101008256645316; Validation Loss = 0.03472159645893173\n",
            "Cost after 296183 iterations : Training Loss =  0.028100998208775242; Validation Loss = 0.03472158786316593\n",
            "Cost after 296184 iterations : Training Loss =  0.028100988161020787; Validation Loss = 0.03472157926748705\n",
            "Cost after 296185 iterations : Training Loss =  0.028100978113381987; Validation Loss = 0.034721570671895303\n",
            "Cost after 296186 iterations : Training Loss =  0.028100968065858857; Validation Loss = 0.03472156207639045\n",
            "Cost after 296187 iterations : Training Loss =  0.028100958018451274; Validation Loss = 0.03472155348097287\n",
            "Cost after 296188 iterations : Training Loss =  0.028100947971159276; Validation Loss = 0.03472154488564207\n",
            "Cost after 296189 iterations : Training Loss =  0.028100937923982956; Validation Loss = 0.03472153629039857\n",
            "Cost after 296190 iterations : Training Loss =  0.02810092787692228; Validation Loss = 0.034721527695241464\n",
            "Cost after 296191 iterations : Training Loss =  0.0281009178299771; Validation Loss = 0.034721519100171655\n",
            "Cost after 296192 iterations : Training Loss =  0.028100907783147715; Validation Loss = 0.03472151050518861\n",
            "Cost after 296193 iterations : Training Loss =  0.028100897736433816; Validation Loss = 0.03472150191029289\n",
            "Cost after 296194 iterations : Training Loss =  0.028100887689835492; Validation Loss = 0.03472149331548382\n",
            "Cost after 296195 iterations : Training Loss =  0.028100877643352732; Validation Loss = 0.034721484720761996\n",
            "Cost after 296196 iterations : Training Loss =  0.028100867596985754; Validation Loss = 0.03472147612612697\n",
            "Cost after 296197 iterations : Training Loss =  0.028100857550734243; Validation Loss = 0.03472146753157879\n",
            "Cost after 296198 iterations : Training Loss =  0.028100847504598382; Validation Loss = 0.03472145893711755\n",
            "Cost after 296199 iterations : Training Loss =  0.0281008374585782; Validation Loss = 0.03472145034274328\n",
            "Cost after 296200 iterations : Training Loss =  0.02810082741267348; Validation Loss = 0.03472144174845607\n",
            "Cost after 296201 iterations : Training Loss =  0.028100817366884405; Validation Loss = 0.034721433154255625\n",
            "Cost after 296202 iterations : Training Loss =  0.0281008073212109; Validation Loss = 0.03472142456014242\n",
            "Cost after 296203 iterations : Training Loss =  0.028100797275653067; Validation Loss = 0.03472141596611621\n",
            "Cost after 296204 iterations : Training Loss =  0.028100787230210773; Validation Loss = 0.034721407372176646\n",
            "Cost after 296205 iterations : Training Loss =  0.02810077718488401; Validation Loss = 0.03472139877832412\n",
            "Cost after 296206 iterations : Training Loss =  0.028100767139672873; Validation Loss = 0.03472139018455866\n",
            "Cost after 296207 iterations : Training Loss =  0.028100757094577326; Validation Loss = 0.03472138159087999\n",
            "Cost after 296208 iterations : Training Loss =  0.028100747049597384; Validation Loss = 0.03472137299728852\n",
            "Cost after 296209 iterations : Training Loss =  0.02810073700473303; Validation Loss = 0.034721364403783664\n",
            "Cost after 296210 iterations : Training Loss =  0.028100726959984328; Validation Loss = 0.0347213558103662\n",
            "Cost after 296211 iterations : Training Loss =  0.028100716915351084; Validation Loss = 0.03472134721703529\n",
            "Cost after 296212 iterations : Training Loss =  0.028100706870833384; Validation Loss = 0.03472133862379159\n",
            "Cost after 296213 iterations : Training Loss =  0.028100696826431192; Validation Loss = 0.03472133003063488\n",
            "Cost after 296214 iterations : Training Loss =  0.02810068678214482; Validation Loss = 0.03472132143756487\n",
            "Cost after 296215 iterations : Training Loss =  0.02810067673797388; Validation Loss = 0.03472131284458188\n",
            "Cost after 296216 iterations : Training Loss =  0.028100666693918613; Validation Loss = 0.03472130425168551\n",
            "Cost after 296217 iterations : Training Loss =  0.02810065664997885; Validation Loss = 0.03472129565887645\n",
            "Cost after 296218 iterations : Training Loss =  0.028100646606154658; Validation Loss = 0.0347212870661544\n",
            "Cost after 296219 iterations : Training Loss =  0.028100636562445904; Validation Loss = 0.0347212784735191\n",
            "Cost after 296220 iterations : Training Loss =  0.028100626518852846; Validation Loss = 0.03472126988097118\n",
            "Cost after 296221 iterations : Training Loss =  0.028100616475375345; Validation Loss = 0.0347212612885096\n",
            "Cost after 296222 iterations : Training Loss =  0.028100606432013328; Validation Loss = 0.03472125269613507\n",
            "Cost after 296223 iterations : Training Loss =  0.028100596388766896; Validation Loss = 0.0347212441038477\n",
            "Cost after 296224 iterations : Training Loss =  0.028100586345636103; Validation Loss = 0.03472123551164719\n",
            "Cost after 296225 iterations : Training Loss =  0.02810057630262081; Validation Loss = 0.03472122691953349\n",
            "Cost after 296226 iterations : Training Loss =  0.028100566259721092; Validation Loss = 0.034721218327506374\n",
            "Cost after 296227 iterations : Training Loss =  0.028100556216936932; Validation Loss = 0.03472120973556671\n",
            "Cost after 296228 iterations : Training Loss =  0.028100546174268236; Validation Loss = 0.03472120114371379\n",
            "Cost after 296229 iterations : Training Loss =  0.028100536131715204; Validation Loss = 0.03472119255194785\n",
            "Cost after 296230 iterations : Training Loss =  0.02810052608927774; Validation Loss = 0.0347211839602689\n",
            "Cost after 296231 iterations : Training Loss =  0.028100516046955706; Validation Loss = 0.034721175368676825\n",
            "Cost after 296232 iterations : Training Loss =  0.02810050600474914; Validation Loss = 0.0347211667771715\n",
            "Cost after 296233 iterations : Training Loss =  0.028100495962658345; Validation Loss = 0.034721158185753186\n",
            "Cost after 296234 iterations : Training Loss =  0.028100485920683052; Validation Loss = 0.034721149594422115\n",
            "Cost after 296235 iterations : Training Loss =  0.02810047587882311; Validation Loss = 0.03472114100317765\n",
            "Cost after 296236 iterations : Training Loss =  0.028100465837078873; Validation Loss = 0.03472113241202012\n",
            "Cost after 296237 iterations : Training Loss =  0.028100455795450185; Validation Loss = 0.03472112382094932\n",
            "Cost after 296238 iterations : Training Loss =  0.028100445753936915; Validation Loss = 0.034721115229965537\n",
            "Cost after 296239 iterations : Training Loss =  0.028100435712539267; Validation Loss = 0.03472110663906896\n",
            "Cost after 296240 iterations : Training Loss =  0.02810042567125707; Validation Loss = 0.034721098048259076\n",
            "Cost after 296241 iterations : Training Loss =  0.02810041563009047; Validation Loss = 0.03472108945753593\n",
            "Cost after 296242 iterations : Training Loss =  0.0281004055890394; Validation Loss = 0.03472108086689983\n",
            "Cost after 296243 iterations : Training Loss =  0.02810039554810376; Validation Loss = 0.034721072276350726\n",
            "Cost after 296244 iterations : Training Loss =  0.028100385507283755; Validation Loss = 0.034721063685888454\n",
            "Cost after 296245 iterations : Training Loss =  0.02810037546657922; Validation Loss = 0.034721055095513036\n",
            "Cost after 296246 iterations : Training Loss =  0.028100365425990315; Validation Loss = 0.03472104650522447\n",
            "Cost after 296247 iterations : Training Loss =  0.028100355385516723; Validation Loss = 0.034721037915023234\n",
            "Cost after 296248 iterations : Training Loss =  0.02810034534515876; Validation Loss = 0.034721029324908476\n",
            "Cost after 296249 iterations : Training Loss =  0.028100335304916367; Validation Loss = 0.03472102073488086\n",
            "Cost after 296250 iterations : Training Loss =  0.02810032526478943; Validation Loss = 0.03472101214494005\n",
            "Cost after 296251 iterations : Training Loss =  0.0281003152247779; Validation Loss = 0.03472100355508585\n",
            "Cost after 296252 iterations : Training Loss =  0.0281003051848821; Validation Loss = 0.034720994965318754\n",
            "Cost after 296253 iterations : Training Loss =  0.028100295145101675; Validation Loss = 0.03472098637563873\n",
            "Cost after 296254 iterations : Training Loss =  0.02810028510543669; Validation Loss = 0.034720977786045375\n",
            "Cost after 296255 iterations : Training Loss =  0.02810027506588745; Validation Loss = 0.03472096919653924\n",
            "Cost after 296256 iterations : Training Loss =  0.028100265026453495; Validation Loss = 0.03472096060711958\n",
            "Cost after 296257 iterations : Training Loss =  0.028100254987135132; Validation Loss = 0.03472095201778698\n",
            "Cost after 296258 iterations : Training Loss =  0.028100244947932156; Validation Loss = 0.03472094342854136\n",
            "Cost after 296259 iterations : Training Loss =  0.028100234908844883; Validation Loss = 0.03472093483938263\n",
            "Cost after 296260 iterations : Training Loss =  0.028100224869872993; Validation Loss = 0.03472092625031052\n",
            "Cost after 296261 iterations : Training Loss =  0.028100214831016518; Validation Loss = 0.03472091766132551\n",
            "Cost after 296262 iterations : Training Loss =  0.028100204792275752; Validation Loss = 0.03472090907242725\n",
            "Cost after 296263 iterations : Training Loss =  0.028100194753650276; Validation Loss = 0.03472090048361584\n",
            "Cost after 296264 iterations : Training Loss =  0.02810018471514034; Validation Loss = 0.03472089189489167\n",
            "Cost after 296265 iterations : Training Loss =  0.028100174676746043; Validation Loss = 0.034720883306254065\n",
            "Cost after 296266 iterations : Training Loss =  0.028100164638467057; Validation Loss = 0.03472087471770334\n",
            "Cost after 296267 iterations : Training Loss =  0.02810015460030365; Validation Loss = 0.0347208661292396\n",
            "Cost after 296268 iterations : Training Loss =  0.028100144562255673; Validation Loss = 0.03472085754086273\n",
            "Cost after 296269 iterations : Training Loss =  0.028100134524323143; Validation Loss = 0.034720848952572596\n",
            "Cost after 296270 iterations : Training Loss =  0.02810012448650621; Validation Loss = 0.03472084036436949\n",
            "Cost after 296271 iterations : Training Loss =  0.028100114448804663; Validation Loss = 0.03472083177625342\n",
            "Cost after 296272 iterations : Training Loss =  0.028100104411218647; Validation Loss = 0.034720823188224004\n",
            "Cost after 296273 iterations : Training Loss =  0.028100094373748084; Validation Loss = 0.03472081460028165\n",
            "Cost after 296274 iterations : Training Loss =  0.02810008433639298; Validation Loss = 0.03472080601242563\n",
            "Cost after 296275 iterations : Training Loss =  0.028100074299153212; Validation Loss = 0.034720797424656696\n",
            "Cost after 296276 iterations : Training Loss =  0.028100064262029198; Validation Loss = 0.03472078883697488\n",
            "Cost after 296277 iterations : Training Loss =  0.028100054225020547; Validation Loss = 0.03472078024937976\n",
            "Cost after 296278 iterations : Training Loss =  0.028100044188127293; Validation Loss = 0.03472077166187199\n",
            "Cost after 296279 iterations : Training Loss =  0.028100034151349623; Validation Loss = 0.034720763074450524\n",
            "Cost after 296280 iterations : Training Loss =  0.028100024114687334; Validation Loss = 0.03472075448711599\n",
            "Cost after 296281 iterations : Training Loss =  0.028100014078140508; Validation Loss = 0.034720745899868406\n",
            "Cost after 296282 iterations : Training Loss =  0.028100004041709148; Validation Loss = 0.034720737312707785\n",
            "Cost after 296283 iterations : Training Loss =  0.028099994005393293; Validation Loss = 0.034720728725633844\n",
            "Cost after 296284 iterations : Training Loss =  0.028099983969192832; Validation Loss = 0.03472072013864687\n",
            "Cost after 296285 iterations : Training Loss =  0.028099973933107848; Validation Loss = 0.03472071155174686\n",
            "Cost after 296286 iterations : Training Loss =  0.02809996389713833; Validation Loss = 0.034720702964933334\n",
            "Cost after 296287 iterations : Training Loss =  0.028099953861284367; Validation Loss = 0.03472069437820693\n",
            "Cost after 296288 iterations : Training Loss =  0.028099943825545665; Validation Loss = 0.03472068579156727\n",
            "Cost after 296289 iterations : Training Loss =  0.028099933789922555; Validation Loss = 0.03472067720501465\n",
            "Cost after 296290 iterations : Training Loss =  0.02809992375441482; Validation Loss = 0.0347206686185486\n",
            "Cost after 296291 iterations : Training Loss =  0.028099913719022544; Validation Loss = 0.034720660032169774\n",
            "Cost after 296292 iterations : Training Loss =  0.028099903683745656; Validation Loss = 0.03472065144587761\n",
            "Cost after 296293 iterations : Training Loss =  0.0280998936485843; Validation Loss = 0.0347206428596723\n",
            "Cost after 296294 iterations : Training Loss =  0.028099883613538417; Validation Loss = 0.03472063427355366\n",
            "Cost after 296295 iterations : Training Loss =  0.02809987357860786; Validation Loss = 0.03472062568752229\n",
            "Cost after 296296 iterations : Training Loss =  0.02809986354379274; Validation Loss = 0.03472061710157753\n",
            "Cost after 296297 iterations : Training Loss =  0.028099853509093303; Validation Loss = 0.03472060851571956\n",
            "Cost after 296298 iterations : Training Loss =  0.02809984347450897; Validation Loss = 0.03472059992994868\n",
            "Cost after 296299 iterations : Training Loss =  0.02809983344004031; Validation Loss = 0.0347205913442642\n",
            "Cost after 296300 iterations : Training Loss =  0.028099823405686838; Validation Loss = 0.03472058275866655\n",
            "Cost after 296301 iterations : Training Loss =  0.02809981337144901; Validation Loss = 0.03472057417315587\n",
            "Cost after 296302 iterations : Training Loss =  0.028099803337326566; Validation Loss = 0.03472056558773217\n",
            "Cost after 296303 iterations : Training Loss =  0.02809979330331946; Validation Loss = 0.034720557002395264\n",
            "Cost after 296304 iterations : Training Loss =  0.028099783269427886; Validation Loss = 0.03472054841714494\n",
            "Cost after 296305 iterations : Training Loss =  0.028099773235651652; Validation Loss = 0.03472053983198176\n",
            "Cost after 296306 iterations : Training Loss =  0.02809976320199094; Validation Loss = 0.03472053124690546\n",
            "Cost after 296307 iterations : Training Loss =  0.02809975316844552; Validation Loss = 0.03472052266191588\n",
            "Cost after 296308 iterations : Training Loss =  0.0280997431350157; Validation Loss = 0.03472051407701312\n",
            "Cost after 296309 iterations : Training Loss =  0.02809973310170118; Validation Loss = 0.03472050549219708\n",
            "Cost after 296310 iterations : Training Loss =  0.028099723068502024; Validation Loss = 0.034720496907468236\n",
            "Cost after 296311 iterations : Training Loss =  0.02809971303541834; Validation Loss = 0.03472048832282574\n",
            "Cost after 296312 iterations : Training Loss =  0.028099703002450044; Validation Loss = 0.03472047973827005\n",
            "Cost after 296313 iterations : Training Loss =  0.028099692969597325; Validation Loss = 0.03472047115380147\n",
            "Cost after 296314 iterations : Training Loss =  0.028099682936859757; Validation Loss = 0.034720462569419616\n",
            "Cost after 296315 iterations : Training Loss =  0.028099672904237694; Validation Loss = 0.03472045398512476\n",
            "Cost after 296316 iterations : Training Loss =  0.028099662871731; Validation Loss = 0.034720445400916504\n",
            "Cost after 296317 iterations : Training Loss =  0.02809965283933991; Validation Loss = 0.03472043681679526\n",
            "Cost after 296318 iterations : Training Loss =  0.028099642807064066; Validation Loss = 0.03472042823276032\n",
            "Cost after 296319 iterations : Training Loss =  0.028099632774903543; Validation Loss = 0.03472041964881272\n",
            "Cost after 296320 iterations : Training Loss =  0.028099622742858547; Validation Loss = 0.03472041106495187\n",
            "Cost after 296321 iterations : Training Loss =  0.028099612710928853; Validation Loss = 0.034720402481177755\n",
            "Cost after 296322 iterations : Training Loss =  0.028099602679114623; Validation Loss = 0.03472039389749065\n",
            "Cost after 296323 iterations : Training Loss =  0.028099592647415798; Validation Loss = 0.03472038531389007\n",
            "Cost after 296324 iterations : Training Loss =  0.02809958261583232; Validation Loss = 0.03472037673037646\n",
            "Cost after 296325 iterations : Training Loss =  0.02809957258436421; Validation Loss = 0.03472036814694991\n",
            "Cost after 296326 iterations : Training Loss =  0.02809956255301166; Validation Loss = 0.034720359563609596\n",
            "Cost after 296327 iterations : Training Loss =  0.02809955252177428; Validation Loss = 0.034720350980356265\n",
            "Cost after 296328 iterations : Training Loss =  0.028099542490652336; Validation Loss = 0.03472034239718973\n",
            "Cost after 296329 iterations : Training Loss =  0.02809953245964587; Validation Loss = 0.03472033381411003\n",
            "Cost after 296330 iterations : Training Loss =  0.028099522428754777; Validation Loss = 0.03472032523111714\n",
            "Cost after 296331 iterations : Training Loss =  0.02809951239797908; Validation Loss = 0.034720316648210806\n",
            "Cost after 296332 iterations : Training Loss =  0.028099502367318622; Validation Loss = 0.03472030806539175\n",
            "Cost after 296333 iterations : Training Loss =  0.02809949233677362; Validation Loss = 0.034720299482659395\n",
            "Cost after 296334 iterations : Training Loss =  0.02809948230634396; Validation Loss = 0.03472029090001361\n",
            "Cost after 296335 iterations : Training Loss =  0.02809947227602968; Validation Loss = 0.034720282317454935\n",
            "Cost after 296336 iterations : Training Loss =  0.028099462245830882; Validation Loss = 0.03472027373498273\n",
            "Cost after 296337 iterations : Training Loss =  0.028099452215747233; Validation Loss = 0.03472026515259749\n",
            "Cost after 296338 iterations : Training Loss =  0.02809944218577915; Validation Loss = 0.03472025657029903\n",
            "Cost after 296339 iterations : Training Loss =  0.028099432155926387; Validation Loss = 0.034720247988087295\n",
            "Cost after 296340 iterations : Training Loss =  0.028099422126189003; Validation Loss = 0.03472023940596253\n",
            "Cost after 296341 iterations : Training Loss =  0.028099412096566998; Validation Loss = 0.0347202308239244\n",
            "Cost after 296342 iterations : Training Loss =  0.02809940206706019; Validation Loss = 0.03472022224197292\n",
            "Cost after 296343 iterations : Training Loss =  0.02809939203766893; Validation Loss = 0.03472021366010843\n",
            "Cost after 296344 iterations : Training Loss =  0.02809938200839301; Validation Loss = 0.034720205078330645\n",
            "Cost after 296345 iterations : Training Loss =  0.028099371979232318; Validation Loss = 0.03472019649663959\n",
            "Cost after 296346 iterations : Training Loss =  0.028099361950187123; Validation Loss = 0.03472018791503552\n",
            "Cost after 296347 iterations : Training Loss =  0.028099351921257273; Validation Loss = 0.034720179333518265\n",
            "Cost after 296348 iterations : Training Loss =  0.028099341892442686; Validation Loss = 0.034720170752087935\n",
            "Cost after 296349 iterations : Training Loss =  0.028099331863743544; Validation Loss = 0.034720162170744015\n",
            "Cost after 296350 iterations : Training Loss =  0.028099321835159743; Validation Loss = 0.034720153589487206\n",
            "Cost after 296351 iterations : Training Loss =  0.028099311806691177; Validation Loss = 0.034720145008317015\n",
            "Cost after 296352 iterations : Training Loss =  0.028099301778338002; Validation Loss = 0.03472013642723336\n",
            "Cost after 296353 iterations : Training Loss =  0.02809929175010027; Validation Loss = 0.034720127846236876\n",
            "Cost after 296354 iterations : Training Loss =  0.028099281721977763; Validation Loss = 0.03472011926532703\n",
            "Cost after 296355 iterations : Training Loss =  0.028099271693970716; Validation Loss = 0.034720110684503765\n",
            "Cost after 296356 iterations : Training Loss =  0.028099261666078856; Validation Loss = 0.03472010210376774\n",
            "Cost after 296357 iterations : Training Loss =  0.028099251638302465; Validation Loss = 0.03472009352311813\n",
            "Cost after 296358 iterations : Training Loss =  0.028099241610641364; Validation Loss = 0.034720084942555345\n",
            "Cost after 296359 iterations : Training Loss =  0.028099231583095588; Validation Loss = 0.034720076362079286\n",
            "Cost after 296360 iterations : Training Loss =  0.028099221555665063; Validation Loss = 0.034720067781689845\n",
            "Cost after 296361 iterations : Training Loss =  0.02809921152834999; Validation Loss = 0.03472005920138747\n",
            "Cost after 296362 iterations : Training Loss =  0.028099201501150237; Validation Loss = 0.03472005062117131\n",
            "Cost after 296363 iterations : Training Loss =  0.028099191474065827; Validation Loss = 0.03472004204104266\n",
            "Cost after 296364 iterations : Training Loss =  0.02809918144709665; Validation Loss = 0.034720033461000395\n",
            "Cost after 296365 iterations : Training Loss =  0.02809917142024283; Validation Loss = 0.0347200248810448\n",
            "Cost after 296366 iterations : Training Loss =  0.02809916139350443; Validation Loss = 0.03472001630117623\n",
            "Cost after 296367 iterations : Training Loss =  0.02809915136688126; Validation Loss = 0.034720007721394344\n",
            "Cost after 296368 iterations : Training Loss =  0.02809914134037332; Validation Loss = 0.034719999141698904\n",
            "Cost after 296369 iterations : Training Loss =  0.02809913131398087; Validation Loss = 0.03471999056209012\n",
            "Cost after 296370 iterations : Training Loss =  0.028099121287703782; Validation Loss = 0.03471998198256864\n",
            "Cost after 296371 iterations : Training Loss =  0.028099111261541746; Validation Loss = 0.034719973403133916\n",
            "Cost after 296372 iterations : Training Loss =  0.02809910123549516; Validation Loss = 0.03471996482378584\n",
            "Cost after 296373 iterations : Training Loss =  0.02809909120956401; Validation Loss = 0.03471995624452426\n",
            "Cost after 296374 iterations : Training Loss =  0.028099081183747977; Validation Loss = 0.03471994766534946\n",
            "Cost after 296375 iterations : Training Loss =  0.02809907115804732; Validation Loss = 0.0347199390862615\n",
            "Cost after 296376 iterations : Training Loss =  0.02809906113246198; Validation Loss = 0.03471993050726015\n",
            "Cost after 296377 iterations : Training Loss =  0.028099051106992; Validation Loss = 0.034719921928345596\n",
            "Cost after 296378 iterations : Training Loss =  0.02809904108163727; Validation Loss = 0.034719913349518045\n",
            "Cost after 296379 iterations : Training Loss =  0.028099031056397803; Validation Loss = 0.03471990477077707\n",
            "Cost after 296380 iterations : Training Loss =  0.02809902103127375; Validation Loss = 0.03471989619212258\n",
            "Cost after 296381 iterations : Training Loss =  0.028099011006264857; Validation Loss = 0.03471988761355519\n",
            "Cost after 296382 iterations : Training Loss =  0.028099000981371296; Validation Loss = 0.03471987903507459\n",
            "Cost after 296383 iterations : Training Loss =  0.028098990956593083; Validation Loss = 0.03471987045668063\n",
            "Cost after 296384 iterations : Training Loss =  0.028098980931930223; Validation Loss = 0.034719861878373344\n",
            "Cost after 296385 iterations : Training Loss =  0.028098970907382583; Validation Loss = 0.034719853300152786\n",
            "Cost after 296386 iterations : Training Loss =  0.028098960882950108; Validation Loss = 0.034719844722019\n",
            "Cost after 296387 iterations : Training Loss =  0.02809895085863309; Validation Loss = 0.03471983614397211\n",
            "Cost after 296388 iterations : Training Loss =  0.028098940834431346; Validation Loss = 0.03471982756601174\n",
            "Cost after 296389 iterations : Training Loss =  0.028098930810344702; Validation Loss = 0.03471981898813812\n",
            "Cost after 296390 iterations : Training Loss =  0.028098920786373563; Validation Loss = 0.034719810410351465\n",
            "Cost after 296391 iterations : Training Loss =  0.028098910762517637; Validation Loss = 0.034719801832651165\n",
            "Cost after 296392 iterations : Training Loss =  0.028098900738777; Validation Loss = 0.03471979325503793\n",
            "Cost after 296393 iterations : Training Loss =  0.02809889071515162; Validation Loss = 0.03471978467751115\n",
            "Cost after 296394 iterations : Training Loss =  0.028098880691641463; Validation Loss = 0.03471977610007109\n",
            "Cost after 296395 iterations : Training Loss =  0.02809887066824669; Validation Loss = 0.0347197675227179\n",
            "Cost after 296396 iterations : Training Loss =  0.028098860644967076; Validation Loss = 0.03471975894545134\n",
            "Cost after 296397 iterations : Training Loss =  0.02809885062180281; Validation Loss = 0.03471975036827164\n",
            "Cost after 296398 iterations : Training Loss =  0.028098840598753838; Validation Loss = 0.03471974179117847\n",
            "Cost after 296399 iterations : Training Loss =  0.0280988305758201; Validation Loss = 0.034719733214171725\n",
            "Cost after 296400 iterations : Training Loss =  0.028098820553001513; Validation Loss = 0.034719724637252564\n",
            "Cost after 296401 iterations : Training Loss =  0.028098810530298387; Validation Loss = 0.03471971606041958\n",
            "Cost after 296402 iterations : Training Loss =  0.0280988005077104; Validation Loss = 0.03471970748367356\n",
            "Cost after 296403 iterations : Training Loss =  0.028098790485237734; Validation Loss = 0.034719698907013846\n",
            "Cost after 296404 iterations : Training Loss =  0.028098780462880252; Validation Loss = 0.0347196903304413\n",
            "Cost after 296405 iterations : Training Loss =  0.028098770440638077; Validation Loss = 0.03471968175395532\n",
            "Cost after 296406 iterations : Training Loss =  0.02809876041851118; Validation Loss = 0.034719673177555976\n",
            "Cost after 296407 iterations : Training Loss =  0.028098750396499634; Validation Loss = 0.0347196646012434\n",
            "Cost after 296408 iterations : Training Loss =  0.028098740374603116; Validation Loss = 0.034719656025017545\n",
            "Cost after 296409 iterations : Training Loss =  0.028098730352822027; Validation Loss = 0.03471964744887845\n",
            "Cost after 296410 iterations : Training Loss =  0.02809872033115612; Validation Loss = 0.034719638872825975\n",
            "Cost after 296411 iterations : Training Loss =  0.028098710309605517; Validation Loss = 0.0347196302968604\n",
            "Cost after 296412 iterations : Training Loss =  0.02809870028817008; Validation Loss = 0.034719621720981024\n",
            "Cost after 296413 iterations : Training Loss =  0.028098690266849923; Validation Loss = 0.034719613145188684\n",
            "Cost after 296414 iterations : Training Loss =  0.028098680245644957; Validation Loss = 0.03471960456948315\n",
            "Cost after 296415 iterations : Training Loss =  0.0280986702245553; Validation Loss = 0.03471959599386427\n",
            "Cost after 296416 iterations : Training Loss =  0.028098660203580875; Validation Loss = 0.03471958741833181\n",
            "Cost after 296417 iterations : Training Loss =  0.028098650182721643; Validation Loss = 0.03471957884288617\n",
            "Cost after 296418 iterations : Training Loss =  0.028098640161977752; Validation Loss = 0.03471957026752739\n",
            "Cost after 296419 iterations : Training Loss =  0.02809863014134905; Validation Loss = 0.03471956169225498\n",
            "Cost after 296420 iterations : Training Loss =  0.028098620120835513; Validation Loss = 0.03471955311706962\n",
            "Cost after 296421 iterations : Training Loss =  0.028098610100437238; Validation Loss = 0.034719544541970654\n",
            "Cost after 296422 iterations : Training Loss =  0.028098600080154253; Validation Loss = 0.034719535966958655\n",
            "Cost after 296423 iterations : Training Loss =  0.028098590059986488; Validation Loss = 0.03471952739203346\n",
            "Cost after 296424 iterations : Training Loss =  0.028098580039933898; Validation Loss = 0.03471951881719469\n",
            "Cost after 296425 iterations : Training Loss =  0.028098570019996553; Validation Loss = 0.0347195102424428\n",
            "Cost after 296426 iterations : Training Loss =  0.02809856000017446; Validation Loss = 0.03471950166777719\n",
            "Cost after 296427 iterations : Training Loss =  0.028098549980467468; Validation Loss = 0.03471949309319858\n",
            "Cost after 296428 iterations : Training Loss =  0.028098539960875822; Validation Loss = 0.03471948451870675\n",
            "Cost after 296429 iterations : Training Loss =  0.02809852994139934; Validation Loss = 0.0347194759443011\n",
            "Cost after 296430 iterations : Training Loss =  0.028098519922038143; Validation Loss = 0.03471946736998278\n",
            "Cost after 296431 iterations : Training Loss =  0.028098509902792147; Validation Loss = 0.03471945879575081\n",
            "Cost after 296432 iterations : Training Loss =  0.028098499883661452; Validation Loss = 0.03471945022160564\n",
            "Cost after 296433 iterations : Training Loss =  0.028098489864645717; Validation Loss = 0.03471944164754699\n",
            "Cost after 296434 iterations : Training Loss =  0.028098479845745393; Validation Loss = 0.03471943307357538\n",
            "Cost after 296435 iterations : Training Loss =  0.028098469826960293; Validation Loss = 0.03471942449969022\n",
            "Cost after 296436 iterations : Training Loss =  0.02809845980829028; Validation Loss = 0.034719415925891756\n",
            "Cost after 296437 iterations : Training Loss =  0.028098449789735493; Validation Loss = 0.03471940735218015\n",
            "Cost after 296438 iterations : Training Loss =  0.028098439771295967; Validation Loss = 0.03471939877855484\n",
            "Cost after 296439 iterations : Training Loss =  0.02809842975297153; Validation Loss = 0.03471939020501624\n",
            "Cost after 296440 iterations : Training Loss =  0.028098419734762372; Validation Loss = 0.03471938163156444\n",
            "Cost after 296441 iterations : Training Loss =  0.028098409716668483; Validation Loss = 0.03471937305819922\n",
            "Cost after 296442 iterations : Training Loss =  0.028098399698689713; Validation Loss = 0.03471936448492076\n",
            "Cost after 296443 iterations : Training Loss =  0.02809838968082624; Validation Loss = 0.0347193559117289\n",
            "Cost after 296444 iterations : Training Loss =  0.028098379663077808; Validation Loss = 0.034719347338623585\n",
            "Cost after 296445 iterations : Training Loss =  0.028098369645444685; Validation Loss = 0.034719338765605325\n",
            "Cost after 296446 iterations : Training Loss =  0.028098359627926603; Validation Loss = 0.03471933019267339\n",
            "Cost after 296447 iterations : Training Loss =  0.028098349610523814; Validation Loss = 0.03471932161982809\n",
            "Cost after 296448 iterations : Training Loss =  0.028098339593236227; Validation Loss = 0.034719313047069676\n",
            "Cost after 296449 iterations : Training Loss =  0.02809832957606381; Validation Loss = 0.03471930447439798\n",
            "Cost after 296450 iterations : Training Loss =  0.028098319559006556; Validation Loss = 0.034719295901812885\n",
            "Cost after 296451 iterations : Training Loss =  0.028098309542064576; Validation Loss = 0.03471928732931432\n",
            "Cost after 296452 iterations : Training Loss =  0.02809829952523763; Validation Loss = 0.03471927875690239\n",
            "Cost after 296453 iterations : Training Loss =  0.028098289508525994; Validation Loss = 0.03471927018457709\n",
            "Cost after 296454 iterations : Training Loss =  0.028098279491929425; Validation Loss = 0.03471926161233861\n",
            "Cost after 296455 iterations : Training Loss =  0.028098269475448117; Validation Loss = 0.034719253040186936\n",
            "Cost after 296456 iterations : Training Loss =  0.02809825945908201; Validation Loss = 0.034719244468121385\n",
            "Cost after 296457 iterations : Training Loss =  0.028098249442830917; Validation Loss = 0.03471923589614282\n",
            "Cost after 296458 iterations : Training Loss =  0.02809823942669517; Validation Loss = 0.03471922732425069\n",
            "Cost after 296459 iterations : Training Loss =  0.0280982294106746; Validation Loss = 0.03471921875244526\n",
            "Cost after 296460 iterations : Training Loss =  0.02809821939476908; Validation Loss = 0.03471921018072663\n",
            "Cost after 296461 iterations : Training Loss =  0.02809820937897871; Validation Loss = 0.03471920160909477\n",
            "Cost after 296462 iterations : Training Loss =  0.028098199363303714; Validation Loss = 0.03471919303754942\n",
            "Cost after 296463 iterations : Training Loss =  0.028098189347743552; Validation Loss = 0.03471918446609088\n",
            "Cost after 296464 iterations : Training Loss =  0.028098179332298787; Validation Loss = 0.034719175894718825\n",
            "Cost after 296465 iterations : Training Loss =  0.028098169316969082; Validation Loss = 0.034719167323433435\n",
            "Cost after 296466 iterations : Training Loss =  0.02809815930175469; Validation Loss = 0.034719158752234656\n",
            "Cost after 296467 iterations : Training Loss =  0.02809814928665522; Validation Loss = 0.034719150181122364\n",
            "Cost after 296468 iterations : Training Loss =  0.0280981392716711; Validation Loss = 0.034719141610096996\n",
            "Cost after 296469 iterations : Training Loss =  0.02809812925680206; Validation Loss = 0.03471913303915802\n",
            "Cost after 296470 iterations : Training Loss =  0.028098119242048174; Validation Loss = 0.0347191244683057\n",
            "Cost after 296471 iterations : Training Loss =  0.02809810922740943; Validation Loss = 0.03471911589754016\n",
            "Cost after 296472 iterations : Training Loss =  0.02809809921288589; Validation Loss = 0.034719107326861426\n",
            "Cost after 296473 iterations : Training Loss =  0.028098089198477356; Validation Loss = 0.03471909875626881\n",
            "Cost after 296474 iterations : Training Loss =  0.028098079184184128; Validation Loss = 0.03471909018576301\n",
            "Cost after 296475 iterations : Training Loss =  0.028098069170005978; Validation Loss = 0.03471908161534392\n",
            "Cost after 296476 iterations : Training Loss =  0.02809805915594295; Validation Loss = 0.03471907304501147\n",
            "Cost after 296477 iterations : Training Loss =  0.028098049141995138; Validation Loss = 0.03471906447476568\n",
            "Cost after 296478 iterations : Training Loss =  0.02809803912816219; Validation Loss = 0.03471905590460648\n",
            "Cost after 296479 iterations : Training Loss =  0.02809802911444481; Validation Loss = 0.03471904733453381\n",
            "Cost after 296480 iterations : Training Loss =  0.028098019100842263; Validation Loss = 0.034719038764547724\n",
            "Cost after 296481 iterations : Training Loss =  0.028098009087355012; Validation Loss = 0.034719030194648406\n",
            "Cost after 296482 iterations : Training Loss =  0.028097999073982766; Validation Loss = 0.0347190216248354\n",
            "Cost after 296483 iterations : Training Loss =  0.028097989060725696; Validation Loss = 0.034719013055109646\n",
            "Cost after 296484 iterations : Training Loss =  0.028097979047583727; Validation Loss = 0.03471900448547013\n",
            "Cost after 296485 iterations : Training Loss =  0.028097969034556948; Validation Loss = 0.03471899591591739\n",
            "Cost after 296486 iterations : Training Loss =  0.02809795902164527; Validation Loss = 0.034718987346450916\n",
            "Cost after 296487 iterations : Training Loss =  0.028097949008848704; Validation Loss = 0.03471897877707118\n",
            "Cost after 296488 iterations : Training Loss =  0.028097938996167304; Validation Loss = 0.03471897020777826\n",
            "Cost after 296489 iterations : Training Loss =  0.028097928983600945; Validation Loss = 0.03471896163857176\n",
            "Cost after 296490 iterations : Training Loss =  0.028097918971149733; Validation Loss = 0.03471895306945217\n",
            "Cost after 296491 iterations : Training Loss =  0.028097908958813773; Validation Loss = 0.034718944500418854\n",
            "Cost after 296492 iterations : Training Loss =  0.02809789894659275; Validation Loss = 0.03471893593147232\n",
            "Cost after 296493 iterations : Training Loss =  0.028097888934486868; Validation Loss = 0.0347189273626123\n",
            "Cost after 296494 iterations : Training Loss =  0.02809787892249615; Validation Loss = 0.03471891879383885\n",
            "Cost after 296495 iterations : Training Loss =  0.028097868910620508; Validation Loss = 0.03471891022515203\n",
            "Cost after 296496 iterations : Training Loss =  0.02809785889886002; Validation Loss = 0.034718901656551886\n",
            "Cost after 296497 iterations : Training Loss =  0.028097848887214595; Validation Loss = 0.034718893088038334\n",
            "Cost after 296498 iterations : Training Loss =  0.028097838875684258; Validation Loss = 0.034718884519611276\n",
            "Cost after 296499 iterations : Training Loss =  0.02809782886426905; Validation Loss = 0.03471887595127095\n",
            "Cost after 296500 iterations : Training Loss =  0.028097818852969022; Validation Loss = 0.034718867383017375\n",
            "Cost after 296501 iterations : Training Loss =  0.028097808841783933; Validation Loss = 0.03471885881485008\n",
            "Cost after 296502 iterations : Training Loss =  0.028097798830714064; Validation Loss = 0.03471885024676947\n",
            "Cost after 296503 iterations : Training Loss =  0.028097788819759308; Validation Loss = 0.03471884167877543\n",
            "Cost after 296504 iterations : Training Loss =  0.028097778808919532; Validation Loss = 0.03471883311086793\n",
            "Cost after 296505 iterations : Training Loss =  0.028097768798194877; Validation Loss = 0.0347188245430472\n",
            "Cost after 296506 iterations : Training Loss =  0.028097758787585435; Validation Loss = 0.03471881597531287\n",
            "Cost after 296507 iterations : Training Loss =  0.028097748777090942; Validation Loss = 0.034718807407665225\n",
            "Cost after 296508 iterations : Training Loss =  0.028097738766711677; Validation Loss = 0.034718798840104176\n",
            "Cost after 296509 iterations : Training Loss =  0.028097728756447317; Validation Loss = 0.03471879027262982\n",
            "Cost after 296510 iterations : Training Loss =  0.02809771874629818; Validation Loss = 0.034718781705241945\n",
            "Cost after 296511 iterations : Training Loss =  0.02809770873626414; Validation Loss = 0.034718773137940605\n",
            "Cost after 296512 iterations : Training Loss =  0.028097698726345093; Validation Loss = 0.03471876457072608\n",
            "Cost after 296513 iterations : Training Loss =  0.028097688716541218; Validation Loss = 0.03471875600359792\n",
            "Cost after 296514 iterations : Training Loss =  0.028097678706852314; Validation Loss = 0.034718747436556494\n",
            "Cost after 296515 iterations : Training Loss =  0.02809766869727857; Validation Loss = 0.034718738869601466\n",
            "Cost after 296516 iterations : Training Loss =  0.028097658687819948; Validation Loss = 0.034718730302732946\n",
            "Cost after 296517 iterations : Training Loss =  0.028097648678476292; Validation Loss = 0.03471872173595151\n",
            "Cost after 296518 iterations : Training Loss =  0.028097638669247874; Validation Loss = 0.03471871316925606\n",
            "Cost after 296519 iterations : Training Loss =  0.028097628660134315; Validation Loss = 0.03471870460264734\n",
            "Cost after 296520 iterations : Training Loss =  0.028097618651135917; Validation Loss = 0.034718696036125464\n",
            "Cost after 296521 iterations : Training Loss =  0.02809760864225255; Validation Loss = 0.0347186874696901\n",
            "Cost after 296522 iterations : Training Loss =  0.028097598633484357; Validation Loss = 0.03471867890334127\n",
            "Cost after 296523 iterations : Training Loss =  0.028097588624831162; Validation Loss = 0.03471867033707894\n",
            "Cost after 296524 iterations : Training Loss =  0.028097578616292918; Validation Loss = 0.03471866177090343\n",
            "Cost after 296525 iterations : Training Loss =  0.028097568607869827; Validation Loss = 0.03471865320481438\n",
            "Cost after 296526 iterations : Training Loss =  0.028097558599561884; Validation Loss = 0.03471864463881164\n",
            "Cost after 296527 iterations : Training Loss =  0.028097548591368898; Validation Loss = 0.03471863607289573\n",
            "Cost after 296528 iterations : Training Loss =  0.028097538583290987; Validation Loss = 0.034718627507066245\n",
            "Cost after 296529 iterations : Training Loss =  0.028097528575328178; Validation Loss = 0.034718618941323415\n",
            "Cost after 296530 iterations : Training Loss =  0.02809751856748036; Validation Loss = 0.03471861037566707\n",
            "Cost after 296531 iterations : Training Loss =  0.028097508559747634; Validation Loss = 0.03471860181009737\n",
            "Cost after 296532 iterations : Training Loss =  0.028097498552129993; Validation Loss = 0.03471859324461449\n",
            "Cost after 296533 iterations : Training Loss =  0.028097488544627347; Validation Loss = 0.03471858467921798\n",
            "Cost after 296534 iterations : Training Loss =  0.028097478537239717; Validation Loss = 0.0347185761139076\n",
            "Cost after 296535 iterations : Training Loss =  0.02809746852996711; Validation Loss = 0.03471856754868413\n",
            "Cost after 296536 iterations : Training Loss =  0.02809745852280963; Validation Loss = 0.03471855898354717\n",
            "Cost after 296537 iterations : Training Loss =  0.02809744851576713; Validation Loss = 0.034718550418496856\n",
            "Cost after 296538 iterations : Training Loss =  0.028097438508839688; Validation Loss = 0.03471854185353295\n",
            "Cost after 296539 iterations : Training Loss =  0.02809742850202737; Validation Loss = 0.03471853328865572\n",
            "Cost after 296540 iterations : Training Loss =  0.02809741849532997; Validation Loss = 0.034718524723864885\n",
            "Cost after 296541 iterations : Training Loss =  0.028097408488747727; Validation Loss = 0.034718516159160794\n",
            "Cost after 296542 iterations : Training Loss =  0.028097398482280347; Validation Loss = 0.03471850759454331\n",
            "Cost after 296543 iterations : Training Loss =  0.02809738847592819; Validation Loss = 0.0347184990300123\n",
            "Cost after 296544 iterations : Training Loss =  0.02809737846969092; Validation Loss = 0.03471849046556802\n",
            "Cost after 296545 iterations : Training Loss =  0.028097368463568757; Validation Loss = 0.034718481901210074\n",
            "Cost after 296546 iterations : Training Loss =  0.028097358457561533; Validation Loss = 0.03471847333693867\n",
            "Cost after 296547 iterations : Training Loss =  0.028097348451669384; Validation Loss = 0.03471846477275372\n",
            "Cost after 296548 iterations : Training Loss =  0.028097338445892274; Validation Loss = 0.03471845620865523\n",
            "Cost after 296549 iterations : Training Loss =  0.02809732844023023; Validation Loss = 0.03471844764464359\n",
            "Cost after 296550 iterations : Training Loss =  0.028097318434683115; Validation Loss = 0.034718439080718526\n",
            "Cost after 296551 iterations : Training Loss =  0.028097308429250928; Validation Loss = 0.034718430516879506\n",
            "Cost after 296552 iterations : Training Loss =  0.028097298423934016; Validation Loss = 0.03471842195312734\n",
            "Cost after 296553 iterations : Training Loss =  0.028097288418731995; Validation Loss = 0.03471841338946196\n",
            "Cost after 296554 iterations : Training Loss =  0.02809727841364494; Validation Loss = 0.03471840482588239\n",
            "Cost after 296555 iterations : Training Loss =  0.028097268408672896; Validation Loss = 0.03471839626239001\n",
            "Cost after 296556 iterations : Training Loss =  0.028097258403815994; Validation Loss = 0.03471838769898373\n",
            "Cost after 296557 iterations : Training Loss =  0.02809724839907406; Validation Loss = 0.034718379135664085\n",
            "Cost after 296558 iterations : Training Loss =  0.028097238394447055; Validation Loss = 0.03471837057243112\n",
            "Cost after 296559 iterations : Training Loss =  0.028097228389935; Validation Loss = 0.03471836200928439\n",
            "Cost after 296560 iterations : Training Loss =  0.028097218385538053; Validation Loss = 0.03471835344622441\n",
            "Cost after 296561 iterations : Training Loss =  0.02809720838125598; Validation Loss = 0.034718344883251144\n",
            "Cost after 296562 iterations : Training Loss =  0.0280971983770891; Validation Loss = 0.034718336320364335\n",
            "Cost after 296563 iterations : Training Loss =  0.028097188373037176; Validation Loss = 0.034718327757563866\n",
            "Cost after 296564 iterations : Training Loss =  0.028097178369100244; Validation Loss = 0.03471831919485021\n",
            "Cost after 296565 iterations : Training Loss =  0.028097168365278247; Validation Loss = 0.034718310632222624\n",
            "Cost after 296566 iterations : Training Loss =  0.028097158361571173; Validation Loss = 0.03471830206968209\n",
            "Cost after 296567 iterations : Training Loss =  0.028097148357979235; Validation Loss = 0.03471829350722791\n",
            "Cost after 296568 iterations : Training Loss =  0.028097138354502213; Validation Loss = 0.03471828494485998\n",
            "Cost after 296569 iterations : Training Loss =  0.028097128351140293; Validation Loss = 0.03471827638257846\n",
            "Cost after 296570 iterations : Training Loss =  0.028097118347893146; Validation Loss = 0.03471826782038398\n",
            "Cost after 296571 iterations : Training Loss =  0.028097108344761115; Validation Loss = 0.034718259258275694\n",
            "Cost after 296572 iterations : Training Loss =  0.028097098341744196; Validation Loss = 0.034718250696254\n",
            "Cost after 296573 iterations : Training Loss =  0.028097088338842093; Validation Loss = 0.03471824213431862\n",
            "Cost after 296574 iterations : Training Loss =  0.028097078336054994; Validation Loss = 0.03471823357247001\n",
            "Cost after 296575 iterations : Training Loss =  0.02809706833338283; Validation Loss = 0.03471822501070774\n",
            "Cost after 296576 iterations : Training Loss =  0.02809705833082573; Validation Loss = 0.034718216449032224\n",
            "Cost after 296577 iterations : Training Loss =  0.028097048328383604; Validation Loss = 0.03471820788744319\n",
            "Cost after 296578 iterations : Training Loss =  0.028097038326056336; Validation Loss = 0.03471819932594055\n",
            "Cost after 296579 iterations : Training Loss =  0.028097028323844194; Validation Loss = 0.03471819076452464\n",
            "Cost after 296580 iterations : Training Loss =  0.0280970183217469; Validation Loss = 0.03471818220319483\n",
            "Cost after 296581 iterations : Training Loss =  0.028097008319764666; Validation Loss = 0.03471817364195191\n",
            "Cost after 296582 iterations : Training Loss =  0.02809699831789733; Validation Loss = 0.03471816508079527\n",
            "Cost after 296583 iterations : Training Loss =  0.028096988316145075; Validation Loss = 0.034718156519725346\n",
            "Cost after 296584 iterations : Training Loss =  0.02809697831450759; Validation Loss = 0.03471814795874163\n",
            "Cost after 296585 iterations : Training Loss =  0.02809696831298522; Validation Loss = 0.03471813939784476\n",
            "Cost after 296586 iterations : Training Loss =  0.028096958311577862; Validation Loss = 0.03471813083703441\n",
            "Cost after 296587 iterations : Training Loss =  0.02809694831028524; Validation Loss = 0.03471812227631009\n",
            "Cost after 296588 iterations : Training Loss =  0.02809693830910779; Validation Loss = 0.034718113715672454\n",
            "Cost after 296589 iterations : Training Loss =  0.028096928308045215; Validation Loss = 0.034718105155121214\n",
            "Cost after 296590 iterations : Training Loss =  0.028096918307097606; Validation Loss = 0.03471809659465651\n",
            "Cost after 296591 iterations : Training Loss =  0.028096908306264927; Validation Loss = 0.03471808803427825\n",
            "Cost after 296592 iterations : Training Loss =  0.028096898305547256; Validation Loss = 0.03471807947398661\n",
            "Cost after 296593 iterations : Training Loss =  0.028096888304944407; Validation Loss = 0.03471807091378149\n",
            "Cost after 296594 iterations : Training Loss =  0.028096878304456605; Validation Loss = 0.034718062353662815\n",
            "Cost after 296595 iterations : Training Loss =  0.028096868304083724; Validation Loss = 0.03471805379363042\n",
            "Cost after 296596 iterations : Training Loss =  0.028096858303825904; Validation Loss = 0.03471804523368492\n",
            "Cost after 296597 iterations : Training Loss =  0.028096848303682816; Validation Loss = 0.034718036673825704\n",
            "Cost after 296598 iterations : Training Loss =  0.02809683830365481; Validation Loss = 0.03471802811405306\n",
            "Cost after 296599 iterations : Training Loss =  0.028096828303741685; Validation Loss = 0.034718019554367034\n",
            "Cost after 296600 iterations : Training Loss =  0.028096818303943515; Validation Loss = 0.034718010994767144\n",
            "Cost after 296601 iterations : Training Loss =  0.028096808304260264; Validation Loss = 0.03471800243525384\n",
            "Cost after 296602 iterations : Training Loss =  0.02809679830469194; Validation Loss = 0.03471799387582677\n",
            "Cost after 296603 iterations : Training Loss =  0.028096788305238617; Validation Loss = 0.03471798531648663\n",
            "Cost after 296604 iterations : Training Loss =  0.028096778305900193; Validation Loss = 0.03471797675723247\n",
            "Cost after 296605 iterations : Training Loss =  0.0280967683066767; Validation Loss = 0.03471796819806531\n",
            "Cost after 296606 iterations : Training Loss =  0.02809675830756814; Validation Loss = 0.034717959638984316\n",
            "Cost after 296607 iterations : Training Loss =  0.028096748308574462; Validation Loss = 0.034717951079989914\n",
            "Cost after 296608 iterations : Training Loss =  0.028096738309695758; Validation Loss = 0.034717942521082255\n",
            "Cost after 296609 iterations : Training Loss =  0.028096728310931986; Validation Loss = 0.03471793396226042\n",
            "Cost after 296610 iterations : Training Loss =  0.028096718312283126; Validation Loss = 0.03471792540352548\n",
            "Cost after 296611 iterations : Training Loss =  0.02809670831374909; Validation Loss = 0.03471791684487665\n",
            "Cost after 296612 iterations : Training Loss =  0.028096698315330144; Validation Loss = 0.034717908286314596\n",
            "Cost after 296613 iterations : Training Loss =  0.028096688317025983; Validation Loss = 0.03471789972783878\n",
            "Cost after 296614 iterations : Training Loss =  0.02809667831883689; Validation Loss = 0.03471789116944959\n",
            "Cost after 296615 iterations : Training Loss =  0.02809666832076253; Validation Loss = 0.03471788261114679\n",
            "Cost after 296616 iterations : Training Loss =  0.028096658322803184; Validation Loss = 0.03471787405293061\n",
            "Cost after 296617 iterations : Training Loss =  0.02809664832495857; Validation Loss = 0.034717865494800625\n",
            "Cost after 296618 iterations : Training Loss =  0.028096638327229116; Validation Loss = 0.03471785693675725\n",
            "Cost after 296619 iterations : Training Loss =  0.028096628329614407; Validation Loss = 0.03471784837880017\n",
            "Cost after 296620 iterations : Training Loss =  0.028096618332114645; Validation Loss = 0.034717839820929845\n",
            "Cost after 296621 iterations : Training Loss =  0.028096608334729815; Validation Loss = 0.03471783126314584\n",
            "Cost after 296622 iterations : Training Loss =  0.0280965983374599; Validation Loss = 0.03471782270544835\n",
            "Cost after 296623 iterations : Training Loss =  0.028096588340304877; Validation Loss = 0.034717814147837193\n",
            "Cost after 296624 iterations : Training Loss =  0.028096578343264744; Validation Loss = 0.03471780559031223\n",
            "Cost after 296625 iterations : Training Loss =  0.028096568346339407; Validation Loss = 0.03471779703287404\n",
            "Cost after 296626 iterations : Training Loss =  0.028096558349529205; Validation Loss = 0.03471778847552217\n",
            "Cost after 296627 iterations : Training Loss =  0.02809654835283364; Validation Loss = 0.03471777991825723\n",
            "Cost after 296628 iterations : Training Loss =  0.028096538356253073; Validation Loss = 0.03471777136107849\n",
            "Cost after 296629 iterations : Training Loss =  0.028096528359787407; Validation Loss = 0.0347177628039861\n",
            "Cost after 296630 iterations : Training Loss =  0.0280965183634366; Validation Loss = 0.03471775424698014\n",
            "Cost after 296631 iterations : Training Loss =  0.028096508367200682; Validation Loss = 0.034717745690060614\n",
            "Cost after 296632 iterations : Training Loss =  0.028096498371079772; Validation Loss = 0.034717737133227636\n",
            "Cost after 296633 iterations : Training Loss =  0.028096488375073534; Validation Loss = 0.03471772857648109\n",
            "Cost after 296634 iterations : Training Loss =  0.02809647837918229; Validation Loss = 0.03471772001982066\n",
            "Cost after 296635 iterations : Training Loss =  0.028096468383405952; Validation Loss = 0.03471771146324699\n",
            "Cost after 296636 iterations : Training Loss =  0.028096458387744466; Validation Loss = 0.03471770290675968\n",
            "Cost after 296637 iterations : Training Loss =  0.02809644839219792; Validation Loss = 0.034717694350358805\n",
            "Cost after 296638 iterations : Training Loss =  0.028096438396766083; Validation Loss = 0.034717685794044266\n",
            "Cost after 296639 iterations : Training Loss =  0.02809642840144923; Validation Loss = 0.03471767723781649\n",
            "Cost after 296640 iterations : Training Loss =  0.028096418406247207; Validation Loss = 0.034717668681675015\n",
            "Cost after 296641 iterations : Training Loss =  0.028096408411160134; Validation Loss = 0.03471766012561981\n",
            "Cost after 296642 iterations : Training Loss =  0.028096398416187923; Validation Loss = 0.034717651569650954\n",
            "Cost after 296643 iterations : Training Loss =  0.028096388421330613; Validation Loss = 0.034717643013768645\n",
            "Cost after 296644 iterations : Training Loss =  0.028096378426588058; Validation Loss = 0.03471763445797297\n",
            "Cost after 296645 iterations : Training Loss =  0.02809636843196033; Validation Loss = 0.03471762590226354\n",
            "Cost after 296646 iterations : Training Loss =  0.028096358437447684; Validation Loss = 0.034717617346640445\n",
            "Cost after 296647 iterations : Training Loss =  0.02809634844304964; Validation Loss = 0.03471760879110406\n",
            "Cost after 296648 iterations : Training Loss =  0.028096338448766574; Validation Loss = 0.03471760023565367\n",
            "Cost after 296649 iterations : Training Loss =  0.02809632845459838; Validation Loss = 0.03471759168029004\n",
            "Cost after 296650 iterations : Training Loss =  0.02809631846054505; Validation Loss = 0.03471758312501296\n",
            "Cost after 296651 iterations : Training Loss =  0.028096308466606584; Validation Loss = 0.034717574569821955\n",
            "Cost after 296652 iterations : Training Loss =  0.028096298472782825; Validation Loss = 0.03471756601471737\n",
            "Cost after 296653 iterations : Training Loss =  0.028096288479074007; Validation Loss = 0.03471755745969965\n",
            "Cost after 296654 iterations : Training Loss =  0.028096278485480117; Validation Loss = 0.034717548904767914\n",
            "Cost after 296655 iterations : Training Loss =  0.02809626849200106; Validation Loss = 0.03471754034992262\n",
            "Cost after 296656 iterations : Training Loss =  0.02809625849863676; Validation Loss = 0.03471753179516406\n",
            "Cost after 296657 iterations : Training Loss =  0.0280962485053874; Validation Loss = 0.03471752324049168\n",
            "Cost after 296658 iterations : Training Loss =  0.0280962385122528; Validation Loss = 0.03471751468590559\n",
            "Cost after 296659 iterations : Training Loss =  0.028096228519233102; Validation Loss = 0.03471750613140604\n",
            "Cost after 296660 iterations : Training Loss =  0.028096218526328164; Validation Loss = 0.03471749757699318\n",
            "Cost after 296661 iterations : Training Loss =  0.028096208533538142; Validation Loss = 0.03471748902266635\n",
            "Cost after 296662 iterations : Training Loss =  0.028096198540862993; Validation Loss = 0.03471748046842608\n",
            "Cost after 296663 iterations : Training Loss =  0.028096188548302536; Validation Loss = 0.03471747191427218\n",
            "Cost after 296664 iterations : Training Loss =  0.028096178555857033; Validation Loss = 0.034717463360204884\n",
            "Cost after 296665 iterations : Training Loss =  0.028096168563526324; Validation Loss = 0.03471745480622346\n",
            "Cost after 296666 iterations : Training Loss =  0.028096158571310564; Validation Loss = 0.03471744625232907\n",
            "Cost after 296667 iterations : Training Loss =  0.028096148579209345; Validation Loss = 0.034717437698520794\n",
            "Cost after 296668 iterations : Training Loss =  0.028096138587223207; Validation Loss = 0.03471742914479882\n",
            "Cost after 296669 iterations : Training Loss =  0.02809612859535172; Validation Loss = 0.03471742059116344\n",
            "Cost after 296670 iterations : Training Loss =  0.028096118603595177; Validation Loss = 0.034717412037614144\n",
            "Cost after 296671 iterations : Training Loss =  0.02809610861195339; Validation Loss = 0.03471740348415152\n",
            "Cost after 296672 iterations : Training Loss =  0.02809609862042648; Validation Loss = 0.03471739493077517\n",
            "Cost after 296673 iterations : Training Loss =  0.028096088629014363; Validation Loss = 0.034717386377485365\n",
            "Cost after 296674 iterations : Training Loss =  0.02809607863771698; Validation Loss = 0.03471737782428179\n",
            "Cost after 296675 iterations : Training Loss =  0.02809606864653456; Validation Loss = 0.03471736927116459\n",
            "Cost after 296676 iterations : Training Loss =  0.028096058655466924; Validation Loss = 0.034717360718133826\n",
            "Cost after 296677 iterations : Training Loss =  0.0280960486645141; Validation Loss = 0.03471735216518952\n",
            "Cost after 296678 iterations : Training Loss =  0.02809603867367602; Validation Loss = 0.034717343612331676\n",
            "Cost after 296679 iterations : Training Loss =  0.028096028682952755; Validation Loss = 0.034717335059560164\n",
            "Cost after 296680 iterations : Training Loss =  0.028096018692344373; Validation Loss = 0.0347173265068753\n",
            "Cost after 296681 iterations : Training Loss =  0.02809600870185065; Validation Loss = 0.03471731795427632\n",
            "Cost after 296682 iterations : Training Loss =  0.028095998711471934; Validation Loss = 0.03471730940176392\n",
            "Cost after 296683 iterations : Training Loss =  0.028095988721207714; Validation Loss = 0.03471730084933783\n",
            "Cost after 296684 iterations : Training Loss =  0.028095978731058627; Validation Loss = 0.03471729229699836\n",
            "Cost after 296685 iterations : Training Loss =  0.028095968741024175; Validation Loss = 0.034717283744744774\n",
            "Cost after 296686 iterations : Training Loss =  0.028095958751104516; Validation Loss = 0.034717275192578174\n",
            "Cost after 296687 iterations : Training Loss =  0.02809594876129975; Validation Loss = 0.034717266640497575\n",
            "Cost after 296688 iterations : Training Loss =  0.028095938771609662; Validation Loss = 0.03471725808850339\n",
            "Cost after 296689 iterations : Training Loss =  0.028095928782034454; Validation Loss = 0.03471724953659554\n",
            "Cost after 296690 iterations : Training Loss =  0.028095918792573973; Validation Loss = 0.03471724098477409\n",
            "Cost after 296691 iterations : Training Loss =  0.028095908803228337; Validation Loss = 0.03471723243303889\n",
            "Cost after 296692 iterations : Training Loss =  0.028095898813997437; Validation Loss = 0.034717223881390195\n",
            "Cost after 296693 iterations : Training Loss =  0.02809588882488133; Validation Loss = 0.0347172153298278\n",
            "Cost after 296694 iterations : Training Loss =  0.028095878835880023; Validation Loss = 0.034717206778351904\n",
            "Cost after 296695 iterations : Training Loss =  0.028095868846993426; Validation Loss = 0.03471719822696216\n",
            "Cost after 296696 iterations : Training Loss =  0.028095858858221707; Validation Loss = 0.03471718967565903\n",
            "Cost after 296697 iterations : Training Loss =  0.02809584886956476; Validation Loss = 0.03471718112444223\n",
            "Cost after 296698 iterations : Training Loss =  0.02809583888102259; Validation Loss = 0.0347171725733117\n",
            "Cost after 296699 iterations : Training Loss =  0.028095828892595143; Validation Loss = 0.03471716402226771\n",
            "Cost after 296700 iterations : Training Loss =  0.02809581890428252; Validation Loss = 0.03471715547130988\n",
            "Cost after 296701 iterations : Training Loss =  0.02809580891608456; Validation Loss = 0.03471714692043845\n",
            "Cost after 296702 iterations : Training Loss =  0.028095798928001556; Validation Loss = 0.03471713836965348\n",
            "Cost after 296703 iterations : Training Loss =  0.02809578894003324; Validation Loss = 0.03471712981895478\n",
            "Cost after 296704 iterations : Training Loss =  0.02809577895217962; Validation Loss = 0.03471712126834218\n",
            "Cost after 296705 iterations : Training Loss =  0.028095768964440767; Validation Loss = 0.03471711271781635\n",
            "Cost after 296706 iterations : Training Loss =  0.028095758976816786; Validation Loss = 0.03471710416737682\n",
            "Cost after 296707 iterations : Training Loss =  0.02809574898930756; Validation Loss = 0.03471709561702329\n",
            "Cost after 296708 iterations : Training Loss =  0.02809573900191302; Validation Loss = 0.034717087066756414\n",
            "Cost after 296709 iterations : Training Loss =  0.028095729014633263; Validation Loss = 0.03471707851657606\n",
            "Cost after 296710 iterations : Training Loss =  0.028095719027468182; Validation Loss = 0.034717069966481974\n",
            "Cost after 296711 iterations : Training Loss =  0.028095709040418023; Validation Loss = 0.034717061416473946\n",
            "Cost after 296712 iterations : Training Loss =  0.028095699053482633; Validation Loss = 0.03471705286655253\n",
            "Cost after 296713 iterations : Training Loss =  0.028095689066661756; Validation Loss = 0.034717044316717544\n",
            "Cost after 296714 iterations : Training Loss =  0.02809567907995587; Validation Loss = 0.03471703576696882\n",
            "Cost after 296715 iterations : Training Loss =  0.028095669093364625; Validation Loss = 0.034717027217306184\n",
            "Cost after 296716 iterations : Training Loss =  0.02809565910688821; Validation Loss = 0.03471701866772998\n",
            "Cost after 296717 iterations : Training Loss =  0.028095649120526377; Validation Loss = 0.03471701011824025\n",
            "Cost after 296718 iterations : Training Loss =  0.02809563913427939; Validation Loss = 0.03471700156883692\n",
            "Cost after 296719 iterations : Training Loss =  0.02809562914814716; Validation Loss = 0.03471699301951984\n",
            "Cost after 296720 iterations : Training Loss =  0.028095619162129645; Validation Loss = 0.03471698447028925\n",
            "Cost after 296721 iterations : Training Loss =  0.028095609176226884; Validation Loss = 0.03471697592114495\n",
            "Cost after 296722 iterations : Training Loss =  0.0280955991904389; Validation Loss = 0.034716967372086786\n",
            "Cost after 296723 iterations : Training Loss =  0.028095589204765643; Validation Loss = 0.03471695882311497\n",
            "Cost after 296724 iterations : Training Loss =  0.028095579219207016; Validation Loss = 0.034716950274229504\n",
            "Cost after 296725 iterations : Training Loss =  0.028095569233763235; Validation Loss = 0.03471694172543055\n",
            "Cost after 296726 iterations : Training Loss =  0.028095559248434113; Validation Loss = 0.03471693317671758\n",
            "Cost after 296727 iterations : Training Loss =  0.028095549263219777; Validation Loss = 0.034716924628091166\n",
            "Cost after 296728 iterations : Training Loss =  0.028095539278120172; Validation Loss = 0.03471691607955122\n",
            "Cost after 296729 iterations : Training Loss =  0.028095529293135177; Validation Loss = 0.034716907531097387\n",
            "Cost after 296730 iterations : Training Loss =  0.028095519308265083; Validation Loss = 0.034716898982729796\n",
            "Cost after 296731 iterations : Training Loss =  0.028095509323509647; Validation Loss = 0.03471689043444862\n",
            "Cost after 296732 iterations : Training Loss =  0.02809549933886885; Validation Loss = 0.034716881886253964\n",
            "Cost after 296733 iterations : Training Loss =  0.028095489354342862; Validation Loss = 0.03471687333814532\n",
            "Cost after 296734 iterations : Training Loss =  0.028095479369931575; Validation Loss = 0.03471686479012333\n",
            "Cost after 296735 iterations : Training Loss =  0.028095469385634845; Validation Loss = 0.03471685624218746\n",
            "Cost after 296736 iterations : Training Loss =  0.028095459401453075; Validation Loss = 0.03471684769433778\n",
            "Cost after 296737 iterations : Training Loss =  0.02809544941738589; Validation Loss = 0.034716839146574384\n",
            "Cost after 296738 iterations : Training Loss =  0.02809543943343352; Validation Loss = 0.03471683059889773\n",
            "Cost after 296739 iterations : Training Loss =  0.02809542944959579; Validation Loss = 0.03471682205130692\n",
            "Cost after 296740 iterations : Training Loss =  0.028095419465872618; Validation Loss = 0.03471681350380266\n",
            "Cost after 296741 iterations : Training Loss =  0.028095409482264442; Validation Loss = 0.03471680495638468\n",
            "Cost after 296742 iterations : Training Loss =  0.02809539949877074; Validation Loss = 0.03471679640905306\n",
            "Cost after 296743 iterations : Training Loss =  0.02809538951539192; Validation Loss = 0.03471678786180793\n",
            "Cost after 296744 iterations : Training Loss =  0.0280953795321277; Validation Loss = 0.03471677931464869\n",
            "Cost after 296745 iterations : Training Loss =  0.028095369548978212; Validation Loss = 0.034716770767575726\n",
            "Cost after 296746 iterations : Training Loss =  0.02809535956594335; Validation Loss = 0.03471676222058932\n",
            "Cost after 296747 iterations : Training Loss =  0.02809534958302326; Validation Loss = 0.0347167536736892\n",
            "Cost after 296748 iterations : Training Loss =  0.02809533960021776; Validation Loss = 0.0347167451268752\n",
            "Cost after 296749 iterations : Training Loss =  0.028095329617527143; Validation Loss = 0.034716736580147914\n",
            "Cost after 296750 iterations : Training Loss =  0.028095319634951075; Validation Loss = 0.03471672803350652\n",
            "Cost after 296751 iterations : Training Loss =  0.028095309652489714; Validation Loss = 0.03471671948695167\n",
            "Cost after 296752 iterations : Training Loss =  0.028095299670143; Validation Loss = 0.03471671094048341\n",
            "Cost after 296753 iterations : Training Loss =  0.028095289687911102; Validation Loss = 0.03471670239410101\n",
            "Cost after 296754 iterations : Training Loss =  0.028095279705793886; Validation Loss = 0.03471669384780522\n",
            "Cost after 296755 iterations : Training Loss =  0.028095269723791186; Validation Loss = 0.03471668530159552\n",
            "Cost after 296756 iterations : Training Loss =  0.028095259741903317; Validation Loss = 0.03471667675547219\n",
            "Cost after 296757 iterations : Training Loss =  0.028095249760130156; Validation Loss = 0.03471666820943522\n",
            "Cost after 296758 iterations : Training Loss =  0.02809523977847156; Validation Loss = 0.03471665966348432\n",
            "Cost after 296759 iterations : Training Loss =  0.028095229796927655; Validation Loss = 0.03471665111761965\n",
            "Cost after 296760 iterations : Training Loss =  0.028095219815498527; Validation Loss = 0.034716642571841275\n",
            "Cost after 296761 iterations : Training Loss =  0.028095209834183902; Validation Loss = 0.034716634026149176\n",
            "Cost after 296762 iterations : Training Loss =  0.028095199852984185; Validation Loss = 0.03471662548054345\n",
            "Cost after 296763 iterations : Training Loss =  0.02809518987189902; Validation Loss = 0.034716616935024376\n",
            "Cost after 296764 iterations : Training Loss =  0.02809517989092845; Validation Loss = 0.034716608389591404\n",
            "Cost after 296765 iterations : Training Loss =  0.028095169910072625; Validation Loss = 0.03471659984424433\n",
            "Cost after 296766 iterations : Training Loss =  0.028095159929331422; Validation Loss = 0.03471659129898376\n",
            "Cost after 296767 iterations : Training Loss =  0.028095149948704892; Validation Loss = 0.034716582753809615\n",
            "Cost after 296768 iterations : Training Loss =  0.02809513996819312; Validation Loss = 0.03471657420872156\n",
            "Cost after 296769 iterations : Training Loss =  0.028095129987795918; Validation Loss = 0.03471656566372006\n",
            "Cost after 296770 iterations : Training Loss =  0.02809512000751334; Validation Loss = 0.03471655711880462\n",
            "Cost after 296771 iterations : Training Loss =  0.02809511002734551; Validation Loss = 0.03471654857397538\n",
            "Cost after 296772 iterations : Training Loss =  0.02809510004729227; Validation Loss = 0.03471654002923263\n",
            "Cost after 296773 iterations : Training Loss =  0.02809509006735362; Validation Loss = 0.03471653148457615\n",
            "Cost after 296774 iterations : Training Loss =  0.028095080087529808; Validation Loss = 0.03471652294000563\n",
            "Cost after 296775 iterations : Training Loss =  0.028095070107820547; Validation Loss = 0.0347165143955217\n",
            "Cost after 296776 iterations : Training Loss =  0.02809506012822595; Validation Loss = 0.03471650585112367\n",
            "Cost after 296777 iterations : Training Loss =  0.02809505014874597; Validation Loss = 0.034716497306812436\n",
            "Cost after 296778 iterations : Training Loss =  0.028095040169380584; Validation Loss = 0.03471648876258711\n",
            "Cost after 296779 iterations : Training Loss =  0.028095030190129995; Validation Loss = 0.034716480218448156\n",
            "Cost after 296780 iterations : Training Loss =  0.028095020210994037; Validation Loss = 0.03471647167439562\n",
            "Cost after 296781 iterations : Training Loss =  0.02809501023197265; Validation Loss = 0.03471646313042929\n",
            "Cost after 296782 iterations : Training Loss =  0.028095000253065733; Validation Loss = 0.03471645458654914\n",
            "Cost after 296783 iterations : Training Loss =  0.02809499027427368; Validation Loss = 0.03471644604275518\n",
            "Cost after 296784 iterations : Training Loss =  0.02809498029559625; Validation Loss = 0.03471643749904754\n",
            "Cost after 296785 iterations : Training Loss =  0.028094970317033445; Validation Loss = 0.03471642895542625\n",
            "Cost after 296786 iterations : Training Loss =  0.02809496033858527; Validation Loss = 0.03471642041189115\n",
            "Cost after 296787 iterations : Training Loss =  0.028094950360251735; Validation Loss = 0.03471641186844245\n",
            "Cost after 296788 iterations : Training Loss =  0.02809494038203269; Validation Loss = 0.03471640332507973\n",
            "Cost after 296789 iterations : Training Loss =  0.028094930403928395; Validation Loss = 0.03471639478180349\n",
            "Cost after 296790 iterations : Training Loss =  0.028094920425938762; Validation Loss = 0.03471638623861343\n",
            "Cost after 296791 iterations : Training Loss =  0.028094910448063715; Validation Loss = 0.03471637769550979\n",
            "Cost after 296792 iterations : Training Loss =  0.028094900470303236; Validation Loss = 0.034716369152491974\n",
            "Cost after 296793 iterations : Training Loss =  0.02809489049265748; Validation Loss = 0.03471636060956072\n",
            "Cost after 296794 iterations : Training Loss =  0.028094880515126274; Validation Loss = 0.03471635206671568\n",
            "Cost after 296795 iterations : Training Loss =  0.02809487053770964; Validation Loss = 0.0347163435239567\n",
            "Cost after 296796 iterations : Training Loss =  0.0280948605604077; Validation Loss = 0.03471633498128424\n",
            "Cost after 296797 iterations : Training Loss =  0.028094850583220407; Validation Loss = 0.03471632643869789\n",
            "Cost after 296798 iterations : Training Loss =  0.028094840606147628; Validation Loss = 0.03471631789619829\n",
            "Cost after 296799 iterations : Training Loss =  0.028094830629189566; Validation Loss = 0.03471630935378411\n",
            "Cost after 296800 iterations : Training Loss =  0.02809482065234603; Validation Loss = 0.034716300811456366\n",
            "Cost after 296801 iterations : Training Loss =  0.028094810675617112; Validation Loss = 0.03471629226921503\n",
            "Cost after 296802 iterations : Training Loss =  0.028094800699002796; Validation Loss = 0.03471628372705959\n",
            "Cost after 296803 iterations : Training Loss =  0.028094790722503208; Validation Loss = 0.034716275184990944\n",
            "Cost after 296804 iterations : Training Loss =  0.02809478074611806; Validation Loss = 0.034716266643008344\n",
            "Cost after 296805 iterations : Training Loss =  0.02809477076984748; Validation Loss = 0.03471625810111187\n",
            "Cost after 296806 iterations : Training Loss =  0.02809476079369172; Validation Loss = 0.03471624955930198\n",
            "Cost after 296807 iterations : Training Loss =  0.028094750817650428; Validation Loss = 0.03471624101757791\n",
            "Cost after 296808 iterations : Training Loss =  0.02809474084172371; Validation Loss = 0.03471623247594039\n",
            "Cost after 296809 iterations : Training Loss =  0.028094730865911646; Validation Loss = 0.03471622393438905\n",
            "Cost after 296810 iterations : Training Loss =  0.028094720890214125; Validation Loss = 0.034716215392923785\n",
            "Cost after 296811 iterations : Training Loss =  0.028094710914631282; Validation Loss = 0.03471620685154502\n",
            "Cost after 296812 iterations : Training Loss =  0.02809470093916291; Validation Loss = 0.034716198310252126\n",
            "Cost after 296813 iterations : Training Loss =  0.028094690963809194; Validation Loss = 0.034716189769045495\n",
            "Cost after 296814 iterations : Training Loss =  0.028094680988570035; Validation Loss = 0.034716181227925226\n",
            "Cost after 296815 iterations : Training Loss =  0.028094671013445573; Validation Loss = 0.03471617268689111\n",
            "Cost after 296816 iterations : Training Loss =  0.0280946610384355; Validation Loss = 0.03471616414594283\n",
            "Cost after 296817 iterations : Training Loss =  0.028094651063540192; Validation Loss = 0.034716155605081435\n",
            "Cost after 296818 iterations : Training Loss =  0.028094641088759382; Validation Loss = 0.03471614706430617\n",
            "Cost after 296819 iterations : Training Loss =  0.028094631114093147; Validation Loss = 0.0347161385236168\n",
            "Cost after 296820 iterations : Training Loss =  0.028094621139541625; Validation Loss = 0.034716129983013905\n",
            "Cost after 296821 iterations : Training Loss =  0.028094611165104495; Validation Loss = 0.03471612144249711\n",
            "Cost after 296822 iterations : Training Loss =  0.02809460119078195; Validation Loss = 0.03471611290206664\n",
            "Cost after 296823 iterations : Training Loss =  0.028094591216574104; Validation Loss = 0.03471610436172222\n",
            "Cost after 296824 iterations : Training Loss =  0.028094581242480716; Validation Loss = 0.034716095821464066\n",
            "Cost after 296825 iterations : Training Loss =  0.028094571268501917; Validation Loss = 0.034716087281292204\n",
            "Cost after 296826 iterations : Training Loss =  0.028094561294637867; Validation Loss = 0.034716078741206746\n",
            "Cost after 296827 iterations : Training Loss =  0.028094551320888086; Validation Loss = 0.03471607020120717\n",
            "Cost after 296828 iterations : Training Loss =  0.028094541347253053; Validation Loss = 0.034716061661294144\n",
            "Cost after 296829 iterations : Training Loss =  0.028094531373732495; Validation Loss = 0.03471605312146719\n",
            "Cost after 296830 iterations : Training Loss =  0.028094521400326547; Validation Loss = 0.03471604458172625\n",
            "Cost after 296831 iterations : Training Loss =  0.02809451142703511; Validation Loss = 0.03471603604207175\n",
            "Cost after 296832 iterations : Training Loss =  0.028094501453858337; Validation Loss = 0.034716027502502904\n",
            "Cost after 296833 iterations : Training Loss =  0.02809449148079592; Validation Loss = 0.034716018963020887\n",
            "Cost after 296834 iterations : Training Loss =  0.028094481507848237; Validation Loss = 0.03471601042362468\n",
            "Cost after 296835 iterations : Training Loss =  0.02809447153501507; Validation Loss = 0.034716001884315104\n",
            "Cost after 296836 iterations : Training Loss =  0.02809446156229652; Validation Loss = 0.03471599334509134\n",
            "Cost after 296837 iterations : Training Loss =  0.028094451589692485; Validation Loss = 0.03471598480595383\n",
            "Cost after 296838 iterations : Training Loss =  0.028094441617202825; Validation Loss = 0.03471597626690263\n",
            "Cost after 296839 iterations : Training Loss =  0.028094431644827972; Validation Loss = 0.034715967727937606\n",
            "Cost after 296840 iterations : Training Loss =  0.028094421672567562; Validation Loss = 0.03471595918905897\n",
            "Cost after 296841 iterations : Training Loss =  0.028094411700421512; Validation Loss = 0.03471595065026639\n",
            "Cost after 296842 iterations : Training Loss =  0.02809440172839025; Validation Loss = 0.034715942111559955\n",
            "Cost after 296843 iterations : Training Loss =  0.028094391756473422; Validation Loss = 0.034715933572939665\n",
            "Cost after 296844 iterations : Training Loss =  0.02809438178467115; Validation Loss = 0.034715925034405744\n",
            "Cost after 296845 iterations : Training Loss =  0.028094371812983483; Validation Loss = 0.034715916495957864\n",
            "Cost after 296846 iterations : Training Loss =  0.028094361841410245; Validation Loss = 0.034715907957595986\n",
            "Cost after 296847 iterations : Training Loss =  0.0280943518699516; Validation Loss = 0.03471589941932048\n",
            "Cost after 296848 iterations : Training Loss =  0.028094341898607402; Validation Loss = 0.03471589088113109\n",
            "Cost after 296849 iterations : Training Loss =  0.028094331927377814; Validation Loss = 0.03471588234302803\n",
            "Cost after 296850 iterations : Training Loss =  0.028094321956262776; Validation Loss = 0.034715873805011\n",
            "Cost after 296851 iterations : Training Loss =  0.028094311985262247; Validation Loss = 0.034715865267080294\n",
            "Cost after 296852 iterations : Training Loss =  0.028094302014376217; Validation Loss = 0.03471585672923548\n",
            "Cost after 296853 iterations : Training Loss =  0.02809429204360474; Validation Loss = 0.03471584819147729\n",
            "Cost after 296854 iterations : Training Loss =  0.028094282072947757; Validation Loss = 0.03471583965380473\n",
            "Cost after 296855 iterations : Training Loss =  0.02809427210240527; Validation Loss = 0.03471583111621892\n",
            "Cost after 296856 iterations : Training Loss =  0.028094262131977313; Validation Loss = 0.03471582257871906\n",
            "Cost after 296857 iterations : Training Loss =  0.02809425216166399; Validation Loss = 0.03471581404130522\n",
            "Cost after 296858 iterations : Training Loss =  0.028094242191465012; Validation Loss = 0.03471580550397763\n",
            "Cost after 296859 iterations : Training Loss =  0.028094232221380525; Validation Loss = 0.03471579696673655\n",
            "Cost after 296860 iterations : Training Loss =  0.028094222251410703; Validation Loss = 0.034715788429581285\n",
            "Cost after 296861 iterations : Training Loss =  0.02809421228155542; Validation Loss = 0.034715779892512226\n",
            "Cost after 296862 iterations : Training Loss =  0.028094202311814517; Validation Loss = 0.03471577135552931\n",
            "Cost after 296863 iterations : Training Loss =  0.028094192342188264; Validation Loss = 0.03471576281863285\n",
            "Cost after 296864 iterations : Training Loss =  0.02809418237267635; Validation Loss = 0.034715754281821956\n",
            "Cost after 296865 iterations : Training Loss =  0.02809417240327901; Validation Loss = 0.034715745745097525\n",
            "Cost after 296866 iterations : Training Loss =  0.028094162433996064; Validation Loss = 0.03471573720845954\n",
            "Cost after 296867 iterations : Training Loss =  0.02809415246482779; Validation Loss = 0.03471572867190749\n",
            "Cost after 296868 iterations : Training Loss =  0.028094142495773983; Validation Loss = 0.034715720135441636\n",
            "Cost after 296869 iterations : Training Loss =  0.028094132526834726; Validation Loss = 0.034715711599062074\n",
            "Cost after 296870 iterations : Training Loss =  0.02809412255800987; Validation Loss = 0.03471570306276857\n",
            "Cost after 296871 iterations : Training Loss =  0.02809411258929945; Validation Loss = 0.03471569452656126\n",
            "Cost after 296872 iterations : Training Loss =  0.028094102620703533; Validation Loss = 0.034715685990439664\n",
            "Cost after 296873 iterations : Training Loss =  0.02809409265222227; Validation Loss = 0.03471567745440483\n",
            "Cost after 296874 iterations : Training Loss =  0.028094082683855354; Validation Loss = 0.034715668918455726\n",
            "Cost after 296875 iterations : Training Loss =  0.02809407271560287; Validation Loss = 0.034715660382593154\n",
            "Cost after 296876 iterations : Training Loss =  0.028094062747464968; Validation Loss = 0.034715651846816395\n",
            "Cost after 296877 iterations : Training Loss =  0.028094052779441686; Validation Loss = 0.03471564331112606\n",
            "Cost after 296878 iterations : Training Loss =  0.028094042811532702; Validation Loss = 0.03471563477552168\n",
            "Cost after 296879 iterations : Training Loss =  0.028094032843738286; Validation Loss = 0.03471562624000346\n",
            "Cost after 296880 iterations : Training Loss =  0.028094022876058226; Validation Loss = 0.03471561770457141\n",
            "Cost after 296881 iterations : Training Loss =  0.028094012908492814; Validation Loss = 0.03471560916922572\n",
            "Cost after 296882 iterations : Training Loss =  0.02809400294104174; Validation Loss = 0.034715600633965824\n",
            "Cost after 296883 iterations : Training Loss =  0.028093992973705156; Validation Loss = 0.0347155920987923\n",
            "Cost after 296884 iterations : Training Loss =  0.028093983006483105; Validation Loss = 0.034715583563704897\n",
            "Cost after 296885 iterations : Training Loss =  0.028093973039375567; Validation Loss = 0.03471557502870365\n",
            "Cost after 296886 iterations : Training Loss =  0.028093963072382332; Validation Loss = 0.0347155664937884\n",
            "Cost after 296887 iterations : Training Loss =  0.028093953105503604; Validation Loss = 0.03471555795895937\n",
            "Cost after 296888 iterations : Training Loss =  0.02809394313873959; Validation Loss = 0.034715549424216716\n",
            "Cost after 296889 iterations : Training Loss =  0.028093933172089858; Validation Loss = 0.034715540889560005\n",
            "Cost after 296890 iterations : Training Loss =  0.02809392320555456; Validation Loss = 0.034715532354989496\n",
            "Cost after 296891 iterations : Training Loss =  0.028093913239133777; Validation Loss = 0.034715523820505044\n",
            "Cost after 296892 iterations : Training Loss =  0.028093903272827365; Validation Loss = 0.034715515286106426\n",
            "Cost after 296893 iterations : Training Loss =  0.02809389330663543; Validation Loss = 0.03471550675179429\n",
            "Cost after 296894 iterations : Training Loss =  0.02809388334055805; Validation Loss = 0.03471549821756853\n",
            "Cost after 296895 iterations : Training Loss =  0.028093873374595155; Validation Loss = 0.03471548968342875\n",
            "Cost after 296896 iterations : Training Loss =  0.02809386340874664; Validation Loss = 0.03471548114937507\n",
            "Cost after 296897 iterations : Training Loss =  0.02809385344301243; Validation Loss = 0.03471547261540734\n",
            "Cost after 296898 iterations : Training Loss =  0.028093843477392914; Validation Loss = 0.03471546408152563\n",
            "Cost after 296899 iterations : Training Loss =  0.028093833511887763; Validation Loss = 0.03471545554773032\n",
            "Cost after 296900 iterations : Training Loss =  0.028093823546496987; Validation Loss = 0.03471544701402127\n",
            "Cost after 296901 iterations : Training Loss =  0.02809381358122065; Validation Loss = 0.034715438480397875\n",
            "Cost after 296902 iterations : Training Loss =  0.028093803616058868; Validation Loss = 0.0347154299468607\n",
            "Cost after 296903 iterations : Training Loss =  0.028093793651011594; Validation Loss = 0.03471542141341002\n",
            "Cost after 296904 iterations : Training Loss =  0.028093783686078615; Validation Loss = 0.034715412880045045\n",
            "Cost after 296905 iterations : Training Loss =  0.028093773721259978; Validation Loss = 0.034715404346766295\n",
            "Cost after 296906 iterations : Training Loss =  0.028093763756555916; Validation Loss = 0.03471539581357377\n",
            "Cost after 296907 iterations : Training Loss =  0.028093753791966342; Validation Loss = 0.0347153872804671\n",
            "Cost after 296908 iterations : Training Loss =  0.028093743827491052; Validation Loss = 0.03471537874744694\n",
            "Cost after 296909 iterations : Training Loss =  0.028093733863130292; Validation Loss = 0.03471537021451255\n",
            "Cost after 296910 iterations : Training Loss =  0.02809372389888387; Validation Loss = 0.03471536168166455\n",
            "Cost after 296911 iterations : Training Loss =  0.028093713934751987; Validation Loss = 0.0347153531489026\n",
            "Cost after 296912 iterations : Training Loss =  0.028093703970734622; Validation Loss = 0.03471534461622641\n",
            "Cost after 296913 iterations : Training Loss =  0.028093694006831486; Validation Loss = 0.034715336083636736\n",
            "Cost after 296914 iterations : Training Loss =  0.028093684043042785; Validation Loss = 0.034715327551133136\n",
            "Cost after 296915 iterations : Training Loss =  0.028093674079368653; Validation Loss = 0.03471531901871539\n",
            "Cost after 296916 iterations : Training Loss =  0.028093664115808808; Validation Loss = 0.03471531048638379\n",
            "Cost after 296917 iterations : Training Loss =  0.028093654152363437; Validation Loss = 0.03471530195413854\n",
            "Cost after 296918 iterations : Training Loss =  0.028093644189032582; Validation Loss = 0.03471529342197925\n",
            "Cost after 296919 iterations : Training Loss =  0.02809363422581598; Validation Loss = 0.03471528488990618\n",
            "Cost after 296920 iterations : Training Loss =  0.028093624262713893; Validation Loss = 0.03471527635791915\n",
            "Cost after 296921 iterations : Training Loss =  0.02809361429972609; Validation Loss = 0.034715267826018276\n",
            "Cost after 296922 iterations : Training Loss =  0.02809360433685273; Validation Loss = 0.03471525929420333\n",
            "Cost after 296923 iterations : Training Loss =  0.028093594374093915; Validation Loss = 0.034715250762474906\n",
            "Cost after 296924 iterations : Training Loss =  0.02809358441144944; Validation Loss = 0.03471524223083238\n",
            "Cost after 296925 iterations : Training Loss =  0.028093574448919276; Validation Loss = 0.03471523369927551\n",
            "Cost after 296926 iterations : Training Loss =  0.028093564486503606; Validation Loss = 0.034715225167805196\n",
            "Cost after 296927 iterations : Training Loss =  0.02809355452420245; Validation Loss = 0.03471521663642073\n",
            "Cost after 296928 iterations : Training Loss =  0.028093544562015575; Validation Loss = 0.03471520810512262\n",
            "Cost after 296929 iterations : Training Loss =  0.028093534599943; Validation Loss = 0.03471519957391016\n",
            "Cost after 296930 iterations : Training Loss =  0.028093524637985014; Validation Loss = 0.03471519104278406\n",
            "Cost after 296931 iterations : Training Loss =  0.02809351467614135; Validation Loss = 0.03471518251174384\n",
            "Cost after 296932 iterations : Training Loss =  0.028093504714411993; Validation Loss = 0.0347151739807898\n",
            "Cost after 296933 iterations : Training Loss =  0.028093494752797144; Validation Loss = 0.034715165449922196\n",
            "Cost after 296934 iterations : Training Loss =  0.02809348479129658; Validation Loss = 0.034715156919140226\n",
            "Cost after 296935 iterations : Training Loss =  0.028093474829910443; Validation Loss = 0.03471514838844459\n",
            "Cost after 296936 iterations : Training Loss =  0.02809346486863873; Validation Loss = 0.03471513985783512\n",
            "Cost after 296937 iterations : Training Loss =  0.028093454907481365; Validation Loss = 0.034715131327311716\n",
            "Cost after 296938 iterations : Training Loss =  0.028093444946438444; Validation Loss = 0.03471512279687436\n",
            "Cost after 296939 iterations : Training Loss =  0.028093434985509914; Validation Loss = 0.03471511426652297\n",
            "Cost after 296940 iterations : Training Loss =  0.028093425024695674; Validation Loss = 0.034715105736257454\n",
            "Cost after 296941 iterations : Training Loss =  0.028093415063995958; Validation Loss = 0.034715097206078266\n",
            "Cost after 296942 iterations : Training Loss =  0.028093405103410452; Validation Loss = 0.03471508867598525\n",
            "Cost after 296943 iterations : Training Loss =  0.028093395142939463; Validation Loss = 0.03471508014597828\n",
            "Cost after 296944 iterations : Training Loss =  0.028093385182582812; Validation Loss = 0.034715071616057344\n",
            "Cost after 296945 iterations : Training Loss =  0.02809337522234045; Validation Loss = 0.034715063086222464\n",
            "Cost after 296946 iterations : Training Loss =  0.02809336526221259; Validation Loss = 0.03471505455647354\n",
            "Cost after 296947 iterations : Training Loss =  0.028093355302198947; Validation Loss = 0.0347150460268108\n",
            "Cost after 296948 iterations : Training Loss =  0.028093345342299868; Validation Loss = 0.03471503749723378\n",
            "Cost after 296949 iterations : Training Loss =  0.02809333538251491; Validation Loss = 0.03471502896774332\n",
            "Cost after 296950 iterations : Training Loss =  0.028093325422844487; Validation Loss = 0.0347150204383385\n",
            "Cost after 296951 iterations : Training Loss =  0.028093315463288276; Validation Loss = 0.03471501190901999\n",
            "Cost after 296952 iterations : Training Loss =  0.028093305503846686; Validation Loss = 0.034715003379787376\n",
            "Cost after 296953 iterations : Training Loss =  0.02809329554451924; Validation Loss = 0.03471499485064103\n",
            "Cost after 296954 iterations : Training Loss =  0.02809328558530627; Validation Loss = 0.03471498632158062\n",
            "Cost after 296955 iterations : Training Loss =  0.02809327562620765; Validation Loss = 0.034714977792606336\n",
            "Cost after 296956 iterations : Training Loss =  0.028093265667223432; Validation Loss = 0.03471496926371792\n",
            "Cost after 296957 iterations : Training Loss =  0.028093255708353332; Validation Loss = 0.03471496073491562\n",
            "Cost after 296958 iterations : Training Loss =  0.028093245749597765; Validation Loss = 0.03471495220619981\n",
            "Cost after 296959 iterations : Training Loss =  0.028093235790956444; Validation Loss = 0.03471494367756983\n",
            "Cost after 296960 iterations : Training Loss =  0.02809322583242952; Validation Loss = 0.03471493514902577\n",
            "Cost after 296961 iterations : Training Loss =  0.02809321587401709; Validation Loss = 0.03471492662056777\n",
            "Cost after 296962 iterations : Training Loss =  0.02809320591571884; Validation Loss = 0.03471491809219569\n",
            "Cost after 296963 iterations : Training Loss =  0.02809319595753503; Validation Loss = 0.03471490956390973\n",
            "Cost after 296964 iterations : Training Loss =  0.02809318599946556; Validation Loss = 0.03471490103570993\n",
            "Cost after 296965 iterations : Training Loss =  0.028093176041510286; Validation Loss = 0.034714892507595775\n",
            "Cost after 296966 iterations : Training Loss =  0.02809316608366942; Validation Loss = 0.03471488397956803\n",
            "Cost after 296967 iterations : Training Loss =  0.02809315612594293; Validation Loss = 0.0347148754516264\n",
            "Cost after 296968 iterations : Training Loss =  0.028093146168330838; Validation Loss = 0.03471486692377075\n",
            "Cost after 296969 iterations : Training Loss =  0.02809313621083296; Validation Loss = 0.034714858396001386\n",
            "Cost after 296970 iterations : Training Loss =  0.028093126253449504; Validation Loss = 0.03471484986831762\n",
            "Cost after 296971 iterations : Training Loss =  0.028093116296180318; Validation Loss = 0.034714841340720164\n",
            "Cost after 296972 iterations : Training Loss =  0.028093106339025457; Validation Loss = 0.034714832813208744\n",
            "Cost after 296973 iterations : Training Loss =  0.028093096381985014; Validation Loss = 0.03471482428578313\n",
            "Cost after 296974 iterations : Training Loss =  0.028093086425058884; Validation Loss = 0.03471481575844374\n",
            "Cost after 296975 iterations : Training Loss =  0.028093076468246925; Validation Loss = 0.03471480723119046\n",
            "Cost after 296976 iterations : Training Loss =  0.02809306651154938; Validation Loss = 0.03471479870402331\n",
            "Cost after 296977 iterations : Training Loss =  0.028093056554966193; Validation Loss = 0.03471479017694209\n",
            "Cost after 296978 iterations : Training Loss =  0.028093046598497335; Validation Loss = 0.03471478164994686\n",
            "Cost after 296979 iterations : Training Loss =  0.028093036642142813; Validation Loss = 0.03471477312303773\n",
            "Cost after 296980 iterations : Training Loss =  0.02809302668590251; Validation Loss = 0.03471476459621438\n",
            "Cost after 296981 iterations : Training Loss =  0.02809301672977656; Validation Loss = 0.03471475606947759\n",
            "Cost after 296982 iterations : Training Loss =  0.028093006773764837; Validation Loss = 0.034714747542826385\n",
            "Cost after 296983 iterations : Training Loss =  0.02809299681786756; Validation Loss = 0.034714739016261306\n",
            "Cost after 296984 iterations : Training Loss =  0.02809298686208455; Validation Loss = 0.03471473048978219\n",
            "Cost after 296985 iterations : Training Loss =  0.028092976906415847; Validation Loss = 0.03471472196338893\n",
            "Cost after 296986 iterations : Training Loss =  0.028092966950861397; Validation Loss = 0.03471471343708185\n",
            "Cost after 296987 iterations : Training Loss =  0.028092956995421383; Validation Loss = 0.034714704910860705\n",
            "Cost after 296988 iterations : Training Loss =  0.02809294704009557; Validation Loss = 0.0347146963847259\n",
            "Cost after 296989 iterations : Training Loss =  0.0280929370848841; Validation Loss = 0.034714687858676894\n",
            "Cost after 296990 iterations : Training Loss =  0.028092927129786886; Validation Loss = 0.03471467933271424\n",
            "Cost after 296991 iterations : Training Loss =  0.028092917174803965; Validation Loss = 0.03471467080683731\n",
            "Cost after 296992 iterations : Training Loss =  0.028092907219935445; Validation Loss = 0.03471466228104645\n",
            "Cost after 296993 iterations : Training Loss =  0.02809289726518111; Validation Loss = 0.034714653755341385\n",
            "Cost after 296994 iterations : Training Loss =  0.02809288731054117; Validation Loss = 0.03471464522972246\n",
            "Cost after 296995 iterations : Training Loss =  0.02809287735601544; Validation Loss = 0.03471463670418983\n",
            "Cost after 296996 iterations : Training Loss =  0.02809286740160399; Validation Loss = 0.03471462817874278\n",
            "Cost after 296997 iterations : Training Loss =  0.028092857447306864; Validation Loss = 0.0347146196533819\n",
            "Cost after 296998 iterations : Training Loss =  0.02809284749312406; Validation Loss = 0.03471461112810692\n",
            "Cost after 296999 iterations : Training Loss =  0.02809283753905554; Validation Loss = 0.03471460260291808\n",
            "Cost after 297000 iterations : Training Loss =  0.028092827585101177; Validation Loss = 0.03471459407781507\n",
            "Cost after 297001 iterations : Training Loss =  0.028092817631261303; Validation Loss = 0.034714585552798244\n",
            "Cost after 297002 iterations : Training Loss =  0.028092807677535553; Validation Loss = 0.03471457702786728\n",
            "Cost after 297003 iterations : Training Loss =  0.02809279772392411; Validation Loss = 0.03471456850302253\n",
            "Cost after 297004 iterations : Training Loss =  0.02809278777042692; Validation Loss = 0.03471455997826352\n",
            "Cost after 297005 iterations : Training Loss =  0.028092777817044112; Validation Loss = 0.0347145514535907\n",
            "Cost after 297006 iterations : Training Loss =  0.028092767863775462; Validation Loss = 0.034714542929003986\n",
            "Cost after 297007 iterations : Training Loss =  0.02809275791062116; Validation Loss = 0.0347145344045029\n",
            "Cost after 297008 iterations : Training Loss =  0.028092747957581002; Validation Loss = 0.03471452588008826\n",
            "Cost after 297009 iterations : Training Loss =  0.02809273800465528; Validation Loss = 0.034714517355759075\n",
            "Cost after 297010 iterations : Training Loss =  0.028092728051843735; Validation Loss = 0.03471450883151644\n",
            "Cost after 297011 iterations : Training Loss =  0.028092718099146485; Validation Loss = 0.034714500307359654\n",
            "Cost after 297012 iterations : Training Loss =  0.028092708146563485; Validation Loss = 0.03471449178328894\n",
            "Cost after 297013 iterations : Training Loss =  0.028092698194094795; Validation Loss = 0.03471448325930387\n",
            "Cost after 297014 iterations : Training Loss =  0.028092688241740223; Validation Loss = 0.03471447473540509\n",
            "Cost after 297015 iterations : Training Loss =  0.02809267828950011; Validation Loss = 0.03471446621159215\n",
            "Cost after 297016 iterations : Training Loss =  0.02809266833737422; Validation Loss = 0.03471445768786532\n",
            "Cost after 297017 iterations : Training Loss =  0.028092658385362387; Validation Loss = 0.03471444916422436\n",
            "Cost after 297018 iterations : Training Loss =  0.028092648433464963; Validation Loss = 0.03471444064066959\n",
            "Cost after 297019 iterations : Training Loss =  0.02809263848168192; Validation Loss = 0.034714432117200655\n",
            "Cost after 297020 iterations : Training Loss =  0.028092628530012775; Validation Loss = 0.03471442359381753\n",
            "Cost after 297021 iterations : Training Loss =  0.028092618578458187; Validation Loss = 0.0347144150705205\n",
            "Cost after 297022 iterations : Training Loss =  0.02809260862701766; Validation Loss = 0.03471440654730961\n",
            "Cost after 297023 iterations : Training Loss =  0.02809259867569161; Validation Loss = 0.034714398024184294\n",
            "Cost after 297024 iterations : Training Loss =  0.02809258872447956; Validation Loss = 0.03471438950114544\n",
            "Cost after 297025 iterations : Training Loss =  0.028092578773381886; Validation Loss = 0.034714380978192454\n",
            "Cost after 297026 iterations : Training Loss =  0.02809256882239836; Validation Loss = 0.034714372455325555\n",
            "Cost after 297027 iterations : Training Loss =  0.028092558871529213; Validation Loss = 0.0347143639325442\n",
            "Cost after 297028 iterations : Training Loss =  0.028092548920774122; Validation Loss = 0.034714355409848796\n",
            "Cost after 297029 iterations : Training Loss =  0.02809253897013345; Validation Loss = 0.03471434688723969\n",
            "Cost after 297030 iterations : Training Loss =  0.028092529019606904; Validation Loss = 0.03471433836471635\n",
            "Cost after 297031 iterations : Training Loss =  0.0280925190691946; Validation Loss = 0.03471432984227923\n",
            "Cost after 297032 iterations : Training Loss =  0.028092509118896505; Validation Loss = 0.03471432131992801\n",
            "Cost after 297033 iterations : Training Loss =  0.028092499168712707; Validation Loss = 0.03471431279766262\n",
            "Cost after 297034 iterations : Training Loss =  0.028092489218643175; Validation Loss = 0.034714304275482964\n",
            "Cost after 297035 iterations : Training Loss =  0.028092479268687746; Validation Loss = 0.03471429575338977\n",
            "Cost after 297036 iterations : Training Loss =  0.028092469318846677; Validation Loss = 0.034714287231382264\n",
            "Cost after 297037 iterations : Training Loss =  0.028092459369119697; Validation Loss = 0.03471427870946109\n",
            "Cost after 297038 iterations : Training Loss =  0.028092449419507087; Validation Loss = 0.034714270187625455\n",
            "Cost after 297039 iterations : Training Loss =  0.028092439470008525; Validation Loss = 0.03471426166587601\n",
            "Cost after 297040 iterations : Training Loss =  0.028092429520624355; Validation Loss = 0.0347142531442125\n",
            "Cost after 297041 iterations : Training Loss =  0.028092419571354277; Validation Loss = 0.03471424462263483\n",
            "Cost after 297042 iterations : Training Loss =  0.028092409622198465; Validation Loss = 0.034714236101143145\n",
            "Cost after 297043 iterations : Training Loss =  0.028092399673156767; Validation Loss = 0.03471422757973742\n",
            "Cost after 297044 iterations : Training Loss =  0.0280923897242294; Validation Loss = 0.03471421905841788\n",
            "Cost after 297045 iterations : Training Loss =  0.028092379775416242; Validation Loss = 0.03471421053718422\n",
            "Cost after 297046 iterations : Training Loss =  0.028092369826717235; Validation Loss = 0.0347142020160364\n",
            "Cost after 297047 iterations : Training Loss =  0.028092359878132408; Validation Loss = 0.03471419349497447\n",
            "Cost after 297048 iterations : Training Loss =  0.028092349929661874; Validation Loss = 0.034714184973998745\n",
            "Cost after 297049 iterations : Training Loss =  0.028092339981305572; Validation Loss = 0.0347141764531085\n",
            "Cost after 297050 iterations : Training Loss =  0.028092330033063453; Validation Loss = 0.034714167932304286\n",
            "Cost after 297051 iterations : Training Loss =  0.028092320084935406; Validation Loss = 0.03471415941158621\n",
            "Cost after 297052 iterations : Training Loss =  0.028092310136921702; Validation Loss = 0.03471415089095402\n",
            "Cost after 297053 iterations : Training Loss =  0.02809230018902207; Validation Loss = 0.03471414237040743\n",
            "Cost after 297054 iterations : Training Loss =  0.02809229024123679; Validation Loss = 0.03471413384994722\n",
            "Cost after 297055 iterations : Training Loss =  0.028092280293565565; Validation Loss = 0.03471412532957269\n",
            "Cost after 297056 iterations : Training Loss =  0.02809227034600855; Validation Loss = 0.03471411680928446\n",
            "Cost after 297057 iterations : Training Loss =  0.028092260398565855; Validation Loss = 0.034714108289082\n",
            "Cost after 297058 iterations : Training Loss =  0.028092250451237172; Validation Loss = 0.034714099768965626\n",
            "Cost after 297059 iterations : Training Loss =  0.028092240504022783; Validation Loss = 0.034714091248935264\n",
            "Cost after 297060 iterations : Training Loss =  0.028092230556922497; Validation Loss = 0.034714082728990625\n",
            "Cost after 297061 iterations : Training Loss =  0.028092220609936436; Validation Loss = 0.03471407420913186\n",
            "Cost after 297062 iterations : Training Loss =  0.02809221066306473; Validation Loss = 0.034714065689358836\n",
            "Cost after 297063 iterations : Training Loss =  0.028092200716306994; Validation Loss = 0.03471405716967187\n",
            "Cost after 297064 iterations : Training Loss =  0.028092190769663468; Validation Loss = 0.03471404865007099\n",
            "Cost after 297065 iterations : Training Loss =  0.02809218082313413; Validation Loss = 0.03471404013055602\n",
            "Cost after 297066 iterations : Training Loss =  0.028092170876719044; Validation Loss = 0.034714031611126746\n",
            "Cost after 297067 iterations : Training Loss =  0.02809216093041806; Validation Loss = 0.03471402309178353\n",
            "Cost after 297068 iterations : Training Loss =  0.0280921509842312; Validation Loss = 0.03471401457252666\n",
            "Cost after 297069 iterations : Training Loss =  0.0280921410381586; Validation Loss = 0.034714006053355236\n",
            "Cost after 297070 iterations : Training Loss =  0.028092131092200226; Validation Loss = 0.03471399753426977\n",
            "Cost after 297071 iterations : Training Loss =  0.028092121146355845; Validation Loss = 0.034713989015270265\n",
            "Cost after 297072 iterations : Training Loss =  0.028092111200625686; Validation Loss = 0.03471398049635655\n",
            "Cost after 297073 iterations : Training Loss =  0.028092101255009744; Validation Loss = 0.034713971977528936\n",
            "Cost after 297074 iterations : Training Loss =  0.028092091309508075; Validation Loss = 0.03471396345878726\n",
            "Cost after 297075 iterations : Training Loss =  0.02809208136412038; Validation Loss = 0.03471395494013164\n",
            "Cost after 297076 iterations : Training Loss =  0.028092071418846905; Validation Loss = 0.03471394642156152\n",
            "Cost after 297077 iterations : Training Loss =  0.028092061473687657; Validation Loss = 0.03471393790307754\n",
            "Cost after 297078 iterations : Training Loss =  0.028092051528642345; Validation Loss = 0.03471392938467964\n",
            "Cost after 297079 iterations : Training Loss =  0.02809204158371148; Validation Loss = 0.034713920866367314\n",
            "Cost after 297080 iterations : Training Loss =  0.02809203163889451; Validation Loss = 0.03471391234814098\n",
            "Cost after 297081 iterations : Training Loss =  0.028092021694191817; Validation Loss = 0.03471390383000083\n",
            "Cost after 297082 iterations : Training Loss =  0.028092011749603327; Validation Loss = 0.03471389531194651\n",
            "Cost after 297083 iterations : Training Loss =  0.028092001805128906; Validation Loss = 0.03471388679397817\n",
            "Cost after 297084 iterations : Training Loss =  0.0280919918607687; Validation Loss = 0.034713878276095574\n",
            "Cost after 297085 iterations : Training Loss =  0.02809198191652253; Validation Loss = 0.034713869758298606\n",
            "Cost after 297086 iterations : Training Loss =  0.02809197197239069; Validation Loss = 0.03471386124058775\n",
            "Cost after 297087 iterations : Training Loss =  0.028091962028372774; Validation Loss = 0.0347138527229629\n",
            "Cost after 297088 iterations : Training Loss =  0.028091952084469084; Validation Loss = 0.03471384420542394\n",
            "Cost after 297089 iterations : Training Loss =  0.02809194214067954; Validation Loss = 0.03471383568797109\n",
            "Cost after 297090 iterations : Training Loss =  0.028091932197004264; Validation Loss = 0.03471382717060379\n",
            "Cost after 297091 iterations : Training Loss =  0.02809192225344297; Validation Loss = 0.03471381865332273\n",
            "Cost after 297092 iterations : Training Loss =  0.02809191230999573; Validation Loss = 0.03471381013612739\n",
            "Cost after 297093 iterations : Training Loss =  0.02809190236666284; Validation Loss = 0.034713801619017895\n",
            "Cost after 297094 iterations : Training Loss =  0.028091892423443926; Validation Loss = 0.0347137931019943\n",
            "Cost after 297095 iterations : Training Loss =  0.028091882480339187; Validation Loss = 0.03471378458505678\n",
            "Cost after 297096 iterations : Training Loss =  0.028091872537348433; Validation Loss = 0.03471377606820493\n",
            "Cost after 297097 iterations : Training Loss =  0.028091862594472012; Validation Loss = 0.03471376755143925\n",
            "Cost after 297098 iterations : Training Loss =  0.028091852651709673; Validation Loss = 0.034713759034759394\n",
            "Cost after 297099 iterations : Training Loss =  0.02809184270906146; Validation Loss = 0.03471375051816536\n",
            "Cost after 297100 iterations : Training Loss =  0.028091832766527294; Validation Loss = 0.03471374200165734\n",
            "Cost after 297101 iterations : Training Loss =  0.02809182282410732; Validation Loss = 0.03471373348523504\n",
            "Cost after 297102 iterations : Training Loss =  0.028091812881801497; Validation Loss = 0.03471372496889857\n",
            "Cost after 297103 iterations : Training Loss =  0.02809180293960971; Validation Loss = 0.034713716452648\n",
            "Cost after 297104 iterations : Training Loss =  0.02809179299753204; Validation Loss = 0.03471370793648333\n",
            "Cost after 297105 iterations : Training Loss =  0.028091783055568486; Validation Loss = 0.03471369942040468\n",
            "Cost after 297106 iterations : Training Loss =  0.028091773113719076; Validation Loss = 0.03471369090441171\n",
            "Cost after 297107 iterations : Training Loss =  0.028091763171983808; Validation Loss = 0.03471368238850448\n",
            "Cost after 297108 iterations : Training Loss =  0.028091753230362438; Validation Loss = 0.03471367387268339\n",
            "Cost after 297109 iterations : Training Loss =  0.028091743288855442; Validation Loss = 0.03471366535694841\n",
            "Cost after 297110 iterations : Training Loss =  0.028091733347462417; Validation Loss = 0.034713656841298575\n",
            "Cost after 297111 iterations : Training Loss =  0.02809172340618358; Validation Loss = 0.03471364832573499\n",
            "Cost after 297112 iterations : Training Loss =  0.02809171346501877; Validation Loss = 0.034713639810257366\n",
            "Cost after 297113 iterations : Training Loss =  0.028091703523967965; Validation Loss = 0.03471363129486554\n",
            "Cost after 297114 iterations : Training Loss =  0.02809169358303141; Validation Loss = 0.034713622779559375\n",
            "Cost after 297115 iterations : Training Loss =  0.02809168364220885; Validation Loss = 0.03471361426433937\n",
            "Cost after 297116 iterations : Training Loss =  0.028091673701500543; Validation Loss = 0.034713605749205234\n",
            "Cost after 297117 iterations : Training Loss =  0.028091663760906185; Validation Loss = 0.034713597234156686\n",
            "Cost after 297118 iterations : Training Loss =  0.02809165382042595; Validation Loss = 0.03471358871919428\n",
            "Cost after 297119 iterations : Training Loss =  0.028091643880059834; Validation Loss = 0.03471358020431778\n",
            "Cost after 297120 iterations : Training Loss =  0.028091633939807754; Validation Loss = 0.034713571689527115\n",
            "Cost after 297121 iterations : Training Loss =  0.028091623999669878; Validation Loss = 0.03471356317482224\n",
            "Cost after 297122 iterations : Training Loss =  0.028091614059645966; Validation Loss = 0.034713554660203066\n",
            "Cost after 297123 iterations : Training Loss =  0.028091604119736276; Validation Loss = 0.03471354614567013\n",
            "Cost after 297124 iterations : Training Loss =  0.028091594179940442; Validation Loss = 0.03471353763122297\n",
            "Cost after 297125 iterations : Training Loss =  0.028091584240258864; Validation Loss = 0.03471352911686147\n",
            "Cost after 297126 iterations : Training Loss =  0.028091574300691327; Validation Loss = 0.03471352060258628\n",
            "Cost after 297127 iterations : Training Loss =  0.02809156436123788; Validation Loss = 0.0347135120883965\n",
            "Cost after 297128 iterations : Training Loss =  0.028091554421898386; Validation Loss = 0.03471350357429277\n",
            "Cost after 297129 iterations : Training Loss =  0.028091544482673172; Validation Loss = 0.034713495060274734\n",
            "Cost after 297130 iterations : Training Loss =  0.02809153454356188; Validation Loss = 0.034713486546342286\n",
            "Cost after 297131 iterations : Training Loss =  0.028091524604564714; Validation Loss = 0.03471347803249645\n",
            "Cost after 297132 iterations : Training Loss =  0.028091514665681567; Validation Loss = 0.03471346951873602\n",
            "Cost after 297133 iterations : Training Loss =  0.028091504726912554; Validation Loss = 0.03471346100506146\n",
            "Cost after 297134 iterations : Training Loss =  0.028091494788257582; Validation Loss = 0.034713452491472445\n",
            "Cost after 297135 iterations : Training Loss =  0.028091484849716686; Validation Loss = 0.03471344397796969\n",
            "Cost after 297136 iterations : Training Loss =  0.028091474911289813; Validation Loss = 0.0347134354645528\n",
            "Cost after 297137 iterations : Training Loss =  0.02809146497297701; Validation Loss = 0.034713426951221565\n",
            "Cost after 297138 iterations : Training Loss =  0.028091455034778275; Validation Loss = 0.03471341843797609\n",
            "Cost after 297139 iterations : Training Loss =  0.028091445096693562; Validation Loss = 0.034713409924816405\n",
            "Cost after 297140 iterations : Training Loss =  0.02809143515872297; Validation Loss = 0.03471340141174281\n",
            "Cost after 297141 iterations : Training Loss =  0.028091425220866426; Validation Loss = 0.03471339289875466\n",
            "Cost after 297142 iterations : Training Loss =  0.0280914152831239; Validation Loss = 0.03471338438585259\n",
            "Cost after 297143 iterations : Training Loss =  0.02809140534549544; Validation Loss = 0.03471337587303645\n",
            "Cost after 297144 iterations : Training Loss =  0.028091395407981095; Validation Loss = 0.03471336736030617\n",
            "Cost after 297145 iterations : Training Loss =  0.028091385470580717; Validation Loss = 0.034713358847661545\n",
            "Cost after 297146 iterations : Training Loss =  0.02809137553329436; Validation Loss = 0.03471335033510305\n",
            "Cost after 297147 iterations : Training Loss =  0.028091365596122155; Validation Loss = 0.034713341822629876\n",
            "Cost after 297148 iterations : Training Loss =  0.02809135565906379; Validation Loss = 0.03471333331024281\n",
            "Cost after 297149 iterations : Training Loss =  0.028091345722119673; Validation Loss = 0.034713324797941994\n",
            "Cost after 297150 iterations : Training Loss =  0.028091335785289523; Validation Loss = 0.03471331628572665\n",
            "Cost after 297151 iterations : Training Loss =  0.028091325848573403; Validation Loss = 0.03471330777359688\n",
            "Cost after 297152 iterations : Training Loss =  0.02809131591197137; Validation Loss = 0.034713299261553074\n",
            "Cost after 297153 iterations : Training Loss =  0.02809130597548327; Validation Loss = 0.034713290749595276\n",
            "Cost after 297154 iterations : Training Loss =  0.02809129603910931; Validation Loss = 0.03471328223772319\n",
            "Cost after 297155 iterations : Training Loss =  0.02809128610284932; Validation Loss = 0.03471327372593692\n",
            "Cost after 297156 iterations : Training Loss =  0.02809127616670334; Validation Loss = 0.03471326521423643\n",
            "Cost after 297157 iterations : Training Loss =  0.028091266230671433; Validation Loss = 0.03471325670262225\n",
            "Cost after 297158 iterations : Training Loss =  0.028091256294753602; Validation Loss = 0.03471324819109327\n",
            "Cost after 297159 iterations : Training Loss =  0.028091246358949686; Validation Loss = 0.03471323967965049\n",
            "Cost after 297160 iterations : Training Loss =  0.028091236423259884; Validation Loss = 0.03471323116829333\n",
            "Cost after 297161 iterations : Training Loss =  0.02809122648768403; Validation Loss = 0.03471322265702223\n",
            "Cost after 297162 iterations : Training Loss =  0.028091216552222287; Validation Loss = 0.03471321414583659\n",
            "Cost after 297163 iterations : Training Loss =  0.028091206616874566; Validation Loss = 0.034713205634736934\n",
            "Cost after 297164 iterations : Training Loss =  0.02809119668164066; Validation Loss = 0.03471319712372289\n",
            "Cost after 297165 iterations : Training Loss =  0.02809118674652098; Validation Loss = 0.0347131886127951\n",
            "Cost after 297166 iterations : Training Loss =  0.028091176811515246; Validation Loss = 0.034713180101953074\n",
            "Cost after 297167 iterations : Training Loss =  0.028091166876623448; Validation Loss = 0.03471317159119663\n",
            "Cost after 297168 iterations : Training Loss =  0.028091156941845764; Validation Loss = 0.03471316308052591\n",
            "Cost after 297169 iterations : Training Loss =  0.02809114700718215; Validation Loss = 0.03471315456994111\n",
            "Cost after 297170 iterations : Training Loss =  0.028091137072632392; Validation Loss = 0.03471314605944221\n",
            "Cost after 297171 iterations : Training Loss =  0.02809112713819668; Validation Loss = 0.03471313754902886\n",
            "Cost after 297172 iterations : Training Loss =  0.028091117203874978; Validation Loss = 0.03471312903870139\n",
            "Cost after 297173 iterations : Training Loss =  0.028091107269667327; Validation Loss = 0.034713120528459526\n",
            "Cost after 297174 iterations : Training Loss =  0.0280910973355737; Validation Loss = 0.03471311201830357\n",
            "Cost after 297175 iterations : Training Loss =  0.028091087401593842; Validation Loss = 0.03471310350823359\n",
            "Cost after 297176 iterations : Training Loss =  0.028091077467728334; Validation Loss = 0.034713094998249316\n",
            "Cost after 297177 iterations : Training Loss =  0.02809106753397656; Validation Loss = 0.03471308648835094\n",
            "Cost after 297178 iterations : Training Loss =  0.028091057600338875; Validation Loss = 0.03471307797853859\n",
            "Cost after 297179 iterations : Training Loss =  0.028091047666815173; Validation Loss = 0.03471306946881183\n",
            "Cost after 297180 iterations : Training Loss =  0.028091037733405457; Validation Loss = 0.034713060959171094\n",
            "Cost after 297181 iterations : Training Loss =  0.02809102780010964; Validation Loss = 0.03471305244961584\n",
            "Cost after 297182 iterations : Training Loss =  0.028091017866927994; Validation Loss = 0.034713043940146564\n",
            "Cost after 297183 iterations : Training Loss =  0.028091007933860185; Validation Loss = 0.034713035430763116\n",
            "Cost after 297184 iterations : Training Loss =  0.028090998000906517; Validation Loss = 0.034713026921465086\n",
            "Cost after 297185 iterations : Training Loss =  0.028090988068066737; Validation Loss = 0.034713018412253036\n",
            "Cost after 297186 iterations : Training Loss =  0.02809097813534094; Validation Loss = 0.034713009903126674\n",
            "Cost after 297187 iterations : Training Loss =  0.028090968202729036; Validation Loss = 0.034713001394086236\n",
            "Cost after 297188 iterations : Training Loss =  0.02809095827023116; Validation Loss = 0.034712992885131286\n",
            "Cost after 297189 iterations : Training Loss =  0.028090948337847324; Validation Loss = 0.0347129843762626\n",
            "Cost after 297190 iterations : Training Loss =  0.028090938405577425; Validation Loss = 0.034712975867479415\n",
            "Cost after 297191 iterations : Training Loss =  0.02809092847342154; Validation Loss = 0.034712967358781946\n",
            "Cost after 297192 iterations : Training Loss =  0.028090918541379524; Validation Loss = 0.034712958850170436\n",
            "Cost after 297193 iterations : Training Loss =  0.028090908609451577; Validation Loss = 0.034712950341644455\n",
            "Cost after 297194 iterations : Training Loss =  0.028090898677637584; Validation Loss = 0.0347129418332046\n",
            "Cost after 297195 iterations : Training Loss =  0.02809088874593755; Validation Loss = 0.0347129333248504\n",
            "Cost after 297196 iterations : Training Loss =  0.02809087881435139; Validation Loss = 0.0347129248165819\n",
            "Cost after 297197 iterations : Training Loss =  0.028090868882879365; Validation Loss = 0.034712916308399226\n",
            "Cost after 297198 iterations : Training Loss =  0.028090858951521174; Validation Loss = 0.034712907800302305\n",
            "Cost after 297199 iterations : Training Loss =  0.028090849020276986; Validation Loss = 0.034712899292291134\n",
            "Cost after 297200 iterations : Training Loss =  0.028090839089146745; Validation Loss = 0.03471289078436555\n",
            "Cost after 297201 iterations : Training Loss =  0.028090829158130368; Validation Loss = 0.03471288227652591\n",
            "Cost after 297202 iterations : Training Loss =  0.028090819227228125; Validation Loss = 0.03471287376877193\n",
            "Cost after 297203 iterations : Training Loss =  0.028090809296439704; Validation Loss = 0.03471286526110399\n",
            "Cost after 297204 iterations : Training Loss =  0.02809079936576537; Validation Loss = 0.03471285675352154\n",
            "Cost after 297205 iterations : Training Loss =  0.028090789435204856; Validation Loss = 0.03471284824602493\n",
            "Cost after 297206 iterations : Training Loss =  0.028090779504758318; Validation Loss = 0.03471283973861442\n",
            "Cost after 297207 iterations : Training Loss =  0.028090769574425755; Validation Loss = 0.03471283123128924\n",
            "Cost after 297208 iterations : Training Loss =  0.02809075964420707; Validation Loss = 0.034712822724050206\n",
            "Cost after 297209 iterations : Training Loss =  0.028090749714102442; Validation Loss = 0.03471281421689682\n",
            "Cost after 297210 iterations : Training Loss =  0.028090739784111706; Validation Loss = 0.03471280570982887\n",
            "Cost after 297211 iterations : Training Loss =  0.028090729854234928; Validation Loss = 0.03471279720284679\n",
            "Cost after 297212 iterations : Training Loss =  0.028090719924471968; Validation Loss = 0.03471278869595049\n",
            "Cost after 297213 iterations : Training Loss =  0.02809070999482302; Validation Loss = 0.03471278018914012\n",
            "Cost after 297214 iterations : Training Loss =  0.02809070006528808; Validation Loss = 0.034712771682415225\n",
            "Cost after 297215 iterations : Training Loss =  0.028090690135867055; Validation Loss = 0.03471276317577631\n",
            "Cost after 297216 iterations : Training Loss =  0.02809068020655996; Validation Loss = 0.034712754669223\n",
            "Cost after 297217 iterations : Training Loss =  0.028090670277366718; Validation Loss = 0.034712746162755506\n",
            "Cost after 297218 iterations : Training Loss =  0.0280906603482875; Validation Loss = 0.03471273765637376\n",
            "Cost after 297219 iterations : Training Loss =  0.028090650419322124; Validation Loss = 0.03471272915007788\n",
            "Cost after 297220 iterations : Training Loss =  0.02809064049047069; Validation Loss = 0.03471272064386769\n",
            "Cost after 297221 iterations : Training Loss =  0.028090630561733165; Validation Loss = 0.03471271213774318\n",
            "Cost after 297222 iterations : Training Loss =  0.028090620633109575; Validation Loss = 0.03471270363170469\n",
            "Cost after 297223 iterations : Training Loss =  0.02809061070459995; Validation Loss = 0.034712695125751564\n",
            "Cost after 297224 iterations : Training Loss =  0.028090600776204195; Validation Loss = 0.03471268661988439\n",
            "Cost after 297225 iterations : Training Loss =  0.028090590847922436; Validation Loss = 0.03471267811410264\n",
            "Cost after 297226 iterations : Training Loss =  0.028090580919754644; Validation Loss = 0.03471266960840698\n",
            "Cost after 297227 iterations : Training Loss =  0.02809057099170053; Validation Loss = 0.03471266110279695\n",
            "Cost after 297228 iterations : Training Loss =  0.028090561063760525; Validation Loss = 0.03471265259727252\n",
            "Cost after 297229 iterations : Training Loss =  0.028090551135934283; Validation Loss = 0.03471264409183401\n",
            "Cost after 297230 iterations : Training Loss =  0.02809054120822214; Validation Loss = 0.03471263558648098\n",
            "Cost after 297231 iterations : Training Loss =  0.028090531280623818; Validation Loss = 0.03471262708121374\n",
            "Cost after 297232 iterations : Training Loss =  0.028090521353139348; Validation Loss = 0.034712618576032354\n",
            "Cost after 297233 iterations : Training Loss =  0.02809051142576882; Validation Loss = 0.03471261007093667\n",
            "Cost after 297234 iterations : Training Loss =  0.028090501498512256; Validation Loss = 0.034712601565926836\n",
            "Cost after 297235 iterations : Training Loss =  0.02809049157136948; Validation Loss = 0.034712593061002675\n",
            "Cost after 297236 iterations : Training Loss =  0.028090481644340734; Validation Loss = 0.034712584556164015\n",
            "Cost after 297237 iterations : Training Loss =  0.028090471717425812; Validation Loss = 0.03471257605141106\n",
            "Cost after 297238 iterations : Training Loss =  0.02809046179062466; Validation Loss = 0.03471256754674427\n",
            "Cost after 297239 iterations : Training Loss =  0.028090451863937594; Validation Loss = 0.034712559042163034\n",
            "Cost after 297240 iterations : Training Loss =  0.028090441937364475; Validation Loss = 0.034712550537667475\n",
            "Cost after 297241 iterations : Training Loss =  0.028090432010905008; Validation Loss = 0.03471254203325751\n",
            "Cost after 297242 iterations : Training Loss =  0.02809042208455953; Validation Loss = 0.0347125335289333\n",
            "Cost after 297243 iterations : Training Loss =  0.028090412158328016; Validation Loss = 0.03471252502469502\n",
            "Cost after 297244 iterations : Training Loss =  0.02809040223221035; Validation Loss = 0.03471251652054237\n",
            "Cost after 297245 iterations : Training Loss =  0.028090392306206503; Validation Loss = 0.034712508016475574\n",
            "Cost after 297246 iterations : Training Loss =  0.028090382380316612; Validation Loss = 0.03471249951249419\n",
            "Cost after 297247 iterations : Training Loss =  0.028090372454540564; Validation Loss = 0.03471249100859869\n",
            "Cost after 297248 iterations : Training Loss =  0.028090362528878497; Validation Loss = 0.034712482504788755\n",
            "Cost after 297249 iterations : Training Loss =  0.02809035260333021; Validation Loss = 0.03471247400106466\n",
            "Cost after 297250 iterations : Training Loss =  0.028090342677895782; Validation Loss = 0.034712465497426306\n",
            "Cost after 297251 iterations : Training Loss =  0.028090332752575282; Validation Loss = 0.03471245699387342\n",
            "Cost after 297252 iterations : Training Loss =  0.02809032282736867; Validation Loss = 0.03471244849040649\n",
            "Cost after 297253 iterations : Training Loss =  0.02809031290227578; Validation Loss = 0.034712439987025404\n",
            "Cost after 297254 iterations : Training Loss =  0.028090302977296915; Validation Loss = 0.03471243148372964\n",
            "Cost after 297255 iterations : Training Loss =  0.028090293052431946; Validation Loss = 0.03471242298051995\n",
            "Cost after 297256 iterations : Training Loss =  0.028090283127680728; Validation Loss = 0.034712414477395453\n",
            "Cost after 297257 iterations : Training Loss =  0.02809027320304343; Validation Loss = 0.03471240597435708\n",
            "Cost after 297258 iterations : Training Loss =  0.028090263278520038; Validation Loss = 0.03471239747140425\n",
            "Cost after 297259 iterations : Training Loss =  0.028090253354110466; Validation Loss = 0.03471238896853735\n",
            "Cost after 297260 iterations : Training Loss =  0.02809024342981477; Validation Loss = 0.034712380465755824\n",
            "Cost after 297261 iterations : Training Loss =  0.028090233505632948; Validation Loss = 0.034712371963059985\n",
            "Cost after 297262 iterations : Training Loss =  0.028090223581564874; Validation Loss = 0.03471236346045009\n",
            "Cost after 297263 iterations : Training Loss =  0.028090213657610708; Validation Loss = 0.034712354957925594\n",
            "Cost after 297264 iterations : Training Loss =  0.02809020373377045; Validation Loss = 0.034712346455486785\n",
            "Cost after 297265 iterations : Training Loss =  0.028090193810044028; Validation Loss = 0.03471233795313403\n",
            "Cost after 297266 iterations : Training Loss =  0.028090183886431485; Validation Loss = 0.03471232945086689\n",
            "Cost after 297267 iterations : Training Loss =  0.028090173962932747; Validation Loss = 0.03471232094868543\n",
            "Cost after 297268 iterations : Training Loss =  0.028090164039547894; Validation Loss = 0.03471231244658959\n",
            "Cost after 297269 iterations : Training Loss =  0.028090154116276828; Validation Loss = 0.0347123039445796\n",
            "Cost after 297270 iterations : Training Loss =  0.02809014419311966; Validation Loss = 0.034712295442655146\n",
            "Cost after 297271 iterations : Training Loss =  0.02809013427007629; Validation Loss = 0.03471228694081649\n",
            "Cost after 297272 iterations : Training Loss =  0.028090124347146833; Validation Loss = 0.03471227843906325\n",
            "Cost after 297273 iterations : Training Loss =  0.028090114424331097; Validation Loss = 0.03471226993739598\n",
            "Cost after 297274 iterations : Training Loss =  0.028090104501629302; Validation Loss = 0.03471226143581416\n",
            "Cost after 297275 iterations : Training Loss =  0.028090094579041325; Validation Loss = 0.0347122529343181\n",
            "Cost after 297276 iterations : Training Loss =  0.028090084656567198; Validation Loss = 0.034712244432907845\n",
            "Cost after 297277 iterations : Training Loss =  0.02809007473420687; Validation Loss = 0.034712235931583284\n",
            "Cost after 297278 iterations : Training Loss =  0.028090064811960382; Validation Loss = 0.03471222743034431\n",
            "Cost after 297279 iterations : Training Loss =  0.028090054889827694; Validation Loss = 0.034712218929190665\n",
            "Cost after 297280 iterations : Training Loss =  0.028090044967808935; Validation Loss = 0.0347122104281231\n",
            "Cost after 297281 iterations : Training Loss =  0.028090035045903797; Validation Loss = 0.03471220192714117\n",
            "Cost after 297282 iterations : Training Loss =  0.028090025124112693; Validation Loss = 0.03471219342624484\n",
            "Cost after 297283 iterations : Training Loss =  0.028090015202435342; Validation Loss = 0.03471218492543408\n",
            "Cost after 297284 iterations : Training Loss =  0.02809000528087184; Validation Loss = 0.034712176424709355\n",
            "Cost after 297285 iterations : Training Loss =  0.028089995359422124; Validation Loss = 0.0347121679240703\n",
            "Cost after 297286 iterations : Training Loss =  0.028089985438086128; Validation Loss = 0.03471215942351655\n",
            "Cost after 297287 iterations : Training Loss =  0.028089975516864204; Validation Loss = 0.03471215092304852\n",
            "Cost after 297288 iterations : Training Loss =  0.028089965595755766; Validation Loss = 0.03471214242266633\n",
            "Cost after 297289 iterations : Training Loss =  0.028089955674761413; Validation Loss = 0.03471213392236972\n",
            "Cost after 297290 iterations : Training Loss =  0.028089945753880727; Validation Loss = 0.03471212542215869\n",
            "Cost after 297291 iterations : Training Loss =  0.028089935833113915; Validation Loss = 0.03471211692203344\n",
            "Cost after 297292 iterations : Training Loss =  0.028089925912460956; Validation Loss = 0.034712108421993626\n",
            "Cost after 297293 iterations : Training Loss =  0.028089915991921767; Validation Loss = 0.034712099922039705\n",
            "Cost after 297294 iterations : Training Loss =  0.028089906071496324; Validation Loss = 0.03471209142217158\n",
            "Cost after 297295 iterations : Training Loss =  0.028089896151184738; Validation Loss = 0.034712082922388914\n",
            "Cost after 297296 iterations : Training Loss =  0.028089886230986995; Validation Loss = 0.0347120744226918\n",
            "Cost after 297297 iterations : Training Loss =  0.028089876310902984; Validation Loss = 0.03471206592308072\n",
            "Cost after 297298 iterations : Training Loss =  0.028089866390932677; Validation Loss = 0.0347120574235549\n",
            "Cost after 297299 iterations : Training Loss =  0.028089856471076268; Validation Loss = 0.03471204892411509\n",
            "Cost after 297300 iterations : Training Loss =  0.028089846551333734; Validation Loss = 0.034712040424760664\n",
            "Cost after 297301 iterations : Training Loss =  0.0280898366317049; Validation Loss = 0.03471203192549235\n",
            "Cost after 297302 iterations : Training Loss =  0.028089826712189864; Validation Loss = 0.034712023426309094\n",
            "Cost after 297303 iterations : Training Loss =  0.028089816792788748; Validation Loss = 0.034712014927211356\n",
            "Cost after 297304 iterations : Training Loss =  0.028089806873501322; Validation Loss = 0.034712006428199806\n",
            "Cost after 297305 iterations : Training Loss =  0.028089796954327624; Validation Loss = 0.034711997929273715\n",
            "Cost after 297306 iterations : Training Loss =  0.028089787035267585; Validation Loss = 0.034711989430433174\n",
            "Cost after 297307 iterations : Training Loss =  0.02808977711632167; Validation Loss = 0.0347119809316784\n",
            "Cost after 297308 iterations : Training Loss =  0.02808976719748934; Validation Loss = 0.03471197243300925\n",
            "Cost after 297309 iterations : Training Loss =  0.028089757278770836; Validation Loss = 0.03471196393442565\n",
            "Cost after 297310 iterations : Training Loss =  0.02808974736016614; Validation Loss = 0.034711955435927956\n",
            "Cost after 297311 iterations : Training Loss =  0.028089737441675162; Validation Loss = 0.03471194693751559\n",
            "Cost after 297312 iterations : Training Loss =  0.028089727523298053; Validation Loss = 0.034711938439189044\n",
            "Cost after 297313 iterations : Training Loss =  0.02808971760503465; Validation Loss = 0.034711929940948016\n",
            "Cost after 297314 iterations : Training Loss =  0.028089707686884936; Validation Loss = 0.034711921442792455\n",
            "Cost after 297315 iterations : Training Loss =  0.028089697768849186; Validation Loss = 0.03471191294472262\n",
            "Cost after 297316 iterations : Training Loss =  0.028089687850927005; Validation Loss = 0.03471190444673831\n",
            "Cost after 297317 iterations : Training Loss =  0.028089677933118704; Validation Loss = 0.03471189594883998\n",
            "Cost after 297318 iterations : Training Loss =  0.028089668015424195; Validation Loss = 0.034711887451027025\n",
            "Cost after 297319 iterations : Training Loss =  0.02808965809784339; Validation Loss = 0.034711878953299816\n",
            "Cost after 297320 iterations : Training Loss =  0.028089648180376355; Validation Loss = 0.034711870455658225\n",
            "Cost after 297321 iterations : Training Loss =  0.02808963826302313; Validation Loss = 0.03471186195810228\n",
            "Cost after 297322 iterations : Training Loss =  0.028089628345783626; Validation Loss = 0.0347118534606322\n",
            "Cost after 297323 iterations : Training Loss =  0.02808961842865792; Validation Loss = 0.034711844963247404\n",
            "Cost after 297324 iterations : Training Loss =  0.028089608511645898; Validation Loss = 0.0347118364659485\n",
            "Cost after 297325 iterations : Training Loss =  0.028089598594747626; Validation Loss = 0.03471182796873495\n",
            "Cost after 297326 iterations : Training Loss =  0.028089588677963195; Validation Loss = 0.03471181947160726\n",
            "Cost after 297327 iterations : Training Loss =  0.028089578761292425; Validation Loss = 0.03471181097456493\n",
            "Cost after 297328 iterations : Training Loss =  0.028089568844735482; Validation Loss = 0.03471180247760863\n",
            "Cost after 297329 iterations : Training Loss =  0.02808955892829227; Validation Loss = 0.034711793980737635\n",
            "Cost after 297330 iterations : Training Loss =  0.028089549011962776; Validation Loss = 0.0347117854839526\n",
            "Cost after 297331 iterations : Training Loss =  0.02808953909574707; Validation Loss = 0.03471177698725302\n",
            "Cost after 297332 iterations : Training Loss =  0.028089529179645138; Validation Loss = 0.03471176849063892\n",
            "Cost after 297333 iterations : Training Loss =  0.028089519263656784; Validation Loss = 0.03471175999411049\n",
            "Cost after 297334 iterations : Training Loss =  0.02808950934778225; Validation Loss = 0.03471175149766753\n",
            "Cost after 297335 iterations : Training Loss =  0.028089499432021606; Validation Loss = 0.03471174300131034\n",
            "Cost after 297336 iterations : Training Loss =  0.02808948951637446; Validation Loss = 0.03471173450503891\n",
            "Cost after 297337 iterations : Training Loss =  0.028089479600841245; Validation Loss = 0.03471172600885247\n",
            "Cost after 297338 iterations : Training Loss =  0.028089469685421753; Validation Loss = 0.0347117175127521\n",
            "Cost after 297339 iterations : Training Loss =  0.028089459770115833; Validation Loss = 0.03471170901673726\n",
            "Cost after 297340 iterations : Training Loss =  0.028089449854923742; Validation Loss = 0.03471170052080793\n",
            "Cost after 297341 iterations : Training Loss =  0.028089439939845418; Validation Loss = 0.03471169202496416\n",
            "Cost after 297342 iterations : Training Loss =  0.02808943002488083; Validation Loss = 0.034711683529206165\n",
            "Cost after 297343 iterations : Training Loss =  0.028089420110029937; Validation Loss = 0.03471167503353379\n",
            "Cost after 297344 iterations : Training Loss =  0.028089410195292725; Validation Loss = 0.034711666537946745\n",
            "Cost after 297345 iterations : Training Loss =  0.02808940028066929; Validation Loss = 0.0347116580424453\n",
            "Cost after 297346 iterations : Training Loss =  0.028089390366159563; Validation Loss = 0.03471164954702961\n",
            "Cost after 297347 iterations : Training Loss =  0.028089380451763537; Validation Loss = 0.03471164105169987\n",
            "Cost after 297348 iterations : Training Loss =  0.02808937053748124; Validation Loss = 0.0347116325564556\n",
            "Cost after 297349 iterations : Training Loss =  0.028089360623312635; Validation Loss = 0.03471162406129652\n",
            "Cost after 297350 iterations : Training Loss =  0.028089350709257922; Validation Loss = 0.03471161556622316\n",
            "Cost after 297351 iterations : Training Loss =  0.0280893407953166; Validation Loss = 0.03471160707123555\n",
            "Cost after 297352 iterations : Training Loss =  0.02808933088148924; Validation Loss = 0.03471159857633347\n",
            "Cost after 297353 iterations : Training Loss =  0.02808932096777546; Validation Loss = 0.034711590081517085\n",
            "Cost after 297354 iterations : Training Loss =  0.02808931105417549; Validation Loss = 0.0347115815867861\n",
            "Cost after 297355 iterations : Training Loss =  0.028089301140689065; Validation Loss = 0.03471157309214073\n",
            "Cost after 297356 iterations : Training Loss =  0.028089291227316477; Validation Loss = 0.03471156459758103\n",
            "Cost after 297357 iterations : Training Loss =  0.028089281314057586; Validation Loss = 0.03471155610310689\n",
            "Cost after 297358 iterations : Training Loss =  0.028089271400912445; Validation Loss = 0.03471154760871836\n",
            "Cost after 297359 iterations : Training Loss =  0.028089261487880917; Validation Loss = 0.034711539114415\n",
            "Cost after 297360 iterations : Training Loss =  0.028089251574963076; Validation Loss = 0.034711530620197564\n",
            "Cost after 297361 iterations : Training Loss =  0.028089241662158964; Validation Loss = 0.03471152212606598\n",
            "Cost after 297362 iterations : Training Loss =  0.02808923174946866; Validation Loss = 0.03471151363201973\n",
            "Cost after 297363 iterations : Training Loss =  0.028089221836891803; Validation Loss = 0.034711505138059144\n",
            "Cost after 297364 iterations : Training Loss =  0.028089211924428814; Validation Loss = 0.03471149664418429\n",
            "Cost after 297365 iterations : Training Loss =  0.028089202012079494; Validation Loss = 0.03471148815039498\n",
            "Cost after 297366 iterations : Training Loss =  0.02808919209984384; Validation Loss = 0.034711479656690934\n",
            "Cost after 297367 iterations : Training Loss =  0.02808918218772197; Validation Loss = 0.034711471163072494\n",
            "Cost after 297368 iterations : Training Loss =  0.028089172275713625; Validation Loss = 0.03471146266953976\n",
            "Cost after 297369 iterations : Training Loss =  0.02808916236381904; Validation Loss = 0.03471145417609264\n",
            "Cost after 297370 iterations : Training Loss =  0.02808915245203807; Validation Loss = 0.03471144568273097\n",
            "Cost after 297371 iterations : Training Loss =  0.028089142540370895; Validation Loss = 0.03471143718945485\n",
            "Cost after 297372 iterations : Training Loss =  0.028089132628817286; Validation Loss = 0.034711428696264514\n",
            "Cost after 297373 iterations : Training Loss =  0.02808912271737759; Validation Loss = 0.03471142020315964\n",
            "Cost after 297374 iterations : Training Loss =  0.02808911280605123; Validation Loss = 0.03471141171014042\n",
            "Cost after 297375 iterations : Training Loss =  0.028089102894838777; Validation Loss = 0.034711403217206595\n",
            "Cost after 297376 iterations : Training Loss =  0.028089092983739926; Validation Loss = 0.034711394724358295\n",
            "Cost after 297377 iterations : Training Loss =  0.02808908307275481; Validation Loss = 0.034711386231595726\n",
            "Cost after 297378 iterations : Training Loss =  0.028089073161883155; Validation Loss = 0.03471137773891858\n",
            "Cost after 297379 iterations : Training Loss =  0.028089063251125482; Validation Loss = 0.03471136924632715\n",
            "Cost after 297380 iterations : Training Loss =  0.028089053340481358; Validation Loss = 0.034711360753821155\n",
            "Cost after 297381 iterations : Training Loss =  0.028089043429950725; Validation Loss = 0.034711352261400714\n",
            "Cost after 297382 iterations : Training Loss =  0.028089033519533863; Validation Loss = 0.034711343769066004\n",
            "Cost after 297383 iterations : Training Loss =  0.028089023609230705; Validation Loss = 0.034711335276816684\n",
            "Cost after 297384 iterations : Training Loss =  0.028089013699041182; Validation Loss = 0.034711326784652864\n",
            "Cost after 297385 iterations : Training Loss =  0.028089003788965332; Validation Loss = 0.0347113182925744\n",
            "Cost after 297386 iterations : Training Loss =  0.028088993879003214; Validation Loss = 0.03471130980058152\n",
            "Cost after 297387 iterations : Training Loss =  0.028088983969154595; Validation Loss = 0.03471130130867422\n",
            "Cost after 297388 iterations : Training Loss =  0.02808897405941973; Validation Loss = 0.0347112928168528\n",
            "Cost after 297389 iterations : Training Loss =  0.028088964149798556; Validation Loss = 0.03471128432511665\n",
            "Cost after 297390 iterations : Training Loss =  0.028088954240290863; Validation Loss = 0.034711275833466004\n",
            "Cost after 297391 iterations : Training Loss =  0.028088944330896943; Validation Loss = 0.03471126734190121\n",
            "Cost after 297392 iterations : Training Loss =  0.028088934421616547; Validation Loss = 0.03471125885042156\n",
            "Cost after 297393 iterations : Training Loss =  0.028088924512449945; Validation Loss = 0.034711250359027654\n",
            "Cost after 297394 iterations : Training Loss =  0.028088914603396985; Validation Loss = 0.03471124186771933\n",
            "Cost after 297395 iterations : Training Loss =  0.028088904694457694; Validation Loss = 0.034711233376496724\n",
            "Cost after 297396 iterations : Training Loss =  0.0280888947856318; Validation Loss = 0.034711224885359326\n",
            "Cost after 297397 iterations : Training Loss =  0.028088884876919767; Validation Loss = 0.034711216394307735\n",
            "Cost after 297398 iterations : Training Loss =  0.028088874968321357; Validation Loss = 0.034711207903341555\n",
            "Cost after 297399 iterations : Training Loss =  0.028088865059836505; Validation Loss = 0.03471119941246071\n",
            "Cost after 297400 iterations : Training Loss =  0.02808885515146543; Validation Loss = 0.03471119092166567\n",
            "Cost after 297401 iterations : Training Loss =  0.02808884524320781; Validation Loss = 0.03471118243095625\n",
            "Cost after 297402 iterations : Training Loss =  0.028088835335063855; Validation Loss = 0.03471117394033212\n",
            "Cost after 297403 iterations : Training Loss =  0.028088825427033625; Validation Loss = 0.034711165449793725\n",
            "Cost after 297404 iterations : Training Loss =  0.028088815519117002; Validation Loss = 0.03471115695934091\n",
            "Cost after 297405 iterations : Training Loss =  0.028088805611313914; Validation Loss = 0.0347111484689734\n",
            "Cost after 297406 iterations : Training Loss =  0.028088795703624495; Validation Loss = 0.03471113997869159\n",
            "Cost after 297407 iterations : Training Loss =  0.028088785796048784; Validation Loss = 0.03471113148849506\n",
            "Cost after 297408 iterations : Training Loss =  0.028088775888586517; Validation Loss = 0.03471112299838411\n",
            "Cost after 297409 iterations : Training Loss =  0.02808876598123804; Validation Loss = 0.03471111450835868\n",
            "Cost after 297410 iterations : Training Loss =  0.028088756074003116; Validation Loss = 0.034711106018419174\n",
            "Cost after 297411 iterations : Training Loss =  0.028088746166881725; Validation Loss = 0.03471109752856491\n",
            "Cost after 297412 iterations : Training Loss =  0.028088736259874143; Validation Loss = 0.03471108903879637\n",
            "Cost after 297413 iterations : Training Loss =  0.028088726352980004; Validation Loss = 0.03471108054911309\n",
            "Cost after 297414 iterations : Training Loss =  0.028088716446199515; Validation Loss = 0.03471107205951537\n",
            "Cost after 297415 iterations : Training Loss =  0.028088706539532615; Validation Loss = 0.03471106357000326\n",
            "Cost after 297416 iterations : Training Loss =  0.028088696632979332; Validation Loss = 0.03471105508057671\n",
            "Cost after 297417 iterations : Training Loss =  0.028088686726539684; Validation Loss = 0.0347110465912357\n",
            "Cost after 297418 iterations : Training Loss =  0.02808867682021365; Validation Loss = 0.03471103810197979\n",
            "Cost after 297419 iterations : Training Loss =  0.028088666914001234; Validation Loss = 0.03471102961280959\n",
            "Cost after 297420 iterations : Training Loss =  0.028088657007902383; Validation Loss = 0.034711021123725185\n",
            "Cost after 297421 iterations : Training Loss =  0.02808864710191724; Validation Loss = 0.03471101263472605\n",
            "Cost after 297422 iterations : Training Loss =  0.028088637196045464; Validation Loss = 0.0347110041458123\n",
            "Cost after 297423 iterations : Training Loss =  0.02808862729028756; Validation Loss = 0.03471099565698447\n",
            "Cost after 297424 iterations : Training Loss =  0.028088617384643105; Validation Loss = 0.03471098716824201\n",
            "Cost after 297425 iterations : Training Loss =  0.028088607479112182; Validation Loss = 0.034710978679585\n",
            "Cost after 297426 iterations : Training Loss =  0.02808859757369486; Validation Loss = 0.03471097019101367\n",
            "Cost after 297427 iterations : Training Loss =  0.02808858766839134; Validation Loss = 0.034710961702527424\n",
            "Cost after 297428 iterations : Training Loss =  0.028088577763201165; Validation Loss = 0.0347109532141269\n",
            "Cost after 297429 iterations : Training Loss =  0.0280885678581246; Validation Loss = 0.03471094472581195\n",
            "Cost after 297430 iterations : Training Loss =  0.02808855795316186; Validation Loss = 0.034710936237581966\n",
            "Cost after 297431 iterations : Training Loss =  0.028088548048312435; Validation Loss = 0.03471092774943795\n",
            "Cost after 297432 iterations : Training Loss =  0.028088538143576782; Validation Loss = 0.03471091926137932\n",
            "Cost after 297433 iterations : Training Loss =  0.02808852823895451; Validation Loss = 0.03471091077340618\n",
            "Cost after 297434 iterations : Training Loss =  0.028088518334446044; Validation Loss = 0.03471090228551875\n",
            "Cost after 297435 iterations : Training Loss =  0.02808850843005098; Validation Loss = 0.034710893797716504\n",
            "Cost after 297436 iterations : Training Loss =  0.02808849852576952; Validation Loss = 0.03471088530999996\n",
            "Cost after 297437 iterations : Training Loss =  0.02808848862160164; Validation Loss = 0.03471087682236886\n",
            "Cost after 297438 iterations : Training Loss =  0.028088478717547345; Validation Loss = 0.034710868334823355\n",
            "Cost after 297439 iterations : Training Loss =  0.02808846881360656; Validation Loss = 0.03471085984736319\n",
            "Cost after 297440 iterations : Training Loss =  0.028088458909779554; Validation Loss = 0.034710851359988804\n",
            "Cost after 297441 iterations : Training Loss =  0.028088449006065913; Validation Loss = 0.03471084287269942\n",
            "Cost after 297442 iterations : Training Loss =  0.028088439102465924; Validation Loss = 0.03471083438549564\n",
            "Cost after 297443 iterations : Training Loss =  0.028088429198979473; Validation Loss = 0.034710825898377394\n",
            "Cost after 297444 iterations : Training Loss =  0.0280884192956065; Validation Loss = 0.034710817411344644\n",
            "Cost after 297445 iterations : Training Loss =  0.028088409392347326; Validation Loss = 0.034710808924397284\n",
            "Cost after 297446 iterations : Training Loss =  0.028088399489201502; Validation Loss = 0.03471080043753586\n",
            "Cost after 297447 iterations : Training Loss =  0.028088389586169323; Validation Loss = 0.03471079195075938\n",
            "Cost after 297448 iterations : Training Loss =  0.028088379683250692; Validation Loss = 0.03471078346406862\n",
            "Cost after 297449 iterations : Training Loss =  0.028088369780445502; Validation Loss = 0.03471077497746339\n",
            "Cost after 297450 iterations : Training Loss =  0.028088359877754022; Validation Loss = 0.03471076649094326\n",
            "Cost after 297451 iterations : Training Loss =  0.028088349975176095; Validation Loss = 0.034710758004508756\n",
            "Cost after 297452 iterations : Training Loss =  0.028088340072711562; Validation Loss = 0.03471074951815974\n",
            "Cost after 297453 iterations : Training Loss =  0.028088330170360665; Validation Loss = 0.03471074103189639\n",
            "Cost after 297454 iterations : Training Loss =  0.028088320268123357; Validation Loss = 0.03471073254571819\n",
            "Cost after 297455 iterations : Training Loss =  0.028088310365999514; Validation Loss = 0.034710724059625934\n",
            "Cost after 297456 iterations : Training Loss =  0.028088300463989246; Validation Loss = 0.03471071557361907\n",
            "Cost after 297457 iterations : Training Loss =  0.028088290562092562; Validation Loss = 0.03471070708769736\n",
            "Cost after 297458 iterations : Training Loss =  0.028088280660309467; Validation Loss = 0.03471069860186161\n",
            "Cost after 297459 iterations : Training Loss =  0.028088270758639823; Validation Loss = 0.034710690116110984\n",
            "Cost after 297460 iterations : Training Loss =  0.028088260857083727; Validation Loss = 0.034710681630445835\n",
            "Cost after 297461 iterations : Training Loss =  0.028088250955641155; Validation Loss = 0.03471067314486628\n",
            "Cost after 297462 iterations : Training Loss =  0.02808824105431203; Validation Loss = 0.03471066465937205\n",
            "Cost after 297463 iterations : Training Loss =  0.02808823115309666; Validation Loss = 0.03471065617396341\n",
            "Cost after 297464 iterations : Training Loss =  0.02808822125199467; Validation Loss = 0.03471064768863998\n",
            "Cost after 297465 iterations : Training Loss =  0.02808821135100609; Validation Loss = 0.03471063920340188\n",
            "Cost after 297466 iterations : Training Loss =  0.028088201450131245; Validation Loss = 0.034710630718249744\n",
            "Cost after 297467 iterations : Training Loss =  0.028088191549369826; Validation Loss = 0.03471062223318259\n",
            "Cost after 297468 iterations : Training Loss =  0.028088181648722014; Validation Loss = 0.034710613748200767\n",
            "Cost after 297469 iterations : Training Loss =  0.028088171748187785; Validation Loss = 0.03471060526330484\n",
            "Cost after 297470 iterations : Training Loss =  0.028088161847766916; Validation Loss = 0.034710596778494064\n",
            "Cost after 297471 iterations : Training Loss =  0.028088151947459547; Validation Loss = 0.03471058829376907\n",
            "Cost after 297472 iterations : Training Loss =  0.028088142047265827; Validation Loss = 0.03471057980912931\n",
            "Cost after 297473 iterations : Training Loss =  0.028088132147185453; Validation Loss = 0.03471057132457519\n",
            "Cost after 297474 iterations : Training Loss =  0.02808812224721884; Validation Loss = 0.034710562840106575\n",
            "Cost after 297475 iterations : Training Loss =  0.028088112347365555; Validation Loss = 0.03471055435572314\n",
            "Cost after 297476 iterations : Training Loss =  0.02808810244762591; Validation Loss = 0.03471054587142536\n",
            "Cost after 297477 iterations : Training Loss =  0.028088092547999485; Validation Loss = 0.03471053738721299\n",
            "Cost after 297478 iterations : Training Loss =  0.028088082648486907; Validation Loss = 0.03471052890308594\n",
            "Cost after 297479 iterations : Training Loss =  0.02808807274908775; Validation Loss = 0.03471052041904454\n",
            "Cost after 297480 iterations : Training Loss =  0.028088062849802017; Validation Loss = 0.03471051193508854\n",
            "Cost after 297481 iterations : Training Loss =  0.02808805295062985; Validation Loss = 0.03471050345121789\n",
            "Cost after 297482 iterations : Training Loss =  0.028088043051571133; Validation Loss = 0.03471049496743285\n",
            "Cost after 297483 iterations : Training Loss =  0.028088033152625893; Validation Loss = 0.034710486483732914\n",
            "Cost after 297484 iterations : Training Loss =  0.02808802325379423; Validation Loss = 0.03471047800011857\n",
            "Cost after 297485 iterations : Training Loss =  0.02808801335507598; Validation Loss = 0.03471046951658969\n",
            "Cost after 297486 iterations : Training Loss =  0.028088003456471375; Validation Loss = 0.034710461033145956\n",
            "Cost after 297487 iterations : Training Loss =  0.02808799355798008; Validation Loss = 0.03471045254978803\n",
            "Cost after 297488 iterations : Training Loss =  0.028087983659602373; Validation Loss = 0.03471044406651523\n",
            "Cost after 297489 iterations : Training Loss =  0.028087973761338186; Validation Loss = 0.03471043558332831\n",
            "Cost after 297490 iterations : Training Loss =  0.028087963863187352; Validation Loss = 0.034710427100226544\n",
            "Cost after 297491 iterations : Training Loss =  0.02808795396515016; Validation Loss = 0.03471041861721018\n",
            "Cost after 297492 iterations : Training Loss =  0.028087944067226443; Validation Loss = 0.03471041013427928\n",
            "Cost after 297493 iterations : Training Loss =  0.028087934169416066; Validation Loss = 0.034710401651433694\n",
            "Cost after 297494 iterations : Training Loss =  0.02808792427171929; Validation Loss = 0.0347103931686735\n",
            "Cost after 297495 iterations : Training Loss =  0.028087914374135953; Validation Loss = 0.03471038468599881\n",
            "Cost after 297496 iterations : Training Loss =  0.028087904476666172; Validation Loss = 0.034710376203409526\n",
            "Cost after 297497 iterations : Training Loss =  0.028087894579309818; Validation Loss = 0.034710367720905896\n",
            "Cost after 297498 iterations : Training Loss =  0.028087884682067004; Validation Loss = 0.03471035923848744\n",
            "Cost after 297499 iterations : Training Loss =  0.028087874784937537; Validation Loss = 0.03471035075615415\n",
            "Cost after 297500 iterations : Training Loss =  0.028087864887921584; Validation Loss = 0.034710342273906736\n",
            "Cost after 297501 iterations : Training Loss =  0.028087854991019033; Validation Loss = 0.034710333791744374\n",
            "Cost after 297502 iterations : Training Loss =  0.02808784509423008; Validation Loss = 0.034710325309668014\n",
            "Cost after 297503 iterations : Training Loss =  0.0280878351975545; Validation Loss = 0.03471031682767658\n",
            "Cost after 297504 iterations : Training Loss =  0.02808782530099238; Validation Loss = 0.034710308345770595\n",
            "Cost after 297505 iterations : Training Loss =  0.028087815404543828; Validation Loss = 0.03471029986394984\n",
            "Cost after 297506 iterations : Training Loss =  0.028087805508208656; Validation Loss = 0.034710291382214854\n",
            "Cost after 297507 iterations : Training Loss =  0.02808779561198704; Validation Loss = 0.034710282900565034\n",
            "Cost after 297508 iterations : Training Loss =  0.028087785715878746; Validation Loss = 0.03471027441900097\n",
            "Cost after 297509 iterations : Training Loss =  0.02808777581988398; Validation Loss = 0.03471026593752198\n",
            "Cost after 297510 iterations : Training Loss =  0.02808776592400276; Validation Loss = 0.03471025745612869\n",
            "Cost after 297511 iterations : Training Loss =  0.028087756028234833; Validation Loss = 0.034710248974820807\n",
            "Cost after 297512 iterations : Training Loss =  0.02808774613258045; Validation Loss = 0.034710240493597874\n",
            "Cost after 297513 iterations : Training Loss =  0.02808773623703954; Validation Loss = 0.03471023201246094\n",
            "Cost after 297514 iterations : Training Loss =  0.02808772634161195; Validation Loss = 0.03471022353140914\n",
            "Cost after 297515 iterations : Training Loss =  0.028087716446297963; Validation Loss = 0.03471021505044282\n",
            "Cost after 297516 iterations : Training Loss =  0.028087706551097318; Validation Loss = 0.03471020656956166\n",
            "Cost after 297517 iterations : Training Loss =  0.028087696656010228; Validation Loss = 0.03471019808876609\n",
            "Cost after 297518 iterations : Training Loss =  0.0280876867610365; Validation Loss = 0.034710189608055886\n",
            "Cost after 297519 iterations : Training Loss =  0.028087676866176185; Validation Loss = 0.03471018112743111\n",
            "Cost after 297520 iterations : Training Loss =  0.028087666971429358; Validation Loss = 0.03471017264689183\n",
            "Cost after 297521 iterations : Training Loss =  0.028087657076795943; Validation Loss = 0.03471016416643759\n",
            "Cost after 297522 iterations : Training Loss =  0.028087647182276007; Validation Loss = 0.03471015568606899\n",
            "Cost after 297523 iterations : Training Loss =  0.02808763728786946; Validation Loss = 0.03471014720578583\n",
            "Cost after 297524 iterations : Training Loss =  0.028087627393576335; Validation Loss = 0.03471013872558804\n",
            "Cost after 297525 iterations : Training Loss =  0.028087617499396724; Validation Loss = 0.03471013024547541\n",
            "Cost after 297526 iterations : Training Loss =  0.0280876076053305; Validation Loss = 0.03471012176544833\n",
            "Cost after 297527 iterations : Training Loss =  0.02808759771137763; Validation Loss = 0.03471011328550661\n",
            "Cost after 297528 iterations : Training Loss =  0.028087587817538236; Validation Loss = 0.03471010480565038\n",
            "Cost after 297529 iterations : Training Loss =  0.02808757792381236; Validation Loss = 0.034710096325879514\n",
            "Cost after 297530 iterations : Training Loss =  0.02808756803019978; Validation Loss = 0.03471008784619399\n",
            "Cost after 297531 iterations : Training Loss =  0.028087558136700663; Validation Loss = 0.034710079366593906\n",
            "Cost after 297532 iterations : Training Loss =  0.028087548243315035; Validation Loss = 0.03471007088707905\n",
            "Cost after 297533 iterations : Training Loss =  0.028087538350042695; Validation Loss = 0.0347100624076493\n",
            "Cost after 297534 iterations : Training Loss =  0.028087528456883892; Validation Loss = 0.034710053928305167\n",
            "Cost after 297535 iterations : Training Loss =  0.028087518563838464; Validation Loss = 0.03471004544904649\n",
            "Cost after 297536 iterations : Training Loss =  0.02808750867090645; Validation Loss = 0.0347100369698732\n",
            "Cost after 297537 iterations : Training Loss =  0.028087498778087864; Validation Loss = 0.034710028490785014\n",
            "Cost after 297538 iterations : Training Loss =  0.028087488885382636; Validation Loss = 0.034710020011782604\n",
            "Cost after 297539 iterations : Training Loss =  0.028087478992790828; Validation Loss = 0.03471001153286526\n",
            "Cost after 297540 iterations : Training Loss =  0.028087469100312505; Validation Loss = 0.03471000305403316\n",
            "Cost after 297541 iterations : Training Loss =  0.028087459207947498; Validation Loss = 0.034709994575287104\n",
            "Cost after 297542 iterations : Training Loss =  0.028087449315695956; Validation Loss = 0.03470998609662568\n",
            "Cost after 297543 iterations : Training Loss =  0.028087439423557785; Validation Loss = 0.034709977618049954\n",
            "Cost after 297544 iterations : Training Loss =  0.028087429531533054; Validation Loss = 0.03470996913955968\n",
            "Cost after 297545 iterations : Training Loss =  0.02808741963962169; Validation Loss = 0.034709960661154816\n",
            "Cost after 297546 iterations : Training Loss =  0.028087409747823783; Validation Loss = 0.03470995218283533\n",
            "Cost after 297547 iterations : Training Loss =  0.028087399856139172; Validation Loss = 0.03470994370460062\n",
            "Cost after 297548 iterations : Training Loss =  0.028087389964567968; Validation Loss = 0.03470993522645191\n",
            "Cost after 297549 iterations : Training Loss =  0.02808738007311023; Validation Loss = 0.034709926748388145\n",
            "Cost after 297550 iterations : Training Loss =  0.028087370181765756; Validation Loss = 0.034709918270409916\n",
            "Cost after 297551 iterations : Training Loss =  0.028087360290534855; Validation Loss = 0.03470990979251722\n",
            "Cost after 297552 iterations : Training Loss =  0.028087350399417167; Validation Loss = 0.03470990131470973\n",
            "Cost after 297553 iterations : Training Loss =  0.02808734050841298; Validation Loss = 0.03470989283698772\n",
            "Cost after 297554 iterations : Training Loss =  0.028087330617522176; Validation Loss = 0.034709884359351094\n",
            "Cost after 297555 iterations : Training Loss =  0.028087320726744736; Validation Loss = 0.034709875881799565\n",
            "Cost after 297556 iterations : Training Loss =  0.028087310836080605; Validation Loss = 0.0347098674043336\n",
            "Cost after 297557 iterations : Training Loss =  0.028087300945529844; Validation Loss = 0.03470985892695274\n",
            "Cost after 297558 iterations : Training Loss =  0.02808729105509255; Validation Loss = 0.03470985044965749\n",
            "Cost after 297559 iterations : Training Loss =  0.028087281164768624; Validation Loss = 0.034709841972447426\n",
            "Cost after 297560 iterations : Training Loss =  0.02808727127455809; Validation Loss = 0.034709833495322875\n",
            "Cost after 297561 iterations : Training Loss =  0.028087261384460886; Validation Loss = 0.034709825018283604\n",
            "Cost after 297562 iterations : Training Loss =  0.028087251494477054; Validation Loss = 0.03470981654132971\n",
            "Cost after 297563 iterations : Training Loss =  0.028087241604606624; Validation Loss = 0.03470980806446127\n",
            "Cost after 297564 iterations : Training Loss =  0.028087231714849503; Validation Loss = 0.03470979958767815\n",
            "Cost after 297565 iterations : Training Loss =  0.028087221825205788; Validation Loss = 0.034709791110980256\n",
            "Cost after 297566 iterations : Training Loss =  0.02808721193567557; Validation Loss = 0.03470978263436737\n",
            "Cost after 297567 iterations : Training Loss =  0.028087202046258492; Validation Loss = 0.034709774157840365\n",
            "Cost after 297568 iterations : Training Loss =  0.02808719215695491; Validation Loss = 0.03470976568139807\n",
            "Cost after 297569 iterations : Training Loss =  0.028087182267764618; Validation Loss = 0.03470975720504174\n",
            "Cost after 297570 iterations : Training Loss =  0.02808717237868777; Validation Loss = 0.03470974872877044\n",
            "Cost after 297571 iterations : Training Loss =  0.02808716248972424; Validation Loss = 0.034709740252584456\n",
            "Cost after 297572 iterations : Training Loss =  0.02808715260087402; Validation Loss = 0.03470973177648405\n",
            "Cost after 297573 iterations : Training Loss =  0.028087142712137223; Validation Loss = 0.03470972330046858\n",
            "Cost after 297574 iterations : Training Loss =  0.02808713282351375; Validation Loss = 0.03470971482453841\n",
            "Cost after 297575 iterations : Training Loss =  0.028087122935003556; Validation Loss = 0.03470970634869393\n",
            "Cost after 297576 iterations : Training Loss =  0.028087113046606875; Validation Loss = 0.03470969787293453\n",
            "Cost after 297577 iterations : Training Loss =  0.02808710315832349; Validation Loss = 0.034709689397260456\n",
            "Cost after 297578 iterations : Training Loss =  0.028087093270153355; Validation Loss = 0.034709680921672086\n",
            "Cost after 297579 iterations : Training Loss =  0.028087083382096537; Validation Loss = 0.034709672446168904\n",
            "Cost after 297580 iterations : Training Loss =  0.02808707349415316; Validation Loss = 0.034709663970750745\n",
            "Cost after 297581 iterations : Training Loss =  0.0280870636063232; Validation Loss = 0.03470965549541825\n",
            "Cost after 297582 iterations : Training Loss =  0.028087053718606418; Validation Loss = 0.03470964702017118\n",
            "Cost after 297583 iterations : Training Loss =  0.028087043831003126; Validation Loss = 0.034709638545009185\n",
            "Cost after 297584 iterations : Training Loss =  0.028087033943513022; Validation Loss = 0.03470963006993249\n",
            "Cost after 297585 iterations : Training Loss =  0.028087024056136275; Validation Loss = 0.0347096215949412\n",
            "Cost after 297586 iterations : Training Loss =  0.028087014168872965; Validation Loss = 0.034709613120035065\n",
            "Cost after 297587 iterations : Training Loss =  0.02808700428172299; Validation Loss = 0.03470960464521438\n",
            "Cost after 297588 iterations : Training Loss =  0.028086994394686267; Validation Loss = 0.03470959617047871\n",
            "Cost after 297589 iterations : Training Loss =  0.028086984507762768; Validation Loss = 0.034709587695828506\n",
            "Cost after 297590 iterations : Training Loss =  0.028086974620952793; Validation Loss = 0.03470957922126352\n",
            "Cost after 297591 iterations : Training Loss =  0.02808696473425603; Validation Loss = 0.03470957074678419\n",
            "Cost after 297592 iterations : Training Loss =  0.028086954847672693; Validation Loss = 0.03470956227239015\n",
            "Cost after 297593 iterations : Training Loss =  0.028086944961202606; Validation Loss = 0.03470955379808136\n",
            "Cost after 297594 iterations : Training Loss =  0.028086935074845786; Validation Loss = 0.034709545323857546\n",
            "Cost after 297595 iterations : Training Loss =  0.028086925188602323; Validation Loss = 0.03470953684971928\n",
            "Cost after 297596 iterations : Training Loss =  0.02808691530247234; Validation Loss = 0.03470952837566638\n",
            "Cost after 297597 iterations : Training Loss =  0.02808690541645537; Validation Loss = 0.03470951990169863\n",
            "Cost after 297598 iterations : Training Loss =  0.028086895530552002; Validation Loss = 0.034709511427816295\n",
            "Cost after 297599 iterations : Training Loss =  0.02808688564476177; Validation Loss = 0.03470950295401927\n",
            "Cost after 297600 iterations : Training Loss =  0.028086875759084966; Validation Loss = 0.03470949448030755\n",
            "Cost after 297601 iterations : Training Loss =  0.028086865873521355; Validation Loss = 0.03470948600668124\n",
            "Cost after 297602 iterations : Training Loss =  0.02808685598807113; Validation Loss = 0.03470947753314009\n",
            "Cost after 297603 iterations : Training Loss =  0.0280868461027341; Validation Loss = 0.034709469059684514\n",
            "Cost after 297604 iterations : Training Loss =  0.028086836217510593; Validation Loss = 0.03470946058631384\n",
            "Cost after 297605 iterations : Training Loss =  0.02808682633240012; Validation Loss = 0.034709452113028796\n",
            "Cost after 297606 iterations : Training Loss =  0.028086816447403233; Validation Loss = 0.03470944363982888\n",
            "Cost after 297607 iterations : Training Loss =  0.028086806562519377; Validation Loss = 0.03470943516671424\n",
            "Cost after 297608 iterations : Training Loss =  0.028086796677748996; Validation Loss = 0.03470942669368482\n",
            "Cost after 297609 iterations : Training Loss =  0.028086786793091775; Validation Loss = 0.03470941822074068\n",
            "Cost after 297610 iterations : Training Loss =  0.02808677690854794; Validation Loss = 0.03470940974788203\n",
            "Cost after 297611 iterations : Training Loss =  0.028086767024117446; Validation Loss = 0.03470940127510842\n",
            "Cost after 297612 iterations : Training Loss =  0.02808675713980009; Validation Loss = 0.03470939280242008\n",
            "Cost after 297613 iterations : Training Loss =  0.028086747255596185; Validation Loss = 0.03470938432981735\n",
            "Cost after 297614 iterations : Training Loss =  0.028086737371505494; Validation Loss = 0.03470937585729973\n",
            "Cost after 297615 iterations : Training Loss =  0.02808672748752811; Validation Loss = 0.03470936738486731\n",
            "Cost after 297616 iterations : Training Loss =  0.028086717603663963; Validation Loss = 0.03470935891251998\n",
            "Cost after 297617 iterations : Training Loss =  0.02808670771991316; Validation Loss = 0.03470935044025813\n",
            "Cost after 297618 iterations : Training Loss =  0.02808669783627557; Validation Loss = 0.034709341968081994\n",
            "Cost after 297619 iterations : Training Loss =  0.02808668795275137; Validation Loss = 0.034709333495990535\n",
            "Cost after 297620 iterations : Training Loss =  0.02808667806934036; Validation Loss = 0.03470932502398451\n",
            "Cost after 297621 iterations : Training Loss =  0.028086668186042635; Validation Loss = 0.03470931655206383\n",
            "Cost after 297622 iterations : Training Loss =  0.028086658302858163; Validation Loss = 0.03470930808022837\n",
            "Cost after 297623 iterations : Training Loss =  0.028086648419787086; Validation Loss = 0.0347092996084781\n",
            "Cost after 297624 iterations : Training Loss =  0.02808663853682912; Validation Loss = 0.03470929113681343\n",
            "Cost after 297625 iterations : Training Loss =  0.028086628653984428; Validation Loss = 0.03470928266523358\n",
            "Cost after 297626 iterations : Training Loss =  0.028086618771253104; Validation Loss = 0.03470927419373924\n",
            "Cost after 297627 iterations : Training Loss =  0.02808660888863509; Validation Loss = 0.03470926572233031\n",
            "Cost after 297628 iterations : Training Loss =  0.028086599006130242; Validation Loss = 0.034709257251006445\n",
            "Cost after 297629 iterations : Training Loss =  0.02808658912373862; Validation Loss = 0.03470924877976807\n",
            "Cost after 297630 iterations : Training Loss =  0.02808657924146033; Validation Loss = 0.03470924030861516\n",
            "Cost after 297631 iterations : Training Loss =  0.02808656935929534; Validation Loss = 0.0347092318375472\n",
            "Cost after 297632 iterations : Training Loss =  0.02808655947724357; Validation Loss = 0.034709223366564554\n",
            "Cost after 297633 iterations : Training Loss =  0.02808654959530508; Validation Loss = 0.03470921489566695\n",
            "Cost after 297634 iterations : Training Loss =  0.02808653971347987; Validation Loss = 0.034709206424854616\n",
            "Cost after 297635 iterations : Training Loss =  0.02808652983176777; Validation Loss = 0.03470919795412748\n",
            "Cost after 297636 iterations : Training Loss =  0.02808651995016903; Validation Loss = 0.03470918948348577\n",
            "Cost after 297637 iterations : Training Loss =  0.02808651006868361; Validation Loss = 0.03470918101292932\n",
            "Cost after 297638 iterations : Training Loss =  0.028086500187311292; Validation Loss = 0.034709172542458\n",
            "Cost after 297639 iterations : Training Loss =  0.028086490306052245; Validation Loss = 0.034709164072072037\n",
            "Cost after 297640 iterations : Training Loss =  0.02808648042490651; Validation Loss = 0.03470915560177144\n",
            "Cost after 297641 iterations : Training Loss =  0.02808647054387403; Validation Loss = 0.034709147131556074\n",
            "Cost after 297642 iterations : Training Loss =  0.028086460662954737; Validation Loss = 0.03470913866142565\n",
            "Cost after 297643 iterations : Training Loss =  0.028086450782148758; Validation Loss = 0.03470913019138088\n",
            "Cost after 297644 iterations : Training Loss =  0.028086440901455893; Validation Loss = 0.03470912172142135\n",
            "Cost after 297645 iterations : Training Loss =  0.02808643102087638; Validation Loss = 0.034709113251546846\n",
            "Cost after 297646 iterations : Training Loss =  0.02808642114041006; Validation Loss = 0.034709104781757524\n",
            "Cost after 297647 iterations : Training Loss =  0.028086411260057013; Validation Loss = 0.034709096312053565\n",
            "Cost after 297648 iterations : Training Loss =  0.0280864013798172; Validation Loss = 0.03470908784243497\n",
            "Cost after 297649 iterations : Training Loss =  0.028086391499690565; Validation Loss = 0.03470907937290143\n",
            "Cost after 297650 iterations : Training Loss =  0.028086381619677116; Validation Loss = 0.034709070903453106\n",
            "Cost after 297651 iterations : Training Loss =  0.02808637173977701; Validation Loss = 0.034709062434090165\n",
            "Cost after 297652 iterations : Training Loss =  0.028086361859990083; Validation Loss = 0.03470905396481242\n",
            "Cost after 297653 iterations : Training Loss =  0.028086351980316393; Validation Loss = 0.03470904549562011\n",
            "Cost after 297654 iterations : Training Loss =  0.02808634210075587; Validation Loss = 0.03470903702651274\n",
            "Cost after 297655 iterations : Training Loss =  0.028086332221308692; Validation Loss = 0.03470902855749081\n",
            "Cost after 297656 iterations : Training Loss =  0.028086322341974685; Validation Loss = 0.034709020088554206\n",
            "Cost after 297657 iterations : Training Loss =  0.02808631246275379; Validation Loss = 0.03470901161970279\n",
            "Cost after 297658 iterations : Training Loss =  0.02808630258364618; Validation Loss = 0.03470900315093649\n",
            "Cost after 297659 iterations : Training Loss =  0.02808629270465189; Validation Loss = 0.034708994682255655\n",
            "Cost after 297660 iterations : Training Loss =  0.02808628282577075; Validation Loss = 0.0347089862136598\n",
            "Cost after 297661 iterations : Training Loss =  0.028086272947002736; Validation Loss = 0.03470897774514922\n",
            "Cost after 297662 iterations : Training Loss =  0.028086263068348053; Validation Loss = 0.034708969276723727\n",
            "Cost after 297663 iterations : Training Loss =  0.028086253189806367; Validation Loss = 0.03470896080838361\n",
            "Cost after 297664 iterations : Training Loss =  0.028086243311378056; Validation Loss = 0.0347089523401284\n",
            "Cost after 297665 iterations : Training Loss =  0.028086233433063018; Validation Loss = 0.03470894387195876\n",
            "Cost after 297666 iterations : Training Loss =  0.028086223554861085; Validation Loss = 0.034708935403874215\n",
            "Cost after 297667 iterations : Training Loss =  0.028086213676772356; Validation Loss = 0.03470892693587474\n",
            "Cost after 297668 iterations : Training Loss =  0.028086203798796866; Validation Loss = 0.03470891846796081\n",
            "Cost after 297669 iterations : Training Loss =  0.02808619392093443; Validation Loss = 0.03470891000013177\n",
            "Cost after 297670 iterations : Training Loss =  0.028086184043185414; Validation Loss = 0.03470890153238846\n",
            "Cost after 297671 iterations : Training Loss =  0.02808617416554945; Validation Loss = 0.03470889306473009\n",
            "Cost after 297672 iterations : Training Loss =  0.028086164288026745; Validation Loss = 0.03470888459715695\n",
            "Cost after 297673 iterations : Training Loss =  0.028086154410617233; Validation Loss = 0.03470887612966905\n",
            "Cost after 297674 iterations : Training Loss =  0.02808614453332083; Validation Loss = 0.034708867662266545\n",
            "Cost after 297675 iterations : Training Loss =  0.028086134656137733; Validation Loss = 0.034708859194948725\n",
            "Cost after 297676 iterations : Training Loss =  0.028086124779067814; Validation Loss = 0.03470885072771651\n",
            "Cost after 297677 iterations : Training Loss =  0.028086114902110944; Validation Loss = 0.0347088422605693\n",
            "Cost after 297678 iterations : Training Loss =  0.028086105025267438; Validation Loss = 0.03470883379350708\n",
            "Cost after 297679 iterations : Training Loss =  0.028086095148536955; Validation Loss = 0.03470882532653042\n",
            "Cost after 297680 iterations : Training Loss =  0.028086085271919817; Validation Loss = 0.03470881685963904\n",
            "Cost after 297681 iterations : Training Loss =  0.02808607539541568; Validation Loss = 0.03470880839283252\n",
            "Cost after 297682 iterations : Training Loss =  0.028086065519024834; Validation Loss = 0.034708799926111346\n",
            "Cost after 297683 iterations : Training Loss =  0.028086055642747187; Validation Loss = 0.03470879145947559\n",
            "Cost after 297684 iterations : Training Loss =  0.028086045766582676; Validation Loss = 0.03470878299292474\n",
            "Cost after 297685 iterations : Training Loss =  0.02808603589053133; Validation Loss = 0.03470877452645931\n",
            "Cost after 297686 iterations : Training Loss =  0.02808602601459322; Validation Loss = 0.03470876606007896\n",
            "Cost after 297687 iterations : Training Loss =  0.028086016138768134; Validation Loss = 0.03470875759378374\n",
            "Cost after 297688 iterations : Training Loss =  0.02808600626305638; Validation Loss = 0.03470874912757388\n",
            "Cost after 297689 iterations : Training Loss =  0.028085996387457724; Validation Loss = 0.034708740661449286\n",
            "Cost after 297690 iterations : Training Loss =  0.028085986511972142; Validation Loss = 0.03470873219540979\n",
            "Cost after 297691 iterations : Training Loss =  0.028085976636599907; Validation Loss = 0.03470872372945531\n",
            "Cost after 297692 iterations : Training Loss =  0.028085966761340737; Validation Loss = 0.034708715263586216\n",
            "Cost after 297693 iterations : Training Loss =  0.028085956886194665; Validation Loss = 0.034708706797802316\n",
            "Cost after 297694 iterations : Training Loss =  0.028085947011161815; Validation Loss = 0.03470869833210375\n",
            "Cost after 297695 iterations : Training Loss =  0.028085937136242103; Validation Loss = 0.034708689866490075\n",
            "Cost after 297696 iterations : Training Loss =  0.028085927261435533; Validation Loss = 0.03470868140096187\n",
            "Cost after 297697 iterations : Training Loss =  0.02808591738674219; Validation Loss = 0.034708672935518735\n",
            "Cost after 297698 iterations : Training Loss =  0.02808590751216205; Validation Loss = 0.03470866447016091\n",
            "Cost after 297699 iterations : Training Loss =  0.028085897637694868; Validation Loss = 0.03470865600488814\n",
            "Cost after 297700 iterations : Training Loss =  0.028085887763340978; Validation Loss = 0.034708647539700745\n",
            "Cost after 297701 iterations : Training Loss =  0.02808587788910022; Validation Loss = 0.03470863907459862\n",
            "Cost after 297702 iterations : Training Loss =  0.028085868014972513; Validation Loss = 0.03470863060958107\n",
            "Cost after 297703 iterations : Training Loss =  0.028085858140958132; Validation Loss = 0.034708622144648954\n",
            "Cost after 297704 iterations : Training Loss =  0.028085848267056783; Validation Loss = 0.03470861367980207\n",
            "Cost after 297705 iterations : Training Loss =  0.028085838393268538; Validation Loss = 0.03470860521504037\n",
            "Cost after 297706 iterations : Training Loss =  0.0280858285195935; Validation Loss = 0.0347085967503639\n",
            "Cost after 297707 iterations : Training Loss =  0.028085818646031546; Validation Loss = 0.0347085882857724\n",
            "Cost after 297708 iterations : Training Loss =  0.028085808772582876; Validation Loss = 0.03470857982126601\n",
            "Cost after 297709 iterations : Training Loss =  0.02808579889924704; Validation Loss = 0.03470857135684498\n",
            "Cost after 297710 iterations : Training Loss =  0.028085789026024637; Validation Loss = 0.03470856289250928\n",
            "Cost after 297711 iterations : Training Loss =  0.02808577915291528; Validation Loss = 0.03470855442825867\n",
            "Cost after 297712 iterations : Training Loss =  0.02808576927991898; Validation Loss = 0.03470854596409348\n",
            "Cost after 297713 iterations : Training Loss =  0.028085759407035792; Validation Loss = 0.034708537500013174\n",
            "Cost after 297714 iterations : Training Loss =  0.028085749534265805; Validation Loss = 0.0347085290360181\n",
            "Cost after 297715 iterations : Training Loss =  0.02808573966160905; Validation Loss = 0.03470852057210821\n",
            "Cost after 297716 iterations : Training Loss =  0.02808572978906516; Validation Loss = 0.03470851210828326\n",
            "Cost after 297717 iterations : Training Loss =  0.028085719916634628; Validation Loss = 0.034708503644543275\n",
            "Cost after 297718 iterations : Training Loss =  0.02808571004431711; Validation Loss = 0.0347084951808889\n",
            "Cost after 297719 iterations : Training Loss =  0.028085700172112673; Validation Loss = 0.034708486717319346\n",
            "Cost after 297720 iterations : Training Loss =  0.028085690300021483; Validation Loss = 0.03470847825383506\n",
            "Cost after 297721 iterations : Training Loss =  0.028085680428043214; Validation Loss = 0.034708469790436104\n",
            "Cost after 297722 iterations : Training Loss =  0.028085670556178256; Validation Loss = 0.03470846132712206\n",
            "Cost after 297723 iterations : Training Loss =  0.028085660684426427; Validation Loss = 0.03470845286389339\n",
            "Cost after 297724 iterations : Training Loss =  0.028085650812787483; Validation Loss = 0.034708444400749786\n",
            "Cost after 297725 iterations : Training Loss =  0.028085640941261914; Validation Loss = 0.034708435937691434\n",
            "Cost after 297726 iterations : Training Loss =  0.02808563106984921; Validation Loss = 0.03470842747471818\n",
            "Cost after 297727 iterations : Training Loss =  0.02808562119854964; Validation Loss = 0.03470841901183007\n",
            "Cost after 297728 iterations : Training Loss =  0.028085611327363418; Validation Loss = 0.034708410549027055\n",
            "Cost after 297729 iterations : Training Loss =  0.028085601456290004; Validation Loss = 0.0347084020863091\n",
            "Cost after 297730 iterations : Training Loss =  0.028085591585329917; Validation Loss = 0.03470839362367645\n",
            "Cost after 297731 iterations : Training Loss =  0.02808558171448277; Validation Loss = 0.03470838516112893\n",
            "Cost after 297732 iterations : Training Loss =  0.02808557184374879; Validation Loss = 0.03470837669866652\n",
            "Cost after 297733 iterations : Training Loss =  0.028085561973127872; Validation Loss = 0.03470836823628931\n",
            "Cost after 297734 iterations : Training Loss =  0.02808555210262016; Validation Loss = 0.034708359773997145\n",
            "Cost after 297735 iterations : Training Loss =  0.028085542232225476; Validation Loss = 0.03470835131179026\n",
            "Cost after 297736 iterations : Training Loss =  0.028085532361943787; Validation Loss = 0.034708342849668616\n",
            "Cost after 297737 iterations : Training Loss =  0.028085522491775285; Validation Loss = 0.03470833438763197\n",
            "Cost after 297738 iterations : Training Loss =  0.02808551262171987; Validation Loss = 0.034708325925680225\n",
            "Cost after 297739 iterations : Training Loss =  0.028085502751777486; Validation Loss = 0.03470831746381364\n",
            "Cost after 297740 iterations : Training Loss =  0.028085492881948275; Validation Loss = 0.034708309002032214\n",
            "Cost after 297741 iterations : Training Loss =  0.028085483012232074; Validation Loss = 0.03470830054033644\n",
            "Cost after 297742 iterations : Training Loss =  0.02808547314262901; Validation Loss = 0.03470829207872564\n",
            "Cost after 297743 iterations : Training Loss =  0.028085463273138957; Validation Loss = 0.03470828361719967\n",
            "Cost after 297744 iterations : Training Loss =  0.02808545340376205; Validation Loss = 0.03470827515575892\n",
            "Cost after 297745 iterations : Training Loss =  0.028085443534498104; Validation Loss = 0.03470826669440352\n",
            "Cost after 297746 iterations : Training Loss =  0.028085433665347367; Validation Loss = 0.03470825823313308\n",
            "Cost after 297747 iterations : Training Loss =  0.028085423796309655; Validation Loss = 0.03470824977194788\n",
            "Cost after 297748 iterations : Training Loss =  0.028085413927385044; Validation Loss = 0.03470824131084767\n",
            "Cost after 297749 iterations : Training Loss =  0.028085404058573526; Validation Loss = 0.03470823284983252\n",
            "Cost after 297750 iterations : Training Loss =  0.02808539418987496; Validation Loss = 0.03470822438890265\n",
            "Cost after 297751 iterations : Training Loss =  0.028085384321289576; Validation Loss = 0.03470821592805749\n",
            "Cost after 297752 iterations : Training Loss =  0.02808537445281728; Validation Loss = 0.03470820746729777\n",
            "Cost after 297753 iterations : Training Loss =  0.028085364584457945; Validation Loss = 0.0347081990066231\n",
            "Cost after 297754 iterations : Training Loss =  0.02808535471621173; Validation Loss = 0.034708190546033406\n",
            "Cost after 297755 iterations : Training Loss =  0.02808534484807852; Validation Loss = 0.03470818208552929\n",
            "Cost after 297756 iterations : Training Loss =  0.028085334980058444; Validation Loss = 0.034708173625110195\n",
            "Cost after 297757 iterations : Training Loss =  0.02808532511215131; Validation Loss = 0.03470816516477603\n",
            "Cost after 297758 iterations : Training Loss =  0.028085315244357298; Validation Loss = 0.03470815670452708\n",
            "Cost after 297759 iterations : Training Loss =  0.028085305376676507; Validation Loss = 0.034708148244363114\n",
            "Cost after 297760 iterations : Training Loss =  0.028085295509108554; Validation Loss = 0.034708139784284435\n",
            "Cost after 297761 iterations : Training Loss =  0.028085285641653787; Validation Loss = 0.034708131324290785\n",
            "Cost after 297762 iterations : Training Loss =  0.028085275774311923; Validation Loss = 0.03470812286438206\n",
            "Cost after 297763 iterations : Training Loss =  0.02808526590708313; Validation Loss = 0.034708114404558725\n",
            "Cost after 297764 iterations : Training Loss =  0.028085256039967487; Validation Loss = 0.03470810594482043\n",
            "Cost after 297765 iterations : Training Loss =  0.02808524617296478; Validation Loss = 0.03470809748516743\n",
            "Cost after 297766 iterations : Training Loss =  0.028085236306075196; Validation Loss = 0.03470808902559933\n",
            "Cost after 297767 iterations : Training Loss =  0.02808522643929862; Validation Loss = 0.034708080566116496\n",
            "Cost after 297768 iterations : Training Loss =  0.028085216572635182; Validation Loss = 0.034708072106718606\n",
            "Cost after 297769 iterations : Training Loss =  0.028085206706084654; Validation Loss = 0.03470806364740597\n",
            "Cost after 297770 iterations : Training Loss =  0.028085196839647178; Validation Loss = 0.034708055188178225\n",
            "Cost after 297771 iterations : Training Loss =  0.02808518697332283; Validation Loss = 0.03470804672903559\n",
            "Cost after 297772 iterations : Training Loss =  0.02808517710711145; Validation Loss = 0.034708038269978245\n",
            "Cost after 297773 iterations : Training Loss =  0.028085167241013138; Validation Loss = 0.034708029811005736\n",
            "Cost after 297774 iterations : Training Loss =  0.028085157375027696; Validation Loss = 0.034708021352118264\n",
            "Cost after 297775 iterations : Training Loss =  0.028085147509155483; Validation Loss = 0.034708012893316265\n",
            "Cost after 297776 iterations : Training Loss =  0.028085137643396235; Validation Loss = 0.03470800443459905\n",
            "Cost after 297777 iterations : Training Loss =  0.02808512777775003; Validation Loss = 0.034707995975967035\n",
            "Cost after 297778 iterations : Training Loss =  0.028085117912216774; Validation Loss = 0.03470798751742008\n",
            "Cost after 297779 iterations : Training Loss =  0.028085108046796533; Validation Loss = 0.03470797905895808\n",
            "Cost after 297780 iterations : Training Loss =  0.028085098181489423; Validation Loss = 0.03470797060058144\n",
            "Cost after 297781 iterations : Training Loss =  0.02808508831629521; Validation Loss = 0.034707962142289626\n",
            "Cost after 297782 iterations : Training Loss =  0.028085078451214152; Validation Loss = 0.034707953684083054\n",
            "Cost after 297783 iterations : Training Loss =  0.02808506858624607; Validation Loss = 0.03470794522596157\n",
            "Cost after 297784 iterations : Training Loss =  0.02808505872139097; Validation Loss = 0.03470793676792528\n",
            "Cost after 297785 iterations : Training Loss =  0.028085048856648826; Validation Loss = 0.034707928309974004\n",
            "Cost after 297786 iterations : Training Loss =  0.02808503899201973; Validation Loss = 0.03470791985210751\n",
            "Cost after 297787 iterations : Training Loss =  0.02808502912750373; Validation Loss = 0.03470791139432642\n",
            "Cost after 297788 iterations : Training Loss =  0.028085019263100662; Validation Loss = 0.034707902936630265\n",
            "Cost after 297789 iterations : Training Loss =  0.02808500939881064; Validation Loss = 0.0347078944790193\n",
            "Cost after 297790 iterations : Training Loss =  0.028084999534633552; Validation Loss = 0.03470788602149335\n",
            "Cost after 297791 iterations : Training Loss =  0.028084989670569577; Validation Loss = 0.03470787756405252\n",
            "Cost after 297792 iterations : Training Loss =  0.028084979806618497; Validation Loss = 0.03470786910669665\n",
            "Cost after 297793 iterations : Training Loss =  0.02808496994278048; Validation Loss = 0.03470786064942601\n",
            "Cost after 297794 iterations : Training Loss =  0.02808496007905533; Validation Loss = 0.034707852192240395\n",
            "Cost after 297795 iterations : Training Loss =  0.028084950215443355; Validation Loss = 0.03470784373513994\n",
            "Cost after 297796 iterations : Training Loss =  0.028084940351944282; Validation Loss = 0.03470783527812425\n",
            "Cost after 297797 iterations : Training Loss =  0.028084930488558278; Validation Loss = 0.03470782682119397\n",
            "Cost after 297798 iterations : Training Loss =  0.028084920625285198; Validation Loss = 0.03470781836434859\n",
            "Cost after 297799 iterations : Training Loss =  0.02808491076212506; Validation Loss = 0.034707809907588295\n",
            "Cost after 297800 iterations : Training Loss =  0.02808490089907805; Validation Loss = 0.03470780145091295\n",
            "Cost after 297801 iterations : Training Loss =  0.028084891036143877; Validation Loss = 0.034707792994323125\n",
            "Cost after 297802 iterations : Training Loss =  0.028084881173322872; Validation Loss = 0.034707784537817796\n",
            "Cost after 297803 iterations : Training Loss =  0.02808487131061472; Validation Loss = 0.03470777608139792\n",
            "Cost after 297804 iterations : Training Loss =  0.02808486144801952; Validation Loss = 0.03470776762506312\n",
            "Cost after 297805 iterations : Training Loss =  0.028084851585537373; Validation Loss = 0.034707759168813385\n",
            "Cost after 297806 iterations : Training Loss =  0.028084841723168257; Validation Loss = 0.03470775071264853\n",
            "Cost after 297807 iterations : Training Loss =  0.02808483186091199; Validation Loss = 0.03470774225656877\n",
            "Cost after 297808 iterations : Training Loss =  0.02808482199876881; Validation Loss = 0.034707733800574056\n",
            "Cost after 297809 iterations : Training Loss =  0.02808481213673854; Validation Loss = 0.03470772534466437\n",
            "Cost after 297810 iterations : Training Loss =  0.028084802274821175; Validation Loss = 0.03470771688883949\n",
            "Cost after 297811 iterations : Training Loss =  0.028084792413016847; Validation Loss = 0.03470770843309993\n",
            "Cost after 297812 iterations : Training Loss =  0.028084782551325564; Validation Loss = 0.03470769997744556\n",
            "Cost after 297813 iterations : Training Loss =  0.028084772689747097; Validation Loss = 0.03470769152187621\n",
            "Cost after 297814 iterations : Training Loss =  0.02808476282828178; Validation Loss = 0.034707683066391576\n",
            "Cost after 297815 iterations : Training Loss =  0.028084752966929322; Validation Loss = 0.03470767461099218\n",
            "Cost after 297816 iterations : Training Loss =  0.028084743105689855; Validation Loss = 0.03470766615567808\n",
            "Cost after 297817 iterations : Training Loss =  0.02808473324456336; Validation Loss = 0.03470765770044896\n",
            "Cost after 297818 iterations : Training Loss =  0.02808472338354974; Validation Loss = 0.03470764924530471\n",
            "Cost after 297819 iterations : Training Loss =  0.0280847135226492; Validation Loss = 0.03470764079024567\n",
            "Cost after 297820 iterations : Training Loss =  0.02808470366186151; Validation Loss = 0.03470763233527154\n",
            "Cost after 297821 iterations : Training Loss =  0.02808469380118682; Validation Loss = 0.0347076238803824\n",
            "Cost after 297822 iterations : Training Loss =  0.028084683940625046; Validation Loss = 0.034707615425578396\n",
            "Cost after 297823 iterations : Training Loss =  0.028084674080176224; Validation Loss = 0.03470760697085928\n",
            "Cost after 297824 iterations : Training Loss =  0.02808466421984043; Validation Loss = 0.03470759851622541\n",
            "Cost after 297825 iterations : Training Loss =  0.028084654359617444; Validation Loss = 0.034707590061676674\n",
            "Cost after 297826 iterations : Training Loss =  0.02808464449950766; Validation Loss = 0.03470758160721274\n",
            "Cost after 297827 iterations : Training Loss =  0.028084634639510712; Validation Loss = 0.03470757315283404\n",
            "Cost after 297828 iterations : Training Loss =  0.02808462477962668; Validation Loss = 0.034707564698540326\n",
            "Cost after 297829 iterations : Training Loss =  0.02808461491985548; Validation Loss = 0.034707556244331486\n",
            "Cost after 297830 iterations : Training Loss =  0.028084605060197332; Validation Loss = 0.03470754779020774\n",
            "Cost after 297831 iterations : Training Loss =  0.02808459520065205; Validation Loss = 0.03470753933616904\n",
            "Cost after 297832 iterations : Training Loss =  0.02808458534121981; Validation Loss = 0.034707530882215155\n",
            "Cost after 297833 iterations : Training Loss =  0.028084575481900447; Validation Loss = 0.03470752242834649\n",
            "Cost after 297834 iterations : Training Loss =  0.028084565622693944; Validation Loss = 0.034707513974562676\n",
            "Cost after 297835 iterations : Training Loss =  0.028084555763600527; Validation Loss = 0.03470750552086409\n",
            "Cost after 297836 iterations : Training Loss =  0.028084545904620034; Validation Loss = 0.034707497067250515\n",
            "Cost after 297837 iterations : Training Loss =  0.028084536045752295; Validation Loss = 0.03470748861372185\n",
            "Cost after 297838 iterations : Training Loss =  0.028084526186997603; Validation Loss = 0.03470748016027831\n",
            "Cost after 297839 iterations : Training Loss =  0.028084516328355874; Validation Loss = 0.03470747170691984\n",
            "Cost after 297840 iterations : Training Loss =  0.028084506469827054; Validation Loss = 0.03470746325364625\n",
            "Cost after 297841 iterations : Training Loss =  0.02808449661141107; Validation Loss = 0.03470745480045764\n",
            "Cost after 297842 iterations : Training Loss =  0.028084486753108148; Validation Loss = 0.03470744634735446\n",
            "Cost after 297843 iterations : Training Loss =  0.028084476894917998; Validation Loss = 0.034707437894335884\n",
            "Cost after 297844 iterations : Training Loss =  0.0280844670368409; Validation Loss = 0.03470742944140243\n",
            "Cost after 297845 iterations : Training Loss =  0.02808445717887661; Validation Loss = 0.03470742098855402\n",
            "Cost after 297846 iterations : Training Loss =  0.028084447321025253; Validation Loss = 0.034707412535790544\n",
            "Cost after 297847 iterations : Training Loss =  0.02808443746328677; Validation Loss = 0.034707404083112195\n",
            "Cost after 297848 iterations : Training Loss =  0.028084427605661336; Validation Loss = 0.034707395630518834\n",
            "Cost after 297849 iterations : Training Loss =  0.028084417748148627; Validation Loss = 0.03470738717801054\n",
            "Cost after 297850 iterations : Training Loss =  0.028084407890749025; Validation Loss = 0.034707378725587164\n",
            "Cost after 297851 iterations : Training Loss =  0.02808439803346222; Validation Loss = 0.03470737027324871\n",
            "Cost after 297852 iterations : Training Loss =  0.028084388176288368; Validation Loss = 0.03470736182099528\n",
            "Cost after 297853 iterations : Training Loss =  0.028084378319227398; Validation Loss = 0.03470735336882725\n",
            "Cost after 297854 iterations : Training Loss =  0.02808436846227932; Validation Loss = 0.034707344916743675\n",
            "Cost after 297855 iterations : Training Loss =  0.028084358605444087; Validation Loss = 0.03470733646474561\n",
            "Cost after 297856 iterations : Training Loss =  0.0280843487487218; Validation Loss = 0.03470732801283242\n",
            "Cost after 297857 iterations : Training Loss =  0.02808433889211243; Validation Loss = 0.03470731956100406\n",
            "Cost after 297858 iterations : Training Loss =  0.028084329035615977; Validation Loss = 0.034707311109260584\n",
            "Cost after 297859 iterations : Training Loss =  0.028084319179232375; Validation Loss = 0.03470730265760228\n",
            "Cost after 297860 iterations : Training Loss =  0.028084309322961732; Validation Loss = 0.03470729420602911\n",
            "Cost after 297861 iterations : Training Loss =  0.02808429946680379; Validation Loss = 0.03470728575454096\n",
            "Cost after 297862 iterations : Training Loss =  0.02808428961075902; Validation Loss = 0.03470727730313732\n",
            "Cost after 297863 iterations : Training Loss =  0.028084279754826903; Validation Loss = 0.034707268851818865\n",
            "Cost after 297864 iterations : Training Loss =  0.02808426989900778; Validation Loss = 0.03470726040058578\n",
            "Cost after 297865 iterations : Training Loss =  0.02808426004330151; Validation Loss = 0.03470725194943755\n",
            "Cost after 297866 iterations : Training Loss =  0.028084250187708118; Validation Loss = 0.034707243498374306\n",
            "Cost after 297867 iterations : Training Loss =  0.028084240332227586; Validation Loss = 0.034707235047395785\n",
            "Cost after 297868 iterations : Training Loss =  0.028084230476860032; Validation Loss = 0.034707226596502175\n",
            "Cost after 297869 iterations : Training Loss =  0.028084220621605344; Validation Loss = 0.03470721814569368\n",
            "Cost after 297870 iterations : Training Loss =  0.028084210766463513; Validation Loss = 0.034707209694970306\n",
            "Cost after 297871 iterations : Training Loss =  0.02808420091143439; Validation Loss = 0.034707201244331944\n",
            "Cost after 297872 iterations : Training Loss =  0.02808419105651832; Validation Loss = 0.03470719279377859\n",
            "Cost after 297873 iterations : Training Loss =  0.028084181201715134; Validation Loss = 0.03470718434330993\n",
            "Cost after 297874 iterations : Training Loss =  0.028084171347024702; Validation Loss = 0.034707175892926476\n",
            "Cost after 297875 iterations : Training Loss =  0.028084161492447163; Validation Loss = 0.0347071674426281\n",
            "Cost after 297876 iterations : Training Loss =  0.02808415163798255; Validation Loss = 0.0347071589924146\n",
            "Cost after 297877 iterations : Training Loss =  0.028084141783630893; Validation Loss = 0.03470715054228591\n",
            "Cost after 297878 iterations : Training Loss =  0.028084131929391944; Validation Loss = 0.03470714209224236\n",
            "Cost after 297879 iterations : Training Loss =  0.02808412207526585; Validation Loss = 0.03470713364228354\n",
            "Cost after 297880 iterations : Training Loss =  0.028084112221252622; Validation Loss = 0.034707125192410114\n",
            "Cost after 297881 iterations : Training Loss =  0.02808410236735229; Validation Loss = 0.03470711674262168\n",
            "Cost after 297882 iterations : Training Loss =  0.02808409251356491; Validation Loss = 0.034707108292917975\n",
            "Cost after 297883 iterations : Training Loss =  0.028084082659890208; Validation Loss = 0.03470709984329904\n",
            "Cost after 297884 iterations : Training Loss =  0.02808407280632849; Validation Loss = 0.034707091393765446\n",
            "Cost after 297885 iterations : Training Loss =  0.02808406295287951; Validation Loss = 0.03470708294431664\n",
            "Cost after 297886 iterations : Training Loss =  0.028084053099543475; Validation Loss = 0.03470707449495259\n",
            "Cost after 297887 iterations : Training Loss =  0.02808404324632035; Validation Loss = 0.034707066045673765\n",
            "Cost after 297888 iterations : Training Loss =  0.028084033393209886; Validation Loss = 0.03470705759647979\n",
            "Cost after 297889 iterations : Training Loss =  0.02808402354021238; Validation Loss = 0.03470704914737098\n",
            "Cost after 297890 iterations : Training Loss =  0.02808401368732778; Validation Loss = 0.03470704069834701\n",
            "Cost after 297891 iterations : Training Loss =  0.028084003834555955; Validation Loss = 0.03470703224940828\n",
            "Cost after 297892 iterations : Training Loss =  0.028083993981896874; Validation Loss = 0.03470702380055408\n",
            "Cost after 297893 iterations : Training Loss =  0.028083984129350745; Validation Loss = 0.03470701535178486\n",
            "Cost after 297894 iterations : Training Loss =  0.028083974276917445; Validation Loss = 0.03470700690310078\n",
            "Cost after 297895 iterations : Training Loss =  0.02808396442459693; Validation Loss = 0.03470699845450158\n",
            "Cost after 297896 iterations : Training Loss =  0.028083954572389373; Validation Loss = 0.03470699000598736\n",
            "Cost after 297897 iterations : Training Loss =  0.028083944720294535; Validation Loss = 0.034706981557558116\n",
            "Cost after 297898 iterations : Training Loss =  0.028083934868312576; Validation Loss = 0.03470697310921368\n",
            "Cost after 297899 iterations : Training Loss =  0.028083925016443356; Validation Loss = 0.03470696466095445\n",
            "Cost after 297900 iterations : Training Loss =  0.028083915164687084; Validation Loss = 0.03470695621277974\n",
            "Cost after 297901 iterations : Training Loss =  0.028083905313043684; Validation Loss = 0.03470694776469053\n",
            "Cost after 297902 iterations : Training Loss =  0.028083895461512985; Validation Loss = 0.03470693931668588\n",
            "Cost after 297903 iterations : Training Loss =  0.02808388561009506; Validation Loss = 0.03470693086876625\n",
            "Cost after 297904 iterations : Training Loss =  0.02808387575879006; Validation Loss = 0.0347069224209315\n",
            "Cost after 297905 iterations : Training Loss =  0.02808386590759789; Validation Loss = 0.03470691397318195\n",
            "Cost after 297906 iterations : Training Loss =  0.028083856056518527; Validation Loss = 0.03470690552551716\n",
            "Cost after 297907 iterations : Training Loss =  0.0280838462055519; Validation Loss = 0.034706897077937296\n",
            "Cost after 297908 iterations : Training Loss =  0.02808383635469821; Validation Loss = 0.034706888630442405\n",
            "Cost after 297909 iterations : Training Loss =  0.02808382650395721; Validation Loss = 0.0347068801830325\n",
            "Cost after 297910 iterations : Training Loss =  0.028083816653329118; Validation Loss = 0.03470687173570735\n",
            "Cost after 297911 iterations : Training Loss =  0.02808380680281373; Validation Loss = 0.03470686328846751\n",
            "Cost after 297912 iterations : Training Loss =  0.028083796952411366; Validation Loss = 0.03470685484131223\n",
            "Cost after 297913 iterations : Training Loss =  0.02808378710212159; Validation Loss = 0.03470684639424211\n",
            "Cost after 297914 iterations : Training Loss =  0.02808377725194471; Validation Loss = 0.034706837947256965\n",
            "Cost after 297915 iterations : Training Loss =  0.02808376740188055; Validation Loss = 0.03470682950035672\n",
            "Cost after 297916 iterations : Training Loss =  0.02808375755192939; Validation Loss = 0.034706821053541115\n",
            "Cost after 297917 iterations : Training Loss =  0.02808374770209085; Validation Loss = 0.03470681260681069\n",
            "Cost after 297918 iterations : Training Loss =  0.0280837378523651; Validation Loss = 0.03470680416016508\n",
            "Cost after 297919 iterations : Training Loss =  0.02808372800275226; Validation Loss = 0.03470679571360456\n",
            "Cost after 297920 iterations : Training Loss =  0.028083718153252217; Validation Loss = 0.034706787267129\n",
            "Cost after 297921 iterations : Training Loss =  0.02808370830386483; Validation Loss = 0.034706778820738526\n",
            "Cost after 297922 iterations : Training Loss =  0.028083698454590342; Validation Loss = 0.03470677037443279\n",
            "Cost after 297923 iterations : Training Loss =  0.028083688605428633; Validation Loss = 0.03470676192821197\n",
            "Cost after 297924 iterations : Training Loss =  0.028083678756379658; Validation Loss = 0.03470675348207591\n",
            "Cost after 297925 iterations : Training Loss =  0.028083668907443488; Validation Loss = 0.034706745036024715\n",
            "Cost after 297926 iterations : Training Loss =  0.028083659058620262; Validation Loss = 0.03470673659005865\n",
            "Cost after 297927 iterations : Training Loss =  0.028083649209909658; Validation Loss = 0.034706728144177544\n",
            "Cost after 297928 iterations : Training Loss =  0.028083639361311887; Validation Loss = 0.03470671969838126\n",
            "Cost after 297929 iterations : Training Loss =  0.02808362951282682; Validation Loss = 0.03470671125266972\n",
            "Cost after 297930 iterations : Training Loss =  0.028083619664454638; Validation Loss = 0.03470670280704328\n",
            "Cost after 297931 iterations : Training Loss =  0.028083609816195173; Validation Loss = 0.034706694361501714\n",
            "Cost after 297932 iterations : Training Loss =  0.028083599968048545; Validation Loss = 0.03470668591604512\n",
            "Cost after 297933 iterations : Training Loss =  0.02808359012001462; Validation Loss = 0.03470667747067334\n",
            "Cost after 297934 iterations : Training Loss =  0.028083580272093577; Validation Loss = 0.03470666902538657\n",
            "Cost after 297935 iterations : Training Loss =  0.028083570424285255; Validation Loss = 0.03470666058018475\n",
            "Cost after 297936 iterations : Training Loss =  0.028083560576589606; Validation Loss = 0.03470665213506751\n",
            "Cost after 297937 iterations : Training Loss =  0.02808355072900692; Validation Loss = 0.03470664369003552\n",
            "Cost after 297938 iterations : Training Loss =  0.028083540881536813; Validation Loss = 0.034706635245088345\n",
            "Cost after 297939 iterations : Training Loss =  0.02808353103417955; Validation Loss = 0.03470662680022604\n",
            "Cost after 297940 iterations : Training Loss =  0.028083521186935034; Validation Loss = 0.03470661835544867\n",
            "Cost after 297941 iterations : Training Loss =  0.028083511339803312; Validation Loss = 0.03470660991075631\n",
            "Cost after 297942 iterations : Training Loss =  0.028083501492784282; Validation Loss = 0.034706601466148894\n",
            "Cost after 297943 iterations : Training Loss =  0.028083491645878034; Validation Loss = 0.03470659302162595\n",
            "Cost after 297944 iterations : Training Loss =  0.02808348179908463; Validation Loss = 0.03470658457718839\n",
            "Cost after 297945 iterations : Training Loss =  0.02808347195240386; Validation Loss = 0.03470657613283558\n",
            "Cost after 297946 iterations : Training Loss =  0.02808346210583591; Validation Loss = 0.034706567688567294\n",
            "Cost after 297947 iterations : Training Loss =  0.028083452259380706; Validation Loss = 0.03470655924438425\n",
            "Cost after 297948 iterations : Training Loss =  0.028083442413038236; Validation Loss = 0.034706550800285875\n",
            "Cost after 297949 iterations : Training Loss =  0.028083432566808494; Validation Loss = 0.03470654235627266\n",
            "Cost after 297950 iterations : Training Loss =  0.028083422720691604; Validation Loss = 0.03470653391234429\n",
            "Cost after 297951 iterations : Training Loss =  0.028083412874687373; Validation Loss = 0.034706525468500796\n",
            "Cost after 297952 iterations : Training Loss =  0.028083403028796007; Validation Loss = 0.03470651702474211\n",
            "Cost after 297953 iterations : Training Loss =  0.02808339318301711; Validation Loss = 0.034706508581068364\n",
            "Cost after 297954 iterations : Training Loss =  0.028083383337351227; Validation Loss = 0.03470650013747934\n",
            "Cost after 297955 iterations : Training Loss =  0.02808337349179798; Validation Loss = 0.03470649169397547\n",
            "Cost after 297956 iterations : Training Loss =  0.02808336364635745; Validation Loss = 0.03470648325055669\n",
            "Cost after 297957 iterations : Training Loss =  0.028083353801029696; Validation Loss = 0.03470647480722248\n",
            "Cost after 297958 iterations : Training Loss =  0.02808334395581472; Validation Loss = 0.034706466363972954\n",
            "Cost after 297959 iterations : Training Loss =  0.02808333411071233; Validation Loss = 0.034706457920808696\n",
            "Cost after 297960 iterations : Training Loss =  0.028083324265722825; Validation Loss = 0.03470644947772893\n",
            "Cost after 297961 iterations : Training Loss =  0.02808331442084593; Validation Loss = 0.034706441034734595\n",
            "Cost after 297962 iterations : Training Loss =  0.02808330457608197; Validation Loss = 0.034706432591824696\n",
            "Cost after 297963 iterations : Training Loss =  0.028083294731430532; Validation Loss = 0.034706424149\n",
            "Cost after 297964 iterations : Training Loss =  0.02808328488689184; Validation Loss = 0.03470641570625994\n",
            "Cost after 297965 iterations : Training Loss =  0.028083275042465876; Validation Loss = 0.034706407263604744\n",
            "Cost after 297966 iterations : Training Loss =  0.02808326519815263; Validation Loss = 0.03470639882103446\n",
            "Cost after 297967 iterations : Training Loss =  0.02808325535395227; Validation Loss = 0.03470639037854919\n",
            "Cost after 297968 iterations : Training Loss =  0.028083245509864407; Validation Loss = 0.03470638193614866\n",
            "Cost after 297969 iterations : Training Loss =  0.028083235665889408; Validation Loss = 0.0347063734938328\n",
            "Cost after 297970 iterations : Training Loss =  0.02808322582202709; Validation Loss = 0.03470636505160215\n",
            "Cost after 297971 iterations : Training Loss =  0.028083215978277388; Validation Loss = 0.03470635660945621\n",
            "Cost after 297972 iterations : Training Loss =  0.028083206134640645; Validation Loss = 0.034706348167395186\n",
            "Cost after 297973 iterations : Training Loss =  0.028083196291116386; Validation Loss = 0.03470633972541914\n",
            "Cost after 297974 iterations : Training Loss =  0.028083186447704883; Validation Loss = 0.034706331283528055\n",
            "Cost after 297975 iterations : Training Loss =  0.02808317660440616; Validation Loss = 0.03470632284172155\n",
            "Cost after 297976 iterations : Training Loss =  0.028083166761219948; Validation Loss = 0.03470631439999994\n",
            "Cost after 297977 iterations : Training Loss =  0.028083156918146664; Validation Loss = 0.034706305958363104\n",
            "Cost after 297978 iterations : Training Loss =  0.028083147075186003; Validation Loss = 0.03470629751681143\n",
            "Cost after 297979 iterations : Training Loss =  0.028083137232338032; Validation Loss = 0.03470628907534429\n",
            "Cost after 297980 iterations : Training Loss =  0.028083127389602766; Validation Loss = 0.034706280633962125\n",
            "Cost after 297981 iterations : Training Loss =  0.028083117546980275; Validation Loss = 0.03470627219266451\n",
            "Cost after 297982 iterations : Training Loss =  0.02808310770447031; Validation Loss = 0.03470626375145219\n",
            "Cost after 297983 iterations : Training Loss =  0.028083097862073116; Validation Loss = 0.03470625531032455\n",
            "Cost after 297984 iterations : Training Loss =  0.028083088019788642; Validation Loss = 0.03470624686928205\n",
            "Cost after 297985 iterations : Training Loss =  0.028083078177616905; Validation Loss = 0.03470623842832396\n",
            "Cost after 297986 iterations : Training Loss =  0.028083068335557827; Validation Loss = 0.034706229987450876\n",
            "Cost after 297987 iterations : Training Loss =  0.02808305849361145; Validation Loss = 0.03470622154666289\n",
            "Cost after 297988 iterations : Training Loss =  0.028083048651777746; Validation Loss = 0.03470621310595957\n",
            "Cost after 297989 iterations : Training Loss =  0.02808303881005668; Validation Loss = 0.03470620466534114\n",
            "Cost after 297990 iterations : Training Loss =  0.028083028968448248; Validation Loss = 0.034706196224807404\n",
            "Cost after 297991 iterations : Training Loss =  0.028083019126952543; Validation Loss = 0.03470618778435856\n",
            "Cost after 297992 iterations : Training Loss =  0.0280830092855697; Validation Loss = 0.03470617934399465\n",
            "Cost after 297993 iterations : Training Loss =  0.028082999444299334; Validation Loss = 0.03470617090371542\n",
            "Cost after 297994 iterations : Training Loss =  0.02808298960314169; Validation Loss = 0.03470616246352129\n",
            "Cost after 297995 iterations : Training Loss =  0.028082979762096644; Validation Loss = 0.03470615402341159\n",
            "Cost after 297996 iterations : Training Loss =  0.028082969921164498; Validation Loss = 0.03470614558338698\n",
            "Cost after 297997 iterations : Training Loss =  0.028082960080344894; Validation Loss = 0.034706137143447405\n",
            "Cost after 297998 iterations : Training Loss =  0.02808295023963779; Validation Loss = 0.03470612870359252\n",
            "Cost after 297999 iterations : Training Loss =  0.028082940399043632; Validation Loss = 0.034706120263822554\n",
            "Cost after 298000 iterations : Training Loss =  0.028082930558561998; Validation Loss = 0.03470611182413711\n",
            "Cost after 298001 iterations : Training Loss =  0.028082920718193127; Validation Loss = 0.03470610338453682\n",
            "Cost after 298002 iterations : Training Loss =  0.02808291087793682; Validation Loss = 0.034706094945021235\n",
            "Cost after 298003 iterations : Training Loss =  0.02808290103779318; Validation Loss = 0.03470608650559051\n",
            "Cost after 298004 iterations : Training Loss =  0.028082891197762243; Validation Loss = 0.034706078066244546\n",
            "Cost after 298005 iterations : Training Loss =  0.028082881357843936; Validation Loss = 0.03470606962698328\n",
            "Cost after 298006 iterations : Training Loss =  0.0280828715180383; Validation Loss = 0.034706061187807065\n",
            "Cost after 298007 iterations : Training Loss =  0.02808286167834539; Validation Loss = 0.03470605274871525\n",
            "Cost after 298008 iterations : Training Loss =  0.028082851838764994; Validation Loss = 0.0347060443097088\n",
            "Cost after 298009 iterations : Training Loss =  0.02808284199929742; Validation Loss = 0.034706035870787\n",
            "Cost after 298010 iterations : Training Loss =  0.028082832159942395; Validation Loss = 0.03470602743194989\n",
            "Cost after 298011 iterations : Training Loss =  0.028082822320700012; Validation Loss = 0.034706018993197744\n",
            "Cost after 298012 iterations : Training Loss =  0.02808281248157032; Validation Loss = 0.03470601055453031\n",
            "Cost after 298013 iterations : Training Loss =  0.028082802642553216; Validation Loss = 0.034706002115947786\n",
            "Cost after 298014 iterations : Training Loss =  0.028082792803648823; Validation Loss = 0.03470599367745038\n",
            "Cost after 298015 iterations : Training Loss =  0.028082782964857042; Validation Loss = 0.03470598523903739\n",
            "Cost after 298016 iterations : Training Loss =  0.028082773126177935; Validation Loss = 0.03470597680070935\n",
            "Cost after 298017 iterations : Training Loss =  0.028082763287611445; Validation Loss = 0.034705968362466\n",
            "Cost after 298018 iterations : Training Loss =  0.028082753449157522; Validation Loss = 0.03470595992430753\n",
            "Cost after 298019 iterations : Training Loss =  0.028082743610816304; Validation Loss = 0.03470595148623385\n",
            "Cost after 298020 iterations : Training Loss =  0.02808273377258777; Validation Loss = 0.03470594304824487\n",
            "Cost after 298021 iterations : Training Loss =  0.028082723934471848; Validation Loss = 0.0347059346103409\n",
            "Cost after 298022 iterations : Training Loss =  0.028082714096468606; Validation Loss = 0.0347059261725216\n",
            "Cost after 298023 iterations : Training Loss =  0.028082704258577867; Validation Loss = 0.03470591773478698\n",
            "Cost after 298024 iterations : Training Loss =  0.028082694420799775; Validation Loss = 0.03470590929713772\n",
            "Cost after 298025 iterations : Training Loss =  0.02808268458313433; Validation Loss = 0.0347059008595727\n",
            "Cost after 298026 iterations : Training Loss =  0.02808267474558158; Validation Loss = 0.03470589242209271\n",
            "Cost after 298027 iterations : Training Loss =  0.0280826649081413; Validation Loss = 0.03470588398469734\n",
            "Cost after 298028 iterations : Training Loss =  0.028082655070813884; Validation Loss = 0.03470587554738669\n",
            "Cost after 298029 iterations : Training Loss =  0.028082645233598918; Validation Loss = 0.03470586711016093\n",
            "Cost after 298030 iterations : Training Loss =  0.028082635396496573; Validation Loss = 0.03470585867302033\n",
            "Cost after 298031 iterations : Training Loss =  0.028082625559506833; Validation Loss = 0.03470585023596454\n",
            "Cost after 298032 iterations : Training Loss =  0.028082615722629826; Validation Loss = 0.0347058417989931\n",
            "Cost after 298033 iterations : Training Loss =  0.028082605885865336; Validation Loss = 0.03470583336210655\n",
            "Cost after 298034 iterations : Training Loss =  0.028082596049213544; Validation Loss = 0.03470582492530508\n",
            "Cost after 298035 iterations : Training Loss =  0.02808258621267421; Validation Loss = 0.03470581648858791\n",
            "Cost after 298036 iterations : Training Loss =  0.028082576376247653; Validation Loss = 0.03470580805195585\n",
            "Cost after 298037 iterations : Training Loss =  0.028082566539933605; Validation Loss = 0.034705799615408754\n",
            "Cost after 298038 iterations : Training Loss =  0.02808255670373221; Validation Loss = 0.03470579117894602\n",
            "Cost after 298039 iterations : Training Loss =  0.02808254686764336; Validation Loss = 0.03470578274256825\n",
            "Cost after 298040 iterations : Training Loss =  0.028082537031667198; Validation Loss = 0.034705774306275215\n",
            "Cost after 298041 iterations : Training Loss =  0.02808252719580355; Validation Loss = 0.03470576587006731\n",
            "Cost after 298042 iterations : Training Loss =  0.028082517360052656; Validation Loss = 0.03470575743394384\n",
            "Cost after 298043 iterations : Training Loss =  0.02808250752441416; Validation Loss = 0.03470574899790515\n",
            "Cost after 298044 iterations : Training Loss =  0.028082497688888368; Validation Loss = 0.03470574056195157\n",
            "Cost after 298045 iterations : Training Loss =  0.02808248785347525; Validation Loss = 0.03470573212608276\n",
            "Cost after 298046 iterations : Training Loss =  0.02808247801817457; Validation Loss = 0.03470572369029844\n",
            "Cost after 298047 iterations : Training Loss =  0.02808246818298651; Validation Loss = 0.03470571525459897\n",
            "Cost after 298048 iterations : Training Loss =  0.028082458347911156; Validation Loss = 0.03470570681898432\n",
            "Cost after 298049 iterations : Training Loss =  0.02808244851294825; Validation Loss = 0.0347056983834544\n",
            "Cost after 298050 iterations : Training Loss =  0.028082438678097957; Validation Loss = 0.034705689948009384\n",
            "Cost after 298051 iterations : Training Loss =  0.02808242884336035; Validation Loss = 0.034705681512649036\n",
            "Cost after 298052 iterations : Training Loss =  0.028082419008735208; Validation Loss = 0.03470567307737356\n",
            "Cost after 298053 iterations : Training Loss =  0.028082409174222802; Validation Loss = 0.03470566464218263\n",
            "Cost after 298054 iterations : Training Loss =  0.02808239933982286; Validation Loss = 0.03470565620707656\n",
            "Cost after 298055 iterations : Training Loss =  0.028082389505535574; Validation Loss = 0.03470564777205525\n",
            "Cost after 298056 iterations : Training Loss =  0.028082379671360718; Validation Loss = 0.034705639337118684\n",
            "Cost after 298057 iterations : Training Loss =  0.028082369837298508; Validation Loss = 0.0347056309022671\n",
            "Cost after 298058 iterations : Training Loss =  0.028082360003348916; Validation Loss = 0.03470562246750017\n",
            "Cost after 298059 iterations : Training Loss =  0.028082350169511945; Validation Loss = 0.0347056140328182\n",
            "Cost after 298060 iterations : Training Loss =  0.02808234033578741; Validation Loss = 0.03470560559822061\n",
            "Cost after 298061 iterations : Training Loss =  0.028082330502175484; Validation Loss = 0.03470559716370822\n",
            "Cost after 298062 iterations : Training Loss =  0.02808232066867633; Validation Loss = 0.034705588729280305\n",
            "Cost after 298063 iterations : Training Loss =  0.02808231083528948; Validation Loss = 0.034705580294937415\n",
            "Cost after 298064 iterations : Training Loss =  0.02808230100201533; Validation Loss = 0.03470557186067899\n",
            "Cost after 298065 iterations : Training Loss =  0.028082291168853604; Validation Loss = 0.034705563426505354\n",
            "Cost after 298066 iterations : Training Loss =  0.028082281335804676; Validation Loss = 0.034705554992416704\n",
            "Cost after 298067 iterations : Training Loss =  0.028082271502868048; Validation Loss = 0.03470554655841243\n",
            "Cost after 298068 iterations : Training Loss =  0.028082261670044094; Validation Loss = 0.03470553812449343\n",
            "Cost after 298069 iterations : Training Loss =  0.028082251837332747; Validation Loss = 0.034705529690658925\n",
            "Cost after 298070 iterations : Training Loss =  0.02808224200473387; Validation Loss = 0.034705521256909226\n",
            "Cost after 298071 iterations : Training Loss =  0.028082232172247684; Validation Loss = 0.034705512823244224\n",
            "Cost after 298072 iterations : Training Loss =  0.02808222233987399; Validation Loss = 0.03470550438966388\n",
            "Cost after 298073 iterations : Training Loss =  0.028082212507612767; Validation Loss = 0.03470549595616839\n",
            "Cost after 298074 iterations : Training Loss =  0.028082202675464157; Validation Loss = 0.03470548752275765\n",
            "Cost after 298075 iterations : Training Loss =  0.02808219284342801; Validation Loss = 0.03470547908943173\n",
            "Cost after 298076 iterations : Training Loss =  0.02808218301150452; Validation Loss = 0.03470547065619017\n",
            "Cost after 298077 iterations : Training Loss =  0.028082173179693536; Validation Loss = 0.03470546222303362\n",
            "Cost after 298078 iterations : Training Loss =  0.028082163347995036; Validation Loss = 0.03470545378996183\n",
            "Cost after 298079 iterations : Training Loss =  0.028082153516409195; Validation Loss = 0.03470544535697472\n",
            "Cost after 298080 iterations : Training Loss =  0.02808214368493576; Validation Loss = 0.0347054369240723\n",
            "Cost after 298081 iterations : Training Loss =  0.02808213385357496; Validation Loss = 0.03470542849125457\n",
            "Cost after 298082 iterations : Training Loss =  0.028082124022326713; Validation Loss = 0.03470542005852169\n",
            "Cost after 298083 iterations : Training Loss =  0.028082114191190995; Validation Loss = 0.034705411625873615\n",
            "Cost after 298084 iterations : Training Loss =  0.02808210436016776; Validation Loss = 0.034705403193310086\n",
            "Cost after 298085 iterations : Training Loss =  0.028082094529256998; Validation Loss = 0.03470539476083151\n",
            "Cost after 298086 iterations : Training Loss =  0.028082084698458867; Validation Loss = 0.03470538632843768\n",
            "Cost after 298087 iterations : Training Loss =  0.02808207486777325; Validation Loss = 0.034705377896128646\n",
            "Cost after 298088 iterations : Training Loss =  0.028082065037200187; Validation Loss = 0.03470536946390406\n",
            "Cost after 298089 iterations : Training Loss =  0.028082055206739537; Validation Loss = 0.03470536103176452\n",
            "Cost after 298090 iterations : Training Loss =  0.028082045376391498; Validation Loss = 0.034705352599709395\n",
            "Cost after 298091 iterations : Training Loss =  0.028082035546156053; Validation Loss = 0.03470534416773934\n",
            "Cost after 298092 iterations : Training Loss =  0.028082025716032993; Validation Loss = 0.03470533573585355\n",
            "Cost after 298093 iterations : Training Loss =  0.028082015886022473; Validation Loss = 0.0347053273040526\n",
            "Cost after 298094 iterations : Training Loss =  0.02808200605612452; Validation Loss = 0.03470531887233674\n",
            "Cost after 298095 iterations : Training Loss =  0.02808199622633903; Validation Loss = 0.03470531044070532\n",
            "Cost after 298096 iterations : Training Loss =  0.02808198639666606; Validation Loss = 0.03470530200915857\n",
            "Cost after 298097 iterations : Training Loss =  0.028081976567105652; Validation Loss = 0.03470529357769654\n",
            "Cost after 298098 iterations : Training Loss =  0.028081966737657785; Validation Loss = 0.034705285146319334\n",
            "Cost after 298099 iterations : Training Loss =  0.028081956908322395; Validation Loss = 0.03470527671502692\n",
            "Cost after 298100 iterations : Training Loss =  0.028081947079099445; Validation Loss = 0.034705268283818855\n",
            "Cost after 298101 iterations : Training Loss =  0.028081937249989038; Validation Loss = 0.03470525985269611\n",
            "Cost after 298102 iterations : Training Loss =  0.02808192742099114; Validation Loss = 0.03470525142165747\n",
            "Cost after 298103 iterations : Training Loss =  0.028081917592105713; Validation Loss = 0.034705242990704085\n",
            "Cost after 298104 iterations : Training Loss =  0.028081907763332796; Validation Loss = 0.0347052345598351\n",
            "Cost after 298105 iterations : Training Loss =  0.028081897934672417; Validation Loss = 0.0347052261290509\n",
            "Cost after 298106 iterations : Training Loss =  0.028081888106124577; Validation Loss = 0.034705217698351214\n",
            "Cost after 298107 iterations : Training Loss =  0.02808187827768915; Validation Loss = 0.03470520926773652\n",
            "Cost after 298108 iterations : Training Loss =  0.028081868449366238; Validation Loss = 0.03470520083720644\n",
            "Cost after 298109 iterations : Training Loss =  0.028081858621155843; Validation Loss = 0.03470519240676102\n",
            "Cost after 298110 iterations : Training Loss =  0.028081848793057966; Validation Loss = 0.0347051839764004\n",
            "Cost after 298111 iterations : Training Loss =  0.028081838965072492; Validation Loss = 0.034705175546124595\n",
            "Cost after 298112 iterations : Training Loss =  0.028081829137199605; Validation Loss = 0.03470516711593329\n",
            "Cost after 298113 iterations : Training Loss =  0.02808181930943912; Validation Loss = 0.03470515868582666\n",
            "Cost after 298114 iterations : Training Loss =  0.028081809481791224; Validation Loss = 0.03470515025580504\n",
            "Cost after 298115 iterations : Training Loss =  0.02808179965425569; Validation Loss = 0.03470514182586793\n",
            "Cost after 298116 iterations : Training Loss =  0.02808178982683278; Validation Loss = 0.034705133396015365\n",
            "Cost after 298117 iterations : Training Loss =  0.028081779999522185; Validation Loss = 0.03470512496624768\n",
            "Cost after 298118 iterations : Training Loss =  0.02808177017232414; Validation Loss = 0.034705116536564405\n",
            "Cost after 298119 iterations : Training Loss =  0.028081760345238616; Validation Loss = 0.03470510810696611\n",
            "Cost after 298120 iterations : Training Loss =  0.02808175051826556; Validation Loss = 0.03470509967745235\n",
            "Cost after 298121 iterations : Training Loss =  0.028081740691404955; Validation Loss = 0.03470509124802329\n",
            "Cost after 298122 iterations : Training Loss =  0.028081730864656797; Validation Loss = 0.03470508281867895\n",
            "Cost after 298123 iterations : Training Loss =  0.02808172103802109; Validation Loss = 0.034705074389419464\n",
            "Cost after 298124 iterations : Training Loss =  0.02808171121149792; Validation Loss = 0.03470506596024463\n",
            "Cost after 298125 iterations : Training Loss =  0.02808170138508713; Validation Loss = 0.03470505753115428\n",
            "Cost after 298126 iterations : Training Loss =  0.028081691558788847; Validation Loss = 0.03470504910214855\n",
            "Cost after 298127 iterations : Training Loss =  0.028081681732603142; Validation Loss = 0.03470504067322776\n",
            "Cost after 298128 iterations : Training Loss =  0.028081671906529767; Validation Loss = 0.03470503224439162\n",
            "Cost after 298129 iterations : Training Loss =  0.028081662080568854; Validation Loss = 0.03470502381564018\n",
            "Cost after 298130 iterations : Training Loss =  0.028081652254720504; Validation Loss = 0.0347050153869734\n",
            "Cost after 298131 iterations : Training Loss =  0.02808164242898455; Validation Loss = 0.0347050069583912\n",
            "Cost after 298132 iterations : Training Loss =  0.028081632603360957; Validation Loss = 0.03470499852989392\n",
            "Cost after 298133 iterations : Training Loss =  0.028081622777850042; Validation Loss = 0.03470499010148108\n",
            "Cost after 298134 iterations : Training Loss =  0.028081612952451353; Validation Loss = 0.034704981673152974\n",
            "Cost after 298135 iterations : Training Loss =  0.02808160312716523; Validation Loss = 0.03470497324490961\n",
            "Cost after 298136 iterations : Training Loss =  0.028081593301991493; Validation Loss = 0.03470496481675079\n",
            "Cost after 298137 iterations : Training Loss =  0.02808158347693021; Validation Loss = 0.03470495638867691\n",
            "Cost after 298138 iterations : Training Loss =  0.02808157365198162; Validation Loss = 0.034704947960687675\n",
            "Cost after 298139 iterations : Training Loss =  0.02808156382714515; Validation Loss = 0.034704939532783044\n",
            "Cost after 298140 iterations : Training Loss =  0.028081554002421197; Validation Loss = 0.03470493110496319\n",
            "Cost after 298141 iterations : Training Loss =  0.02808154417780969; Validation Loss = 0.034704922677227655\n",
            "Cost after 298142 iterations : Training Loss =  0.028081534353310813; Validation Loss = 0.03470491424957715\n",
            "Cost after 298143 iterations : Training Loss =  0.028081524528924207; Validation Loss = 0.03470490582201116\n",
            "Cost after 298144 iterations : Training Loss =  0.028081514704650003; Validation Loss = 0.034704897394529685\n",
            "Cost after 298145 iterations : Training Loss =  0.028081504880488325; Validation Loss = 0.03470488896713306\n",
            "Cost after 298146 iterations : Training Loss =  0.028081495056439074; Validation Loss = 0.03470488053982085\n",
            "Cost after 298147 iterations : Training Loss =  0.028081485232502243; Validation Loss = 0.03470487211259332\n",
            "Cost after 298148 iterations : Training Loss =  0.028081475408677774; Validation Loss = 0.034704863685450935\n",
            "Cost after 298149 iterations : Training Loss =  0.028081465584965788; Validation Loss = 0.034704855258392765\n",
            "Cost after 298150 iterations : Training Loss =  0.028081455761366274; Validation Loss = 0.03470484683141958\n",
            "Cost after 298151 iterations : Training Loss =  0.028081445937879045; Validation Loss = 0.03470483840453079\n",
            "Cost after 298152 iterations : Training Loss =  0.02808143611450435; Validation Loss = 0.03470482997772688\n",
            "Cost after 298153 iterations : Training Loss =  0.02808142629124215; Validation Loss = 0.034704821551007556\n",
            "Cost after 298154 iterations : Training Loss =  0.028081416468092257; Validation Loss = 0.03470481312437305\n",
            "Cost after 298155 iterations : Training Loss =  0.02808140664505484; Validation Loss = 0.03470480469782289\n",
            "Cost after 298156 iterations : Training Loss =  0.028081396822129776; Validation Loss = 0.03470479627135723\n",
            "Cost after 298157 iterations : Training Loss =  0.028081386999317195; Validation Loss = 0.0347047878449766\n",
            "Cost after 298158 iterations : Training Loss =  0.02808137717661699; Validation Loss = 0.034704779418680276\n",
            "Cost after 298159 iterations : Training Loss =  0.028081367354029264; Validation Loss = 0.03470477099246886\n",
            "Cost after 298160 iterations : Training Loss =  0.0280813575315538; Validation Loss = 0.034704762566341966\n",
            "Cost after 298161 iterations : Training Loss =  0.02808134770919094; Validation Loss = 0.03470475414029988\n",
            "Cost after 298162 iterations : Training Loss =  0.028081337886940366; Validation Loss = 0.034704745714342275\n",
            "Cost after 298163 iterations : Training Loss =  0.02808132806480216; Validation Loss = 0.03470473728846939\n",
            "Cost after 298164 iterations : Training Loss =  0.028081318242776498; Validation Loss = 0.03470472886268111\n",
            "Cost after 298165 iterations : Training Loss =  0.028081308420863173; Validation Loss = 0.034704720436977435\n",
            "Cost after 298166 iterations : Training Loss =  0.028081298599062273; Validation Loss = 0.034704712011358725\n",
            "Cost after 298167 iterations : Training Loss =  0.028081288777373703; Validation Loss = 0.03470470358582442\n",
            "Cost after 298168 iterations : Training Loss =  0.028081278955797668; Validation Loss = 0.034704695160374714\n",
            "Cost after 298169 iterations : Training Loss =  0.02808126913433384; Validation Loss = 0.03470468673500962\n",
            "Cost after 298170 iterations : Training Loss =  0.028081259312982505; Validation Loss = 0.034704678309728836\n",
            "Cost after 298171 iterations : Training Loss =  0.028081249491743572; Validation Loss = 0.03470466988453334\n",
            "Cost after 298172 iterations : Training Loss =  0.028081239670617028; Validation Loss = 0.034704661459422165\n",
            "Cost after 298173 iterations : Training Loss =  0.028081229849602964; Validation Loss = 0.034704653034395495\n",
            "Cost after 298174 iterations : Training Loss =  0.02808122002870115; Validation Loss = 0.03470464460945379\n",
            "Cost after 298175 iterations : Training Loss =  0.028081210207911784; Validation Loss = 0.034704636184596446\n",
            "Cost after 298176 iterations : Training Loss =  0.028081200387234672; Validation Loss = 0.03470462775982382\n",
            "Cost after 298177 iterations : Training Loss =  0.028081190566670064; Validation Loss = 0.03470461933513574\n",
            "Cost after 298178 iterations : Training Loss =  0.028081180746217953; Validation Loss = 0.034704610910532556\n",
            "Cost after 298179 iterations : Training Loss =  0.028081170925878057; Validation Loss = 0.03470460248601389\n",
            "Cost after 298180 iterations : Training Loss =  0.028081161105650586; Validation Loss = 0.034704594061580094\n",
            "Cost after 298181 iterations : Training Loss =  0.02808115128553554; Validation Loss = 0.03470458563723053\n",
            "Cost after 298182 iterations : Training Loss =  0.028081141465532772; Validation Loss = 0.03470457721296537\n",
            "Cost after 298183 iterations : Training Loss =  0.02808113164564247; Validation Loss = 0.03470456878878536\n",
            "Cost after 298184 iterations : Training Loss =  0.028081121825864516; Validation Loss = 0.03470456036468985\n",
            "Cost after 298185 iterations : Training Loss =  0.028081112006198984; Validation Loss = 0.03470455194067878\n",
            "Cost after 298186 iterations : Training Loss =  0.028081102186645758; Validation Loss = 0.034704543516752465\n",
            "Cost after 298187 iterations : Training Loss =  0.02808109236720494; Validation Loss = 0.03470453509291053\n",
            "Cost after 298188 iterations : Training Loss =  0.02808108254787646; Validation Loss = 0.034704526669153475\n",
            "Cost after 298189 iterations : Training Loss =  0.0280810727286602; Validation Loss = 0.03470451824548098\n",
            "Cost after 298190 iterations : Training Loss =  0.028081062909556507; Validation Loss = 0.034704509821893\n",
            "Cost after 298191 iterations : Training Loss =  0.028081053090565064; Validation Loss = 0.03470450139838952\n",
            "Cost after 298192 iterations : Training Loss =  0.02808104327168618; Validation Loss = 0.03470449297497077\n",
            "Cost after 298193 iterations : Training Loss =  0.028081033452919457; Validation Loss = 0.0347044845516366\n",
            "Cost after 298194 iterations : Training Loss =  0.028081023634265102; Validation Loss = 0.03470447612838704\n",
            "Cost after 298195 iterations : Training Loss =  0.02808101381572321; Validation Loss = 0.03470446770522205\n",
            "Cost after 298196 iterations : Training Loss =  0.02808100399729358; Validation Loss = 0.03470445928214177\n",
            "Cost after 298197 iterations : Training Loss =  0.02808099417897634; Validation Loss = 0.03470445085914602\n",
            "Cost after 298198 iterations : Training Loss =  0.028080984360771423; Validation Loss = 0.034704442436235036\n",
            "Cost after 298199 iterations : Training Loss =  0.028080974542678907; Validation Loss = 0.03470443401340855\n",
            "Cost after 298200 iterations : Training Loss =  0.028080964724698624; Validation Loss = 0.03470442559066674\n",
            "Cost after 298201 iterations : Training Loss =  0.028080954906830838; Validation Loss = 0.034704417168009455\n",
            "Cost after 298202 iterations : Training Loss =  0.02808094508907529; Validation Loss = 0.034704408745436684\n",
            "Cost after 298203 iterations : Training Loss =  0.02808093527143215; Validation Loss = 0.0347044003229485\n",
            "Cost after 298204 iterations : Training Loss =  0.028080925453901406; Validation Loss = 0.03470439190054491\n",
            "Cost after 298205 iterations : Training Loss =  0.028080915636482847; Validation Loss = 0.0347043834782261\n",
            "Cost after 298206 iterations : Training Loss =  0.028080905819176594; Validation Loss = 0.034704375055991984\n",
            "Cost after 298207 iterations : Training Loss =  0.02808089600198288; Validation Loss = 0.03470436663384225\n",
            "Cost after 298208 iterations : Training Loss =  0.028080886184901486; Validation Loss = 0.03470435821177716\n",
            "Cost after 298209 iterations : Training Loss =  0.028080876367932273; Validation Loss = 0.03470434978979691\n",
            "Cost after 298210 iterations : Training Loss =  0.02808086655107546; Validation Loss = 0.03470434136790083\n",
            "Cost after 298211 iterations : Training Loss =  0.028080856734330983; Validation Loss = 0.03470433294608966\n",
            "Cost after 298212 iterations : Training Loss =  0.028080846917698854; Validation Loss = 0.034704324524362874\n",
            "Cost after 298213 iterations : Training Loss =  0.028080837101178955; Validation Loss = 0.03470431610272072\n",
            "Cost after 298214 iterations : Training Loss =  0.02808082728477146; Validation Loss = 0.03470430768116333\n",
            "Cost after 298215 iterations : Training Loss =  0.028080817468476255; Validation Loss = 0.03470429925969053\n",
            "Cost after 298216 iterations : Training Loss =  0.028080807652293538; Validation Loss = 0.0347042908383021\n",
            "Cost after 298217 iterations : Training Loss =  0.028080797836222988; Validation Loss = 0.034704282416998314\n",
            "Cost after 298218 iterations : Training Loss =  0.02808078802026472; Validation Loss = 0.034704273995779064\n",
            "Cost after 298219 iterations : Training Loss =  0.028080778204418805; Validation Loss = 0.03470426557464446\n",
            "Cost after 298220 iterations : Training Loss =  0.028080768388685368; Validation Loss = 0.034704257153594494\n",
            "Cost after 298221 iterations : Training Loss =  0.02808075857306405; Validation Loss = 0.0347042487326292\n",
            "Cost after 298222 iterations : Training Loss =  0.028080748757555094; Validation Loss = 0.03470424031174812\n",
            "Cost after 298223 iterations : Training Loss =  0.028080738942158387; Validation Loss = 0.03470423189095177\n",
            "Cost after 298224 iterations : Training Loss =  0.02808072912687411; Validation Loss = 0.03470422347024\n",
            "Cost after 298225 iterations : Training Loss =  0.02808071931170201; Validation Loss = 0.034704215049613\n",
            "Cost after 298226 iterations : Training Loss =  0.028080709496642417; Validation Loss = 0.03470420662907026\n",
            "Cost after 298227 iterations : Training Loss =  0.02808069968169494; Validation Loss = 0.03470419820861214\n",
            "Cost after 298228 iterations : Training Loss =  0.028080689866859757; Validation Loss = 0.034704189788238735\n",
            "Cost after 298229 iterations : Training Loss =  0.02808068005213709; Validation Loss = 0.03470418136794991\n",
            "Cost after 298230 iterations : Training Loss =  0.028080670237526518; Validation Loss = 0.03470417294774537\n",
            "Cost after 298231 iterations : Training Loss =  0.028080660423028335; Validation Loss = 0.03470416452762571\n",
            "Cost after 298232 iterations : Training Loss =  0.02808065060864249; Validation Loss = 0.0347041561075905\n",
            "Cost after 298233 iterations : Training Loss =  0.028080640794368877; Validation Loss = 0.03470414768763987\n",
            "Cost after 298234 iterations : Training Loss =  0.028080630980207473; Validation Loss = 0.034704139267773666\n",
            "Cost after 298235 iterations : Training Loss =  0.0280806211661585; Validation Loss = 0.03470413084799232\n",
            "Cost after 298236 iterations : Training Loss =  0.028080611352221802; Validation Loss = 0.03470412242829538\n",
            "Cost after 298237 iterations : Training Loss =  0.02808060153839727; Validation Loss = 0.034704114008682854\n",
            "Cost after 298238 iterations : Training Loss =  0.028080591724685133; Validation Loss = 0.03470410558915477\n",
            "Cost after 298239 iterations : Training Loss =  0.028080581911085246; Validation Loss = 0.034704097169711616\n",
            "Cost after 298240 iterations : Training Loss =  0.028080572097597702; Validation Loss = 0.03470408875035277\n",
            "Cost after 298241 iterations : Training Loss =  0.028080562284222354; Validation Loss = 0.03470408033107869\n",
            "Cost after 298242 iterations : Training Loss =  0.028080552470959332; Validation Loss = 0.03470407191188897\n",
            "Cost after 298243 iterations : Training Loss =  0.028080542657808613; Validation Loss = 0.03470406349278398\n",
            "Cost after 298244 iterations : Training Loss =  0.028080532844770172; Validation Loss = 0.03470405507376337\n",
            "Cost after 298245 iterations : Training Loss =  0.028080523031843933; Validation Loss = 0.03470404665482731\n",
            "Cost after 298246 iterations : Training Loss =  0.028080513219029993; Validation Loss = 0.03470403823597594\n",
            "Cost after 298247 iterations : Training Loss =  0.028080503406328394; Validation Loss = 0.034704029817208965\n",
            "Cost after 298248 iterations : Training Loss =  0.028080493593738983; Validation Loss = 0.034704021398526505\n",
            "Cost after 298249 iterations : Training Loss =  0.02808048378126201; Validation Loss = 0.03470401297992866\n",
            "Cost after 298250 iterations : Training Loss =  0.02808047396889715; Validation Loss = 0.03470400456141527\n",
            "Cost after 298251 iterations : Training Loss =  0.02808046415664443; Validation Loss = 0.03470399614298666\n",
            "Cost after 298252 iterations : Training Loss =  0.028080454344504197; Validation Loss = 0.034703987724642525\n",
            "Cost after 298253 iterations : Training Loss =  0.028080444532476136; Validation Loss = 0.03470397930638253\n",
            "Cost after 298254 iterations : Training Loss =  0.028080434720560318; Validation Loss = 0.03470397088820783\n",
            "Cost after 298255 iterations : Training Loss =  0.028080424908756823; Validation Loss = 0.034703962470117145\n",
            "Cost after 298256 iterations : Training Loss =  0.02808041509706555; Validation Loss = 0.034703954052111165\n",
            "Cost after 298257 iterations : Training Loss =  0.028080405285486533; Validation Loss = 0.034703945634189486\n",
            "Cost after 298258 iterations : Training Loss =  0.028080395474019852; Validation Loss = 0.03470393721635244\n",
            "Cost after 298259 iterations : Training Loss =  0.028080385662665307; Validation Loss = 0.034703928798600174\n",
            "Cost after 298260 iterations : Training Loss =  0.02808037585142318; Validation Loss = 0.034703920380932424\n",
            "Cost after 298261 iterations : Training Loss =  0.028080366040293136; Validation Loss = 0.034703911963349085\n",
            "Cost after 298262 iterations : Training Loss =  0.028080356229275336; Validation Loss = 0.034703903545850366\n",
            "Cost after 298263 iterations : Training Loss =  0.028080346418369852; Validation Loss = 0.03470389512843613\n",
            "Cost after 298264 iterations : Training Loss =  0.02808033660757665; Validation Loss = 0.03470388671110641\n",
            "Cost after 298265 iterations : Training Loss =  0.028080326796895663; Validation Loss = 0.03470387829386086\n",
            "Cost after 298266 iterations : Training Loss =  0.028080316986326872; Validation Loss = 0.034703869876700405\n",
            "Cost after 298267 iterations : Training Loss =  0.028080307175870244; Validation Loss = 0.03470386145962416\n",
            "Cost after 298268 iterations : Training Loss =  0.02808029736552608; Validation Loss = 0.03470385304263243\n",
            "Cost after 298269 iterations : Training Loss =  0.028080287555293976; Validation Loss = 0.03470384462572542\n",
            "Cost after 298270 iterations : Training Loss =  0.028080277745174232; Validation Loss = 0.03470383620890266\n",
            "Cost after 298271 iterations : Training Loss =  0.02808026793516654; Validation Loss = 0.03470382779216451\n",
            "Cost after 298272 iterations : Training Loss =  0.02808025812527121; Validation Loss = 0.03470381937551088\n",
            "Cost after 298273 iterations : Training Loss =  0.028080248315488214; Validation Loss = 0.03470381095894178\n",
            "Cost after 298274 iterations : Training Loss =  0.028080238505817322; Validation Loss = 0.03470380254245742\n",
            "Cost after 298275 iterations : Training Loss =  0.02808022869625859; Validation Loss = 0.03470379412605759\n",
            "Cost after 298276 iterations : Training Loss =  0.028080218886812235; Validation Loss = 0.03470378570974219\n",
            "Cost after 298277 iterations : Training Loss =  0.028080209077477963; Validation Loss = 0.03470377729351125\n",
            "Cost after 298278 iterations : Training Loss =  0.028080199268255973; Validation Loss = 0.03470376887736472\n",
            "Cost after 298279 iterations : Training Loss =  0.02808018945914625; Validation Loss = 0.03470376046130285\n",
            "Cost after 298280 iterations : Training Loss =  0.0280801796501487; Validation Loss = 0.03470375204532504\n",
            "Cost after 298281 iterations : Training Loss =  0.028080169841263426; Validation Loss = 0.03470374362943209\n",
            "Cost after 298282 iterations : Training Loss =  0.028080160032490288; Validation Loss = 0.03470373521362359\n",
            "Cost after 298283 iterations : Training Loss =  0.02808015022382943; Validation Loss = 0.0347037267978997\n",
            "Cost after 298284 iterations : Training Loss =  0.028080140415280806; Validation Loss = 0.03470371838226022\n",
            "Cost after 298285 iterations : Training Loss =  0.02808013060684436; Validation Loss = 0.03470370996670514\n",
            "Cost after 298286 iterations : Training Loss =  0.028080120798520087; Validation Loss = 0.03470370155123488\n",
            "Cost after 298287 iterations : Training Loss =  0.028080110990307938; Validation Loss = 0.03470369313584892\n",
            "Cost after 298288 iterations : Training Loss =  0.028080101182208286; Validation Loss = 0.03470368472054768\n",
            "Cost after 298289 iterations : Training Loss =  0.02808009137422059; Validation Loss = 0.034703676305330736\n",
            "Cost after 298290 iterations : Training Loss =  0.028080081566345224; Validation Loss = 0.034703667890198193\n",
            "Cost after 298291 iterations : Training Loss =  0.028080071758581963; Validation Loss = 0.03470365947515039\n",
            "Cost after 298292 iterations : Training Loss =  0.02808006195093097; Validation Loss = 0.03470365106018692\n",
            "Cost after 298293 iterations : Training Loss =  0.02808005214339215; Validation Loss = 0.03470364264530791\n",
            "Cost after 298294 iterations : Training Loss =  0.028080042335965505; Validation Loss = 0.03470363423051333\n",
            "Cost after 298295 iterations : Training Loss =  0.028080032528651128; Validation Loss = 0.03470362581580353\n",
            "Cost after 298296 iterations : Training Loss =  0.028080022721448886; Validation Loss = 0.03470361740117782\n",
            "Cost after 298297 iterations : Training Loss =  0.02808001291435889; Validation Loss = 0.034703608986637095\n",
            "Cost after 298298 iterations : Training Loss =  0.028080003107381; Validation Loss = 0.034703600572180075\n",
            "Cost after 298299 iterations : Training Loss =  0.028079993300515343; Validation Loss = 0.0347035921578082\n",
            "Cost after 298300 iterations : Training Loss =  0.02807998349376196; Validation Loss = 0.0347035837435207\n",
            "Cost after 298301 iterations : Training Loss =  0.028079973687120677; Validation Loss = 0.034703575329317726\n",
            "Cost after 298302 iterations : Training Loss =  0.028079963880591502; Validation Loss = 0.034703566915199005\n",
            "Cost after 298303 iterations : Training Loss =  0.02807995407417469; Validation Loss = 0.03470355850116515\n",
            "Cost after 298304 iterations : Training Loss =  0.028079944267869982; Validation Loss = 0.03470355008721569\n",
            "Cost after 298305 iterations : Training Loss =  0.02807993446167749; Validation Loss = 0.03470354167335053\n",
            "Cost after 298306 iterations : Training Loss =  0.028079924655597027; Validation Loss = 0.0347035332595699\n",
            "Cost after 298307 iterations : Training Loss =  0.028079914849628906; Validation Loss = 0.03470352484587363\n",
            "Cost after 298308 iterations : Training Loss =  0.028079905043772828; Validation Loss = 0.03470351643226203\n",
            "Cost after 298309 iterations : Training Loss =  0.028079895238029003; Validation Loss = 0.03470350801873454\n",
            "Cost after 298310 iterations : Training Loss =  0.02807988543239743; Validation Loss = 0.03470349960529221\n",
            "Cost after 298311 iterations : Training Loss =  0.028079875626877797; Validation Loss = 0.03470349119193353\n",
            "Cost after 298312 iterations : Training Loss =  0.0280798658214705; Validation Loss = 0.034703482778659984\n",
            "Cost after 298313 iterations : Training Loss =  0.02807985601617535; Validation Loss = 0.03470347436547095\n",
            "Cost after 298314 iterations : Training Loss =  0.028079846210992368; Validation Loss = 0.03470346595236585\n",
            "Cost after 298315 iterations : Training Loss =  0.028079836405921554; Validation Loss = 0.034703457539345325\n",
            "Cost after 298316 iterations : Training Loss =  0.02807982660096286; Validation Loss = 0.034703449126409255\n",
            "Cost after 298317 iterations : Training Loss =  0.02807981679611629; Validation Loss = 0.03470344071355784\n",
            "Cost after 298318 iterations : Training Loss =  0.028079806991382072; Validation Loss = 0.03470343230079075\n",
            "Cost after 298319 iterations : Training Loss =  0.02807979718675984; Validation Loss = 0.034703423888108154\n",
            "Cost after 298320 iterations : Training Loss =  0.028079787382249873; Validation Loss = 0.0347034154755103\n",
            "Cost after 298321 iterations : Training Loss =  0.028079777577851947; Validation Loss = 0.03470340706299675\n",
            "Cost after 298322 iterations : Training Loss =  0.028079767773566206; Validation Loss = 0.0347033986505675\n",
            "Cost after 298323 iterations : Training Loss =  0.028079757969392594; Validation Loss = 0.03470339023822297\n",
            "Cost after 298324 iterations : Training Loss =  0.028079748165331212; Validation Loss = 0.034703381825962776\n",
            "Cost after 298325 iterations : Training Loss =  0.02807973836138192; Validation Loss = 0.0347033734137871\n",
            "Cost after 298326 iterations : Training Loss =  0.028079728557544897; Validation Loss = 0.03470336500169557\n",
            "Cost after 298327 iterations : Training Loss =  0.028079718753819913; Validation Loss = 0.0347033565896888\n",
            "Cost after 298328 iterations : Training Loss =  0.028079708950207095; Validation Loss = 0.034703348177766505\n",
            "Cost after 298329 iterations : Training Loss =  0.02807969914670637; Validation Loss = 0.03470333976592849\n",
            "Cost after 298330 iterations : Training Loss =  0.0280796893433178; Validation Loss = 0.0347033313541752\n",
            "Cost after 298331 iterations : Training Loss =  0.028079679540041404; Validation Loss = 0.034703322942506255\n",
            "Cost after 298332 iterations : Training Loss =  0.028079669736877158; Validation Loss = 0.03470331453092183\n",
            "Cost after 298333 iterations : Training Loss =  0.028079659933825052; Validation Loss = 0.03470330611942157\n",
            "Cost after 298334 iterations : Training Loss =  0.028079650130884953; Validation Loss = 0.03470329770800583\n",
            "Cost after 298335 iterations : Training Loss =  0.02807964032805716; Validation Loss = 0.03470328929667431\n",
            "Cost after 298336 iterations : Training Loss =  0.028079630525341407; Validation Loss = 0.03470328088542757\n",
            "Cost after 298337 iterations : Training Loss =  0.028079620722737786; Validation Loss = 0.0347032724742651\n",
            "Cost after 298338 iterations : Training Loss =  0.028079610920246398; Validation Loss = 0.03470326406318721\n",
            "Cost after 298339 iterations : Training Loss =  0.028079601117866917; Validation Loss = 0.03470325565219354\n",
            "Cost after 298340 iterations : Training Loss =  0.02807959131559965; Validation Loss = 0.03470324724128471\n",
            "Cost after 298341 iterations : Training Loss =  0.028079581513444585; Validation Loss = 0.03470323883045995\n",
            "Cost after 298342 iterations : Training Loss =  0.028079571711401602; Validation Loss = 0.03470323041971995\n",
            "Cost after 298343 iterations : Training Loss =  0.028079561909470724; Validation Loss = 0.034703222009064304\n",
            "Cost after 298344 iterations : Training Loss =  0.02807955210765199; Validation Loss = 0.03470321359849291\n",
            "Cost after 298345 iterations : Training Loss =  0.028079542305945386; Validation Loss = 0.03470320518800617\n",
            "Cost after 298346 iterations : Training Loss =  0.028079532504350843; Validation Loss = 0.034703196777603626\n",
            "Cost after 298347 iterations : Training Loss =  0.02807952270286859; Validation Loss = 0.03470318836728573\n",
            "Cost after 298348 iterations : Training Loss =  0.028079512901498238; Validation Loss = 0.034703179957052244\n",
            "Cost after 298349 iterations : Training Loss =  0.02807950310023998; Validation Loss = 0.03470317154690298\n",
            "Cost after 298350 iterations : Training Loss =  0.028079493299093986; Validation Loss = 0.03470316313683824\n",
            "Cost after 298351 iterations : Training Loss =  0.028079483498060075; Validation Loss = 0.03470315472685791\n",
            "Cost after 298352 iterations : Training Loss =  0.028079473697138212; Validation Loss = 0.03470314631696199\n",
            "Cost after 298353 iterations : Training Loss =  0.028079463896328375; Validation Loss = 0.03470313790715049\n",
            "Cost after 298354 iterations : Training Loss =  0.028079454095630715; Validation Loss = 0.03470312949742358\n",
            "Cost after 298355 iterations : Training Loss =  0.028079444295045195; Validation Loss = 0.034703121087780925\n",
            "Cost after 298356 iterations : Training Loss =  0.02807943449457175; Validation Loss = 0.03470311267822269\n",
            "Cost after 298357 iterations : Training Loss =  0.028079424694210386; Validation Loss = 0.034703104268749244\n",
            "Cost after 298358 iterations : Training Loss =  0.02807941489396113; Validation Loss = 0.03470309585935965\n",
            "Cost after 298359 iterations : Training Loss =  0.02807940509382396; Validation Loss = 0.03470308745005507\n",
            "Cost after 298360 iterations : Training Loss =  0.028079395293798937; Validation Loss = 0.034703079040834584\n",
            "Cost after 298361 iterations : Training Loss =  0.02807938549388592; Validation Loss = 0.034703070631698474\n",
            "Cost after 298362 iterations : Training Loss =  0.028079375694085074; Validation Loss = 0.03470306222264685\n",
            "Cost after 298363 iterations : Training Loss =  0.028079365894396215; Validation Loss = 0.0347030538136794\n",
            "Cost after 298364 iterations : Training Loss =  0.028079356094819482; Validation Loss = 0.03470304540479644\n",
            "Cost after 298365 iterations : Training Loss =  0.028079346295354874; Validation Loss = 0.03470303699599808\n",
            "Cost after 298366 iterations : Training Loss =  0.02807933649600234; Validation Loss = 0.03470302858728413\n",
            "Cost after 298367 iterations : Training Loss =  0.028079326696761872; Validation Loss = 0.03470302017865443\n",
            "Cost after 298368 iterations : Training Loss =  0.02807931689763346; Validation Loss = 0.034703011770109086\n",
            "Cost after 298369 iterations : Training Loss =  0.028079307098617216; Validation Loss = 0.03470300336164843\n",
            "Cost after 298370 iterations : Training Loss =  0.028079297299712955; Validation Loss = 0.03470299495327219\n",
            "Cost after 298371 iterations : Training Loss =  0.028079287500920824; Validation Loss = 0.03470298654498026\n",
            "Cost after 298372 iterations : Training Loss =  0.02807927770224074; Validation Loss = 0.03470297813677267\n",
            "Cost after 298373 iterations : Training Loss =  0.028079267903672687; Validation Loss = 0.03470296972864963\n",
            "Cost after 298374 iterations : Training Loss =  0.028079258105216845; Validation Loss = 0.03470296132061092\n",
            "Cost after 298375 iterations : Training Loss =  0.02807924830687294; Validation Loss = 0.034702952912656466\n",
            "Cost after 298376 iterations : Training Loss =  0.02807923850864117; Validation Loss = 0.034702944504786344\n",
            "Cost after 298377 iterations : Training Loss =  0.028079228710521483; Validation Loss = 0.034702936097000724\n",
            "Cost after 298378 iterations : Training Loss =  0.028079218912513717; Validation Loss = 0.034702927689299605\n",
            "Cost after 298379 iterations : Training Loss =  0.028079209114618198; Validation Loss = 0.034702919281682953\n",
            "Cost after 298380 iterations : Training Loss =  0.028079199316834537; Validation Loss = 0.03470291087415048\n",
            "Cost after 298381 iterations : Training Loss =  0.028079189519162967; Validation Loss = 0.034702902466702476\n",
            "Cost after 298382 iterations : Training Loss =  0.0280791797216037; Validation Loss = 0.03470289405933922\n",
            "Cost after 298383 iterations : Training Loss =  0.02807916992415625; Validation Loss = 0.034702885652059826\n",
            "Cost after 298384 iterations : Training Loss =  0.028079160126820952; Validation Loss = 0.034702877244864754\n",
            "Cost after 298385 iterations : Training Loss =  0.028079150329597735; Validation Loss = 0.03470286883775422\n",
            "Cost after 298386 iterations : Training Loss =  0.02807914053248648; Validation Loss = 0.03470286043072819\n",
            "Cost after 298387 iterations : Training Loss =  0.028079130735487384; Validation Loss = 0.034702852023786486\n",
            "Cost after 298388 iterations : Training Loss =  0.028079120938600172; Validation Loss = 0.03470284361692905\n",
            "Cost after 298389 iterations : Training Loss =  0.02807911114182517; Validation Loss = 0.03470283521015639\n",
            "Cost after 298390 iterations : Training Loss =  0.028079101345162102; Validation Loss = 0.03470282680346786\n",
            "Cost after 298391 iterations : Training Loss =  0.028079091548611167; Validation Loss = 0.034702818396863754\n",
            "Cost after 298392 iterations : Training Loss =  0.02807908175217225; Validation Loss = 0.03470280999034405\n",
            "Cost after 298393 iterations : Training Loss =  0.02807907195584528; Validation Loss = 0.034702801583908614\n",
            "Cost after 298394 iterations : Training Loss =  0.02807906215963049; Validation Loss = 0.03470279317755767\n",
            "Cost after 298395 iterations : Training Loss =  0.028079052363527673; Validation Loss = 0.03470278477129111\n",
            "Cost after 298396 iterations : Training Loss =  0.028079042567536884; Validation Loss = 0.0347027763651088\n",
            "Cost after 298397 iterations : Training Loss =  0.028079032771658078; Validation Loss = 0.03470276795901067\n",
            "Cost after 298398 iterations : Training Loss =  0.028079022975891453; Validation Loss = 0.03470275955299736\n",
            "Cost after 298399 iterations : Training Loss =  0.02807901318023674; Validation Loss = 0.03470275114706829\n",
            "Cost after 298400 iterations : Training Loss =  0.02807900338469403; Validation Loss = 0.034702742741223536\n",
            "Cost after 298401 iterations : Training Loss =  0.02807899358926351; Validation Loss = 0.034702734335463246\n",
            "Cost after 298402 iterations : Training Loss =  0.028078983793944863; Validation Loss = 0.03470272592978725\n",
            "Cost after 298403 iterations : Training Loss =  0.028078973998738203; Validation Loss = 0.03470271752419544\n",
            "Cost after 298404 iterations : Training Loss =  0.02807896420364375; Validation Loss = 0.034702709118688294\n",
            "Cost after 298405 iterations : Training Loss =  0.028078954408661178; Validation Loss = 0.03470270071326528\n",
            "Cost after 298406 iterations : Training Loss =  0.028078944613790617; Validation Loss = 0.03470269230792651\n",
            "Cost after 298407 iterations : Training Loss =  0.028078934819032102; Validation Loss = 0.034702683902672206\n",
            "Cost after 298408 iterations : Training Loss =  0.028078925024385627; Validation Loss = 0.0347026754975022\n",
            "Cost after 298409 iterations : Training Loss =  0.028078915229851172; Validation Loss = 0.03470266709241663\n",
            "Cost after 298410 iterations : Training Loss =  0.028078905435428705; Validation Loss = 0.03470265868741548\n",
            "Cost after 298411 iterations : Training Loss =  0.02807889564111825; Validation Loss = 0.03470265028249887\n",
            "Cost after 298412 iterations : Training Loss =  0.0280788858469198; Validation Loss = 0.0347026418776668\n",
            "Cost after 298413 iterations : Training Loss =  0.028078876052833428; Validation Loss = 0.03470263347291856\n",
            "Cost after 298414 iterations : Training Loss =  0.028078866258858946; Validation Loss = 0.03470262506825468\n",
            "Cost after 298415 iterations : Training Loss =  0.028078856464996634; Validation Loss = 0.03470261666367544\n",
            "Cost after 298416 iterations : Training Loss =  0.028078846671246122; Validation Loss = 0.03470260825918038\n",
            "Cost after 298417 iterations : Training Loss =  0.028078836877607812; Validation Loss = 0.034702599854769645\n",
            "Cost after 298418 iterations : Training Loss =  0.028078827084081368; Validation Loss = 0.03470259145044334\n",
            "Cost after 298419 iterations : Training Loss =  0.028078817290666924; Validation Loss = 0.03470258304620146\n",
            "Cost after 298420 iterations : Training Loss =  0.028078807497364543; Validation Loss = 0.0347025746420437\n",
            "Cost after 298421 iterations : Training Loss =  0.02807879770417418; Validation Loss = 0.03470256623797041\n",
            "Cost after 298422 iterations : Training Loss =  0.02807878791109573; Validation Loss = 0.03470255783398147\n",
            "Cost after 298423 iterations : Training Loss =  0.028078778118129283; Validation Loss = 0.03470254943007707\n",
            "Cost after 298424 iterations : Training Loss =  0.02807876832527492; Validation Loss = 0.03470254102625673\n",
            "Cost after 298425 iterations : Training Loss =  0.028078758532532368; Validation Loss = 0.034702532622521085\n",
            "Cost after 298426 iterations : Training Loss =  0.02807874873990194; Validation Loss = 0.03470252421886934\n",
            "Cost after 298427 iterations : Training Loss =  0.028078738947383428; Validation Loss = 0.0347025158153019\n",
            "Cost after 298428 iterations : Training Loss =  0.028078729154977045; Validation Loss = 0.03470250741181923\n",
            "Cost after 298429 iterations : Training Loss =  0.02807871936268258; Validation Loss = 0.03470249900842084\n",
            "Cost after 298430 iterations : Training Loss =  0.028078709570500005; Validation Loss = 0.03470249060510693\n",
            "Cost after 298431 iterations : Training Loss =  0.028078699778429347; Validation Loss = 0.03470248220187695\n",
            "Cost after 298432 iterations : Training Loss =  0.02807868998647093; Validation Loss = 0.034702473798731516\n",
            "Cost after 298433 iterations : Training Loss =  0.028078680194624245; Validation Loss = 0.034702465395670186\n",
            "Cost after 298434 iterations : Training Loss =  0.028078670402889673; Validation Loss = 0.034702456992693385\n",
            "Cost after 298435 iterations : Training Loss =  0.02807866061126708; Validation Loss = 0.034702448589800976\n",
            "Cost after 298436 iterations : Training Loss =  0.028078650819756447; Validation Loss = 0.034702440186992575\n",
            "Cost after 298437 iterations : Training Loss =  0.028078641028357713; Validation Loss = 0.034702431784268614\n",
            "Cost after 298438 iterations : Training Loss =  0.02807863123707098; Validation Loss = 0.034702423381628995\n",
            "Cost after 298439 iterations : Training Loss =  0.0280786214458962; Validation Loss = 0.03470241497907383\n",
            "Cost after 298440 iterations : Training Loss =  0.02807861165483338; Validation Loss = 0.03470240657660304\n",
            "Cost after 298441 iterations : Training Loss =  0.02807860186388255; Validation Loss = 0.034702398174216754\n",
            "Cost after 298442 iterations : Training Loss =  0.02807859207304369; Validation Loss = 0.034702389771914664\n",
            "Cost after 298443 iterations : Training Loss =  0.028078582282316775; Validation Loss = 0.03470238136969668\n",
            "Cost after 298444 iterations : Training Loss =  0.02807857249170179; Validation Loss = 0.034702372967563226\n",
            "Cost after 298445 iterations : Training Loss =  0.028078562701198854; Validation Loss = 0.03470236456551377\n",
            "Cost after 298446 iterations : Training Loss =  0.028078552910807887; Validation Loss = 0.03470235616354887\n",
            "Cost after 298447 iterations : Training Loss =  0.02807854312052877; Validation Loss = 0.03470234776166839\n",
            "Cost after 298448 iterations : Training Loss =  0.02807853333036163; Validation Loss = 0.03470233935987185\n",
            "Cost after 298449 iterations : Training Loss =  0.02807852354030638; Validation Loss = 0.03470233095815951\n",
            "Cost after 298450 iterations : Training Loss =  0.02807851375036323; Validation Loss = 0.03470232255653212\n",
            "Cost after 298451 iterations : Training Loss =  0.028078503960531895; Validation Loss = 0.034702314154988395\n",
            "Cost after 298452 iterations : Training Loss =  0.028078494170812617; Validation Loss = 0.034702305753529464\n",
            "Cost after 298453 iterations : Training Loss =  0.02807848438120519; Validation Loss = 0.03470229735215478\n",
            "Cost after 298454 iterations : Training Loss =  0.028078474591709788; Validation Loss = 0.03470228895086438\n",
            "Cost after 298455 iterations : Training Loss =  0.0280784648023262; Validation Loss = 0.034702280549658036\n",
            "Cost after 298456 iterations : Training Loss =  0.02807845501305482; Validation Loss = 0.03470227214853617\n",
            "Cost after 298457 iterations : Training Loss =  0.02807844522389508; Validation Loss = 0.0347022637474985\n",
            "Cost after 298458 iterations : Training Loss =  0.028078435434847445; Validation Loss = 0.03470225534654516\n",
            "Cost after 298459 iterations : Training Loss =  0.02807842564591168; Validation Loss = 0.03470224694567635\n",
            "Cost after 298460 iterations : Training Loss =  0.028078415857087907; Validation Loss = 0.034702238544891725\n",
            "Cost after 298461 iterations : Training Loss =  0.02807840606837594; Validation Loss = 0.03470223014419138\n",
            "Cost after 298462 iterations : Training Loss =  0.02807839627977604; Validation Loss = 0.034702221743575276\n",
            "Cost after 298463 iterations : Training Loss =  0.028078386491287972; Validation Loss = 0.0347022133430433\n",
            "Cost after 298464 iterations : Training Loss =  0.028078376702911894; Validation Loss = 0.03470220494259589\n",
            "Cost after 298465 iterations : Training Loss =  0.02807836691464771; Validation Loss = 0.03470219654223263\n",
            "Cost after 298466 iterations : Training Loss =  0.028078357126495597; Validation Loss = 0.034702188141953895\n",
            "Cost after 298467 iterations : Training Loss =  0.028078347338455156; Validation Loss = 0.03470217974175955\n",
            "Cost after 298468 iterations : Training Loss =  0.028078337550526723; Validation Loss = 0.03470217134164903\n",
            "Cost after 298469 iterations : Training Loss =  0.028078327762710298; Validation Loss = 0.03470216294162274\n",
            "Cost after 298470 iterations : Training Loss =  0.028078317975005686; Validation Loss = 0.03470215454168137\n",
            "Cost after 298471 iterations : Training Loss =  0.0280783081874131; Validation Loss = 0.034702146141823924\n",
            "Cost after 298472 iterations : Training Loss =  0.028078298399932304; Validation Loss = 0.03470213774205097\n",
            "Cost after 298473 iterations : Training Loss =  0.028078288612563587; Validation Loss = 0.03470212934236217\n",
            "Cost after 298474 iterations : Training Loss =  0.02807827882530654; Validation Loss = 0.034702120942757715\n",
            "Cost after 298475 iterations : Training Loss =  0.028078269038161574; Validation Loss = 0.03470211254323746\n",
            "Cost after 298476 iterations : Training Loss =  0.028078259251128515; Validation Loss = 0.03470210414380164\n",
            "Cost after 298477 iterations : Training Loss =  0.028078249464207367; Validation Loss = 0.03470209574444984\n",
            "Cost after 298478 iterations : Training Loss =  0.02807823967739807; Validation Loss = 0.03470208734518229\n",
            "Cost after 298479 iterations : Training Loss =  0.028078229890700763; Validation Loss = 0.034702078945999255\n",
            "Cost after 298480 iterations : Training Loss =  0.028078220104115197; Validation Loss = 0.03470207054690019\n",
            "Cost after 298481 iterations : Training Loss =  0.028078210317641683; Validation Loss = 0.03470206214788566\n",
            "Cost after 298482 iterations : Training Loss =  0.028078200531279993; Validation Loss = 0.03470205374895516\n",
            "Cost after 298483 iterations : Training Loss =  0.0280781907450302; Validation Loss = 0.034702045350109005\n",
            "Cost after 298484 iterations : Training Loss =  0.02807818095889227; Validation Loss = 0.03470203695134719\n",
            "Cost after 298485 iterations : Training Loss =  0.028078171172866373; Validation Loss = 0.03470202855266971\n",
            "Cost after 298486 iterations : Training Loss =  0.028078161386952256; Validation Loss = 0.03470202015407672\n",
            "Cost after 298487 iterations : Training Loss =  0.028078151601150028; Validation Loss = 0.034702011755567745\n",
            "Cost after 298488 iterations : Training Loss =  0.028078141815459704; Validation Loss = 0.03470200335714299\n",
            "Cost after 298489 iterations : Training Loss =  0.028078132029881263; Validation Loss = 0.03470199495880277\n",
            "Cost after 298490 iterations : Training Loss =  0.028078122244414833; Validation Loss = 0.034701986560546454\n",
            "Cost after 298491 iterations : Training Loss =  0.028078112459060167; Validation Loss = 0.03470197816237466\n",
            "Cost after 298492 iterations : Training Loss =  0.028078102673817225; Validation Loss = 0.03470196976428715\n",
            "Cost after 298493 iterations : Training Loss =  0.028078092888686425; Validation Loss = 0.03470196136628362\n",
            "Cost after 298494 iterations : Training Loss =  0.0280780831036674; Validation Loss = 0.03470195296836443\n",
            "Cost after 298495 iterations : Training Loss =  0.028078073318760225; Validation Loss = 0.034701944570529525\n",
            "Cost after 298496 iterations : Training Loss =  0.028078063533964994; Validation Loss = 0.034701936172779016\n",
            "Cost after 298497 iterations : Training Loss =  0.02807805374928158; Validation Loss = 0.034701927775112745\n",
            "Cost after 298498 iterations : Training Loss =  0.028078043964710055; Validation Loss = 0.034701919377530595\n",
            "Cost after 298499 iterations : Training Loss =  0.028078034180250552; Validation Loss = 0.03470191098003256\n",
            "Cost after 298500 iterations : Training Loss =  0.028078024395902703; Validation Loss = 0.03470190258261923\n",
            "Cost after 298501 iterations : Training Loss =  0.02807801461166681; Validation Loss = 0.03470189418529002\n",
            "Cost after 298502 iterations : Training Loss =  0.028078004827542698; Validation Loss = 0.0347018857880449\n",
            "Cost after 298503 iterations : Training Loss =  0.028077995043530698; Validation Loss = 0.034701877390884026\n",
            "Cost after 298504 iterations : Training Loss =  0.028077985259630355; Validation Loss = 0.034701868993807616\n",
            "Cost after 298505 iterations : Training Loss =  0.028077975475841892; Validation Loss = 0.034701860596815445\n",
            "Cost after 298506 iterations : Training Loss =  0.02807796569216533; Validation Loss = 0.034701852199907506\n",
            "Cost after 298507 iterations : Training Loss =  0.028077955908600686; Validation Loss = 0.034701843803083485\n",
            "Cost after 298508 iterations : Training Loss =  0.028077946125147753; Validation Loss = 0.03470183540634392\n",
            "Cost after 298509 iterations : Training Loss =  0.028077936341806723; Validation Loss = 0.034701827009688706\n",
            "Cost after 298510 iterations : Training Loss =  0.02807792655857758; Validation Loss = 0.034701818613117594\n",
            "Cost after 298511 iterations : Training Loss =  0.028077916775460277; Validation Loss = 0.03470181021663078\n",
            "Cost after 298512 iterations : Training Loss =  0.02807790699245491; Validation Loss = 0.03470180182022812\n",
            "Cost after 298513 iterations : Training Loss =  0.028077897209561303; Validation Loss = 0.034701793423909603\n",
            "Cost after 298514 iterations : Training Loss =  0.028077887426779507; Validation Loss = 0.03470178502767567\n",
            "Cost after 298515 iterations : Training Loss =  0.0280778776441096; Validation Loss = 0.03470177663152589\n",
            "Cost after 298516 iterations : Training Loss =  0.028077867861551503; Validation Loss = 0.0347017682354602\n",
            "Cost after 298517 iterations : Training Loss =  0.028077858079105327; Validation Loss = 0.034701759839478846\n",
            "Cost after 298518 iterations : Training Loss =  0.028077848296770864; Validation Loss = 0.0347017514435815\n",
            "Cost after 298519 iterations : Training Loss =  0.028077838514548443; Validation Loss = 0.0347017430477688\n",
            "Cost after 298520 iterations : Training Loss =  0.028077828732437676; Validation Loss = 0.034701734652040205\n",
            "Cost after 298521 iterations : Training Loss =  0.028077818950438812; Validation Loss = 0.034701726256395914\n",
            "Cost after 298522 iterations : Training Loss =  0.028077809168551814; Validation Loss = 0.03470171786083542\n",
            "Cost after 298523 iterations : Training Loss =  0.028077799386776615; Validation Loss = 0.03470170946535947\n",
            "Cost after 298524 iterations : Training Loss =  0.02807778960511334; Validation Loss = 0.03470170106996744\n",
            "Cost after 298525 iterations : Training Loss =  0.028077779823561858; Validation Loss = 0.03470169267465987\n",
            "Cost after 298526 iterations : Training Loss =  0.02807777004212211; Validation Loss = 0.034701684279436436\n",
            "Cost after 298527 iterations : Training Loss =  0.028077760260794213; Validation Loss = 0.03470167588429753\n",
            "Cost after 298528 iterations : Training Loss =  0.028077750479578074; Validation Loss = 0.03470166748924258\n",
            "Cost after 298529 iterations : Training Loss =  0.028077740698473963; Validation Loss = 0.03470165909427194\n",
            "Cost after 298530 iterations : Training Loss =  0.028077730917481586; Validation Loss = 0.034701650699385626\n",
            "Cost after 298531 iterations : Training Loss =  0.02807772113660102; Validation Loss = 0.034701642304583384\n",
            "Cost after 298532 iterations : Training Loss =  0.028077711355832282; Validation Loss = 0.03470163390986543\n",
            "Cost after 298533 iterations : Training Loss =  0.02807770157517524; Validation Loss = 0.03470162551523157\n",
            "Cost after 298534 iterations : Training Loss =  0.028077691794630138; Validation Loss = 0.034701617120682184\n",
            "Cost after 298535 iterations : Training Loss =  0.028077682014196755; Validation Loss = 0.0347016087262168\n",
            "Cost after 298536 iterations : Training Loss =  0.028077672233875358; Validation Loss = 0.03470160033183561\n",
            "Cost after 298537 iterations : Training Loss =  0.028077662453665716; Validation Loss = 0.03470159193753895\n",
            "Cost after 298538 iterations : Training Loss =  0.02807765267356784; Validation Loss = 0.03470158354332637\n",
            "Cost after 298539 iterations : Training Loss =  0.028077642893581785; Validation Loss = 0.03470157514919808\n",
            "Cost after 298540 iterations : Training Loss =  0.02807763311370751; Validation Loss = 0.03470156675515387\n",
            "Cost after 298541 iterations : Training Loss =  0.028077623333945; Validation Loss = 0.034701558361193915\n",
            "Cost after 298542 iterations : Training Loss =  0.028077613554294344; Validation Loss = 0.03470154996731785\n",
            "Cost after 298543 iterations : Training Loss =  0.028077603774755468; Validation Loss = 0.03470154157352649\n",
            "Cost after 298544 iterations : Training Loss =  0.028077593995328436; Validation Loss = 0.034701533179818805\n",
            "Cost after 298545 iterations : Training Loss =  0.028077584216013318; Validation Loss = 0.0347015247861958\n",
            "Cost after 298546 iterations : Training Loss =  0.028077574436809773; Validation Loss = 0.034701516392656395\n",
            "Cost after 298547 iterations : Training Loss =  0.028077564657718122; Validation Loss = 0.03470150799920185\n",
            "Cost after 298548 iterations : Training Loss =  0.028077554878738243; Validation Loss = 0.03470149960583124\n",
            "Cost after 298549 iterations : Training Loss =  0.028077545099870097; Validation Loss = 0.03470149121254485\n",
            "Cost after 298550 iterations : Training Loss =  0.028077535321114008; Validation Loss = 0.03470148281934229\n",
            "Cost after 298551 iterations : Training Loss =  0.028077525542469464; Validation Loss = 0.034701474426224295\n",
            "Cost after 298552 iterations : Training Loss =  0.028077515763936575; Validation Loss = 0.034701466033190416\n",
            "Cost after 298553 iterations : Training Loss =  0.028077505985515793; Validation Loss = 0.034701457640240616\n",
            "Cost after 298554 iterations : Training Loss =  0.028077496207206676; Validation Loss = 0.03470144924737513\n",
            "Cost after 298555 iterations : Training Loss =  0.02807748642900936; Validation Loss = 0.03470144085459404\n",
            "Cost after 298556 iterations : Training Loss =  0.028077476650923767; Validation Loss = 0.03470143246189694\n",
            "Cost after 298557 iterations : Training Loss =  0.02807746687295004; Validation Loss = 0.0347014240692841\n",
            "Cost after 298558 iterations : Training Loss =  0.028077457095088004; Validation Loss = 0.034701415676755304\n",
            "Cost after 298559 iterations : Training Loss =  0.0280774473173377; Validation Loss = 0.03470140728431079\n",
            "Cost after 298560 iterations : Training Loss =  0.028077437539699233; Validation Loss = 0.034701398891950674\n",
            "Cost after 298561 iterations : Training Loss =  0.028077427762172535; Validation Loss = 0.034701390499674245\n",
            "Cost after 298562 iterations : Training Loss =  0.028077417984757647; Validation Loss = 0.034701382107482304\n",
            "Cost after 298563 iterations : Training Loss =  0.028077408207454586; Validation Loss = 0.03470137371537434\n",
            "Cost after 298564 iterations : Training Loss =  0.02807739843026321; Validation Loss = 0.03470136532335086\n",
            "Cost after 298565 iterations : Training Loss =  0.028077388653183585; Validation Loss = 0.03470135693141143\n",
            "Cost after 298566 iterations : Training Loss =  0.028077378876215683; Validation Loss = 0.034701348539556524\n",
            "Cost after 298567 iterations : Training Loss =  0.028077369099359664; Validation Loss = 0.03470134014778532\n",
            "Cost after 298568 iterations : Training Loss =  0.02807735932261531; Validation Loss = 0.03470133175609833\n",
            "Cost after 298569 iterations : Training Loss =  0.028077349545982757; Validation Loss = 0.03470132336449543\n",
            "Cost after 298570 iterations : Training Loss =  0.028077339769461994; Validation Loss = 0.034701314972977244\n",
            "Cost after 298571 iterations : Training Loss =  0.028077329993052926; Validation Loss = 0.034701306581542656\n",
            "Cost after 298572 iterations : Training Loss =  0.02807732021675567; Validation Loss = 0.034701298190192346\n",
            "Cost after 298573 iterations : Training Loss =  0.028077310440570137; Validation Loss = 0.03470128979892657\n",
            "Cost after 298574 iterations : Training Loss =  0.028077300664496395; Validation Loss = 0.0347012814077447\n",
            "Cost after 298575 iterations : Training Loss =  0.028077290888534303; Validation Loss = 0.034701273016647056\n",
            "Cost after 298576 iterations : Training Loss =  0.02807728111268403; Validation Loss = 0.034701264625633436\n",
            "Cost after 298577 iterations : Training Loss =  0.028077271336945563; Validation Loss = 0.03470125623470413\n",
            "Cost after 298578 iterations : Training Loss =  0.028077261561318918; Validation Loss = 0.03470124784385908\n",
            "Cost after 298579 iterations : Training Loss =  0.028077251785803715; Validation Loss = 0.034701239453097936\n",
            "Cost after 298580 iterations : Training Loss =  0.028077242010400436; Validation Loss = 0.0347012310624211\n",
            "Cost after 298581 iterations : Training Loss =  0.028077232235108874; Validation Loss = 0.03470122267182857\n",
            "Cost after 298582 iterations : Training Loss =  0.028077222459929076; Validation Loss = 0.03470121428131997\n",
            "Cost after 298583 iterations : Training Loss =  0.028077212684861068; Validation Loss = 0.034701205890895724\n",
            "Cost after 298584 iterations : Training Loss =  0.028077202909904706; Validation Loss = 0.03470119750055565\n",
            "Cost after 298585 iterations : Training Loss =  0.028077193135060092; Validation Loss = 0.03470118911029969\n",
            "Cost after 298586 iterations : Training Loss =  0.02807718336032721; Validation Loss = 0.03470118072012771\n",
            "Cost after 298587 iterations : Training Loss =  0.02807717358570614; Validation Loss = 0.03470117233003985\n",
            "Cost after 298588 iterations : Training Loss =  0.028077163811196734; Validation Loss = 0.03470116394003643\n",
            "Cost after 298589 iterations : Training Loss =  0.028077154036799105; Validation Loss = 0.034701155550117053\n",
            "Cost after 298590 iterations : Training Loss =  0.028077144262513083; Validation Loss = 0.03470114716028174\n",
            "Cost after 298591 iterations : Training Loss =  0.028077134488338915; Validation Loss = 0.034701138770530764\n",
            "Cost after 298592 iterations : Training Loss =  0.028077124714276397; Validation Loss = 0.034701130380863775\n",
            "Cost after 298593 iterations : Training Loss =  0.028077114940325634; Validation Loss = 0.03470112199128121\n",
            "Cost after 298594 iterations : Training Loss =  0.02807710516648661; Validation Loss = 0.03470111360178255\n",
            "Cost after 298595 iterations : Training Loss =  0.02807709539275928; Validation Loss = 0.03470110521236834\n",
            "Cost after 298596 iterations : Training Loss =  0.0280770856191437; Validation Loss = 0.0347010968230378\n",
            "Cost after 298597 iterations : Training Loss =  0.028077075845639704; Validation Loss = 0.03470108843379144\n",
            "Cost after 298598 iterations : Training Loss =  0.028077066072247536; Validation Loss = 0.03470108004462953\n",
            "Cost after 298599 iterations : Training Loss =  0.028077056298967125; Validation Loss = 0.03470107165555142\n",
            "Cost after 298600 iterations : Training Loss =  0.028077046525798344; Validation Loss = 0.0347010632665574\n",
            "Cost after 298601 iterations : Training Loss =  0.028077036752741282; Validation Loss = 0.03470105487764775\n",
            "Cost after 298602 iterations : Training Loss =  0.028077026979795985; Validation Loss = 0.0347010464888223\n",
            "Cost after 298603 iterations : Training Loss =  0.028077017206962346; Validation Loss = 0.03470103810008078\n",
            "Cost after 298604 iterations : Training Loss =  0.028077007434240468; Validation Loss = 0.03470102971142345\n",
            "Cost after 298605 iterations : Training Loss =  0.02807699766163019; Validation Loss = 0.034701021322850294\n",
            "Cost after 298606 iterations : Training Loss =  0.02807698788913163; Validation Loss = 0.034701012934361466\n",
            "Cost after 298607 iterations : Training Loss =  0.028076978116744825; Validation Loss = 0.034701004545956475\n",
            "Cost after 298608 iterations : Training Loss =  0.028076968344469745; Validation Loss = 0.03470099615763577\n",
            "Cost after 298609 iterations : Training Loss =  0.028076958572306344; Validation Loss = 0.03470098776939918\n",
            "Cost after 298610 iterations : Training Loss =  0.02807694880025452; Validation Loss = 0.03470097938124652\n",
            "Cost after 298611 iterations : Training Loss =  0.028076939028314495; Validation Loss = 0.03470097099317807\n",
            "Cost after 298612 iterations : Training Loss =  0.02807692925648624; Validation Loss = 0.0347009626051941\n",
            "Cost after 298613 iterations : Training Loss =  0.028076919484769532; Validation Loss = 0.03470095421729392\n",
            "Cost after 298614 iterations : Training Loss =  0.028076909713164535; Validation Loss = 0.03470094582947814\n",
            "Cost after 298615 iterations : Training Loss =  0.028076899941671254; Validation Loss = 0.03470093744174613\n",
            "Cost after 298616 iterations : Training Loss =  0.028076890170289685; Validation Loss = 0.03470092905409827\n",
            "Cost after 298617 iterations : Training Loss =  0.028076880399019816; Validation Loss = 0.034700920666534955\n",
            "Cost after 298618 iterations : Training Loss =  0.02807687062786151; Validation Loss = 0.034700912279055335\n",
            "Cost after 298619 iterations : Training Loss =  0.028076860856815048; Validation Loss = 0.03470090389165987\n",
            "Cost after 298620 iterations : Training Loss =  0.028076851085880185; Validation Loss = 0.03470089550434852\n",
            "Cost after 298621 iterations : Training Loss =  0.02807684131505698; Validation Loss = 0.034700887117121576\n",
            "Cost after 298622 iterations : Training Loss =  0.028076831544345572; Validation Loss = 0.03470087872997828\n",
            "Cost after 298623 iterations : Training Loss =  0.028076821773745767; Validation Loss = 0.03470087034291917\n",
            "Cost after 298624 iterations : Training Loss =  0.028076812003257588; Validation Loss = 0.03470086195594426\n",
            "Cost after 298625 iterations : Training Loss =  0.02807680223288115; Validation Loss = 0.03470085356905377\n",
            "Cost after 298626 iterations : Training Loss =  0.02807679246261625; Validation Loss = 0.03470084518224696\n",
            "Cost after 298627 iterations : Training Loss =  0.028076782692463113; Validation Loss = 0.034700836795524846\n",
            "Cost after 298628 iterations : Training Loss =  0.028076772922421656; Validation Loss = 0.03470082840888627\n",
            "Cost after 298629 iterations : Training Loss =  0.02807676315249187; Validation Loss = 0.03470082002233213\n",
            "Cost after 298630 iterations : Training Loss =  0.02807675338267361; Validation Loss = 0.034700811635861806\n",
            "Cost after 298631 iterations : Training Loss =  0.028076743612967206; Validation Loss = 0.03470080324947567\n",
            "Cost after 298632 iterations : Training Loss =  0.02807673384337237; Validation Loss = 0.034700794863174005\n",
            "Cost after 298633 iterations : Training Loss =  0.028076724073889225; Validation Loss = 0.0347007864769561\n",
            "Cost after 298634 iterations : Training Loss =  0.02807671430451772; Validation Loss = 0.03470077809082263\n",
            "Cost after 298635 iterations : Training Loss =  0.028076704535257853; Validation Loss = 0.03470076970477325\n",
            "Cost after 298636 iterations : Training Loss =  0.02807669476610971; Validation Loss = 0.034700761318807775\n",
            "Cost after 298637 iterations : Training Loss =  0.028076684997073113; Validation Loss = 0.03470075293292614\n",
            "Cost after 298638 iterations : Training Loss =  0.028076675228148266; Validation Loss = 0.034700744547129\n",
            "Cost after 298639 iterations : Training Loss =  0.02807666545933507; Validation Loss = 0.03470073616141556\n",
            "Cost after 298640 iterations : Training Loss =  0.028076655690633362; Validation Loss = 0.03470072777578661\n",
            "Cost after 298641 iterations : Training Loss =  0.02807664592204337; Validation Loss = 0.034700719390241525\n",
            "Cost after 298642 iterations : Training Loss =  0.028076636153565135; Validation Loss = 0.034700711004780754\n",
            "Cost after 298643 iterations : Training Loss =  0.028076626385198474; Validation Loss = 0.03470070261940358\n",
            "Cost after 298644 iterations : Training Loss =  0.02807661661694354; Validation Loss = 0.03470069423411074\n",
            "Cost after 298645 iterations : Training Loss =  0.02807660684880014; Validation Loss = 0.034700685848902034\n",
            "Cost after 298646 iterations : Training Loss =  0.02807659708076834; Validation Loss = 0.034700677463777335\n",
            "Cost after 298647 iterations : Training Loss =  0.028076587312848234; Validation Loss = 0.03470066907873657\n",
            "Cost after 298648 iterations : Training Loss =  0.028076577545039765; Validation Loss = 0.03470066069378015\n",
            "Cost after 298649 iterations : Training Loss =  0.02807656777734284; Validation Loss = 0.034700652308907697\n",
            "Cost after 298650 iterations : Training Loss =  0.028076558009757818; Validation Loss = 0.03470064392411914\n",
            "Cost after 298651 iterations : Training Loss =  0.028076548242284192; Validation Loss = 0.03470063553941506\n",
            "Cost after 298652 iterations : Training Loss =  0.02807653847492227; Validation Loss = 0.03470062715479498\n",
            "Cost after 298653 iterations : Training Loss =  0.028076528707672008; Validation Loss = 0.034700618770259\n",
            "Cost after 298654 iterations : Training Loss =  0.028076518940533247; Validation Loss = 0.03470061038580695\n",
            "Cost after 298655 iterations : Training Loss =  0.028076509173506185; Validation Loss = 0.03470060200143908\n",
            "Cost after 298656 iterations : Training Loss =  0.028076499406590787; Validation Loss = 0.034700593617155134\n",
            "Cost after 298657 iterations : Training Loss =  0.028076489639786897; Validation Loss = 0.03470058523295537\n",
            "Cost after 298658 iterations : Training Loss =  0.0280764798730948; Validation Loss = 0.034700576848839794\n",
            "Cost after 298659 iterations : Training Loss =  0.02807647010651418; Validation Loss = 0.034700568464808165\n",
            "Cost after 298660 iterations : Training Loss =  0.028076460340045132; Validation Loss = 0.03470056008086068\n",
            "Cost after 298661 iterations : Training Loss =  0.02807645057368789; Validation Loss = 0.034700551696997296\n",
            "Cost after 298662 iterations : Training Loss =  0.028076440807442087; Validation Loss = 0.03470054331321769\n",
            "Cost after 298663 iterations : Training Loss =  0.02807643104130795; Validation Loss = 0.03470053492952258\n",
            "Cost after 298664 iterations : Training Loss =  0.0280764212752855; Validation Loss = 0.034700526545911305\n",
            "Cost after 298665 iterations : Training Loss =  0.02807641150937459; Validation Loss = 0.034700518162383856\n",
            "Cost after 298666 iterations : Training Loss =  0.028076401743575327; Validation Loss = 0.034700509778940784\n",
            "Cost after 298667 iterations : Training Loss =  0.028076391977887576; Validation Loss = 0.034700501395581874\n",
            "Cost after 298668 iterations : Training Loss =  0.02807638221231154; Validation Loss = 0.034700493012306814\n",
            "Cost after 298669 iterations : Training Loss =  0.028076372446847037; Validation Loss = 0.034700484629115874\n",
            "Cost after 298670 iterations : Training Loss =  0.028076362681494094; Validation Loss = 0.0347004762460091\n",
            "Cost after 298671 iterations : Training Loss =  0.028076352916252902; Validation Loss = 0.034700467862986285\n",
            "Cost after 298672 iterations : Training Loss =  0.0280763431511232; Validation Loss = 0.03470045948004761\n",
            "Cost after 298673 iterations : Training Loss =  0.028076333386105018; Validation Loss = 0.03470045109719262\n",
            "Cost after 298674 iterations : Training Loss =  0.02807632362119858; Validation Loss = 0.03470044271442202\n",
            "Cost after 298675 iterations : Training Loss =  0.02807631385640373; Validation Loss = 0.034700434331735565\n",
            "Cost after 298676 iterations : Training Loss =  0.028076304091720374; Validation Loss = 0.03470042594913305\n",
            "Cost after 298677 iterations : Training Loss =  0.02807629432714859; Validation Loss = 0.03470041756661461\n",
            "Cost after 298678 iterations : Training Loss =  0.028076284562688462; Validation Loss = 0.03470040918418022\n",
            "Cost after 298679 iterations : Training Loss =  0.028076274798339948; Validation Loss = 0.03470040080182973\n",
            "Cost after 298680 iterations : Training Loss =  0.028076265034103067; Validation Loss = 0.03470039241956352\n",
            "Cost after 298681 iterations : Training Loss =  0.028076255269977615; Validation Loss = 0.03470038403738114\n",
            "Cost after 298682 iterations : Training Loss =  0.028076245505963743; Validation Loss = 0.034700375655282785\n",
            "Cost after 298683 iterations : Training Loss =  0.02807623574206157; Validation Loss = 0.03470036727326861\n",
            "Cost after 298684 iterations : Training Loss =  0.02807622597827094; Validation Loss = 0.034700358891338476\n",
            "Cost after 298685 iterations : Training Loss =  0.028076216214591977; Validation Loss = 0.03470035050949248\n",
            "Cost after 298686 iterations : Training Loss =  0.02807620645102437; Validation Loss = 0.03470034212773043\n",
            "Cost after 298687 iterations : Training Loss =  0.02807619668756851; Validation Loss = 0.034700333746052206\n",
            "Cost after 298688 iterations : Training Loss =  0.028076186924224124; Validation Loss = 0.03470032536445822\n",
            "Cost after 298689 iterations : Training Loss =  0.02807617716099133; Validation Loss = 0.03470031698294822\n",
            "Cost after 298690 iterations : Training Loss =  0.02807616739787013; Validation Loss = 0.034700308601522636\n",
            "Cost after 298691 iterations : Training Loss =  0.028076157634860543; Validation Loss = 0.03470030022018073\n",
            "Cost after 298692 iterations : Training Loss =  0.028076147871962435; Validation Loss = 0.034700291838922946\n",
            "Cost after 298693 iterations : Training Loss =  0.028076138109175936; Validation Loss = 0.03470028345774885\n",
            "Cost after 298694 iterations : Training Loss =  0.028076128346500955; Validation Loss = 0.034700275076658994\n",
            "Cost after 298695 iterations : Training Loss =  0.028076118583937597; Validation Loss = 0.03470026669565328\n",
            "Cost after 298696 iterations : Training Loss =  0.028076108821485767; Validation Loss = 0.03470025831473127\n",
            "Cost after 298697 iterations : Training Loss =  0.028076099059145553; Validation Loss = 0.034700249933893826\n",
            "Cost after 298698 iterations : Training Loss =  0.028076089296916868; Validation Loss = 0.0347002415531401\n",
            "Cost after 298699 iterations : Training Loss =  0.028076079534799653; Validation Loss = 0.03470023317247048\n",
            "Cost after 298700 iterations : Training Loss =  0.028076069772794057; Validation Loss = 0.03470022479188472\n",
            "Cost after 298701 iterations : Training Loss =  0.02807606001090006; Validation Loss = 0.03470021641138296\n",
            "Cost after 298702 iterations : Training Loss =  0.028076050249117506; Validation Loss = 0.034700208030965435\n",
            "Cost after 298703 iterations : Training Loss =  0.02807604048744657; Validation Loss = 0.034700199650631876\n",
            "Cost after 298704 iterations : Training Loss =  0.02807603072588716; Validation Loss = 0.03470019127038231\n",
            "Cost after 298705 iterations : Training Loss =  0.028076020964439334; Validation Loss = 0.03470018289021691\n",
            "Cost after 298706 iterations : Training Loss =  0.02807601120310288; Validation Loss = 0.03470017451013549\n",
            "Cost after 298707 iterations : Training Loss =  0.028076001441878235; Validation Loss = 0.03470016613013785\n",
            "Cost after 298708 iterations : Training Loss =  0.02807599168076497; Validation Loss = 0.03470015775022448\n",
            "Cost after 298709 iterations : Training Loss =  0.028075981919763234; Validation Loss = 0.03470014937039494\n",
            "Cost after 298710 iterations : Training Loss =  0.028075972158873107; Validation Loss = 0.034700140990649476\n",
            "Cost after 298711 iterations : Training Loss =  0.028075962398094477; Validation Loss = 0.034700132610988116\n",
            "Cost after 298712 iterations : Training Loss =  0.028075952637427425; Validation Loss = 0.034700124231410466\n",
            "Cost after 298713 iterations : Training Loss =  0.028075942876871856; Validation Loss = 0.03470011585191695\n",
            "Cost after 298714 iterations : Training Loss =  0.0280759331164279; Validation Loss = 0.034700107472507624\n",
            "Cost after 298715 iterations : Training Loss =  0.028075923356095236; Validation Loss = 0.03470009909318233\n",
            "Cost after 298716 iterations : Training Loss =  0.02807591359587442; Validation Loss = 0.03470009071394098\n",
            "Cost after 298717 iterations : Training Loss =  0.02807590383576492; Validation Loss = 0.034700082334783396\n",
            "Cost after 298718 iterations : Training Loss =  0.028075894075767018; Validation Loss = 0.034700073955710016\n",
            "Cost after 298719 iterations : Training Loss =  0.028075884315880678; Validation Loss = 0.0347000655767203\n",
            "Cost after 298720 iterations : Training Loss =  0.028075874556105815; Validation Loss = 0.034700057197815014\n",
            "Cost after 298721 iterations : Training Loss =  0.02807586479644241; Validation Loss = 0.034700048818993856\n",
            "Cost after 298722 iterations : Training Loss =  0.028075855036890546; Validation Loss = 0.03470004044025671\n",
            "Cost after 298723 iterations : Training Loss =  0.028075845277450284; Validation Loss = 0.03470003206160319\n",
            "Cost after 298724 iterations : Training Loss =  0.028075835518121525; Validation Loss = 0.03470002368303386\n",
            "Cost after 298725 iterations : Training Loss =  0.028075825758904133; Validation Loss = 0.03470001530454829\n",
            "Cost after 298726 iterations : Training Loss =  0.028075815999798356; Validation Loss = 0.034700006926147206\n",
            "Cost after 298727 iterations : Training Loss =  0.028075806240804142; Validation Loss = 0.0346999985478296\n",
            "Cost after 298728 iterations : Training Loss =  0.028075796481921454; Validation Loss = 0.03469999016959611\n",
            "Cost after 298729 iterations : Training Loss =  0.02807578672315014; Validation Loss = 0.034699981791446725\n",
            "Cost after 298730 iterations : Training Loss =  0.028075776964490408; Validation Loss = 0.034699973413381206\n",
            "Cost after 298731 iterations : Training Loss =  0.02807576720594213; Validation Loss = 0.03469996503539984\n",
            "Cost after 298732 iterations : Training Loss =  0.028075757447505435; Validation Loss = 0.0346999566575026\n",
            "Cost after 298733 iterations : Training Loss =  0.02807574768918013; Validation Loss = 0.03469994827968926\n",
            "Cost after 298734 iterations : Training Loss =  0.028075737930966436; Validation Loss = 0.03469993990195978\n",
            "Cost after 298735 iterations : Training Loss =  0.028075728172864172; Validation Loss = 0.034699931524314345\n",
            "Cost after 298736 iterations : Training Loss =  0.028075718414873406; Validation Loss = 0.03469992314675263\n",
            "Cost after 298737 iterations : Training Loss =  0.02807570865699423; Validation Loss = 0.0346999147692753\n",
            "Cost after 298738 iterations : Training Loss =  0.02807569889922644; Validation Loss = 0.03469990639188162\n",
            "Cost after 298739 iterations : Training Loss =  0.02807568914157027; Validation Loss = 0.03469989801457213\n",
            "Cost after 298740 iterations : Training Loss =  0.028075679384025408; Validation Loss = 0.03469988963734653\n",
            "Cost after 298741 iterations : Training Loss =  0.02807566962659218; Validation Loss = 0.03469988126020512\n",
            "Cost after 298742 iterations : Training Loss =  0.02807565986927038; Validation Loss = 0.03469987288314755\n",
            "Cost after 298743 iterations : Training Loss =  0.02807565011206002; Validation Loss = 0.034699864506174016\n",
            "Cost after 298744 iterations : Training Loss =  0.028075640354961132; Validation Loss = 0.034699856129284286\n",
            "Cost after 298745 iterations : Training Loss =  0.028075630597973904; Validation Loss = 0.03469984775247859\n",
            "Cost after 298746 iterations : Training Loss =  0.028075620841097965; Validation Loss = 0.03469983937575671\n",
            "Cost after 298747 iterations : Training Loss =  0.028075611084333568; Validation Loss = 0.03469983099911869\n",
            "Cost after 298748 iterations : Training Loss =  0.028075601327680735; Validation Loss = 0.03469982262256515\n",
            "Cost after 298749 iterations : Training Loss =  0.02807559157113929; Validation Loss = 0.03469981424609546\n",
            "Cost after 298750 iterations : Training Loss =  0.028075581814709285; Validation Loss = 0.034699805869709714\n",
            "Cost after 298751 iterations : Training Loss =  0.028075572058390806; Validation Loss = 0.03469979749340762\n",
            "Cost after 298752 iterations : Training Loss =  0.02807556230218385; Validation Loss = 0.03469978911718985\n",
            "Cost after 298753 iterations : Training Loss =  0.028075552546088282; Validation Loss = 0.034699780741055876\n",
            "Cost after 298754 iterations : Training Loss =  0.028075542790104106; Validation Loss = 0.03469977236500577\n",
            "Cost after 298755 iterations : Training Loss =  0.02807553303423155; Validation Loss = 0.03469976398903977\n",
            "Cost after 298756 iterations : Training Loss =  0.02807552327847044; Validation Loss = 0.034699755613157766\n",
            "Cost after 298757 iterations : Training Loss =  0.028075513522820672; Validation Loss = 0.03469974723735955\n",
            "Cost after 298758 iterations : Training Loss =  0.028075503767282482; Validation Loss = 0.034699738861645506\n",
            "Cost after 298759 iterations : Training Loss =  0.028075494011855717; Validation Loss = 0.03469973048601515\n",
            "Cost after 298760 iterations : Training Loss =  0.02807548425654049; Validation Loss = 0.03469972211046913\n",
            "Cost after 298761 iterations : Training Loss =  0.028075474501336503; Validation Loss = 0.0346997137350067\n",
            "Cost after 298762 iterations : Training Loss =  0.028075464746244158; Validation Loss = 0.03469970535962844\n",
            "Cost after 298763 iterations : Training Loss =  0.028075454991263248; Validation Loss = 0.03469969698433427\n",
            "Cost after 298764 iterations : Training Loss =  0.028075445236393697; Validation Loss = 0.0346996886091239\n",
            "Cost after 298765 iterations : Training Loss =  0.028075435481635692; Validation Loss = 0.034699680233997325\n",
            "Cost after 298766 iterations : Training Loss =  0.028075425726989036; Validation Loss = 0.034699671858954985\n",
            "Cost after 298767 iterations : Training Loss =  0.0280754159724539; Validation Loss = 0.03469966348399648\n",
            "Cost after 298768 iterations : Training Loss =  0.028075406218030233; Validation Loss = 0.03469965510912191\n",
            "Cost after 298769 iterations : Training Loss =  0.028075396463718; Validation Loss = 0.03469964673433091\n",
            "Cost after 298770 iterations : Training Loss =  0.028075386709517196; Validation Loss = 0.03469963835962418\n",
            "Cost after 298771 iterations : Training Loss =  0.02807537695542786; Validation Loss = 0.03469962998500141\n",
            "Cost after 298772 iterations : Training Loss =  0.028075367201449857; Validation Loss = 0.03469962161046261\n",
            "Cost after 298773 iterations : Training Loss =  0.028075357447583344; Validation Loss = 0.034699613236007794\n",
            "Cost after 298774 iterations : Training Loss =  0.028075347693828325; Validation Loss = 0.03469960486163703\n",
            "Cost after 298775 iterations : Training Loss =  0.02807533794018472; Validation Loss = 0.03469959648735\n",
            "Cost after 298776 iterations : Training Loss =  0.02807532818665259; Validation Loss = 0.03469958811314692\n",
            "Cost after 298777 iterations : Training Loss =  0.02807531843323176; Validation Loss = 0.03469957973902771\n",
            "Cost after 298778 iterations : Training Loss =  0.028075308679922407; Validation Loss = 0.03469957136499274\n",
            "Cost after 298779 iterations : Training Loss =  0.028075298926724557; Validation Loss = 0.034699562991041405\n",
            "Cost after 298780 iterations : Training Loss =  0.02807528917363797; Validation Loss = 0.03469955461717418\n",
            "Cost after 298781 iterations : Training Loss =  0.028075279420663027; Validation Loss = 0.03469954624339072\n",
            "Cost after 298782 iterations : Training Loss =  0.02807526966779934; Validation Loss = 0.034699537869691294\n",
            "Cost after 298783 iterations : Training Loss =  0.028075259915047116; Validation Loss = 0.03469952949607579\n",
            "Cost after 298784 iterations : Training Loss =  0.028075250162406404; Validation Loss = 0.03469952112254401\n",
            "Cost after 298785 iterations : Training Loss =  0.028075240409877047; Validation Loss = 0.034699512749096266\n",
            "Cost after 298786 iterations : Training Loss =  0.02807523065745906; Validation Loss = 0.03469950437573247\n",
            "Cost after 298787 iterations : Training Loss =  0.028075220905152555; Validation Loss = 0.03469949600245285\n",
            "Cost after 298788 iterations : Training Loss =  0.028075211152957445; Validation Loss = 0.03469948762925709\n",
            "Cost after 298789 iterations : Training Loss =  0.028075201400873714; Validation Loss = 0.03469947925614514\n",
            "Cost after 298790 iterations : Training Loss =  0.02807519164890142; Validation Loss = 0.03469947088311714\n",
            "Cost after 298791 iterations : Training Loss =  0.028075181897040537; Validation Loss = 0.03469946251017302\n",
            "Cost after 298792 iterations : Training Loss =  0.028075172145291087; Validation Loss = 0.03469945413731281\n",
            "Cost after 298793 iterations : Training Loss =  0.02807516239365301; Validation Loss = 0.03469944576453636\n",
            "Cost after 298794 iterations : Training Loss =  0.02807515264212623; Validation Loss = 0.03469943739184428\n",
            "Cost after 298795 iterations : Training Loss =  0.028075142890710923; Validation Loss = 0.03469942901923565\n",
            "Cost after 298796 iterations : Training Loss =  0.028075133139407172; Validation Loss = 0.03469942064671127\n",
            "Cost after 298797 iterations : Training Loss =  0.028075123388214697; Validation Loss = 0.03469941227427042\n",
            "Cost after 298798 iterations : Training Loss =  0.028075113637133668; Validation Loss = 0.03469940390191379\n",
            "Cost after 298799 iterations : Training Loss =  0.02807510388616404; Validation Loss = 0.03469939552964099\n",
            "Cost after 298800 iterations : Training Loss =  0.028075094135305716; Validation Loss = 0.03469938715745205\n",
            "Cost after 298801 iterations : Training Loss =  0.028075084384558892; Validation Loss = 0.03469937878534717\n",
            "Cost after 298802 iterations : Training Loss =  0.02807507463392339; Validation Loss = 0.034699370413326144\n",
            "Cost after 298803 iterations : Training Loss =  0.028075064883399337; Validation Loss = 0.03469936204138908\n",
            "Cost after 298804 iterations : Training Loss =  0.028075055132986505; Validation Loss = 0.034699353669535714\n",
            "Cost after 298805 iterations : Training Loss =  0.02807504538268519; Validation Loss = 0.0346993452977663\n",
            "Cost after 298806 iterations : Training Loss =  0.028075035632495388; Validation Loss = 0.034699336926080834\n",
            "Cost after 298807 iterations : Training Loss =  0.028075025882416737; Validation Loss = 0.034699328554479256\n",
            "Cost after 298808 iterations : Training Loss =  0.028075016132449556; Validation Loss = 0.034699320182961714\n",
            "Cost after 298809 iterations : Training Loss =  0.028075006382593845; Validation Loss = 0.03469931181152802\n",
            "Cost after 298810 iterations : Training Loss =  0.028074996632849365; Validation Loss = 0.034699303440178124\n",
            "Cost after 298811 iterations : Training Loss =  0.02807498688321628; Validation Loss = 0.034699295068912187\n",
            "Cost after 298812 iterations : Training Loss =  0.028074977133694636; Validation Loss = 0.03469928669773004\n",
            "Cost after 298813 iterations : Training Loss =  0.028074967384284336; Validation Loss = 0.03469927832663186\n",
            "Cost after 298814 iterations : Training Loss =  0.02807495763498557; Validation Loss = 0.03469926995561752\n",
            "Cost after 298815 iterations : Training Loss =  0.02807494788579788; Validation Loss = 0.03469926158468723\n",
            "Cost after 298816 iterations : Training Loss =  0.02807493813672171; Validation Loss = 0.034699253213840736\n",
            "Cost after 298817 iterations : Training Loss =  0.028074928387756923; Validation Loss = 0.0346992448430782\n",
            "Cost after 298818 iterations : Training Loss =  0.028074918638903497; Validation Loss = 0.034699236472399406\n",
            "Cost after 298819 iterations : Training Loss =  0.02807490889016141; Validation Loss = 0.03469922810180463\n",
            "Cost after 298820 iterations : Training Loss =  0.028074899141530798; Validation Loss = 0.03469921973129377\n",
            "Cost after 298821 iterations : Training Loss =  0.028074889393011396; Validation Loss = 0.034699211360866455\n",
            "Cost after 298822 iterations : Training Loss =  0.028074879644603506; Validation Loss = 0.03469920299052356\n",
            "Cost after 298823 iterations : Training Loss =  0.02807486989630677; Validation Loss = 0.03469919462026409\n",
            "Cost after 298824 iterations : Training Loss =  0.028074860148121453; Validation Loss = 0.03469918625008881\n",
            "Cost after 298825 iterations : Training Loss =  0.02807485040004765; Validation Loss = 0.034699177879997364\n",
            "Cost after 298826 iterations : Training Loss =  0.028074840652085077; Validation Loss = 0.03469916950998953\n",
            "Cost after 298827 iterations : Training Loss =  0.028074830904233884; Validation Loss = 0.03469916114006571\n",
            "Cost after 298828 iterations : Training Loss =  0.028074821156494015; Validation Loss = 0.034699152770225825\n",
            "Cost after 298829 iterations : Training Loss =  0.028074811408865436; Validation Loss = 0.034699144400469804\n",
            "Cost after 298830 iterations : Training Loss =  0.028074801661348348; Validation Loss = 0.034699136030797426\n",
            "Cost after 298831 iterations : Training Loss =  0.02807479191394256; Validation Loss = 0.03469912766120903\n",
            "Cost after 298832 iterations : Training Loss =  0.02807478216664798; Validation Loss = 0.034699119291704966\n",
            "Cost after 298833 iterations : Training Loss =  0.02807477241946503; Validation Loss = 0.03469911092228447\n",
            "Cost after 298834 iterations : Training Loss =  0.028074762672393203; Validation Loss = 0.03469910255294781\n",
            "Cost after 298835 iterations : Training Loss =  0.028074752925432704; Validation Loss = 0.03469909418369506\n",
            "Cost after 298836 iterations : Training Loss =  0.028074743178583623; Validation Loss = 0.03469908581452608\n",
            "Cost after 298837 iterations : Training Loss =  0.02807473343184576; Validation Loss = 0.034699077445441165\n",
            "Cost after 298838 iterations : Training Loss =  0.028074723685219367; Validation Loss = 0.03469906907643996\n",
            "Cost after 298839 iterations : Training Loss =  0.028074713938704306; Validation Loss = 0.034699060707522586\n",
            "Cost after 298840 iterations : Training Loss =  0.028074704192300518; Validation Loss = 0.034699052338689196\n",
            "Cost after 298841 iterations : Training Loss =  0.028074694446008106; Validation Loss = 0.034699043969939614\n",
            "Cost after 298842 iterations : Training Loss =  0.028074684699826925; Validation Loss = 0.03469903560127416\n",
            "Cost after 298843 iterations : Training Loss =  0.028074674953757147; Validation Loss = 0.034699027232692456\n",
            "Cost after 298844 iterations : Training Loss =  0.028074665207798697; Validation Loss = 0.0346990188641945\n",
            "Cost after 298845 iterations : Training Loss =  0.028074655461951648; Validation Loss = 0.034699010495780516\n",
            "Cost after 298846 iterations : Training Loss =  0.02807464571621583; Validation Loss = 0.03469900212745029\n",
            "Cost after 298847 iterations : Training Loss =  0.028074635970591323; Validation Loss = 0.03469899375920389\n",
            "Cost after 298848 iterations : Training Loss =  0.028074626225078215; Validation Loss = 0.03469898539104131\n",
            "Cost after 298849 iterations : Training Loss =  0.028074616479676243; Validation Loss = 0.03469897702296267\n",
            "Cost after 298850 iterations : Training Loss =  0.02807460673438573; Validation Loss = 0.03469896865496804\n",
            "Cost after 298851 iterations : Training Loss =  0.028074596989206523; Validation Loss = 0.034698960287056935\n",
            "Cost after 298852 iterations : Training Loss =  0.0280745872441386; Validation Loss = 0.034698951919230024\n",
            "Cost after 298853 iterations : Training Loss =  0.028074577499181946; Validation Loss = 0.034698943551486686\n",
            "Cost after 298854 iterations : Training Loss =  0.028074567754336758; Validation Loss = 0.0346989351838272\n",
            "Cost after 298855 iterations : Training Loss =  0.028074558009602745; Validation Loss = 0.03469892681625173\n",
            "Cost after 298856 iterations : Training Loss =  0.02807454826498006; Validation Loss = 0.0346989184487599\n",
            "Cost after 298857 iterations : Training Loss =  0.02807453852046866; Validation Loss = 0.03469891008135186\n",
            "Cost after 298858 iterations : Training Loss =  0.02807452877606861; Validation Loss = 0.03469890171402764\n",
            "Cost after 298859 iterations : Training Loss =  0.028074519031779963; Validation Loss = 0.03469889334678753\n",
            "Cost after 298860 iterations : Training Loss =  0.02807450928760234; Validation Loss = 0.03469888497963132\n",
            "Cost after 298861 iterations : Training Loss =  0.028074499543536198; Validation Loss = 0.03469887661255881\n",
            "Cost after 298862 iterations : Training Loss =  0.02807448979958133; Validation Loss = 0.03469886824557004\n",
            "Cost after 298863 iterations : Training Loss =  0.028074480055737806; Validation Loss = 0.034698859878665285\n",
            "Cost after 298864 iterations : Training Loss =  0.02807447031200553; Validation Loss = 0.034698851511844236\n",
            "Cost after 298865 iterations : Training Loss =  0.028074460568384484; Validation Loss = 0.03469884314510723\n",
            "Cost after 298866 iterations : Training Loss =  0.028074450824874853; Validation Loss = 0.03469883477845367\n",
            "Cost after 298867 iterations : Training Loss =  0.02807444108147641; Validation Loss = 0.0346988264118839\n",
            "Cost after 298868 iterations : Training Loss =  0.02807443133818925; Validation Loss = 0.03469881804539849\n",
            "Cost after 298869 iterations : Training Loss =  0.028074421595013492; Validation Loss = 0.03469880967899662\n",
            "Cost after 298870 iterations : Training Loss =  0.0280744118519489; Validation Loss = 0.03469880131267895\n",
            "Cost after 298871 iterations : Training Loss =  0.02807440210899565; Validation Loss = 0.034698792946444644\n",
            "Cost after 298872 iterations : Training Loss =  0.028074392366153673; Validation Loss = 0.03469878458029431\n",
            "Cost after 298873 iterations : Training Loss =  0.028074382623422857; Validation Loss = 0.03469877621422785\n",
            "Cost after 298874 iterations : Training Loss =  0.02807437288080354; Validation Loss = 0.03469876784824495\n",
            "Cost after 298875 iterations : Training Loss =  0.02807436313829541; Validation Loss = 0.03469875948234588\n",
            "Cost after 298876 iterations : Training Loss =  0.028074353395898543; Validation Loss = 0.034698751116531014\n",
            "Cost after 298877 iterations : Training Loss =  0.028074343653612954; Validation Loss = 0.03469874275079988\n",
            "Cost after 298878 iterations : Training Loss =  0.028074333911438553; Validation Loss = 0.03469873438515221\n",
            "Cost after 298879 iterations : Training Loss =  0.02807432416937556; Validation Loss = 0.03469872601958865\n",
            "Cost after 298880 iterations : Training Loss =  0.02807431442742373; Validation Loss = 0.03469871765410877\n",
            "Cost after 298881 iterations : Training Loss =  0.028074304685583307; Validation Loss = 0.03469870928871295\n",
            "Cost after 298882 iterations : Training Loss =  0.028074294943853974; Validation Loss = 0.03469870092340092\n",
            "Cost after 298883 iterations : Training Loss =  0.028074285202236014; Validation Loss = 0.0346986925581725\n",
            "Cost after 298884 iterations : Training Loss =  0.02807427546072927; Validation Loss = 0.03469868419302796\n",
            "Cost after 298885 iterations : Training Loss =  0.02807426571933372; Validation Loss = 0.03469867582796695\n",
            "Cost after 298886 iterations : Training Loss =  0.02807425597804951; Validation Loss = 0.03469866746299031\n",
            "Cost after 298887 iterations : Training Loss =  0.0280742462368766; Validation Loss = 0.03469865909809724\n",
            "Cost after 298888 iterations : Training Loss =  0.02807423649581489; Validation Loss = 0.03469865073328808\n",
            "Cost after 298889 iterations : Training Loss =  0.028074226754864515; Validation Loss = 0.03469864236856242\n",
            "Cost after 298890 iterations : Training Loss =  0.02807421701402532; Validation Loss = 0.03469863400392086\n",
            "Cost after 298891 iterations : Training Loss =  0.028074207273297333; Validation Loss = 0.03469862563936336\n",
            "Cost after 298892 iterations : Training Loss =  0.02807419753268065; Validation Loss = 0.034698617274889244\n",
            "Cost after 298893 iterations : Training Loss =  0.028074187792175187; Validation Loss = 0.03469860891049899\n",
            "Cost after 298894 iterations : Training Loss =  0.028074178051781022; Validation Loss = 0.03469860054619258\n",
            "Cost after 298895 iterations : Training Loss =  0.02807416831149807; Validation Loss = 0.03469859218197021\n",
            "Cost after 298896 iterations : Training Loss =  0.02807415857132632; Validation Loss = 0.034698583817831224\n",
            "Cost after 298897 iterations : Training Loss =  0.02807414883126591; Validation Loss = 0.034698575453776116\n",
            "Cost after 298898 iterations : Training Loss =  0.028074139091316765; Validation Loss = 0.03469856708980475\n",
            "Cost after 298899 iterations : Training Loss =  0.028074129351478728; Validation Loss = 0.03469855872591738\n",
            "Cost after 298900 iterations : Training Loss =  0.028074119611751925; Validation Loss = 0.03469855036211367\n",
            "Cost after 298901 iterations : Training Loss =  0.02807410987213648; Validation Loss = 0.03469854199839383\n",
            "Cost after 298902 iterations : Training Loss =  0.02807410013263212; Validation Loss = 0.03469853363475787\n",
            "Cost after 298903 iterations : Training Loss =  0.028074090393239114; Validation Loss = 0.034698525271205365\n",
            "Cost after 298904 iterations : Training Loss =  0.02807408065395724; Validation Loss = 0.034698516907736975\n",
            "Cost after 298905 iterations : Training Loss =  0.028074070914786798; Validation Loss = 0.03469850854435221\n",
            "Cost after 298906 iterations : Training Loss =  0.028074061175727364; Validation Loss = 0.034698500181051375\n",
            "Cost after 298907 iterations : Training Loss =  0.028074051436779206; Validation Loss = 0.03469849181783415\n",
            "Cost after 298908 iterations : Training Loss =  0.028074041697942222; Validation Loss = 0.034698483454700814\n",
            "Cost after 298909 iterations : Training Loss =  0.028074031959216643; Validation Loss = 0.03469847509165163\n",
            "Cost after 298910 iterations : Training Loss =  0.02807402222060214; Validation Loss = 0.03469846672868571\n",
            "Cost after 298911 iterations : Training Loss =  0.028074012482098853; Validation Loss = 0.03469845836580361\n",
            "Cost after 298912 iterations : Training Loss =  0.028074002743706843; Validation Loss = 0.03469845000300549\n",
            "Cost after 298913 iterations : Training Loss =  0.028073993005426102; Validation Loss = 0.03469844164029106\n",
            "Cost after 298914 iterations : Training Loss =  0.028073983267256596; Validation Loss = 0.034698433277660486\n",
            "Cost after 298915 iterations : Training Loss =  0.028073973529198052; Validation Loss = 0.03469842491511356\n",
            "Cost after 298916 iterations : Training Loss =  0.028073963791250926; Validation Loss = 0.034698416552650414\n",
            "Cost after 298917 iterations : Training Loss =  0.028073954053414986; Validation Loss = 0.034698408190271354\n",
            "Cost after 298918 iterations : Training Loss =  0.02807394431569027; Validation Loss = 0.03469839982797565\n",
            "Cost after 298919 iterations : Training Loss =  0.028073934578076717; Validation Loss = 0.03469839146576393\n",
            "Cost after 298920 iterations : Training Loss =  0.028073924840574426; Validation Loss = 0.03469838310363592\n",
            "Cost after 298921 iterations : Training Loss =  0.028073915103183238; Validation Loss = 0.03469837474159176\n",
            "Cost after 298922 iterations : Training Loss =  0.028073905365903356; Validation Loss = 0.03469836637963144\n",
            "Cost after 298923 iterations : Training Loss =  0.028073895628734566; Validation Loss = 0.03469835801775501\n",
            "Cost after 298924 iterations : Training Loss =  0.028073885891676947; Validation Loss = 0.03469834965596199\n",
            "Cost after 298925 iterations : Training Loss =  0.0280738761547307; Validation Loss = 0.034698341294252824\n",
            "Cost after 298926 iterations : Training Loss =  0.02807386641789551; Validation Loss = 0.03469833293262751\n",
            "Cost after 298927 iterations : Training Loss =  0.02807385668117159; Validation Loss = 0.03469832457108584\n",
            "Cost after 298928 iterations : Training Loss =  0.028073846944558924; Validation Loss = 0.03469831620962793\n",
            "Cost after 298929 iterations : Training Loss =  0.028073837208057364; Validation Loss = 0.034698307848253944\n",
            "Cost after 298930 iterations : Training Loss =  0.028073827471666927; Validation Loss = 0.034698299486963735\n",
            "Cost after 298931 iterations : Training Loss =  0.028073817735387776; Validation Loss = 0.03469829112575724\n",
            "Cost after 298932 iterations : Training Loss =  0.028073807999219817; Validation Loss = 0.034698282764634894\n",
            "Cost after 298933 iterations : Training Loss =  0.028073798263163; Validation Loss = 0.034698274403595714\n",
            "Cost after 298934 iterations : Training Loss =  0.02807378852721736; Validation Loss = 0.03469826604264064\n",
            "Cost after 298935 iterations : Training Loss =  0.028073778791382918; Validation Loss = 0.034698257681769\n",
            "Cost after 298936 iterations : Training Loss =  0.028073769055659656; Validation Loss = 0.03469824932098134\n",
            "Cost after 298937 iterations : Training Loss =  0.02807375932004757; Validation Loss = 0.03469824096027761\n",
            "Cost after 298938 iterations : Training Loss =  0.028073749584546684; Validation Loss = 0.03469823259965718\n",
            "Cost after 298939 iterations : Training Loss =  0.02807373984915689; Validation Loss = 0.03469822423912073\n",
            "Cost after 298940 iterations : Training Loss =  0.028073730113878363; Validation Loss = 0.03469821587866787\n",
            "Cost after 298941 iterations : Training Loss =  0.028073720378711114; Validation Loss = 0.034698207518298704\n",
            "Cost after 298942 iterations : Training Loss =  0.028073710643654798; Validation Loss = 0.034698199158013476\n",
            "Cost after 298943 iterations : Training Loss =  0.028073700908709656; Validation Loss = 0.03469819079781193\n",
            "Cost after 298944 iterations : Training Loss =  0.02807369117387585; Validation Loss = 0.034698182437694534\n",
            "Cost after 298945 iterations : Training Loss =  0.028073681439153123; Validation Loss = 0.03469817407766048\n",
            "Cost after 298946 iterations : Training Loss =  0.02807367170454163; Validation Loss = 0.03469816571771015\n",
            "Cost after 298947 iterations : Training Loss =  0.028073661970041228; Validation Loss = 0.03469815735784377\n",
            "Cost after 298948 iterations : Training Loss =  0.028073652235651975; Validation Loss = 0.034698148998061146\n",
            "Cost after 298949 iterations : Training Loss =  0.028073642501373976; Validation Loss = 0.03469814063836198\n",
            "Cost after 298950 iterations : Training Loss =  0.028073632767207014; Validation Loss = 0.034698132278746664\n",
            "Cost after 298951 iterations : Training Loss =  0.028073623033151366; Validation Loss = 0.03469812391921525\n",
            "Cost after 298952 iterations : Training Loss =  0.02807361329920675; Validation Loss = 0.03469811555976764\n",
            "Cost after 298953 iterations : Training Loss =  0.02807360356537331; Validation Loss = 0.034698107200403436\n",
            "Cost after 298954 iterations : Training Loss =  0.028073593831651023; Validation Loss = 0.034698098841122904\n",
            "Cost after 298955 iterations : Training Loss =  0.02807358409803999; Validation Loss = 0.034698090481926416\n",
            "Cost after 298956 iterations : Training Loss =  0.028073574364539947; Validation Loss = 0.03469808212281355\n",
            "Cost after 298957 iterations : Training Loss =  0.028073564631151144; Validation Loss = 0.03469807376378464\n",
            "Cost after 298958 iterations : Training Loss =  0.02807355489787335; Validation Loss = 0.03469806540483905\n",
            "Cost after 298959 iterations : Training Loss =  0.02807354516470679; Validation Loss = 0.0346980570459773\n",
            "Cost after 298960 iterations : Training Loss =  0.02807353543165141; Validation Loss = 0.03469804868719951\n",
            "Cost after 298961 iterations : Training Loss =  0.028073525698707115; Validation Loss = 0.03469804032850533\n",
            "Cost after 298962 iterations : Training Loss =  0.02807351596587405; Validation Loss = 0.03469803196989473\n",
            "Cost after 298963 iterations : Training Loss =  0.028073506233152017; Validation Loss = 0.03469802361136766\n",
            "Cost after 298964 iterations : Training Loss =  0.028073496500541157; Validation Loss = 0.03469801525292457\n",
            "Cost after 298965 iterations : Training Loss =  0.028073486768041472; Validation Loss = 0.03469800689456527\n",
            "Cost after 298966 iterations : Training Loss =  0.02807347703565298; Validation Loss = 0.034697998536289666\n",
            "Cost after 298967 iterations : Training Loss =  0.028073467303375357; Validation Loss = 0.03469799017809787\n",
            "Cost after 298968 iterations : Training Loss =  0.028073457571209097; Validation Loss = 0.03469798181998971\n",
            "Cost after 298969 iterations : Training Loss =  0.028073447839153946; Validation Loss = 0.03469797346196548\n",
            "Cost after 298970 iterations : Training Loss =  0.028073438107209917; Validation Loss = 0.03469796510402473\n",
            "Cost after 298971 iterations : Training Loss =  0.02807342837537693; Validation Loss = 0.03469795674616782\n",
            "Cost after 298972 iterations : Training Loss =  0.028073418643655156; Validation Loss = 0.03469794838839437\n",
            "Cost after 298973 iterations : Training Loss =  0.028073408912044334; Validation Loss = 0.03469794003070475\n",
            "Cost after 298974 iterations : Training Loss =  0.028073399180544866; Validation Loss = 0.03469793167309868\n",
            "Cost after 298975 iterations : Training Loss =  0.02807338944915635; Validation Loss = 0.03469792331557658\n",
            "Cost after 298976 iterations : Training Loss =  0.028073379717879172; Validation Loss = 0.034697914958138086\n",
            "Cost after 298977 iterations : Training Loss =  0.02807336998671286; Validation Loss = 0.03469790660078327\n",
            "Cost after 298978 iterations : Training Loss =  0.028073360255657702; Validation Loss = 0.034697898243512305\n",
            "Cost after 298979 iterations : Training Loss =  0.028073350524713755; Validation Loss = 0.034697889886324995\n",
            "Cost after 298980 iterations : Training Loss =  0.028073340793880856; Validation Loss = 0.03469788152922131\n",
            "Cost after 298981 iterations : Training Loss =  0.028073331063159117; Validation Loss = 0.034697873172201306\n",
            "Cost after 298982 iterations : Training Loss =  0.028073321332548445; Validation Loss = 0.03469786481526505\n",
            "Cost after 298983 iterations : Training Loss =  0.028073311602048814; Validation Loss = 0.03469785645841253\n",
            "Cost after 298984 iterations : Training Loss =  0.0280733018716604; Validation Loss = 0.03469784810164369\n",
            "Cost after 298985 iterations : Training Loss =  0.02807329214138303; Validation Loss = 0.03469783974495832\n",
            "Cost after 298986 iterations : Training Loss =  0.028073282411216694; Validation Loss = 0.03469783138835701\n",
            "Cost after 298987 iterations : Training Loss =  0.02807327268116157; Validation Loss = 0.03469782303183927\n",
            "Cost after 298988 iterations : Training Loss =  0.02807326295121759; Validation Loss = 0.03469781467540521\n",
            "Cost after 298989 iterations : Training Loss =  0.028073253221384657; Validation Loss = 0.03469780631905478\n",
            "Cost after 298990 iterations : Training Loss =  0.028073243491662812; Validation Loss = 0.03469779796278833\n",
            "Cost after 298991 iterations : Training Loss =  0.028073233762051914; Validation Loss = 0.034697789606605065\n",
            "Cost after 298992 iterations : Training Loss =  0.028073224032552267; Validation Loss = 0.034697781250505697\n",
            "Cost after 298993 iterations : Training Loss =  0.028073214303163698; Validation Loss = 0.03469777289449025\n",
            "Cost after 298994 iterations : Training Loss =  0.028073204573886214; Validation Loss = 0.03469776453855848\n",
            "Cost after 298995 iterations : Training Loss =  0.028073194844719866; Validation Loss = 0.0346977561827102\n",
            "Cost after 298996 iterations : Training Loss =  0.028073185115664513; Validation Loss = 0.03469774782694548\n",
            "Cost after 298997 iterations : Training Loss =  0.028073175386720252; Validation Loss = 0.03469773947126506\n",
            "Cost after 298998 iterations : Training Loss =  0.028073165657887172; Validation Loss = 0.034697731115667915\n",
            "Cost after 298999 iterations : Training Loss =  0.028073155929165095; Validation Loss = 0.03469772276015436\n",
            "Cost after 299000 iterations : Training Loss =  0.02807314620055404; Validation Loss = 0.03469771440472455\n",
            "Cost after 299001 iterations : Training Loss =  0.028073136472054127; Validation Loss = 0.034697706049378434\n",
            "Cost after 299002 iterations : Training Loss =  0.028073126743665303; Validation Loss = 0.03469769769411589\n",
            "Cost after 299003 iterations : Training Loss =  0.02807311701538755; Validation Loss = 0.034697689338937056\n",
            "Cost after 299004 iterations : Training Loss =  0.02807310728722083; Validation Loss = 0.03469768098384178\n",
            "Cost after 299005 iterations : Training Loss =  0.028073097559165223; Validation Loss = 0.03469767262883045\n",
            "Cost after 299006 iterations : Training Loss =  0.02807308783122069; Validation Loss = 0.03469766427390249\n",
            "Cost after 299007 iterations : Training Loss =  0.02807307810338718; Validation Loss = 0.03469765591905837\n",
            "Cost after 299008 iterations : Training Loss =  0.028073068375664725; Validation Loss = 0.03469764756429804\n",
            "Cost after 299009 iterations : Training Loss =  0.02807305864805338; Validation Loss = 0.03469763920962136\n",
            "Cost after 299010 iterations : Training Loss =  0.028073048920553092; Validation Loss = 0.03469763085502841\n",
            "Cost after 299011 iterations : Training Loss =  0.028073039193163913; Validation Loss = 0.03469762250051883\n",
            "Cost after 299012 iterations : Training Loss =  0.028073029465885684; Validation Loss = 0.034697614146093105\n",
            "Cost after 299013 iterations : Training Loss =  0.028073019738718574; Validation Loss = 0.03469760579175123\n",
            "Cost after 299014 iterations : Training Loss =  0.02807301001166257; Validation Loss = 0.0346975974374929\n",
            "Cost after 299015 iterations : Training Loss =  0.02807300028471758; Validation Loss = 0.03469758908331817\n",
            "Cost after 299016 iterations : Training Loss =  0.02807299055788372; Validation Loss = 0.03469758072922723\n",
            "Cost after 299017 iterations : Training Loss =  0.028072980831160724; Validation Loss = 0.03469757237521986\n",
            "Cost after 299018 iterations : Training Loss =  0.02807297110454905; Validation Loss = 0.03469756402129621\n",
            "Cost after 299019 iterations : Training Loss =  0.02807296137804822; Validation Loss = 0.03469755566745589\n",
            "Cost after 299020 iterations : Training Loss =  0.028072951651658424; Validation Loss = 0.03469754731369947\n",
            "Cost after 299021 iterations : Training Loss =  0.02807294192537971; Validation Loss = 0.03469753896002666\n",
            "Cost after 299022 iterations : Training Loss =  0.02807293219921208; Validation Loss = 0.03469753060643776\n",
            "Cost after 299023 iterations : Training Loss =  0.02807292247315555; Validation Loss = 0.03469752225293244\n",
            "Cost after 299024 iterations : Training Loss =  0.028072912747209924; Validation Loss = 0.03469751389951091\n",
            "Cost after 299025 iterations : Training Loss =  0.02807290302137545; Validation Loss = 0.034697505546172895\n",
            "Cost after 299026 iterations : Training Loss =  0.028072893295651904; Validation Loss = 0.034697497192918404\n",
            "Cost after 299027 iterations : Training Loss =  0.028072883570039557; Validation Loss = 0.03469748883974732\n",
            "Cost after 299028 iterations : Training Loss =  0.02807287384453813; Validation Loss = 0.034697480486660284\n",
            "Cost after 299029 iterations : Training Loss =  0.028072864119147712; Validation Loss = 0.03469747213365659\n",
            "Cost after 299030 iterations : Training Loss =  0.02807285439386837; Validation Loss = 0.03469746378073667\n",
            "Cost after 299031 iterations : Training Loss =  0.028072844668700067; Validation Loss = 0.03469745542790028\n",
            "Cost after 299032 iterations : Training Loss =  0.0280728349436427; Validation Loss = 0.034697447075147804\n",
            "Cost after 299033 iterations : Training Loss =  0.028072825218696536; Validation Loss = 0.03469743872247874\n",
            "Cost after 299034 iterations : Training Loss =  0.02807281549386126; Validation Loss = 0.0346974303698936\n",
            "Cost after 299035 iterations : Training Loss =  0.02807280576913716; Validation Loss = 0.03469742201739178\n",
            "Cost after 299036 iterations : Training Loss =  0.028072796044523923; Validation Loss = 0.03469741366497375\n",
            "Cost after 299037 iterations : Training Loss =  0.028072786320021782; Validation Loss = 0.03469740531263935\n",
            "Cost after 299038 iterations : Training Loss =  0.028072776595630677; Validation Loss = 0.03469739696038832\n",
            "Cost after 299039 iterations : Training Loss =  0.028072766871350522; Validation Loss = 0.03469738860822126\n",
            "Cost after 299040 iterations : Training Loss =  0.028072757147181503; Validation Loss = 0.034697380256137625\n",
            "Cost after 299041 iterations : Training Loss =  0.02807274742312334; Validation Loss = 0.034697371904138075\n",
            "Cost after 299042 iterations : Training Loss =  0.028072737699176363; Validation Loss = 0.034697363552221715\n",
            "Cost after 299043 iterations : Training Loss =  0.028072727975340293; Validation Loss = 0.03469735520038921\n",
            "Cost after 299044 iterations : Training Loss =  0.02807271825161515; Validation Loss = 0.034697346848640294\n",
            "Cost after 299045 iterations : Training Loss =  0.02807270852800105; Validation Loss = 0.03469733849697477\n",
            "Cost after 299046 iterations : Training Loss =  0.028072698804498104; Validation Loss = 0.034697330145392974\n",
            "Cost after 299047 iterations : Training Loss =  0.028072689081105974; Validation Loss = 0.03469732179389517\n",
            "Cost after 299048 iterations : Training Loss =  0.028072679357824936; Validation Loss = 0.034697313442480594\n",
            "Cost after 299049 iterations : Training Loss =  0.028072669634654962; Validation Loss = 0.03469730509114959\n",
            "Cost after 299050 iterations : Training Loss =  0.028072659911595937; Validation Loss = 0.03469729673990246\n",
            "Cost after 299051 iterations : Training Loss =  0.028072650188647942; Validation Loss = 0.03469728838873894\n",
            "Cost after 299052 iterations : Training Loss =  0.028072640465810882; Validation Loss = 0.03469728003765893\n",
            "Cost after 299053 iterations : Training Loss =  0.028072630743084814; Validation Loss = 0.03469727168666232\n",
            "Cost after 299054 iterations : Training Loss =  0.028072621020469854; Validation Loss = 0.03469726333574966\n",
            "Cost after 299055 iterations : Training Loss =  0.02807261129796576; Validation Loss = 0.034697254984920586\n",
            "Cost after 299056 iterations : Training Loss =  0.028072601575572693; Validation Loss = 0.0346972466341754\n",
            "Cost after 299057 iterations : Training Loss =  0.02807259185329062; Validation Loss = 0.0346972382835134\n",
            "Cost after 299058 iterations : Training Loss =  0.028072582131119607; Validation Loss = 0.034697229932935274\n",
            "Cost after 299059 iterations : Training Loss =  0.028072572409059474; Validation Loss = 0.03469722158244046\n",
            "Cost after 299060 iterations : Training Loss =  0.02807256268711036; Validation Loss = 0.03469721323202975\n",
            "Cost after 299061 iterations : Training Loss =  0.028072552965272307; Validation Loss = 0.03469720488170245\n",
            "Cost after 299062 iterations : Training Loss =  0.028072543243545182; Validation Loss = 0.03469719653145866\n",
            "Cost after 299063 iterations : Training Loss =  0.028072533521928997; Validation Loss = 0.03469718818129852\n",
            "Cost after 299064 iterations : Training Loss =  0.028072523800423903; Validation Loss = 0.03469717983122179\n",
            "Cost after 299065 iterations : Training Loss =  0.028072514079029626; Validation Loss = 0.03469717148122882\n",
            "Cost after 299066 iterations : Training Loss =  0.028072504357746362; Validation Loss = 0.034697163131319424\n",
            "Cost after 299067 iterations : Training Loss =  0.028072494636574245; Validation Loss = 0.034697154781493655\n",
            "Cost after 299068 iterations : Training Loss =  0.02807248491551282; Validation Loss = 0.03469714643175145\n",
            "Cost after 299069 iterations : Training Loss =  0.028072475194562463; Validation Loss = 0.03469713808209301\n",
            "Cost after 299070 iterations : Training Loss =  0.028072465473723232; Validation Loss = 0.03469712973251784\n",
            "Cost after 299071 iterations : Training Loss =  0.02807245575299485; Validation Loss = 0.03469712138302677\n",
            "Cost after 299072 iterations : Training Loss =  0.02807244603237748; Validation Loss = 0.03469711303361897\n",
            "Cost after 299073 iterations : Training Loss =  0.02807243631187095; Validation Loss = 0.03469710468429482\n",
            "Cost after 299074 iterations : Training Loss =  0.028072426591475536; Validation Loss = 0.03469709633505424\n",
            "Cost after 299075 iterations : Training Loss =  0.028072416871191046; Validation Loss = 0.03469708798589725\n",
            "Cost after 299076 iterations : Training Loss =  0.028072407151017522; Validation Loss = 0.03469707963682391\n",
            "Cost after 299077 iterations : Training Loss =  0.028072397430954896; Validation Loss = 0.034697071287834336\n",
            "Cost after 299078 iterations : Training Loss =  0.02807238771100325; Validation Loss = 0.034697062938928165\n",
            "Cost after 299079 iterations : Training Loss =  0.028072377991162575; Validation Loss = 0.03469705459010563\n",
            "Cost after 299080 iterations : Training Loss =  0.028072368271432845; Validation Loss = 0.034697046241366915\n",
            "Cost after 299081 iterations : Training Loss =  0.02807235855181408; Validation Loss = 0.03469703789271156\n",
            "Cost after 299082 iterations : Training Loss =  0.02807234883230623; Validation Loss = 0.03469702954413962\n",
            "Cost after 299083 iterations : Training Loss =  0.028072339112909322; Validation Loss = 0.03469702119565139\n",
            "Cost after 299084 iterations : Training Loss =  0.02807232939362347; Validation Loss = 0.03469701284724679\n",
            "Cost after 299085 iterations : Training Loss =  0.028072319674448405; Validation Loss = 0.034697004498925994\n",
            "Cost after 299086 iterations : Training Loss =  0.02807230995538439; Validation Loss = 0.03469699615068873\n",
            "Cost after 299087 iterations : Training Loss =  0.028072300236431334; Validation Loss = 0.0346969878025348\n",
            "Cost after 299088 iterations : Training Loss =  0.028072290517589114; Validation Loss = 0.03469697945446476\n",
            "Cost after 299089 iterations : Training Loss =  0.028072280798857874; Validation Loss = 0.0346969711064777\n",
            "Cost after 299090 iterations : Training Loss =  0.028072271080237604; Validation Loss = 0.034696962758574645\n",
            "Cost after 299091 iterations : Training Loss =  0.028072261361728273; Validation Loss = 0.03469695441075525\n",
            "Cost after 299092 iterations : Training Loss =  0.028072251643329844; Validation Loss = 0.034696946063019374\n",
            "Cost after 299093 iterations : Training Loss =  0.02807224192504238; Validation Loss = 0.034696937715367006\n",
            "Cost after 299094 iterations : Training Loss =  0.02807223220686586; Validation Loss = 0.03469692936779823\n",
            "Cost after 299095 iterations : Training Loss =  0.02807222248880027; Validation Loss = 0.03469692102031299\n",
            "Cost after 299096 iterations : Training Loss =  0.02807221277084561; Validation Loss = 0.034696912672911404\n",
            "Cost after 299097 iterations : Training Loss =  0.02807220305300178; Validation Loss = 0.03469690432559323\n",
            "Cost after 299098 iterations : Training Loss =  0.02807219333526903; Validation Loss = 0.03469689597835886\n",
            "Cost after 299099 iterations : Training Loss =  0.028072183617647035; Validation Loss = 0.03469688763120768\n",
            "Cost after 299100 iterations : Training Loss =  0.028072173900136055; Validation Loss = 0.03469687928414021\n",
            "Cost after 299101 iterations : Training Loss =  0.02807216418273596; Validation Loss = 0.03469687093715649\n",
            "Cost after 299102 iterations : Training Loss =  0.02807215446544685; Validation Loss = 0.03469686259025618\n",
            "Cost after 299103 iterations : Training Loss =  0.02807214474826858; Validation Loss = 0.0346968542434393\n",
            "Cost after 299104 iterations : Training Loss =  0.028072135031201297; Validation Loss = 0.034696845896706144\n",
            "Cost after 299105 iterations : Training Loss =  0.028072125314244984; Validation Loss = 0.03469683755005649\n",
            "Cost after 299106 iterations : Training Loss =  0.028072115597399458; Validation Loss = 0.03469682920349046\n",
            "Cost after 299107 iterations : Training Loss =  0.02807210588066484; Validation Loss = 0.03469682085700827\n",
            "Cost after 299108 iterations : Training Loss =  0.028072096164041122; Validation Loss = 0.034696812510609285\n",
            "Cost after 299109 iterations : Training Loss =  0.02807208644752846; Validation Loss = 0.03469680416429415\n",
            "Cost after 299110 iterations : Training Loss =  0.028072076731126574; Validation Loss = 0.03469679581806247\n",
            "Cost after 299111 iterations : Training Loss =  0.02807206701483562; Validation Loss = 0.034696787471914126\n",
            "Cost after 299112 iterations : Training Loss =  0.02807205729865561; Validation Loss = 0.034696779125849404\n",
            "Cost after 299113 iterations : Training Loss =  0.02807204758258649; Validation Loss = 0.03469677077986817\n",
            "Cost after 299114 iterations : Training Loss =  0.02807203786662817; Validation Loss = 0.03469676243397095\n",
            "Cost after 299115 iterations : Training Loss =  0.02807202815078087; Validation Loss = 0.03469675408815665\n",
            "Cost after 299116 iterations : Training Loss =  0.028072018435044425; Validation Loss = 0.03469674574242614\n",
            "Cost after 299117 iterations : Training Loss =  0.028072008719418888; Validation Loss = 0.034696737396779205\n",
            "Cost after 299118 iterations : Training Loss =  0.02807199900390427; Validation Loss = 0.034696729051215826\n",
            "Cost after 299119 iterations : Training Loss =  0.028071989288500434; Validation Loss = 0.03469672070573601\n",
            "Cost after 299120 iterations : Training Loss =  0.02807197957320754; Validation Loss = 0.03469671236033996\n",
            "Cost after 299121 iterations : Training Loss =  0.02807196985802565; Validation Loss = 0.03469670401502715\n",
            "Cost after 299122 iterations : Training Loss =  0.028071960142954547; Validation Loss = 0.0346966956697979\n",
            "Cost after 299123 iterations : Training Loss =  0.028071950427994336; Validation Loss = 0.03469668732465226\n",
            "Cost after 299124 iterations : Training Loss =  0.028071940713144977; Validation Loss = 0.034696678979590084\n",
            "Cost after 299125 iterations : Training Loss =  0.028071930998406464; Validation Loss = 0.03469667063461164\n",
            "Cost after 299126 iterations : Training Loss =  0.02807192128377903; Validation Loss = 0.034696662289716435\n",
            "Cost after 299127 iterations : Training Loss =  0.028071911569262285; Validation Loss = 0.034696653944904975\n",
            "Cost after 299128 iterations : Training Loss =  0.028071901854856418; Validation Loss = 0.03469664560017702\n",
            "Cost after 299129 iterations : Training Loss =  0.02807189214056154; Validation Loss = 0.034696637255532646\n",
            "Cost after 299130 iterations : Training Loss =  0.028071882426377433; Validation Loss = 0.034696628910971715\n",
            "Cost after 299131 iterations : Training Loss =  0.0280718727123043; Validation Loss = 0.03469662056649419\n",
            "Cost after 299132 iterations : Training Loss =  0.028071862998341995; Validation Loss = 0.03469661222210031\n",
            "Cost after 299133 iterations : Training Loss =  0.028071853284490546; Validation Loss = 0.03469660387778996\n",
            "Cost after 299134 iterations : Training Loss =  0.02807184357075; Validation Loss = 0.0346965955335631\n",
            "Cost after 299135 iterations : Training Loss =  0.028071833857120254; Validation Loss = 0.03469658718941977\n",
            "Cost after 299136 iterations : Training Loss =  0.028071824143601435; Validation Loss = 0.03469657884536016\n",
            "Cost after 299137 iterations : Training Loss =  0.028071814430193496; Validation Loss = 0.03469657050138393\n",
            "Cost after 299138 iterations : Training Loss =  0.028071804716896374; Validation Loss = 0.03469656215749101\n",
            "Cost after 299139 iterations : Training Loss =  0.02807179500371015; Validation Loss = 0.03469655381368205\n",
            "Cost after 299140 iterations : Training Loss =  0.028071785290634782; Validation Loss = 0.03469654546995653\n",
            "Cost after 299141 iterations : Training Loss =  0.02807177557767023; Validation Loss = 0.0346965371263142\n",
            "Cost after 299142 iterations : Training Loss =  0.0280717658648166; Validation Loss = 0.03469652878275559\n",
            "Cost after 299143 iterations : Training Loss =  0.02807175615207372; Validation Loss = 0.034696520439280405\n",
            "Cost after 299144 iterations : Training Loss =  0.028071746439441853; Validation Loss = 0.034696512095888934\n",
            "Cost after 299145 iterations : Training Loss =  0.028071736726920742; Validation Loss = 0.034696503752581064\n",
            "Cost after 299146 iterations : Training Loss =  0.02807172701451045; Validation Loss = 0.03469649540935659\n",
            "Cost after 299147 iterations : Training Loss =  0.028071717302210922; Validation Loss = 0.03469648706621523\n",
            "Cost after 299148 iterations : Training Loss =  0.028071707590022386; Validation Loss = 0.03469647872315768\n",
            "Cost after 299149 iterations : Training Loss =  0.028071697877944606; Validation Loss = 0.034696470380183725\n",
            "Cost after 299150 iterations : Training Loss =  0.02807168816597778; Validation Loss = 0.034696462037293055\n",
            "Cost after 299151 iterations : Training Loss =  0.028071678454121774; Validation Loss = 0.034696453694485985\n",
            "Cost after 299152 iterations : Training Loss =  0.028071668742376668; Validation Loss = 0.034696445351762806\n",
            "Cost after 299153 iterations : Training Loss =  0.02807165903074219; Validation Loss = 0.03469643700912267\n",
            "Cost after 299154 iterations : Training Loss =  0.028071649319218718; Validation Loss = 0.03469642866656616\n",
            "Cost after 299155 iterations : Training Loss =  0.02807163960780602; Validation Loss = 0.03469642032409342\n",
            "Cost after 299156 iterations : Training Loss =  0.028071629896504226; Validation Loss = 0.03469641198170396\n",
            "Cost after 299157 iterations : Training Loss =  0.02807162018531319; Validation Loss = 0.03469640363939807\n",
            "Cost after 299158 iterations : Training Loss =  0.028071610474232924; Validation Loss = 0.0346963952971758\n",
            "Cost after 299159 iterations : Training Loss =  0.02807160076326362; Validation Loss = 0.03469638695503706\n",
            "Cost after 299160 iterations : Training Loss =  0.028071591052405136; Validation Loss = 0.034696378612981506\n",
            "Cost after 299161 iterations : Training Loss =  0.02807158134165742; Validation Loss = 0.03469637027100971\n",
            "Cost after 299162 iterations : Training Loss =  0.028071571631020503; Validation Loss = 0.034696361929120983\n",
            "Cost after 299163 iterations : Training Loss =  0.0280715619204945; Validation Loss = 0.03469635358731602\n",
            "Cost after 299164 iterations : Training Loss =  0.028071552210079242; Validation Loss = 0.03469634524559459\n",
            "Cost after 299165 iterations : Training Loss =  0.02807154249977475; Validation Loss = 0.0346963369039567\n",
            "Cost after 299166 iterations : Training Loss =  0.028071532789581223; Validation Loss = 0.034696328562402505\n",
            "Cost after 299167 iterations : Training Loss =  0.02807152307949838; Validation Loss = 0.03469632022093131\n",
            "Cost after 299168 iterations : Training Loss =  0.028071513369526447; Validation Loss = 0.03469631187954364\n",
            "Cost after 299169 iterations : Training Loss =  0.02807150365966528; Validation Loss = 0.03469630353823982\n",
            "Cost after 299170 iterations : Training Loss =  0.028071493949915004; Validation Loss = 0.03469629519701916\n",
            "Cost after 299171 iterations : Training Loss =  0.028071484240275484; Validation Loss = 0.03469628685588228\n",
            "Cost after 299172 iterations : Training Loss =  0.028071474530746782; Validation Loss = 0.034696278514828476\n",
            "Cost after 299173 iterations : Training Loss =  0.028071464821328832; Validation Loss = 0.0346962701738585\n",
            "Cost after 299174 iterations : Training Loss =  0.028071455112021772; Validation Loss = 0.03469626183297191\n",
            "Cost after 299175 iterations : Training Loss =  0.02807144540282542; Validation Loss = 0.03469625349216895\n",
            "Cost after 299176 iterations : Training Loss =  0.028071435693740023; Validation Loss = 0.03469624515144945\n",
            "Cost after 299177 iterations : Training Loss =  0.02807142598476517; Validation Loss = 0.0346962368108135\n",
            "Cost after 299178 iterations : Training Loss =  0.028071416275901373; Validation Loss = 0.03469622847026083\n",
            "Cost after 299179 iterations : Training Loss =  0.028071406567148315; Validation Loss = 0.03469622012979159\n",
            "Cost after 299180 iterations : Training Loss =  0.02807139685850594; Validation Loss = 0.03469621178940604\n",
            "Cost after 299181 iterations : Training Loss =  0.028071387149974484; Validation Loss = 0.03469620344910361\n",
            "Cost after 299182 iterations : Training Loss =  0.028071377441553764; Validation Loss = 0.0346961951088852\n",
            "Cost after 299183 iterations : Training Loss =  0.02807136773324384; Validation Loss = 0.034696186768749804\n",
            "Cost after 299184 iterations : Training Loss =  0.028071358025044665; Validation Loss = 0.03469617842869799\n",
            "Cost after 299185 iterations : Training Loss =  0.028071348316956315; Validation Loss = 0.034696170088729716\n",
            "Cost after 299186 iterations : Training Loss =  0.028071338608978768; Validation Loss = 0.034696161748844954\n",
            "Cost after 299187 iterations : Training Loss =  0.028071328901111955; Validation Loss = 0.034696153409043286\n",
            "Cost after 299188 iterations : Training Loss =  0.028071319193356082; Validation Loss = 0.03469614506932511\n",
            "Cost after 299189 iterations : Training Loss =  0.02807130948571078; Validation Loss = 0.03469613672969075\n",
            "Cost after 299190 iterations : Training Loss =  0.028071299778176392; Validation Loss = 0.034696128390139726\n",
            "Cost after 299191 iterations : Training Loss =  0.028071290070752743; Validation Loss = 0.03469612005067209\n",
            "Cost after 299192 iterations : Training Loss =  0.02807128036343988; Validation Loss = 0.03469611171128797\n",
            "Cost after 299193 iterations : Training Loss =  0.028071270656237778; Validation Loss = 0.034696103371987384\n",
            "Cost after 299194 iterations : Training Loss =  0.02807126094914645; Validation Loss = 0.03469609503276995\n",
            "Cost after 299195 iterations : Training Loss =  0.028071251242166015; Validation Loss = 0.03469608669363654\n",
            "Cost after 299196 iterations : Training Loss =  0.02807124153529627; Validation Loss = 0.03469607835458623\n",
            "Cost after 299197 iterations : Training Loss =  0.028071231828537286; Validation Loss = 0.03469607001561949\n",
            "Cost after 299198 iterations : Training Loss =  0.028071222121888985; Validation Loss = 0.03469606167673616\n",
            "Cost after 299199 iterations : Training Loss =  0.0280712124153515; Validation Loss = 0.034696053337936295\n",
            "Cost after 299200 iterations : Training Loss =  0.02807120270892478; Validation Loss = 0.03469604499921995\n",
            "Cost after 299201 iterations : Training Loss =  0.028071193002608907; Validation Loss = 0.03469603666058699\n",
            "Cost after 299202 iterations : Training Loss =  0.028071183296403752; Validation Loss = 0.03469602832203754\n",
            "Cost after 299203 iterations : Training Loss =  0.028071173590309348; Validation Loss = 0.03469601998357174\n",
            "Cost after 299204 iterations : Training Loss =  0.028071163884325637; Validation Loss = 0.03469601164518909\n",
            "Cost after 299205 iterations : Training Loss =  0.028071154178452788; Validation Loss = 0.03469600330688987\n",
            "Cost after 299206 iterations : Training Loss =  0.028071144472690525; Validation Loss = 0.03469599496867413\n",
            "Cost after 299207 iterations : Training Loss =  0.028071134767039208; Validation Loss = 0.03469598663054191\n",
            "Cost after 299208 iterations : Training Loss =  0.02807112506149853; Validation Loss = 0.03469597829249324\n",
            "Cost after 299209 iterations : Training Loss =  0.02807111535606865; Validation Loss = 0.03469596995452793\n",
            "Cost after 299210 iterations : Training Loss =  0.028071105650749537; Validation Loss = 0.03469596161664578\n",
            "Cost after 299211 iterations : Training Loss =  0.028071095945541184; Validation Loss = 0.03469595327884744\n",
            "Cost after 299212 iterations : Training Loss =  0.02807108624044352; Validation Loss = 0.03469594494113222\n",
            "Cost after 299213 iterations : Training Loss =  0.028071076535456645; Validation Loss = 0.034695936603500524\n",
            "Cost after 299214 iterations : Training Loss =  0.028071066830580457; Validation Loss = 0.0346959282659526\n",
            "Cost after 299215 iterations : Training Loss =  0.028071057125815084; Validation Loss = 0.0346959199284878\n",
            "Cost after 299216 iterations : Training Loss =  0.028071047421160476; Validation Loss = 0.03469591159110641\n",
            "Cost after 299217 iterations : Training Loss =  0.028071037716616454; Validation Loss = 0.03469590325380849\n",
            "Cost after 299218 iterations : Training Loss =  0.0280710280121832; Validation Loss = 0.03469589491659409\n",
            "Cost after 299219 iterations : Training Loss =  0.028071018307860806; Validation Loss = 0.034695886579463216\n",
            "Cost after 299220 iterations : Training Loss =  0.02807100860364911; Validation Loss = 0.03469587824241546\n",
            "Cost after 299221 iterations : Training Loss =  0.02807099889954804; Validation Loss = 0.03469586990545139\n",
            "Cost after 299222 iterations : Training Loss =  0.028070989195557818; Validation Loss = 0.03469586156857025\n",
            "Cost after 299223 iterations : Training Loss =  0.028070979491678322; Validation Loss = 0.03469585323177332\n",
            "Cost after 299224 iterations : Training Loss =  0.028070969787909465; Validation Loss = 0.03469584489505954\n",
            "Cost after 299225 iterations : Training Loss =  0.02807096008425147; Validation Loss = 0.034695836558428915\n",
            "Cost after 299226 iterations : Training Loss =  0.028070950380704077; Validation Loss = 0.03469582822188197\n",
            "Cost after 299227 iterations : Training Loss =  0.028070940677267366; Validation Loss = 0.03469581988541819\n",
            "Cost after 299228 iterations : Training Loss =  0.028070930973941498; Validation Loss = 0.03469581154903802\n",
            "Cost after 299229 iterations : Training Loss =  0.028070921270726367; Validation Loss = 0.03469580321274156\n",
            "Cost after 299230 iterations : Training Loss =  0.028070911567621856; Validation Loss = 0.03469579487652806\n",
            "Cost after 299231 iterations : Training Loss =  0.028070901864628076; Validation Loss = 0.034695786540398246\n",
            "Cost after 299232 iterations : Training Loss =  0.028070892161744968; Validation Loss = 0.034695778204351775\n",
            "Cost after 299233 iterations : Training Loss =  0.028070882458972705; Validation Loss = 0.03469576986838846\n",
            "Cost after 299234 iterations : Training Loss =  0.028070872756311083; Validation Loss = 0.03469576153250868\n",
            "Cost after 299235 iterations : Training Loss =  0.02807086305376016; Validation Loss = 0.03469575319671257\n",
            "Cost after 299236 iterations : Training Loss =  0.028070853351320077; Validation Loss = 0.03469574486099974\n",
            "Cost after 299237 iterations : Training Loss =  0.028070843648990467; Validation Loss = 0.034695736525370384\n",
            "Cost after 299238 iterations : Training Loss =  0.028070833946771723; Validation Loss = 0.03469572818982428\n",
            "Cost after 299239 iterations : Training Loss =  0.028070824244663638; Validation Loss = 0.03469571985436186\n",
            "Cost after 299240 iterations : Training Loss =  0.028070814542666304; Validation Loss = 0.034695711518982424\n",
            "Cost after 299241 iterations : Training Loss =  0.028070804840779583; Validation Loss = 0.03469570318368691\n",
            "Cost after 299242 iterations : Training Loss =  0.028070795139003582; Validation Loss = 0.03469569484847434\n",
            "Cost after 299243 iterations : Training Loss =  0.02807078543733833; Validation Loss = 0.03469568651334539\n",
            "Cost after 299244 iterations : Training Loss =  0.028070775735783763; Validation Loss = 0.034695678178299806\n",
            "Cost after 299245 iterations : Training Loss =  0.028070766034339743; Validation Loss = 0.03469566984333747\n",
            "Cost after 299246 iterations : Training Loss =  0.028070756333006763; Validation Loss = 0.03469566150845914\n",
            "Cost after 299247 iterations : Training Loss =  0.028070746631784133; Validation Loss = 0.03469565317366357\n",
            "Cost after 299248 iterations : Training Loss =  0.02807073693067227; Validation Loss = 0.034695644838951746\n",
            "Cost after 299249 iterations : Training Loss =  0.02807072722967119; Validation Loss = 0.03469563650432282\n",
            "Cost after 299250 iterations : Training Loss =  0.02807071752878071; Validation Loss = 0.03469562816977759\n",
            "Cost after 299251 iterations : Training Loss =  0.028070707828000998; Validation Loss = 0.03469561983531611\n",
            "Cost after 299252 iterations : Training Loss =  0.02807069812733189; Validation Loss = 0.03469561150093768\n",
            "Cost after 299253 iterations : Training Loss =  0.028070688426773526; Validation Loss = 0.034695603166642476\n",
            "Cost after 299254 iterations : Training Loss =  0.028070678726325765; Validation Loss = 0.034695594832430954\n",
            "Cost after 299255 iterations : Training Loss =  0.0280706690259888; Validation Loss = 0.03469558649830283\n",
            "Cost after 299256 iterations : Training Loss =  0.02807065932576244; Validation Loss = 0.03469557816425797\n",
            "Cost after 299257 iterations : Training Loss =  0.028070649625646805; Validation Loss = 0.03469556983029664\n",
            "Cost after 299258 iterations : Training Loss =  0.02807063992564175; Validation Loss = 0.03469556149641855\n",
            "Cost after 299259 iterations : Training Loss =  0.028070630225747423; Validation Loss = 0.03469555316262412\n",
            "Cost after 299260 iterations : Training Loss =  0.02807062052596379; Validation Loss = 0.03469554482891289\n",
            "Cost after 299261 iterations : Training Loss =  0.02807061082629077; Validation Loss = 0.03469553649528496\n",
            "Cost after 299262 iterations : Training Loss =  0.028070601126728437; Validation Loss = 0.03469552816174069\n",
            "Cost after 299263 iterations : Training Loss =  0.028070591427276914; Validation Loss = 0.034695519828279626\n",
            "Cost after 299264 iterations : Training Loss =  0.028070581727935796; Validation Loss = 0.034695511494901986\n",
            "Cost after 299265 iterations : Training Loss =  0.02807057202870554; Validation Loss = 0.03469550316160751\n",
            "Cost after 299266 iterations : Training Loss =  0.028070562329585852; Validation Loss = 0.034695494828396685\n",
            "Cost after 299267 iterations : Training Loss =  0.0280705526305769; Validation Loss = 0.03469548649526917\n",
            "Cost after 299268 iterations : Training Loss =  0.028070542931678495; Validation Loss = 0.03469547816222476\n",
            "Cost after 299269 iterations : Training Loss =  0.028070533232890833; Validation Loss = 0.03469546982926388\n",
            "Cost after 299270 iterations : Training Loss =  0.02807052353421375; Validation Loss = 0.034695461496386554\n",
            "Cost after 299271 iterations : Training Loss =  0.0280705138356474; Validation Loss = 0.03469545316359218\n",
            "Cost after 299272 iterations : Training Loss =  0.02807050413719171; Validation Loss = 0.03469544483088132\n",
            "Cost after 299273 iterations : Training Loss =  0.028070494438846565; Validation Loss = 0.03469543649825401\n",
            "Cost after 299274 iterations : Training Loss =  0.028070484740612134; Validation Loss = 0.034695428165710016\n",
            "Cost after 299275 iterations : Training Loss =  0.02807047504248839; Validation Loss = 0.03469541983324922\n",
            "Cost after 299276 iterations : Training Loss =  0.028070465344475274; Validation Loss = 0.03469541150087196\n",
            "Cost after 299277 iterations : Training Loss =  0.02807045564657276; Validation Loss = 0.03469540316857816\n",
            "Cost after 299278 iterations : Training Loss =  0.028070445948780862; Validation Loss = 0.034695394836367584\n",
            "Cost after 299279 iterations : Training Loss =  0.028070436251099742; Validation Loss = 0.03469538650424035\n",
            "Cost after 299280 iterations : Training Loss =  0.028070426553529062; Validation Loss = 0.03469537817219647\n",
            "Cost after 299281 iterations : Training Loss =  0.028070416856069164; Validation Loss = 0.03469536984023603\n",
            "Cost after 299282 iterations : Training Loss =  0.028070407158719907; Validation Loss = 0.03469536150835918\n",
            "Cost after 299283 iterations : Training Loss =  0.028070397461481204; Validation Loss = 0.0346953531765656\n",
            "Cost after 299284 iterations : Training Loss =  0.028070387764353177; Validation Loss = 0.03469534484485529\n",
            "Cost after 299285 iterations : Training Loss =  0.028070378067335776; Validation Loss = 0.03469533651322812\n",
            "Cost after 299286 iterations : Training Loss =  0.028070368370429068; Validation Loss = 0.034695328181684644\n",
            "Cost after 299287 iterations : Training Loss =  0.028070358673632993; Validation Loss = 0.034695319850224574\n",
            "Cost after 299288 iterations : Training Loss =  0.02807034897694736; Validation Loss = 0.03469531151884747\n",
            "Cost after 299289 iterations : Training Loss =  0.028070339280372537; Validation Loss = 0.03469530318755404\n",
            "Cost after 299290 iterations : Training Loss =  0.028070329583908232; Validation Loss = 0.03469529485634407\n",
            "Cost after 299291 iterations : Training Loss =  0.028070319887554644; Validation Loss = 0.034695286525217264\n",
            "Cost after 299292 iterations : Training Loss =  0.028070310191311593; Validation Loss = 0.03469527819417369\n",
            "Cost after 299293 iterations : Training Loss =  0.028070300495179206; Validation Loss = 0.034695269863213495\n",
            "Cost after 299294 iterations : Training Loss =  0.028070290799157478; Validation Loss = 0.034695261532336555\n",
            "Cost after 299295 iterations : Training Loss =  0.02807028110324631; Validation Loss = 0.03469525320154291\n",
            "Cost after 299296 iterations : Training Loss =  0.028070271407445724; Validation Loss = 0.034695244870832954\n",
            "Cost after 299297 iterations : Training Loss =  0.028070261711755806; Validation Loss = 0.034695236540206155\n",
            "Cost after 299298 iterations : Training Loss =  0.0280702520161765; Validation Loss = 0.034695228209662594\n",
            "Cost after 299299 iterations : Training Loss =  0.028070242320707747; Validation Loss = 0.03469521987920248\n",
            "Cost after 299300 iterations : Training Loss =  0.02807023262534967; Validation Loss = 0.03469521154882564\n",
            "Cost after 299301 iterations : Training Loss =  0.028070222930102175; Validation Loss = 0.03469520321853215\n",
            "Cost after 299302 iterations : Training Loss =  0.028070213234965252; Validation Loss = 0.03469519488832179\n",
            "Cost after 299303 iterations : Training Loss =  0.028070203539938908; Validation Loss = 0.03469518655819506\n",
            "Cost after 299304 iterations : Training Loss =  0.028070193845023367; Validation Loss = 0.03469517822815136\n",
            "Cost after 299305 iterations : Training Loss =  0.028070184150218187; Validation Loss = 0.03469516989819121\n",
            "Cost after 299306 iterations : Training Loss =  0.028070174455523764; Validation Loss = 0.03469516156831442\n",
            "Cost after 299307 iterations : Training Loss =  0.028070164760939934; Validation Loss = 0.03469515323852106\n",
            "Cost after 299308 iterations : Training Loss =  0.028070155066466658; Validation Loss = 0.03469514490881078\n",
            "Cost after 299309 iterations : Training Loss =  0.02807014537210395; Validation Loss = 0.03469513657918409\n",
            "Cost after 299310 iterations : Training Loss =  0.02807013567785174; Validation Loss = 0.03469512824964053\n",
            "Cost after 299311 iterations : Training Loss =  0.028070125983710205; Validation Loss = 0.03469511992018033\n",
            "Cost after 299312 iterations : Training Loss =  0.02807011628967935; Validation Loss = 0.034695111590803594\n",
            "Cost after 299313 iterations : Training Loss =  0.02807010659575898; Validation Loss = 0.03469510326151013\n",
            "Cost after 299314 iterations : Training Loss =  0.028070096901949212; Validation Loss = 0.034695094932299914\n",
            "Cost after 299315 iterations : Training Loss =  0.02807008720825011; Validation Loss = 0.03469508660317308\n",
            "Cost after 299316 iterations : Training Loss =  0.028070077514661486; Validation Loss = 0.03469507827412963\n",
            "Cost after 299317 iterations : Training Loss =  0.028070067821183497; Validation Loss = 0.034695069945169674\n",
            "Cost after 299318 iterations : Training Loss =  0.028070058127816117; Validation Loss = 0.03469506161629275\n",
            "Cost after 299319 iterations : Training Loss =  0.028070048434559212; Validation Loss = 0.03469505328749929\n",
            "Cost after 299320 iterations : Training Loss =  0.028070038741413037; Validation Loss = 0.03469504495878882\n",
            "Cost after 299321 iterations : Training Loss =  0.02807002904837723; Validation Loss = 0.0346950366301619\n",
            "Cost after 299322 iterations : Training Loss =  0.028070019355452124; Validation Loss = 0.03469502830161861\n",
            "Cost after 299323 iterations : Training Loss =  0.028070009662637622; Validation Loss = 0.03469501997315841\n",
            "Cost after 299324 iterations : Training Loss =  0.028069999969933594; Validation Loss = 0.03469501164478143\n",
            "Cost after 299325 iterations : Training Loss =  0.02806999027734033; Validation Loss = 0.034695003316487755\n",
            "Cost after 299326 iterations : Training Loss =  0.028069980584857438; Validation Loss = 0.03469499498827738\n",
            "Cost after 299327 iterations : Training Loss =  0.028069970892485117; Validation Loss = 0.034694986660150605\n",
            "Cost after 299328 iterations : Training Loss =  0.028069961200223488; Validation Loss = 0.03469497833210681\n",
            "Cost after 299329 iterations : Training Loss =  0.02806995150807229; Validation Loss = 0.03469497000414665\n",
            "Cost after 299330 iterations : Training Loss =  0.028069941816031653; Validation Loss = 0.03469496167626964\n",
            "Cost after 299331 iterations : Training Loss =  0.02806993212410169; Validation Loss = 0.03469495334847588\n",
            "Cost after 299332 iterations : Training Loss =  0.02806992243228219; Validation Loss = 0.03469494502076553\n",
            "Cost after 299333 iterations : Training Loss =  0.02806991274057336; Validation Loss = 0.03469493669313822\n",
            "Cost after 299334 iterations : Training Loss =  0.02806990304897496; Validation Loss = 0.034694928365594346\n",
            "Cost after 299335 iterations : Training Loss =  0.028069893357487184; Validation Loss = 0.03469492003813363\n",
            "Cost after 299336 iterations : Training Loss =  0.028069883666109956; Validation Loss = 0.03469491171075641\n",
            "Cost after 299337 iterations : Training Loss =  0.028069873974843288; Validation Loss = 0.03469490338346232\n",
            "Cost after 299338 iterations : Training Loss =  0.02806986428368713; Validation Loss = 0.034694895056251715\n",
            "Cost after 299339 iterations : Training Loss =  0.028069854592641504; Validation Loss = 0.034694886729123985\n",
            "Cost after 299340 iterations : Training Loss =  0.02806984490170637; Validation Loss = 0.034694878402080034\n",
            "Cost after 299341 iterations : Training Loss =  0.028069835210881983; Validation Loss = 0.03469487007511914\n",
            "Cost after 299342 iterations : Training Loss =  0.028069825520167926; Validation Loss = 0.034694861748241594\n",
            "Cost after 299343 iterations : Training Loss =  0.028069815829564554; Validation Loss = 0.03469485342144717\n",
            "Cost after 299344 iterations : Training Loss =  0.02806980613907166; Validation Loss = 0.034694845094736255\n",
            "Cost after 299345 iterations : Training Loss =  0.028069796448689305; Validation Loss = 0.03469483676810849\n",
            "Cost after 299346 iterations : Training Loss =  0.02806978675841753; Validation Loss = 0.03469482844156398\n",
            "Cost after 299347 iterations : Training Loss =  0.02806977706825622; Validation Loss = 0.03469482011510302\n",
            "Cost after 299348 iterations : Training Loss =  0.028069767378205476; Validation Loss = 0.03469481178872525\n",
            "Cost after 299349 iterations : Training Loss =  0.02806975768826528; Validation Loss = 0.03469480346243077\n",
            "Cost after 299350 iterations : Training Loss =  0.02806974799843563; Validation Loss = 0.03469479513621938\n",
            "Cost after 299351 iterations : Training Loss =  0.028069738308716473; Validation Loss = 0.03469478681009136\n",
            "Cost after 299352 iterations : Training Loss =  0.02806972861910784; Validation Loss = 0.03469477848404674\n",
            "Cost after 299353 iterations : Training Loss =  0.028069718929609785; Validation Loss = 0.0346947701580853\n",
            "Cost after 299354 iterations : Training Loss =  0.028069709240222146; Validation Loss = 0.034694761832207335\n",
            "Cost after 299355 iterations : Training Loss =  0.02806969955094512; Validation Loss = 0.03469475350641215\n",
            "Cost after 299356 iterations : Training Loss =  0.02806968986177857; Validation Loss = 0.034694745180700697\n",
            "Cost after 299357 iterations : Training Loss =  0.028069680172722548; Validation Loss = 0.034694736855072454\n",
            "Cost after 299358 iterations : Training Loss =  0.028069670483777075; Validation Loss = 0.03469472852952728\n",
            "Cost after 299359 iterations : Training Loss =  0.028069660794942053; Validation Loss = 0.03469472020406545\n",
            "Cost after 299360 iterations : Training Loss =  0.02806965110621765; Validation Loss = 0.03469471187868689\n",
            "Cost after 299361 iterations : Training Loss =  0.02806964141760368; Validation Loss = 0.0346947035533919\n",
            "Cost after 299362 iterations : Training Loss =  0.02806963172910021; Validation Loss = 0.03469469522817992\n",
            "Cost after 299363 iterations : Training Loss =  0.028069622040707273; Validation Loss = 0.03469468690305116\n",
            "Cost after 299364 iterations : Training Loss =  0.02806961235242492; Validation Loss = 0.03469467857800564\n",
            "Cost after 299365 iterations : Training Loss =  0.028069602664252923; Validation Loss = 0.03469467025304366\n",
            "Cost after 299366 iterations : Training Loss =  0.028069592976191493; Validation Loss = 0.03469466192816492\n",
            "Cost after 299367 iterations : Training Loss =  0.028069583288240624; Validation Loss = 0.034694653603369464\n",
            "Cost after 299368 iterations : Training Loss =  0.028069573600400292; Validation Loss = 0.03469464527865701\n",
            "Cost after 299369 iterations : Training Loss =  0.028069563912670317; Validation Loss = 0.0346946369540278\n",
            "Cost after 299370 iterations : Training Loss =  0.02806955422505096; Validation Loss = 0.03469462862948205\n",
            "Cost after 299371 iterations : Training Loss =  0.028069544537542; Validation Loss = 0.03469462030501956\n",
            "Cost after 299372 iterations : Training Loss =  0.02806953485014364; Validation Loss = 0.03469461198064025\n",
            "Cost after 299373 iterations : Training Loss =  0.02806952516285564; Validation Loss = 0.034694603656344226\n",
            "Cost after 299374 iterations : Training Loss =  0.028069515475678212; Validation Loss = 0.034694595332131296\n",
            "Cost after 299375 iterations : Training Loss =  0.028069505788611303; Validation Loss = 0.03469458700800185\n",
            "Cost after 299376 iterations : Training Loss =  0.02806949610165485; Validation Loss = 0.03469457868395552\n",
            "Cost after 299377 iterations : Training Loss =  0.02806948641480889; Validation Loss = 0.03469457035999266\n",
            "Cost after 299378 iterations : Training Loss =  0.028069476728073445; Validation Loss = 0.03469456203611292\n",
            "Cost after 299379 iterations : Training Loss =  0.028069467041448457; Validation Loss = 0.03469455371231631\n",
            "Cost after 299380 iterations : Training Loss =  0.028069457354933866; Validation Loss = 0.03469454538860308\n",
            "Cost after 299381 iterations : Training Loss =  0.02806944766852986; Validation Loss = 0.03469453706497306\n",
            "Cost after 299382 iterations : Training Loss =  0.028069437982236344; Validation Loss = 0.03469452874142616\n",
            "Cost after 299383 iterations : Training Loss =  0.0280694282960533; Validation Loss = 0.03469452041796275\n",
            "Cost after 299384 iterations : Training Loss =  0.028069418609980707; Validation Loss = 0.03469451209458269\n",
            "Cost after 299385 iterations : Training Loss =  0.028069408924018626; Validation Loss = 0.03469450377128574\n",
            "Cost after 299386 iterations : Training Loss =  0.028069399238167005; Validation Loss = 0.034694495448071766\n",
            "Cost after 299387 iterations : Training Loss =  0.028069389552425807; Validation Loss = 0.03469448712494112\n",
            "Cost after 299388 iterations : Training Loss =  0.028069379866795058; Validation Loss = 0.03469447880189381\n",
            "Cost after 299389 iterations : Training Loss =  0.02806937018127478; Validation Loss = 0.034694470478929734\n",
            "Cost after 299390 iterations : Training Loss =  0.02806936049586509; Validation Loss = 0.034694462156048836\n",
            "Cost after 299391 iterations : Training Loss =  0.02806935081056586; Validation Loss = 0.034694453833251176\n",
            "Cost after 299392 iterations : Training Loss =  0.028069341125376988; Validation Loss = 0.034694445510536846\n",
            "Cost after 299393 iterations : Training Loss =  0.02806933144029852; Validation Loss = 0.03469443718790565\n",
            "Cost after 299394 iterations : Training Loss =  0.028069321755330626; Validation Loss = 0.0346944288653578\n",
            "Cost after 299395 iterations : Training Loss =  0.028069312070473355; Validation Loss = 0.03469442054289321\n",
            "Cost after 299396 iterations : Training Loss =  0.028069302385726302; Validation Loss = 0.03469441222051169\n",
            "Cost after 299397 iterations : Training Loss =  0.02806929270108982; Validation Loss = 0.03469440389821355\n",
            "Cost after 299398 iterations : Training Loss =  0.02806928301656364; Validation Loss = 0.03469439557599861\n",
            "Cost after 299399 iterations : Training Loss =  0.02806927333214802; Validation Loss = 0.034694387253866726\n",
            "Cost after 299400 iterations : Training Loss =  0.028069263647842884; Validation Loss = 0.0346943789318182\n",
            "Cost after 299401 iterations : Training Loss =  0.028069253963648136; Validation Loss = 0.03469437060985267\n",
            "Cost after 299402 iterations : Training Loss =  0.0280692442795639; Validation Loss = 0.03469436228797049\n",
            "Cost after 299403 iterations : Training Loss =  0.028069234595590107; Validation Loss = 0.03469435396617179\n",
            "Cost after 299404 iterations : Training Loss =  0.028069224911726694; Validation Loss = 0.03469434564445622\n",
            "Cost after 299405 iterations : Training Loss =  0.028069215227973798; Validation Loss = 0.03469433732282387\n",
            "Cost after 299406 iterations : Training Loss =  0.028069205544331313; Validation Loss = 0.03469432900127457\n",
            "Cost after 299407 iterations : Training Loss =  0.02806919586079918; Validation Loss = 0.03469432067980872\n",
            "Cost after 299408 iterations : Training Loss =  0.02806918617737759; Validation Loss = 0.034694312358425766\n",
            "Cost after 299409 iterations : Training Loss =  0.02806917649406651; Validation Loss = 0.03469430403712608\n",
            "Cost after 299410 iterations : Training Loss =  0.028069166810865652; Validation Loss = 0.03469429571591003\n",
            "Cost after 299411 iterations : Training Loss =  0.028069157127775343; Validation Loss = 0.0346942873947768\n",
            "Cost after 299412 iterations : Training Loss =  0.02806914744479542; Validation Loss = 0.034694279073726984\n",
            "Cost after 299413 iterations : Training Loss =  0.028069137761926124; Validation Loss = 0.03469427075276042\n",
            "Cost after 299414 iterations : Training Loss =  0.028069128079167092; Validation Loss = 0.03469426243187694\n",
            "Cost after 299415 iterations : Training Loss =  0.028069118396518437; Validation Loss = 0.03469425411107662\n",
            "Cost after 299416 iterations : Training Loss =  0.028069108713980346; Validation Loss = 0.03469424579035945\n",
            "Cost after 299417 iterations : Training Loss =  0.028069099031552587; Validation Loss = 0.03469423746972571\n",
            "Cost after 299418 iterations : Training Loss =  0.028069089349235188; Validation Loss = 0.03469422914917487\n",
            "Cost after 299419 iterations : Training Loss =  0.028069079667028523; Validation Loss = 0.03469422082870743\n",
            "Cost after 299420 iterations : Training Loss =  0.02806906998493187; Validation Loss = 0.03469421250832308\n",
            "Cost after 299421 iterations : Training Loss =  0.028069060302945946; Validation Loss = 0.03469420418802228\n",
            "Cost after 299422 iterations : Training Loss =  0.028069050621070252; Validation Loss = 0.03469419586780458\n",
            "Cost after 299423 iterations : Training Loss =  0.028069040939305068; Validation Loss = 0.03469418754766991\n",
            "Cost after 299424 iterations : Training Loss =  0.028069031257650288; Validation Loss = 0.0346941792276184\n",
            "Cost after 299425 iterations : Training Loss =  0.028069021576105812; Validation Loss = 0.03469417090765012\n",
            "Cost after 299426 iterations : Training Loss =  0.028069011894671998; Validation Loss = 0.03469416258776499\n",
            "Cost after 299427 iterations : Training Loss =  0.02806900221334835; Validation Loss = 0.03469415426796346\n",
            "Cost after 299428 iterations : Training Loss =  0.028068992532135132; Validation Loss = 0.03469414594824477\n",
            "Cost after 299429 iterations : Training Loss =  0.028068982851032424; Validation Loss = 0.03469413762860948\n",
            "Cost after 299430 iterations : Training Loss =  0.028068973170040094; Validation Loss = 0.034694129309057346\n",
            "Cost after 299431 iterations : Training Loss =  0.02806896348915811; Validation Loss = 0.03469412098958823\n",
            "Cost after 299432 iterations : Training Loss =  0.028068953808386733; Validation Loss = 0.03469411267020222\n",
            "Cost after 299433 iterations : Training Loss =  0.02806894412772544; Validation Loss = 0.03469410435089975\n",
            "Cost after 299434 iterations : Training Loss =  0.028068934447174846; Validation Loss = 0.03469409603168001\n",
            "Cost after 299435 iterations : Training Loss =  0.02806892476673445; Validation Loss = 0.03469408771254364\n",
            "Cost after 299436 iterations : Training Loss =  0.028068915086404557; Validation Loss = 0.03469407939349047\n",
            "Cost after 299437 iterations : Training Loss =  0.028068905406184914; Validation Loss = 0.034694071074520355\n",
            "Cost after 299438 iterations : Training Loss =  0.028068895726075828; Validation Loss = 0.03469406275563348\n",
            "Cost after 299439 iterations : Training Loss =  0.028068886046077; Validation Loss = 0.034694054436830005\n",
            "Cost after 299440 iterations : Training Loss =  0.02806887636618865; Validation Loss = 0.03469404611810994\n",
            "Cost after 299441 iterations : Training Loss =  0.028068866686410617; Validation Loss = 0.034694037799472686\n",
            "Cost after 299442 iterations : Training Loss =  0.028068857006743015; Validation Loss = 0.03469402948091866\n",
            "Cost after 299443 iterations : Training Loss =  0.028068847327185873; Validation Loss = 0.03469402116244794\n",
            "Cost after 299444 iterations : Training Loss =  0.028068837647739014; Validation Loss = 0.03469401284406027\n",
            "Cost after 299445 iterations : Training Loss =  0.028068827968402422; Validation Loss = 0.03469400452575562\n",
            "Cost after 299446 iterations : Training Loss =  0.02806881828917638; Validation Loss = 0.03469399620753443\n",
            "Cost after 299447 iterations : Training Loss =  0.028068808610060694; Validation Loss = 0.0346939878893962\n",
            "Cost after 299448 iterations : Training Loss =  0.02806879893105543; Validation Loss = 0.034693979571341224\n",
            "Cost after 299449 iterations : Training Loss =  0.028068789252160515; Validation Loss = 0.034693971253369295\n",
            "Cost after 299450 iterations : Training Loss =  0.028068779573375908; Validation Loss = 0.034693962935480875\n",
            "Cost after 299451 iterations : Training Loss =  0.028068769894701667; Validation Loss = 0.034693954617675174\n",
            "Cost after 299452 iterations : Training Loss =  0.028068760216137894; Validation Loss = 0.03469394629995315\n",
            "Cost after 299453 iterations : Training Loss =  0.02806875053768436; Validation Loss = 0.03469393798231397\n",
            "Cost after 299454 iterations : Training Loss =  0.028068740859341273; Validation Loss = 0.0346939296647579\n",
            "Cost after 299455 iterations : Training Loss =  0.028068731181108655; Validation Loss = 0.03469392134728509\n",
            "Cost after 299456 iterations : Training Loss =  0.028068721502986223; Validation Loss = 0.034693913029895496\n",
            "Cost after 299457 iterations : Training Loss =  0.02806871182497423; Validation Loss = 0.03469390471258864\n",
            "Cost after 299458 iterations : Training Loss =  0.028068702147072588; Validation Loss = 0.0346938963953652\n",
            "Cost after 299459 iterations : Training Loss =  0.02806869246928136; Validation Loss = 0.034693888078225196\n",
            "Cost after 299460 iterations : Training Loss =  0.028068682791600456; Validation Loss = 0.03469387976116826\n",
            "Cost after 299461 iterations : Training Loss =  0.028068673114029857; Validation Loss = 0.03469387144419433\n",
            "Cost after 299462 iterations : Training Loss =  0.028068663436569673; Validation Loss = 0.034693863127303326\n",
            "Cost after 299463 iterations : Training Loss =  0.028068653759219787; Validation Loss = 0.03469385481049585\n",
            "Cost after 299464 iterations : Training Loss =  0.02806864408198033; Validation Loss = 0.034693846493771364\n",
            "Cost after 299465 iterations : Training Loss =  0.028068634404851064; Validation Loss = 0.03469383817713007\n",
            "Cost after 299466 iterations : Training Loss =  0.028068624727832333; Validation Loss = 0.03469382986057214\n",
            "Cost after 299467 iterations : Training Loss =  0.028068615050923933; Validation Loss = 0.034693821544097087\n",
            "Cost after 299468 iterations : Training Loss =  0.028068605374125775; Validation Loss = 0.03469381322770511\n",
            "Cost after 299469 iterations : Training Loss =  0.02806859569743794; Validation Loss = 0.0346938049113964\n",
            "Cost after 299470 iterations : Training Loss =  0.02806858602086047; Validation Loss = 0.034693796595171036\n",
            "Cost after 299471 iterations : Training Loss =  0.028068576344393456; Validation Loss = 0.03469378827902858\n",
            "Cost after 299472 iterations : Training Loss =  0.028068566668036695; Validation Loss = 0.03469377996296916\n",
            "Cost after 299473 iterations : Training Loss =  0.02806855699179026; Validation Loss = 0.03469377164699306\n",
            "Cost after 299474 iterations : Training Loss =  0.028068547315654186; Validation Loss = 0.034693763331100365\n",
            "Cost after 299475 iterations : Training Loss =  0.0280685376396285; Validation Loss = 0.034693755015290685\n",
            "Cost after 299476 iterations : Training Loss =  0.028068527963713112; Validation Loss = 0.03469374669956381\n",
            "Cost after 299477 iterations : Training Loss =  0.02806851828790802; Validation Loss = 0.03469373838392057\n",
            "Cost after 299478 iterations : Training Loss =  0.02806850861221331; Validation Loss = 0.03469373006836007\n",
            "Cost after 299479 iterations : Training Loss =  0.02806849893662896; Validation Loss = 0.034693721752882815\n",
            "Cost after 299480 iterations : Training Loss =  0.028068489261154732; Validation Loss = 0.03469371343748863\n",
            "Cost after 299481 iterations : Training Loss =  0.028068479585791048; Validation Loss = 0.034693705122177725\n",
            "Cost after 299482 iterations : Training Loss =  0.02806846991053761; Validation Loss = 0.03469369680694985\n",
            "Cost after 299483 iterations : Training Loss =  0.028068460235394405; Validation Loss = 0.03469368849180532\n",
            "Cost after 299484 iterations : Training Loss =  0.028068450560361648; Validation Loss = 0.0346936801767437\n",
            "Cost after 299485 iterations : Training Loss =  0.028068440885439253; Validation Loss = 0.03469367186176514\n",
            "Cost after 299486 iterations : Training Loss =  0.028068431210627093; Validation Loss = 0.03469366354686994\n",
            "Cost after 299487 iterations : Training Loss =  0.028068421535925158; Validation Loss = 0.03469365523205764\n",
            "Cost after 299488 iterations : Training Loss =  0.02806841186133378; Validation Loss = 0.034693646917328735\n",
            "Cost after 299489 iterations : Training Loss =  0.028068402186852474; Validation Loss = 0.034693638602682694\n",
            "Cost after 299490 iterations : Training Loss =  0.028068392512481565; Validation Loss = 0.03469363028811984\n",
            "Cost after 299491 iterations : Training Loss =  0.02806838283822094; Validation Loss = 0.03469362197363997\n",
            "Cost after 299492 iterations : Training Loss =  0.028068373164070723; Validation Loss = 0.03469361365924365\n",
            "Cost after 299493 iterations : Training Loss =  0.028068363490030727; Validation Loss = 0.034693605344930406\n",
            "Cost after 299494 iterations : Training Loss =  0.028068353816101003; Validation Loss = 0.03469359703069998\n",
            "Cost after 299495 iterations : Training Loss =  0.02806834414228162; Validation Loss = 0.03469358871655281\n",
            "Cost after 299496 iterations : Training Loss =  0.028068334468572563; Validation Loss = 0.034693580402488815\n",
            "Cost after 299497 iterations : Training Loss =  0.02806832479497382; Validation Loss = 0.03469357208850799\n",
            "Cost after 299498 iterations : Training Loss =  0.028068315121485297; Validation Loss = 0.03469356377461018\n",
            "Cost after 299499 iterations : Training Loss =  0.028068305448107123; Validation Loss = 0.034693555460795736\n",
            "Cost after 299500 iterations : Training Loss =  0.028068295774839304; Validation Loss = 0.03469354714706416\n",
            "Cost after 299501 iterations : Training Loss =  0.028068286101681744; Validation Loss = 0.034693538833415805\n",
            "Cost after 299502 iterations : Training Loss =  0.028068276428634412; Validation Loss = 0.03469353051985043\n",
            "Cost after 299503 iterations : Training Loss =  0.028068266755697364; Validation Loss = 0.03469352220636808\n",
            "Cost after 299504 iterations : Training Loss =  0.028068257082870755; Validation Loss = 0.03469351389296891\n",
            "Cost after 299505 iterations : Training Loss =  0.028068247410154314; Validation Loss = 0.03469350557965282\n",
            "Cost after 299506 iterations : Training Loss =  0.028068237737548157; Validation Loss = 0.03469349726641995\n",
            "Cost after 299507 iterations : Training Loss =  0.02806822806505228; Validation Loss = 0.0346934889532699\n",
            "Cost after 299508 iterations : Training Loss =  0.028068218392666693; Validation Loss = 0.03469348064020328\n",
            "Cost after 299509 iterations : Training Loss =  0.028068208720391376; Validation Loss = 0.03469347232721948\n",
            "Cost after 299510 iterations : Training Loss =  0.028068199048226345; Validation Loss = 0.034693464014318866\n",
            "Cost after 299511 iterations : Training Loss =  0.028068189376171712; Validation Loss = 0.034693455701501265\n",
            "Cost after 299512 iterations : Training Loss =  0.028068179704227262; Validation Loss = 0.03469344738876692\n",
            "Cost after 299513 iterations : Training Loss =  0.02806817003239304; Validation Loss = 0.0346934390761157\n",
            "Cost after 299514 iterations : Training Loss =  0.0280681603606692; Validation Loss = 0.034693430763547486\n",
            "Cost after 299515 iterations : Training Loss =  0.028068150689055472; Validation Loss = 0.034693422451062335\n",
            "Cost after 299516 iterations : Training Loss =  0.02806814101755214; Validation Loss = 0.03469341413866035\n",
            "Cost after 299517 iterations : Training Loss =  0.028068131346159007; Validation Loss = 0.03469340582634151\n",
            "Cost after 299518 iterations : Training Loss =  0.02806812167487623; Validation Loss = 0.034693397514105766\n",
            "Cost after 299519 iterations : Training Loss =  0.02806811200370374; Validation Loss = 0.0346933892019532\n",
            "Cost after 299520 iterations : Training Loss =  0.028068102332641377; Validation Loss = 0.03469338088988349\n",
            "Cost after 299521 iterations : Training Loss =  0.02806809266168938; Validation Loss = 0.034693372577896765\n",
            "Cost after 299522 iterations : Training Loss =  0.028068082990847593; Validation Loss = 0.034693364265993586\n",
            "Cost after 299523 iterations : Training Loss =  0.028068073320116096; Validation Loss = 0.034693355954172896\n",
            "Cost after 299524 iterations : Training Loss =  0.02806806364949492; Validation Loss = 0.03469334764243594\n",
            "Cost after 299525 iterations : Training Loss =  0.028068053978983775; Validation Loss = 0.03469333933078138\n",
            "Cost after 299526 iterations : Training Loss =  0.028068044308583034; Validation Loss = 0.03469333101921054\n",
            "Cost after 299527 iterations : Training Loss =  0.02806803463829267; Validation Loss = 0.03469332270772253\n",
            "Cost after 299528 iterations : Training Loss =  0.02806802496811236; Validation Loss = 0.03469331439631753\n",
            "Cost after 299529 iterations : Training Loss =  0.028068015298042405; Validation Loss = 0.034693306084995816\n",
            "Cost after 299530 iterations : Training Loss =  0.02806800562808266; Validation Loss = 0.034693297773756994\n",
            "Cost after 299531 iterations : Training Loss =  0.02806799595823315; Validation Loss = 0.03469328946260121\n",
            "Cost after 299532 iterations : Training Loss =  0.02806798628849391; Validation Loss = 0.03469328115152867\n",
            "Cost after 299533 iterations : Training Loss =  0.028067976618864877; Validation Loss = 0.034693272840539406\n",
            "Cost after 299534 iterations : Training Loss =  0.028067966949346216; Validation Loss = 0.03469326452963277\n",
            "Cost after 299535 iterations : Training Loss =  0.028067957279937758; Validation Loss = 0.034693256218809546\n",
            "Cost after 299536 iterations : Training Loss =  0.028067947610639365; Validation Loss = 0.03469324790806904\n",
            "Cost after 299537 iterations : Training Loss =  0.028067937941451314; Validation Loss = 0.034693239597411885\n",
            "Cost after 299538 iterations : Training Loss =  0.028067928272373543; Validation Loss = 0.03469323128683766\n",
            "Cost after 299539 iterations : Training Loss =  0.028067918603406017; Validation Loss = 0.03469322297634671\n",
            "Cost after 299540 iterations : Training Loss =  0.02806790893454867; Validation Loss = 0.034693214665938915\n",
            "Cost after 299541 iterations : Training Loss =  0.028067899265801506; Validation Loss = 0.034693206355613904\n",
            "Cost after 299542 iterations : Training Loss =  0.028067889597164674; Validation Loss = 0.0346931980453722\n",
            "Cost after 299543 iterations : Training Loss =  0.028067879928638073; Validation Loss = 0.03469318973521325\n",
            "Cost after 299544 iterations : Training Loss =  0.02806787026022166; Validation Loss = 0.034693181425137595\n",
            "Cost after 299545 iterations : Training Loss =  0.0280678605919154; Validation Loss = 0.03469317311514452\n",
            "Cost after 299546 iterations : Training Loss =  0.028067850923719485; Validation Loss = 0.03469316480523507\n",
            "Cost after 299547 iterations : Training Loss =  0.02806784125563372; Validation Loss = 0.03469315649540831\n",
            "Cost after 299548 iterations : Training Loss =  0.02806783158765818; Validation Loss = 0.034693148185664886\n",
            "Cost after 299549 iterations : Training Loss =  0.028067821919792874; Validation Loss = 0.03469313987600436\n",
            "Cost after 299550 iterations : Training Loss =  0.028067812252037836; Validation Loss = 0.034693131566426894\n",
            "Cost after 299551 iterations : Training Loss =  0.028067802584392912; Validation Loss = 0.034693123256932426\n",
            "Cost after 299552 iterations : Training Loss =  0.028067792916858254; Validation Loss = 0.03469311494752132\n",
            "Cost after 299553 iterations : Training Loss =  0.028067783249433757; Validation Loss = 0.03469310663819294\n",
            "Cost after 299554 iterations : Training Loss =  0.02806777358211947; Validation Loss = 0.03469309832894758\n",
            "Cost after 299555 iterations : Training Loss =  0.02806776391491562; Validation Loss = 0.03469309001978527\n",
            "Cost after 299556 iterations : Training Loss =  0.028067754247821784; Validation Loss = 0.034693081710706175\n",
            "Cost after 299557 iterations : Training Loss =  0.02806774458083822; Validation Loss = 0.034693073401710385\n",
            "Cost after 299558 iterations : Training Loss =  0.028067734913964734; Validation Loss = 0.034693065092797355\n",
            "Cost after 299559 iterations : Training Loss =  0.028067725247201542; Validation Loss = 0.03469305678396736\n",
            "Cost after 299560 iterations : Training Loss =  0.02806771558054852; Validation Loss = 0.03469304847522068\n",
            "Cost after 299561 iterations : Training Loss =  0.02806770591400582; Validation Loss = 0.0346930401665569\n",
            "Cost after 299562 iterations : Training Loss =  0.028067696247573135; Validation Loss = 0.034693031857976314\n",
            "Cost after 299563 iterations : Training Loss =  0.028067686581250705; Validation Loss = 0.034693023549478494\n",
            "Cost after 299564 iterations : Training Loss =  0.028067676915038607; Validation Loss = 0.03469301524106402\n",
            "Cost after 299565 iterations : Training Loss =  0.02806766724893656; Validation Loss = 0.034693006932732064\n",
            "Cost after 299566 iterations : Training Loss =  0.028067657582944685; Validation Loss = 0.034692998624483176\n",
            "Cost after 299567 iterations : Training Loss =  0.028067647917063017; Validation Loss = 0.03469299031631766\n",
            "Cost after 299568 iterations : Training Loss =  0.028067638251291722; Validation Loss = 0.034692982008234965\n",
            "Cost after 299569 iterations : Training Loss =  0.028067628585630337; Validation Loss = 0.03469297370023537\n",
            "Cost after 299570 iterations : Training Loss =  0.02806761892007942; Validation Loss = 0.034692965392318656\n",
            "Cost after 299571 iterations : Training Loss =  0.02806760925463844; Validation Loss = 0.034692957084485096\n",
            "Cost after 299572 iterations : Training Loss =  0.02806759958930769; Validation Loss = 0.034692948776734726\n",
            "Cost after 299573 iterations : Training Loss =  0.028067589924087297; Validation Loss = 0.03469294046906746\n",
            "Cost after 299574 iterations : Training Loss =  0.028067580258977022; Validation Loss = 0.034692932161483114\n",
            "Cost after 299575 iterations : Training Loss =  0.028067570593976843; Validation Loss = 0.03469292385398187\n",
            "Cost after 299576 iterations : Training Loss =  0.028067560929086924; Validation Loss = 0.034692915546563625\n",
            "Cost after 299577 iterations : Training Loss =  0.028067551264307017; Validation Loss = 0.03469290723922837\n",
            "Cost after 299578 iterations : Training Loss =  0.02806754159963745; Validation Loss = 0.03469289893197579\n",
            "Cost after 299579 iterations : Training Loss =  0.028067531935077925; Validation Loss = 0.03469289062480654\n",
            "Cost after 299580 iterations : Training Loss =  0.028067522270628597; Validation Loss = 0.03469288231772009\n",
            "Cost after 299581 iterations : Training Loss =  0.028067512606289494; Validation Loss = 0.034692874010716884\n",
            "Cost after 299582 iterations : Training Loss =  0.02806750294206057; Validation Loss = 0.03469286570379674\n",
            "Cost after 299583 iterations : Training Loss =  0.028067493277941873; Validation Loss = 0.03469285739695957\n",
            "Cost after 299584 iterations : Training Loss =  0.02806748361393313; Validation Loss = 0.03469284909020566\n",
            "Cost after 299585 iterations : Training Loss =  0.028067473950034744; Validation Loss = 0.03469284078353448\n",
            "Cost after 299586 iterations : Training Loss =  0.028067464286246357; Validation Loss = 0.03469283247694654\n",
            "Cost after 299587 iterations : Training Loss =  0.02806745462256834; Validation Loss = 0.034692824170441075\n",
            "Cost after 299588 iterations : Training Loss =  0.02806744495900028; Validation Loss = 0.03469281586401904\n",
            "Cost after 299589 iterations : Training Loss =  0.028067435295542547; Validation Loss = 0.03469280755767988\n",
            "Cost after 299590 iterations : Training Loss =  0.028067425632194793; Validation Loss = 0.03469279925142387\n",
            "Cost after 299591 iterations : Training Loss =  0.028067415968957357; Validation Loss = 0.03469279094525066\n",
            "Cost after 299592 iterations : Training Loss =  0.02806740630583006; Validation Loss = 0.03469278263916054\n",
            "Cost after 299593 iterations : Training Loss =  0.028067396642812846; Validation Loss = 0.03469277433315371\n",
            "Cost after 299594 iterations : Training Loss =  0.028067386979905674; Validation Loss = 0.03469276602722976\n",
            "Cost after 299595 iterations : Training Loss =  0.028067377317108875; Validation Loss = 0.03469275772138848\n",
            "Cost after 299596 iterations : Training Loss =  0.028067367654421957; Validation Loss = 0.03469274941563055\n",
            "Cost after 299597 iterations : Training Loss =  0.028067357991845385; Validation Loss = 0.03469274110995532\n",
            "Cost after 299598 iterations : Training Loss =  0.028067348329379037; Validation Loss = 0.03469273280436317\n",
            "Cost after 299599 iterations : Training Loss =  0.02806733866702269; Validation Loss = 0.03469272449885439\n",
            "Cost after 299600 iterations : Training Loss =  0.028067329004776497; Validation Loss = 0.03469271619342834\n",
            "Cost after 299601 iterations : Training Loss =  0.028067319342640458; Validation Loss = 0.03469270788808537\n",
            "Cost after 299602 iterations : Training Loss =  0.028067309680614504; Validation Loss = 0.03469269958282499\n",
            "Cost after 299603 iterations : Training Loss =  0.028067300018698713; Validation Loss = 0.03469269127764803\n",
            "Cost after 299604 iterations : Training Loss =  0.028067290356893003; Validation Loss = 0.034692682972553894\n",
            "Cost after 299605 iterations : Training Loss =  0.028067280695197518; Validation Loss = 0.03469267466754281\n",
            "Cost after 299606 iterations : Training Loss =  0.028067271033612094; Validation Loss = 0.03469266636261491\n",
            "Cost after 299607 iterations : Training Loss =  0.02806726137213679; Validation Loss = 0.034692658057769804\n",
            "Cost after 299608 iterations : Training Loss =  0.02806725171077168; Validation Loss = 0.034692649753007496\n",
            "Cost after 299609 iterations : Training Loss =  0.02806724204951668; Validation Loss = 0.03469264144832853\n",
            "Cost after 299610 iterations : Training Loss =  0.028067232388371752; Validation Loss = 0.03469263314373233\n",
            "Cost after 299611 iterations : Training Loss =  0.02806722272733708; Validation Loss = 0.034692624839219394\n",
            "Cost after 299612 iterations : Training Loss =  0.02806721306641236; Validation Loss = 0.0346926165347891\n",
            "Cost after 299613 iterations : Training Loss =  0.028067203405597846; Validation Loss = 0.034692608230442075\n",
            "Cost after 299614 iterations : Training Loss =  0.028067193744893488; Validation Loss = 0.034692599926177604\n",
            "Cost after 299615 iterations : Training Loss =  0.028067184084299094; Validation Loss = 0.03469259162199659\n",
            "Cost after 299616 iterations : Training Loss =  0.028067174423814973; Validation Loss = 0.0346925833178984\n",
            "Cost after 299617 iterations : Training Loss =  0.02806716476344094; Validation Loss = 0.03469257501388314\n",
            "Cost after 299618 iterations : Training Loss =  0.028067155103176963; Validation Loss = 0.0346925667099509\n",
            "Cost after 299619 iterations : Training Loss =  0.02806714544302318; Validation Loss = 0.03469255840610213\n",
            "Cost after 299620 iterations : Training Loss =  0.02806713578297946; Validation Loss = 0.03469255010233533\n",
            "Cost after 299621 iterations : Training Loss =  0.028067126123045818; Validation Loss = 0.03469254179865224\n",
            "Cost after 299622 iterations : Training Loss =  0.028067116463222297; Validation Loss = 0.034692533495052025\n",
            "Cost after 299623 iterations : Training Loss =  0.028067106803508938; Validation Loss = 0.03469252519153486\n",
            "Cost after 299624 iterations : Training Loss =  0.0280670971439056; Validation Loss = 0.034692516888100544\n",
            "Cost after 299625 iterations : Training Loss =  0.02806708748441239; Validation Loss = 0.034692508584749225\n",
            "Cost after 299626 iterations : Training Loss =  0.028067077825029294; Validation Loss = 0.03469250028148111\n",
            "Cost after 299627 iterations : Training Loss =  0.028067068165756243; Validation Loss = 0.03469249197829561\n",
            "Cost after 299628 iterations : Training Loss =  0.028067058506593245; Validation Loss = 0.03469248367519315\n",
            "Cost after 299629 iterations : Training Loss =  0.028067048847540438; Validation Loss = 0.034692475372173646\n",
            "Cost after 299630 iterations : Training Loss =  0.0280670391885977; Validation Loss = 0.03469246706923718\n",
            "Cost after 299631 iterations : Training Loss =  0.028067029529765072; Validation Loss = 0.034692458766383584\n",
            "Cost after 299632 iterations : Training Loss =  0.028067019871042626; Validation Loss = 0.034692450463612856\n",
            "Cost after 299633 iterations : Training Loss =  0.02806701021243005; Validation Loss = 0.03469244216092533\n",
            "Cost after 299634 iterations : Training Loss =  0.02806700055392771; Validation Loss = 0.034692433858320486\n",
            "Cost after 299635 iterations : Training Loss =  0.02806699089553534; Validation Loss = 0.03469242555579869\n",
            "Cost after 299636 iterations : Training Loss =  0.02806698123725317; Validation Loss = 0.03469241725335977\n",
            "Cost after 299637 iterations : Training Loss =  0.028066971579081036; Validation Loss = 0.03469240895100402\n",
            "Cost after 299638 iterations : Training Loss =  0.02806696192101902; Validation Loss = 0.034692400648730984\n",
            "Cost after 299639 iterations : Training Loss =  0.028066952263067085; Validation Loss = 0.0346923923465413\n",
            "Cost after 299640 iterations : Training Loss =  0.028066942605225225; Validation Loss = 0.03469238404443423\n",
            "Cost after 299641 iterations : Training Loss =  0.02806693294749339; Validation Loss = 0.03469237574241003\n",
            "Cost after 299642 iterations : Training Loss =  0.028066923289871603; Validation Loss = 0.03469236744046897\n",
            "Cost after 299643 iterations : Training Loss =  0.028066913632359906; Validation Loss = 0.03469235913861094\n",
            "Cost after 299644 iterations : Training Loss =  0.028066903974958368; Validation Loss = 0.03469235083683585\n",
            "Cost after 299645 iterations : Training Loss =  0.028066894317666857; Validation Loss = 0.03469234253514374\n",
            "Cost after 299646 iterations : Training Loss =  0.02806688466048547; Validation Loss = 0.03469233423353451\n",
            "Cost after 299647 iterations : Training Loss =  0.02806687500341396; Validation Loss = 0.034692325932008335\n",
            "Cost after 299648 iterations : Training Loss =  0.028066865346452724; Validation Loss = 0.0346923176305647\n",
            "Cost after 299649 iterations : Training Loss =  0.028066855689601402; Validation Loss = 0.03469230932920425\n",
            "Cost after 299650 iterations : Training Loss =  0.028066846032860283; Validation Loss = 0.03469230102792688\n",
            "Cost after 299651 iterations : Training Loss =  0.02806683637622908; Validation Loss = 0.03469229272673246\n",
            "Cost after 299652 iterations : Training Loss =  0.028066826719708077; Validation Loss = 0.034692284425620855\n",
            "Cost after 299653 iterations : Training Loss =  0.028066817063297018; Validation Loss = 0.03469227612459217\n",
            "Cost after 299654 iterations : Training Loss =  0.028066807406995967; Validation Loss = 0.0346922678236467\n",
            "Cost after 299655 iterations : Training Loss =  0.02806679775080505; Validation Loss = 0.03469225952278387\n",
            "Cost after 299656 iterations : Training Loss =  0.028066788094724224; Validation Loss = 0.03469225122200404\n",
            "Cost after 299657 iterations : Training Loss =  0.028066778438753454; Validation Loss = 0.03469224292130713\n",
            "Cost after 299658 iterations : Training Loss =  0.02806676878289263; Validation Loss = 0.03469223462069315\n",
            "Cost after 299659 iterations : Training Loss =  0.028066759127141933; Validation Loss = 0.03469222632016212\n",
            "Cost after 299660 iterations : Training Loss =  0.028066749471501284; Validation Loss = 0.034692218019713776\n",
            "Cost after 299661 iterations : Training Loss =  0.02806673981597065; Validation Loss = 0.034692209719348714\n",
            "Cost after 299662 iterations : Training Loss =  0.028066730160550068; Validation Loss = 0.03469220141906658\n",
            "Cost after 299663 iterations : Training Loss =  0.02806672050523948; Validation Loss = 0.03469219311886701\n",
            "Cost after 299664 iterations : Training Loss =  0.02806671085003905; Validation Loss = 0.03469218481875062\n",
            "Cost after 299665 iterations : Training Loss =  0.02806670119494846; Validation Loss = 0.03469217651871724\n",
            "Cost after 299666 iterations : Training Loss =  0.028066691539968144; Validation Loss = 0.034692168218766806\n",
            "Cost after 299667 iterations : Training Loss =  0.02806668188509774; Validation Loss = 0.03469215991889899\n",
            "Cost after 299668 iterations : Training Loss =  0.02806667223033748; Validation Loss = 0.034692151619114445\n",
            "Cost after 299669 iterations : Training Loss =  0.028066662575687164; Validation Loss = 0.03469214331941241\n",
            "Cost after 299670 iterations : Training Loss =  0.028066652921146767; Validation Loss = 0.0346921350197935\n",
            "Cost after 299671 iterations : Training Loss =  0.028066643266716578; Validation Loss = 0.03469212672025773\n",
            "Cost after 299672 iterations : Training Loss =  0.028066633612396315; Validation Loss = 0.03469211842080489\n",
            "Cost after 299673 iterations : Training Loss =  0.02806662395818612; Validation Loss = 0.034692110121434715\n",
            "Cost after 299674 iterations : Training Loss =  0.028066614304085885; Validation Loss = 0.03469210182214756\n",
            "Cost after 299675 iterations : Training Loss =  0.028066604650095663; Validation Loss = 0.034692093522943106\n",
            "Cost after 299676 iterations : Training Loss =  0.028066594996215555; Validation Loss = 0.03469208522382168\n",
            "Cost after 299677 iterations : Training Loss =  0.028066585342445494; Validation Loss = 0.034692076924783155\n",
            "Cost after 299678 iterations : Training Loss =  0.028066575688785376; Validation Loss = 0.03469206862582767\n",
            "Cost after 299679 iterations : Training Loss =  0.028066566035235323; Validation Loss = 0.03469206032695507\n",
            "Cost after 299680 iterations : Training Loss =  0.02806655638179524; Validation Loss = 0.03469205202816511\n",
            "Cost after 299681 iterations : Training Loss =  0.028066546728465147; Validation Loss = 0.034692043729458236\n",
            "Cost after 299682 iterations : Training Loss =  0.028066537075245042; Validation Loss = 0.03469203543083431\n",
            "Cost after 299683 iterations : Training Loss =  0.028066527422135047; Validation Loss = 0.03469202713229338\n",
            "Cost after 299684 iterations : Training Loss =  0.02806651776913507; Validation Loss = 0.034692018833835596\n",
            "Cost after 299685 iterations : Training Loss =  0.028066508116245; Validation Loss = 0.034692010535460394\n",
            "Cost after 299686 iterations : Training Loss =  0.02806649846346506; Validation Loss = 0.03469200223716801\n",
            "Cost after 299687 iterations : Training Loss =  0.02806648881079499; Validation Loss = 0.0346919939389587\n",
            "Cost after 299688 iterations : Training Loss =  0.028066479158234982; Validation Loss = 0.034691985640832014\n",
            "Cost after 299689 iterations : Training Loss =  0.02806646950578503; Validation Loss = 0.034691977342788664\n",
            "Cost after 299690 iterations : Training Loss =  0.028066459853444987; Validation Loss = 0.03469196904482812\n",
            "Cost after 299691 iterations : Training Loss =  0.028066450201215024; Validation Loss = 0.034691960746950405\n",
            "Cost after 299692 iterations : Training Loss =  0.028066440549095004; Validation Loss = 0.034691952449155454\n",
            "Cost after 299693 iterations : Training Loss =  0.02806643089708498; Validation Loss = 0.03469194415144345\n",
            "Cost after 299694 iterations : Training Loss =  0.02806642124518497; Validation Loss = 0.034691935853814235\n",
            "Cost after 299695 iterations : Training Loss =  0.028066411593394902; Validation Loss = 0.03469192755626819\n",
            "Cost after 299696 iterations : Training Loss =  0.02806640194171488; Validation Loss = 0.03469191925880509\n",
            "Cost after 299697 iterations : Training Loss =  0.028066392290144734; Validation Loss = 0.03469191096142472\n",
            "Cost after 299698 iterations : Training Loss =  0.02806638263868471; Validation Loss = 0.034691902664127236\n",
            "Cost after 299699 iterations : Training Loss =  0.028066372987334692; Validation Loss = 0.03469189436691249\n",
            "Cost after 299700 iterations : Training Loss =  0.028066363336094493; Validation Loss = 0.034691886069780846\n",
            "Cost after 299701 iterations : Training Loss =  0.028066353684964428; Validation Loss = 0.03469187777273202\n",
            "Cost after 299702 iterations : Training Loss =  0.028066344033944302; Validation Loss = 0.03469186947576618\n",
            "Cost after 299703 iterations : Training Loss =  0.028066334383034106; Validation Loss = 0.03469186117888295\n",
            "Cost after 299704 iterations : Training Loss =  0.02806632473223401; Validation Loss = 0.03469185288208271\n",
            "Cost after 299705 iterations : Training Loss =  0.028066315081543662; Validation Loss = 0.034691844585365665\n",
            "Cost after 299706 iterations : Training Loss =  0.0280663054309635; Validation Loss = 0.03469183628873119\n",
            "Cost after 299707 iterations : Training Loss =  0.028066295780493276; Validation Loss = 0.034691827992179534\n",
            "Cost after 299708 iterations : Training Loss =  0.028066286130133; Validation Loss = 0.03469181969571072\n",
            "Cost after 299709 iterations : Training Loss =  0.028066276479882676; Validation Loss = 0.034691811399325145\n",
            "Cost after 299710 iterations : Training Loss =  0.028066266829742413; Validation Loss = 0.0346918031030221\n",
            "Cost after 299711 iterations : Training Loss =  0.028066257179712006; Validation Loss = 0.03469179480680202\n",
            "Cost after 299712 iterations : Training Loss =  0.028066247529791533; Validation Loss = 0.034691786510664686\n",
            "Cost after 299713 iterations : Training Loss =  0.0280662378799811; Validation Loss = 0.03469177821461037\n",
            "Cost after 299714 iterations : Training Loss =  0.02806622823028071; Validation Loss = 0.034691769918638954\n",
            "Cost after 299715 iterations : Training Loss =  0.0280662185806901; Validation Loss = 0.03469176162275027\n",
            "Cost after 299716 iterations : Training Loss =  0.0280662089312094; Validation Loss = 0.03469175332694454\n",
            "Cost after 299717 iterations : Training Loss =  0.028066199281838927; Validation Loss = 0.03469174503122149\n",
            "Cost after 299718 iterations : Training Loss =  0.02806618963257823; Validation Loss = 0.03469173673558161\n",
            "Cost after 299719 iterations : Training Loss =  0.028066179983427513; Validation Loss = 0.03469172844002442\n",
            "Cost after 299720 iterations : Training Loss =  0.028066170334386804; Validation Loss = 0.034691720144550395\n",
            "Cost after 299721 iterations : Training Loss =  0.02806616068545602; Validation Loss = 0.034691711849159035\n",
            "Cost after 299722 iterations : Training Loss =  0.02806615103663525; Validation Loss = 0.034691703553850387\n",
            "Cost after 299723 iterations : Training Loss =  0.028066141387924366; Validation Loss = 0.03469169525862461\n",
            "Cost after 299724 iterations : Training Loss =  0.028066131739323277; Validation Loss = 0.03469168696348164\n",
            "Cost after 299725 iterations : Training Loss =  0.02806612209083236; Validation Loss = 0.034691678668421794\n",
            "Cost after 299726 iterations : Training Loss =  0.028066112442451198; Validation Loss = 0.03469167037344491\n",
            "Cost after 299727 iterations : Training Loss =  0.028066102794180186; Validation Loss = 0.03469166207855052\n",
            "Cost after 299728 iterations : Training Loss =  0.028066093146019027; Validation Loss = 0.03469165378373916\n",
            "Cost after 299729 iterations : Training Loss =  0.02806608349796773; Validation Loss = 0.03469164548901061\n",
            "Cost after 299730 iterations : Training Loss =  0.028066073850026376; Validation Loss = 0.03469163719436507\n",
            "Cost after 299731 iterations : Training Loss =  0.028066064202195058; Validation Loss = 0.03469162889980212\n",
            "Cost after 299732 iterations : Training Loss =  0.028066054554473637; Validation Loss = 0.034691620605322104\n",
            "Cost after 299733 iterations : Training Loss =  0.028066044906862084; Validation Loss = 0.034691612310925136\n",
            "Cost after 299734 iterations : Training Loss =  0.028066035259360595; Validation Loss = 0.03469160401661091\n",
            "Cost after 299735 iterations : Training Loss =  0.02806602561196898; Validation Loss = 0.0346915957223797\n",
            "Cost after 299736 iterations : Training Loss =  0.028066015964687194; Validation Loss = 0.034691587428231294\n",
            "Cost after 299737 iterations : Training Loss =  0.02806600631751545; Validation Loss = 0.03469157913416571\n",
            "Cost after 299738 iterations : Training Loss =  0.02806599667045365; Validation Loss = 0.03469157084018268\n",
            "Cost after 299739 iterations : Training Loss =  0.028065987023501597; Validation Loss = 0.034691562546282924\n",
            "Cost after 299740 iterations : Training Loss =  0.02806597737665955; Validation Loss = 0.034691554252465694\n",
            "Cost after 299741 iterations : Training Loss =  0.028065967729927487; Validation Loss = 0.034691545958731355\n",
            "Cost after 299742 iterations : Training Loss =  0.02806595808330532; Validation Loss = 0.03469153766507972\n",
            "Cost after 299743 iterations : Training Loss =  0.02806594843679298; Validation Loss = 0.03469152937151115\n",
            "Cost after 299744 iterations : Training Loss =  0.02806593879039067; Validation Loss = 0.03469152107802525\n",
            "Cost after 299745 iterations : Training Loss =  0.028065929144098306; Validation Loss = 0.03469151278462216\n",
            "Cost after 299746 iterations : Training Loss =  0.028065919497915728; Validation Loss = 0.03469150449130223\n",
            "Cost after 299747 iterations : Training Loss =  0.02806590985184319; Validation Loss = 0.03469149619806472\n",
            "Cost after 299748 iterations : Training Loss =  0.028065900205880557; Validation Loss = 0.034691487904910445\n",
            "Cost after 299749 iterations : Training Loss =  0.028065890560027673; Validation Loss = 0.034691479611838584\n",
            "Cost after 299750 iterations : Training Loss =  0.028065880914284726; Validation Loss = 0.03469147131884998\n",
            "Cost after 299751 iterations : Training Loss =  0.02806587126865186; Validation Loss = 0.034691463025944114\n",
            "Cost after 299752 iterations : Training Loss =  0.028065861623128744; Validation Loss = 0.03469145473312109\n",
            "Cost after 299753 iterations : Training Loss =  0.02806585197771556; Validation Loss = 0.03469144644038065\n",
            "Cost after 299754 iterations : Training Loss =  0.028065842332412227; Validation Loss = 0.03469143814772309\n",
            "Cost after 299755 iterations : Training Loss =  0.02806583268721894; Validation Loss = 0.03469142985514822\n",
            "Cost after 299756 iterations : Training Loss =  0.028065823042135497; Validation Loss = 0.03469142156265652\n",
            "Cost after 299757 iterations : Training Loss =  0.028065813397161857; Validation Loss = 0.03469141327024749\n",
            "Cost after 299758 iterations : Training Loss =  0.028065803752298182; Validation Loss = 0.03469140497792106\n",
            "Cost after 299759 iterations : Training Loss =  0.02806579410754436; Validation Loss = 0.03469139668567751\n",
            "Cost after 299760 iterations : Training Loss =  0.028065784462900402; Validation Loss = 0.0346913883935172\n",
            "Cost after 299761 iterations : Training Loss =  0.02806577481836646; Validation Loss = 0.034691380101439497\n",
            "Cost after 299762 iterations : Training Loss =  0.02806576517394225; Validation Loss = 0.034691371809444344\n",
            "Cost after 299763 iterations : Training Loss =  0.02806575552962793; Validation Loss = 0.034691363517532395\n",
            "Cost after 299764 iterations : Training Loss =  0.028065745885423654; Validation Loss = 0.03469135522570306\n",
            "Cost after 299765 iterations : Training Loss =  0.02806573624132921; Validation Loss = 0.03469134693395661\n",
            "Cost after 299766 iterations : Training Loss =  0.028065726597344704; Validation Loss = 0.03469133864229294\n",
            "Cost after 299767 iterations : Training Loss =  0.028065716953469867; Validation Loss = 0.03469133035071233\n",
            "Cost after 299768 iterations : Training Loss =  0.028065707309705132; Validation Loss = 0.03469132205921424\n",
            "Cost after 299769 iterations : Training Loss =  0.028065697666050084; Validation Loss = 0.034691313767798757\n",
            "Cost after 299770 iterations : Training Loss =  0.028065688022504937; Validation Loss = 0.03469130547646631\n",
            "Cost after 299771 iterations : Training Loss =  0.028065678379069783; Validation Loss = 0.03469129718521672\n",
            "Cost after 299772 iterations : Training Loss =  0.028065668735744488; Validation Loss = 0.034691288894049585\n",
            "Cost after 299773 iterations : Training Loss =  0.028065659092529022; Validation Loss = 0.03469128060296559\n",
            "Cost after 299774 iterations : Training Loss =  0.028065649449423392; Validation Loss = 0.03469127231196445\n",
            "Cost after 299775 iterations : Training Loss =  0.028065639806427605; Validation Loss = 0.03469126402104588\n",
            "Cost after 299776 iterations : Training Loss =  0.02806563016354184; Validation Loss = 0.034691255730210195\n",
            "Cost after 299777 iterations : Training Loss =  0.028065620520765826; Validation Loss = 0.03469124743945733\n",
            "Cost after 299778 iterations : Training Loss =  0.028065610878099653; Validation Loss = 0.03469123914878761\n",
            "Cost after 299779 iterations : Training Loss =  0.028065601235543275; Validation Loss = 0.03469123085820039\n",
            "Cost after 299780 iterations : Training Loss =  0.02806559159309693; Validation Loss = 0.03469122256769588\n",
            "Cost after 299781 iterations : Training Loss =  0.02806558195076039; Validation Loss = 0.03469121427727425\n",
            "Cost after 299782 iterations : Training Loss =  0.028065572308533644; Validation Loss = 0.03469120598693533\n",
            "Cost after 299783 iterations : Training Loss =  0.02806556266641684; Validation Loss = 0.0346911976966792\n",
            "Cost after 299784 iterations : Training Loss =  0.0280655530244098; Validation Loss = 0.03469118940650603\n",
            "Cost after 299785 iterations : Training Loss =  0.02806554338251266; Validation Loss = 0.034691181116415656\n",
            "Cost after 299786 iterations : Training Loss =  0.028065533740725394; Validation Loss = 0.0346911728264079\n",
            "Cost after 299787 iterations : Training Loss =  0.028065524099047834; Validation Loss = 0.034691164536482993\n",
            "Cost after 299788 iterations : Training Loss =  0.02806551445748026; Validation Loss = 0.03469115624664112\n",
            "Cost after 299789 iterations : Training Loss =  0.028065504816022612; Validation Loss = 0.03469114795688162\n",
            "Cost after 299790 iterations : Training Loss =  0.0280654951746747; Validation Loss = 0.03469113966720532\n",
            "Cost after 299791 iterations : Training Loss =  0.02806548553343663; Validation Loss = 0.03469113137761135\n",
            "Cost after 299792 iterations : Training Loss =  0.028065475892308282; Validation Loss = 0.0346911230881005\n",
            "Cost after 299793 iterations : Training Loss =  0.02806546625128994; Validation Loss = 0.034691114798672346\n",
            "Cost after 299794 iterations : Training Loss =  0.028065456610381356; Validation Loss = 0.03469110650932695\n",
            "Cost after 299795 iterations : Training Loss =  0.028065446969582696; Validation Loss = 0.03469109822006432\n",
            "Cost after 299796 iterations : Training Loss =  0.028065437328893786; Validation Loss = 0.03469108993088449\n",
            "Cost after 299797 iterations : Training Loss =  0.02806542768831477; Validation Loss = 0.03469108164178724\n",
            "Cost after 299798 iterations : Training Loss =  0.02806541804784544; Validation Loss = 0.034691073352773\n",
            "Cost after 299799 iterations : Training Loss =  0.028065408407486152; Validation Loss = 0.03469106506384144\n",
            "Cost after 299800 iterations : Training Loss =  0.02806539876723663; Validation Loss = 0.03469105677499257\n",
            "Cost after 299801 iterations : Training Loss =  0.028065389127096853; Validation Loss = 0.034691048486226526\n",
            "Cost after 299802 iterations : Training Loss =  0.028065379487066982; Validation Loss = 0.03469104019754348\n",
            "Cost after 299803 iterations : Training Loss =  0.02806536984714695; Validation Loss = 0.034691031908943064\n",
            "Cost after 299804 iterations : Training Loss =  0.02806536020733657; Validation Loss = 0.03469102362042532\n",
            "Cost after 299805 iterations : Training Loss =  0.028065350567636174; Validation Loss = 0.03469101533199053\n",
            "Cost after 299806 iterations : Training Loss =  0.028065340928045492; Validation Loss = 0.034691007043638505\n",
            "Cost after 299807 iterations : Training Loss =  0.028065331288564713; Validation Loss = 0.034690998755369296\n",
            "Cost after 299808 iterations : Training Loss =  0.02806532164919371; Validation Loss = 0.03469099046718277\n",
            "Cost after 299809 iterations : Training Loss =  0.02806531200993258; Validation Loss = 0.03469098217907892\n",
            "Cost after 299810 iterations : Training Loss =  0.028065302370781253; Validation Loss = 0.03469097389105782\n",
            "Cost after 299811 iterations : Training Loss =  0.028065292731739737; Validation Loss = 0.03469096560311942\n",
            "Cost after 299812 iterations : Training Loss =  0.028065283092807983; Validation Loss = 0.03469095731526371\n",
            "Cost after 299813 iterations : Training Loss =  0.02806527345398603; Validation Loss = 0.03469094902749114\n",
            "Cost after 299814 iterations : Training Loss =  0.028065263815273824; Validation Loss = 0.034690940739801016\n",
            "Cost after 299815 iterations : Training Loss =  0.028065254176671637; Validation Loss = 0.03469093245219384\n",
            "Cost after 299816 iterations : Training Loss =  0.028065244538179063; Validation Loss = 0.034690924164668874\n",
            "Cost after 299817 iterations : Training Loss =  0.028065234899796312; Validation Loss = 0.034690915877227495\n",
            "Cost after 299818 iterations : Training Loss =  0.02806522526152349; Validation Loss = 0.034690907589868515\n",
            "Cost after 299819 iterations : Training Loss =  0.028065215623360357; Validation Loss = 0.03469089930259235\n",
            "Cost after 299820 iterations : Training Loss =  0.028065205985307005; Validation Loss = 0.03469089101539877\n",
            "Cost after 299821 iterations : Training Loss =  0.028065196347363482; Validation Loss = 0.03469088272828797\n",
            "Cost after 299822 iterations : Training Loss =  0.028065186709529778; Validation Loss = 0.03469087444126027\n",
            "Cost after 299823 iterations : Training Loss =  0.028065177071805846; Validation Loss = 0.03469086615431496\n",
            "Cost after 299824 iterations : Training Loss =  0.028065167434191757; Validation Loss = 0.03469085786745245\n",
            "Cost after 299825 iterations : Training Loss =  0.028065157796687345; Validation Loss = 0.03469084958067287\n",
            "Cost after 299826 iterations : Training Loss =  0.02806514815929287; Validation Loss = 0.03469084129397555\n",
            "Cost after 299827 iterations : Training Loss =  0.028065138522008072; Validation Loss = 0.03469083300736144\n",
            "Cost after 299828 iterations : Training Loss =  0.02806512888483308; Validation Loss = 0.034690824720829856\n",
            "Cost after 299829 iterations : Training Loss =  0.028065119247767834; Validation Loss = 0.034690816434381204\n",
            "Cost after 299830 iterations : Training Loss =  0.028065109610812364; Validation Loss = 0.034690808148015055\n",
            "Cost after 299831 iterations : Training Loss =  0.02806509997396678; Validation Loss = 0.03469079986173176\n",
            "Cost after 299832 iterations : Training Loss =  0.028065090337230893; Validation Loss = 0.03469079157553119\n",
            "Cost after 299833 iterations : Training Loss =  0.028065080700604816; Validation Loss = 0.03469078328941354\n",
            "Cost after 299834 iterations : Training Loss =  0.02806507106408841; Validation Loss = 0.034690775003378235\n",
            "Cost after 299835 iterations : Training Loss =  0.028065061427682027; Validation Loss = 0.03469076671742581\n",
            "Cost after 299836 iterations : Training Loss =  0.028065051791385222; Validation Loss = 0.034690758431556085\n",
            "Cost after 299837 iterations : Training Loss =  0.028065042155198224; Validation Loss = 0.034690750145769125\n",
            "Cost after 299838 iterations : Training Loss =  0.02806503251912095; Validation Loss = 0.03469074186006521\n",
            "Cost after 299839 iterations : Training Loss =  0.02806502288315349; Validation Loss = 0.03469073357444363\n",
            "Cost after 299840 iterations : Training Loss =  0.028065013247295767; Validation Loss = 0.03469072528890488\n",
            "Cost after 299841 iterations : Training Loss =  0.028065003611547967; Validation Loss = 0.034690717003448786\n",
            "Cost after 299842 iterations : Training Loss =  0.028064993975909714; Validation Loss = 0.03469070871807564\n",
            "Cost after 299843 iterations : Training Loss =  0.028064984340381325; Validation Loss = 0.0346907004327849\n",
            "Cost after 299844 iterations : Training Loss =  0.028064974704962716; Validation Loss = 0.0346906921475772\n",
            "Cost after 299845 iterations : Training Loss =  0.028064965069653742; Validation Loss = 0.0346906838624521\n",
            "Cost after 299846 iterations : Training Loss =  0.028064955434454673; Validation Loss = 0.034690675577409645\n",
            "Cost after 299847 iterations : Training Loss =  0.028064945799365314; Validation Loss = 0.03469066729245002\n",
            "Cost after 299848 iterations : Training Loss =  0.02806493616438572; Validation Loss = 0.03469065900757309\n",
            "Cost after 299849 iterations : Training Loss =  0.02806492652951579; Validation Loss = 0.03469065072277864\n",
            "Cost after 299850 iterations : Training Loss =  0.028064916894755775; Validation Loss = 0.0346906424380674\n",
            "Cost after 299851 iterations : Training Loss =  0.028064907260105416; Validation Loss = 0.034690634153438454\n",
            "Cost after 299852 iterations : Training Loss =  0.028064897625564716; Validation Loss = 0.03469062586889243\n",
            "Cost after 299853 iterations : Training Loss =  0.02806488799113384; Validation Loss = 0.03469061758442901\n",
            "Cost after 299854 iterations : Training Loss =  0.028064878356812706; Validation Loss = 0.0346906093000484\n",
            "Cost after 299855 iterations : Training Loss =  0.028064868722601305; Validation Loss = 0.034690601015750486\n",
            "Cost after 299856 iterations : Training Loss =  0.028064859088499754; Validation Loss = 0.03469059273153516\n",
            "Cost after 299857 iterations : Training Loss =  0.02806484945450782; Validation Loss = 0.03469058444740275\n",
            "Cost after 299858 iterations : Training Loss =  0.02806483982062567; Validation Loss = 0.034690576163352936\n",
            "Cost after 299859 iterations : Training Loss =  0.028064830186853303; Validation Loss = 0.034690567879385736\n",
            "Cost after 299860 iterations : Training Loss =  0.02806482055319052; Validation Loss = 0.034690559595501436\n",
            "Cost after 299861 iterations : Training Loss =  0.02806481091963761; Validation Loss = 0.03469055131170003\n",
            "Cost after 299862 iterations : Training Loss =  0.028064801286194402; Validation Loss = 0.03469054302798105\n",
            "Cost after 299863 iterations : Training Loss =  0.028064791652860934; Validation Loss = 0.03469053474434499\n",
            "Cost after 299864 iterations : Training Loss =  0.028064782019637104; Validation Loss = 0.03469052646079138\n",
            "Cost after 299865 iterations : Training Loss =  0.028064772386523145; Validation Loss = 0.034690518177320696\n",
            "Cost after 299866 iterations : Training Loss =  0.02806476275351872; Validation Loss = 0.03469050989393255\n",
            "Cost after 299867 iterations : Training Loss =  0.028064753120624168; Validation Loss = 0.03469050161062718\n",
            "Cost after 299868 iterations : Training Loss =  0.028064743487839387; Validation Loss = 0.03469049332740414\n",
            "Cost after 299869 iterations : Training Loss =  0.028064733855164243; Validation Loss = 0.03469048504426417\n",
            "Cost after 299870 iterations : Training Loss =  0.02806472422259874; Validation Loss = 0.034690476761206894\n",
            "Cost after 299871 iterations : Training Loss =  0.028064714590143022; Validation Loss = 0.03469046847823214\n",
            "Cost after 299872 iterations : Training Loss =  0.028064704957797126; Validation Loss = 0.034690460195340125\n",
            "Cost after 299873 iterations : Training Loss =  0.02806469532556077; Validation Loss = 0.03469045191253092\n",
            "Cost after 299874 iterations : Training Loss =  0.028064685693434125; Validation Loss = 0.03469044362980441\n",
            "Cost after 299875 iterations : Training Loss =  0.028064676061417368; Validation Loss = 0.03469043534716041\n",
            "Cost after 299876 iterations : Training Loss =  0.02806466642951019; Validation Loss = 0.03469042706459904\n",
            "Cost after 299877 iterations : Training Loss =  0.02806465679771286; Validation Loss = 0.034690418782120584\n",
            "Cost after 299878 iterations : Training Loss =  0.02806464716602517; Validation Loss = 0.03469041049972455\n",
            "Cost after 299879 iterations : Training Loss =  0.028064637534447164; Validation Loss = 0.034690402217411306\n",
            "Cost after 299880 iterations : Training Loss =  0.028064627902978766; Validation Loss = 0.03469039393518075\n",
            "Cost after 299881 iterations : Training Loss =  0.02806461827162019; Validation Loss = 0.03469038565303322\n",
            "Cost after 299882 iterations : Training Loss =  0.0280646086403713; Validation Loss = 0.03469037737096806\n",
            "Cost after 299883 iterations : Training Loss =  0.028064599009232048; Validation Loss = 0.03469036908898555\n",
            "Cost after 299884 iterations : Training Loss =  0.0280645893782026; Validation Loss = 0.034690360807085985\n",
            "Cost after 299885 iterations : Training Loss =  0.02806457974728277; Validation Loss = 0.03469035252526892\n",
            "Cost after 299886 iterations : Training Loss =  0.028064570116472662; Validation Loss = 0.03469034424353446\n",
            "Cost after 299887 iterations : Training Loss =  0.028064560485772307; Validation Loss = 0.03469033596188284\n",
            "Cost after 299888 iterations : Training Loss =  0.02806455085518145; Validation Loss = 0.03469032768031366\n",
            "Cost after 299889 iterations : Training Loss =  0.028064541224700527; Validation Loss = 0.03469031939882748\n",
            "Cost after 299890 iterations : Training Loss =  0.028064531594329156; Validation Loss = 0.03469031111742385\n",
            "Cost after 299891 iterations : Training Loss =  0.028064521964067557; Validation Loss = 0.034690302836102895\n",
            "Cost after 299892 iterations : Training Loss =  0.02806451233391544; Validation Loss = 0.03469029455486472\n",
            "Cost after 299893 iterations : Training Loss =  0.028064502703873208; Validation Loss = 0.03469028627370912\n",
            "Cost after 299894 iterations : Training Loss =  0.02806449307394061; Validation Loss = 0.034690277992636014\n",
            "Cost after 299895 iterations : Training Loss =  0.028064483444117716; Validation Loss = 0.03469026971164569\n",
            "Cost after 299896 iterations : Training Loss =  0.028064473814404342; Validation Loss = 0.03469026143073782\n",
            "Cost after 299897 iterations : Training Loss =  0.028064464184800856; Validation Loss = 0.034690253149913045\n",
            "Cost after 299898 iterations : Training Loss =  0.02806445455530685; Validation Loss = 0.034690244869170744\n",
            "Cost after 299899 iterations : Training Loss =  0.02806444492592263; Validation Loss = 0.034690236588511114\n",
            "Cost after 299900 iterations : Training Loss =  0.02806443529664808; Validation Loss = 0.03469022830793412\n",
            "Cost after 299901 iterations : Training Loss =  0.028064425667483302; Validation Loss = 0.03469022002743986\n",
            "Cost after 299902 iterations : Training Loss =  0.02806441603842805; Validation Loss = 0.034690211747028146\n",
            "Cost after 299903 iterations : Training Loss =  0.028064406409482563; Validation Loss = 0.03469020346669892\n",
            "Cost after 299904 iterations : Training Loss =  0.028064396780646643; Validation Loss = 0.034690195186452485\n",
            "Cost after 299905 iterations : Training Loss =  0.028064387151920445; Validation Loss = 0.03469018690628886\n",
            "Cost after 299906 iterations : Training Loss =  0.02806437752330389; Validation Loss = 0.03469017862620783\n",
            "Cost after 299907 iterations : Training Loss =  0.028064367894797045; Validation Loss = 0.03469017034620956\n",
            "Cost after 299908 iterations : Training Loss =  0.028064358266399782; Validation Loss = 0.03469016206629359\n",
            "Cost after 299909 iterations : Training Loss =  0.02806434863811227; Validation Loss = 0.03469015378646045\n",
            "Cost after 299910 iterations : Training Loss =  0.02806433900993431; Validation Loss = 0.034690145506710186\n",
            "Cost after 299911 iterations : Training Loss =  0.028064329381866128; Validation Loss = 0.03469013722704208\n",
            "Cost after 299912 iterations : Training Loss =  0.028064319753907496; Validation Loss = 0.03469012894745691\n",
            "Cost after 299913 iterations : Training Loss =  0.02806431012605856; Validation Loss = 0.03469012066795467\n",
            "Cost after 299914 iterations : Training Loss =  0.0280643004983193; Validation Loss = 0.034690112388534734\n",
            "Cost after 299915 iterations : Training Loss =  0.028064290870689627; Validation Loss = 0.03469010410919742\n",
            "Cost after 299916 iterations : Training Loss =  0.028064281243169693; Validation Loss = 0.034690095829942764\n",
            "Cost after 299917 iterations : Training Loss =  0.02806427161575932; Validation Loss = 0.034690087550771014\n",
            "Cost after 299918 iterations : Training Loss =  0.02806426198845863; Validation Loss = 0.03469007927168156\n",
            "Cost after 299919 iterations : Training Loss =  0.02806425236126759; Validation Loss = 0.034690070992674696\n",
            "Cost after 299920 iterations : Training Loss =  0.028064242734186166; Validation Loss = 0.0346900627137505\n",
            "Cost after 299921 iterations : Training Loss =  0.02806423310721441; Validation Loss = 0.034690054434908996\n",
            "Cost after 299922 iterations : Training Loss =  0.028064223480352285; Validation Loss = 0.03469004615615046\n",
            "Cost after 299923 iterations : Training Loss =  0.02806421385359984; Validation Loss = 0.03469003787747448\n",
            "Cost after 299924 iterations : Training Loss =  0.028064204226957008; Validation Loss = 0.03469002959888088\n",
            "Cost after 299925 iterations : Training Loss =  0.028064194600423715; Validation Loss = 0.03469002132037003\n",
            "Cost after 299926 iterations : Training Loss =  0.028064184974000077; Validation Loss = 0.03469001304194202\n",
            "Cost after 299927 iterations : Training Loss =  0.028064175347686206; Validation Loss = 0.034690004763596426\n",
            "Cost after 299928 iterations : Training Loss =  0.028064165721481904; Validation Loss = 0.03468999648533347\n",
            "Cost after 299929 iterations : Training Loss =  0.02806415609538716; Validation Loss = 0.034689988207153216\n",
            "Cost after 299930 iterations : Training Loss =  0.02806414646940209; Validation Loss = 0.03468997992905532\n",
            "Cost after 299931 iterations : Training Loss =  0.028064136843526658; Validation Loss = 0.034689971651040326\n",
            "Cost after 299932 iterations : Training Loss =  0.028064127217760807; Validation Loss = 0.034689963373108\n",
            "Cost after 299933 iterations : Training Loss =  0.028064117592104646; Validation Loss = 0.03468995509525848\n",
            "Cost after 299934 iterations : Training Loss =  0.02806410796655807; Validation Loss = 0.03468994681749141\n",
            "Cost after 299935 iterations : Training Loss =  0.02806409834112111; Validation Loss = 0.034689938539806994\n",
            "Cost after 299936 iterations : Training Loss =  0.02806408871579373; Validation Loss = 0.03468993026220507\n",
            "Cost after 299937 iterations : Training Loss =  0.02806407909057607; Validation Loss = 0.03468992198468579\n",
            "Cost after 299938 iterations : Training Loss =  0.02806406946546792; Validation Loss = 0.034689913707249\n",
            "Cost after 299939 iterations : Training Loss =  0.028064059840469502; Validation Loss = 0.03468990542989483\n",
            "Cost after 299940 iterations : Training Loss =  0.028064050215580554; Validation Loss = 0.03468989715262345\n",
            "Cost after 299941 iterations : Training Loss =  0.028064040590801198; Validation Loss = 0.03468988887543436\n",
            "Cost after 299942 iterations : Training Loss =  0.02806403096613158; Validation Loss = 0.034689880598328296\n",
            "Cost after 299943 iterations : Training Loss =  0.028064021341571502; Validation Loss = 0.03468987232130491\n",
            "Cost after 299944 iterations : Training Loss =  0.02806401171712107; Validation Loss = 0.03468986404436365\n",
            "Cost after 299945 iterations : Training Loss =  0.028064002092780302; Validation Loss = 0.034689855767505266\n",
            "Cost after 299946 iterations : Training Loss =  0.028063992468548894; Validation Loss = 0.03468984749072946\n",
            "Cost after 299947 iterations : Training Loss =  0.028063982844427363; Validation Loss = 0.03468983921403658\n",
            "Cost after 299948 iterations : Training Loss =  0.028063973220415393; Validation Loss = 0.03468983093742603\n",
            "Cost after 299949 iterations : Training Loss =  0.028063963596512892; Validation Loss = 0.03468982266089801\n",
            "Cost after 299950 iterations : Training Loss =  0.028063953972719956; Validation Loss = 0.03468981438445292\n",
            "Cost after 299951 iterations : Training Loss =  0.028063944349036706; Validation Loss = 0.034689806108090124\n",
            "Cost after 299952 iterations : Training Loss =  0.028063934725463074; Validation Loss = 0.03468979783180982\n",
            "Cost after 299953 iterations : Training Loss =  0.028063925101998947; Validation Loss = 0.034689789555612534\n",
            "Cost after 299954 iterations : Training Loss =  0.028063915478644473; Validation Loss = 0.03468978127949755\n",
            "Cost after 299955 iterations : Training Loss =  0.028063905855399515; Validation Loss = 0.03468977300346498\n",
            "Cost after 299956 iterations : Training Loss =  0.028063896232264275; Validation Loss = 0.034689764727515275\n",
            "Cost after 299957 iterations : Training Loss =  0.0280638866092385; Validation Loss = 0.03468975645164833\n",
            "Cost after 299958 iterations : Training Loss =  0.028063876986322362; Validation Loss = 0.034689748175863726\n",
            "Cost after 299959 iterations : Training Loss =  0.0280638673635158; Validation Loss = 0.034689739900161824\n",
            "Cost after 299960 iterations : Training Loss =  0.0280638577408188; Validation Loss = 0.034689731624542475\n",
            "Cost after 299961 iterations : Training Loss =  0.028063848118231434; Validation Loss = 0.03468972334900582\n",
            "Cost after 299962 iterations : Training Loss =  0.028063838495753606; Validation Loss = 0.034689715073551855\n",
            "Cost after 299963 iterations : Training Loss =  0.02806382887338524; Validation Loss = 0.03468970679817996\n",
            "Cost after 299964 iterations : Training Loss =  0.02806381925112661; Validation Loss = 0.03468969852289109\n",
            "Cost after 299965 iterations : Training Loss =  0.028063809628977573; Validation Loss = 0.03468969024768483\n",
            "Cost after 299966 iterations : Training Loss =  0.02806380000693805; Validation Loss = 0.03468968197256071\n",
            "Cost after 299967 iterations : Training Loss =  0.028063790385008083; Validation Loss = 0.03468967369751938\n",
            "Cost after 299968 iterations : Training Loss =  0.02806378076318763; Validation Loss = 0.03468966542256093\n",
            "Cost after 299969 iterations : Training Loss =  0.028063771141476825; Validation Loss = 0.0346896571476847\n",
            "Cost after 299970 iterations : Training Loss =  0.02806376151987561; Validation Loss = 0.03468964887289111\n",
            "Cost after 299971 iterations : Training Loss =  0.028063751898383792; Validation Loss = 0.03468964059818007\n",
            "Cost after 299972 iterations : Training Loss =  0.02806374227700161; Validation Loss = 0.0346896323235517\n",
            "Cost after 299973 iterations : Training Loss =  0.02806373265572916; Validation Loss = 0.03468962404900598\n",
            "Cost after 299974 iterations : Training Loss =  0.028063723034566043; Validation Loss = 0.03468961577454302\n",
            "Cost after 299975 iterations : Training Loss =  0.028063713413512575; Validation Loss = 0.03468960750016236\n",
            "Cost after 299976 iterations : Training Loss =  0.028063703792568707; Validation Loss = 0.034689599225864474\n",
            "Cost after 299977 iterations : Training Loss =  0.028063694171734235; Validation Loss = 0.0346895909516491\n",
            "Cost after 299978 iterations : Training Loss =  0.028063684551009535; Validation Loss = 0.03468958267751642\n",
            "Cost after 299979 iterations : Training Loss =  0.028063674930394272; Validation Loss = 0.03468957440346622\n",
            "Cost after 299980 iterations : Training Loss =  0.028063665309888557; Validation Loss = 0.03468956612949837\n",
            "Cost after 299981 iterations : Training Loss =  0.028063655689492407; Validation Loss = 0.03468955785561296\n",
            "Cost after 299982 iterations : Training Loss =  0.028063646069205767; Validation Loss = 0.03468954958181057\n",
            "Cost after 299983 iterations : Training Loss =  0.028063636449028786; Validation Loss = 0.034689541308090543\n",
            "Cost after 299984 iterations : Training Loss =  0.028063626828961245; Validation Loss = 0.03468953303445315\n",
            "Cost after 299985 iterations : Training Loss =  0.028063617209003217; Validation Loss = 0.03468952476089795\n",
            "Cost after 299986 iterations : Training Loss =  0.02806360758915472; Validation Loss = 0.03468951648742572\n",
            "Cost after 299987 iterations : Training Loss =  0.028063597969415894; Validation Loss = 0.034689508214035984\n",
            "Cost after 299988 iterations : Training Loss =  0.028063588349786547; Validation Loss = 0.034689499940728744\n",
            "Cost after 299989 iterations : Training Loss =  0.028063578730266714; Validation Loss = 0.03468949166750411\n",
            "Cost after 299990 iterations : Training Loss =  0.028063569110856376; Validation Loss = 0.034689483394361835\n",
            "Cost after 299991 iterations : Training Loss =  0.028063559491555656; Validation Loss = 0.034689475121302375\n",
            "Cost after 299992 iterations : Training Loss =  0.028063549872364434; Validation Loss = 0.03468946684832518\n",
            "Cost after 299993 iterations : Training Loss =  0.02806354025328275; Validation Loss = 0.03468945857543106\n",
            "Cost after 299994 iterations : Training Loss =  0.028063530634310517; Validation Loss = 0.034689450302619045\n",
            "Cost after 299995 iterations : Training Loss =  0.028063521015447967; Validation Loss = 0.03468944202988986\n",
            "Cost after 299996 iterations : Training Loss =  0.02806351139669483; Validation Loss = 0.03468943375724323\n",
            "Cost after 299997 iterations : Training Loss =  0.028063501778051152; Validation Loss = 0.034689425484679064\n",
            "Cost after 299998 iterations : Training Loss =  0.028063492159517082; Validation Loss = 0.034689417212197524\n",
            "Cost after 299999 iterations : Training Loss =  0.02806348254109254; Validation Loss = 0.03468940893979812\n",
            "Cost after 300000 iterations : Training Loss =  0.028063472922777506; Validation Loss = 0.03468940066748189\n",
            "Training Complete : min_loss_achieved = 0.03468940066748189; A,B,C = (0.8593484165434271, -1.4619424004139716, 0.12493014749593671)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIWWhRh4TEua",
        "outputId": "d126a209-a8d9-404c-f12f-fff055cb6038"
      },
      "source": [
        "# Model-4 Training\n",
        "quad_results.append(quad_train(quad_data,Q2,Q1,Q0,7))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Quadratic Regression Model using COST = (y-z)**7\n",
            "Cost after 1 iterations : Training Loss =  20964618326.504993; Validation Loss = 17264630598.54053\n",
            "Cost after 2 iterations : Training Loss =  1.4289473734059126e+57; Validation Loss = 1.1737895028281396e+57\n",
            "Cost after 3 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 4 iterations : Training Loss =  57.07571902762977; Validation Loss = 71.00355965448662\n",
            "Cost after 5 iterations : Training Loss =  42823.51832081089; Validation Loss = 36466.96877662951\n",
            "Cost after 6 iterations : Training Loss =  9.2820717800376e+22; Validation Loss = 7.624317259671145e+22\n",
            "Cost after 7 iterations : Training Loss =  1.0787644855821723e+133; Validation Loss = 8.861368485648035e+132\n",
            "Cost after 8 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 9 iterations : Training Loss =  45653618652.80312; Validation Loss = 38057804988.00579\n",
            "Cost after 10 iterations : Training Loss =  1.518949553540315e+59; Validation Loss = 1.2477316438444492e+59\n",
            "Cost after 11 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 12 iterations : Training Loss =  8090430254.805384; Validation Loss = 6760092789.184928\n",
            "Cost after 13 iterations : Training Loss =  4.7000089700763596e+54; Validation Loss = 3.860792650011225e+54\n",
            "Cost after 14 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 15 iterations : Training Loss =  21422457430.416496; Validation Loss = 17905436694.701283\n",
            "Cost after 16 iterations : Training Loss =  1.6195078533628594e+57; Validation Loss = 1.3303357661621026e+57\n",
            "Cost after 17 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 18 iterations : Training Loss =  5214583427.097958; Validation Loss = 4322954940.603857\n",
            "Cost after 19 iterations : Training Loss =  3.3788247951217537e+53; Validation Loss = 2.775499792387978e+53\n",
            "Cost after 20 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 21 iterations : Training Loss =  1523190356.6403415; Validation Loss = 1298106979.0121326\n",
            "Cost after 22 iterations : Training Loss =  2.07208120740133e+50; Validation Loss = 1.7021093176679458e+50\n",
            "Cost after 23 iterations : Training Loss =  1.3350513081971428e+297; Validation Loss = 1.0966603089706659e+297\n",
            "Cost after 24 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 25 iterations : Training Loss =  2467825.5332635497; Validation Loss = 2165721.435593135\n",
            "Cost after 26 iterations : Training Loss =  3.6709127304168536e+33; Validation Loss = 3.015453253094153e+33\n",
            "Cost after 27 iterations : Training Loss =  4.127638829309136e+196; Validation Loss = 3.3905945207993854e+196\n",
            "Cost after 28 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 29 iterations : Training Loss =  80638.53083551583; Validation Loss = 92316.44523571897\n",
            "Cost after 30 iterations : Training Loss =  1.7236405898419037e+24; Validation Loss = 1.4157548917215906e+24\n",
            "Cost after 31 iterations : Training Loss =  4.423204161616217e+140; Validation Loss = 3.6333821393553923e+140\n",
            "Cost after 32 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 33 iterations : Training Loss =  920.4055512131265; Validation Loss = 1266.591264395569\n",
            "Cost after 34 iterations : Training Loss =  67269967723.92467; Validation Loss = 55838295308.55357\n",
            "Cost after 35 iterations : Training Loss =  1.5577330403472407e+60; Validation Loss = 1.2795870294850038e+60\n",
            "Cost after 36 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 37 iterations : Training Loss =  17473.51000105324; Validation Loss = 13912.323016675355\n",
            "Cost after 38 iterations : Training Loss =  4.0238108851150225e+20; Validation Loss = 3.305213191412761e+20\n",
            "Cost after 39 iterations : Training Loss =  7.159473775878602e+118; Validation Loss = 5.881054377251927e+118\n",
            "Cost after 40 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 41 iterations : Training Loss =  599001216.7083333; Validation Loss = 500442369.56256926\n",
            "Cost after 42 iterations : Training Loss =  7.73153867055952e+47; Validation Loss = 6.351007860087722e+47\n",
            "Cost after 43 iterations : Training Loss =  3.602905973058009e+282; Validation Loss = 2.959559607244205e+282\n",
            "Cost after 44 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 45 iterations : Training Loss =  2406796.0155278514; Validation Loss = 2159041.6911092987\n",
            "Cost after 46 iterations : Training Loss =  3.127268344709477e+33; Validation Loss = 2.5688927098551185e+33\n",
            "Cost after 47 iterations : Training Loss =  1.5777864267240933e+196; Validation Loss = 1.2960518768047428e+196\n",
            "Cost after 48 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 49 iterations : Training Loss =  99391844.37342064; Validation Loss = 85020981.60302074\n",
            "Cost after 50 iterations : Training Loss =  1.5961277644802427e+43; Validation Loss = 1.3111349787434837e+43\n",
            "Cost after 51 iterations : Training Loss =  2.7890945734686484e+254; Validation Loss = 2.291064986530018e+254\n",
            "Cost after 52 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 53 iterations : Training Loss =  919383.1450236461; Validation Loss = 938558.0285681916\n",
            "Cost after 54 iterations : Training Loss =  9.011564380397513e+30; Validation Loss = 7.402601338798395e+30\n",
            "Cost after 55 iterations : Training Loss =  9.033549777566057e+180; Validation Loss = 7.420490572625385e+180\n",
            "Cost after 56 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 57 iterations : Training Loss =  8725191.203763347; Validation Loss = 7662557.051316834\n",
            "Cost after 58 iterations : Training Loss =  7.203137310223388e+36; Validation Loss = 5.917008785928899e+36\n",
            "Cost after 59 iterations : Training Loss =  2.3560661868720307e+216; Validation Loss = 1.9353595304481185e+216\n",
            "Cost after 60 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 61 iterations : Training Loss =  19304538.194668382; Validation Loss = 16013474.91151496\n",
            "Cost after 62 iterations : Training Loss =  8.544255828530745e+38; Validation Loss = 7.01849471650951e+38\n",
            "Cost after 63 iterations : Training Loss =  6.563005835390438e+228; Validation Loss = 5.391094482214394e+228\n",
            "Cost after 64 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 65 iterations : Training Loss =  954249661.6609012; Validation Loss = 785741097.4786774\n",
            "Cost after 66 iterations : Training Loss =  1.2581651876975982e+49; Validation Loss = 1.0334907426159606e+49\n",
            "Cost after 67 iterations : Training Loss =  6.6908811488142655e+289; Validation Loss = 5.496135956645674e+289\n",
            "Cost after 68 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 69 iterations : Training Loss =  45594.26895967396; Validation Loss = 51613.098065187514\n",
            "Cost after 70 iterations : Training Loss =  1.2585426106377233e+23; Validation Loss = 1.0336858517472783e+23\n",
            "Cost after 71 iterations : Training Loss =  6.702924751699408e+133; Validation Loss = 5.506028434467011e+133\n",
            "Cost after 72 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 73 iterations : Training Loss =  16011029929.890642; Validation Loss = 13142883427.00728\n",
            "Cost after 74 iterations : Training Loss =  2.8286331476462938e+56; Validation Loss = 2.323527754945288e+56\n",
            "Cost after 75 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 76 iterations : Training Loss =  104440137768.94254; Validation Loss = 86746569558.39046\n",
            "Cost after 77 iterations : Training Loss =  2.1815305710855503e+61; Validation Loss = 1.7920009991636782e+61\n",
            "Cost after 78 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 79 iterations : Training Loss =  277647.2229117167; Validation Loss = 248605.85353666104\n",
            "Cost after 80 iterations : Training Loss =  6.796259431110448e+27; Validation Loss = 5.582560489394851e+27\n",
            "Cost after 81 iterations : Training Loss =  1.662175882788924e+162; Validation Loss = 1.365372391717419e+162\n",
            "Cost after 82 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 83 iterations : Training Loss =  37344772.696966045; Validation Loss = 30740308.02489864\n",
            "Cost after 84 iterations : Training Loss =  4.505776958665994e+40; Validation Loss = 3.701191347135951e+40\n",
            "Cost after 85 iterations : Training Loss =  1.4114824188487117e+239; Validation Loss = 1.1594436038312913e+239\n",
            "Cost after 86 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 87 iterations : Training Loss =  466100670.7101191; Validation Loss = 412222288.65823525\n",
            "Cost after 88 iterations : Training Loss =  1.657140059886021e+47; Validation Loss = 1.3612701499613266e+47\n",
            "Cost after 89 iterations : Training Loss =  3.493112743185204e+278; Validation Loss = 2.869371486575607e+278\n",
            "Cost after 90 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 91 iterations : Training Loss =  133488.50291221685; Validation Loss = 123280.19848587064\n",
            "Cost after 92 iterations : Training Loss =  8.76329071715874e+25; Validation Loss = 7.198387352518422e+25\n",
            "Cost after 93 iterations : Training Loss =  7.639424068683811e+150; Validation Loss = 6.2753040930925005e+150\n",
            "Cost after 94 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 95 iterations : Training Loss =  2209962888.669673; Validation Loss = 1818955397.2740488\n",
            "Cost after 96 iterations : Training Loss =  1.9439276489132747e+51; Validation Loss = 1.5967949006387046e+51\n",
            "Cost after 97 iterations : Training Loss =  9.102022614202647e+302; Validation Loss = 7.476736273558507e+302\n",
            "Cost after 98 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 99 iterations : Training Loss =  40973436.903470874; Validation Loss = 35810140.277778424\n",
            "Cost after 100 iterations : Training Loss =  7.74913572403817e+40; Validation Loss = 6.365536502865964e+40\n",
            "Cost after 101 iterations : Training Loss =  3.65238816010331e+240; Validation Loss = 3.000206148274754e+240\n",
            "Cost after 102 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 103 iterations : Training Loss =  379813054.7484624; Validation Loss = 316324603.08164406\n",
            "Cost after 104 iterations : Training Loss =  5.026214244075387e+46; Validation Loss = 4.1287318712002927e+46\n",
            "Cost after 105 iterations : Training Loss =  2.7195892407444636e+275; Validation Loss = 2.2339707163032127e+275\n",
            "Cost after 106 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 107 iterations : Training Loss =  25950.404943968988; Validation Loss = 29436.751811859267\n",
            "Cost after 108 iterations : Training Loss =  4.22548660771268e+21; Validation Loss = 3.470297620393892e+21\n",
            "Cost after 109 iterations : Training Loss =  9.600994383169767e+124; Validation Loss = 7.886608532480904e+124\n",
            "Cost after 110 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 111 iterations : Training Loss =  719161384.5129967; Validation Loss = 609862994.1002928\n",
            "Cost after 112 iterations : Training Loss =  2.301522198589772e+48; Validation Loss = 1.8905786992210726e+48\n",
            "Cost after 113 iterations : Training Loss =  2.506965380832837e+285; Validation Loss = 2.0593136775306243e+285\n",
            "Cost after 114 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 115 iterations : Training Loss =  113561991.51672895; Validation Loss = 93181168.45546862\n",
            "Cost after 116 iterations : Training Loss =  3.580916452374358e+43; Validation Loss = 2.941510123944268e+43\n",
            "Cost after 117 iterations : Training Loss =  3.5564971505735853e+256; Validation Loss = 2.921437686190576e+256\n",
            "Cost after 118 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 119 iterations : Training Loss =  9010.022718777616; Validation Loss = 10920.341944223632\n",
            "Cost after 120 iterations : Training Loss =  3.008245914572145e+18; Validation Loss = 2.472022759911921e+18\n",
            "Cost after 121 iterations : Training Loss =  1.25006095029122e+106; Validation Loss = 1.02684637080403e+106\n",
            "Cost after 122 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 123 iterations : Training Loss =  4540946.051042394; Validation Loss = 4055665.265201794\n",
            "Cost after 124 iterations : Training Loss =  1.2612797827124186e+35; Validation Loss = 1.0360033274535962e+35\n",
            "Cost after 125 iterations : Training Loss =  6.790875849647457e+205; Validation Loss = 5.578274914959438e+205\n",
            "Cost after 126 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 127 iterations : Training Loss =  7088735710.959991; Validation Loss = 5817600413.8829775\n",
            "Cost after 128 iterations : Training Loss =  2.1306537833198157e+54; Validation Loss = 1.750187676955193e+54\n",
            "Cost after 129 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 130 iterations : Training Loss =  95691.7294727981; Validation Loss = 101909.64322167986\n",
            "Cost after 131 iterations : Training Loss =  1.11740790493343e+25; Validation Loss = 9.17840592246475e+24\n",
            "Cost after 132 iterations : Training Loss =  3.283429400001588e+145; Validation Loss = 2.69712966400133e+145\n",
            "Cost after 133 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 134 iterations : Training Loss =  94828667.4267004; Validation Loss = 82755931.51432143\n",
            "Cost after 135 iterations : Training Loss =  1.1899021386945187e+43; Validation Loss = 9.774481688705528e+42\n",
            "Cost after 136 iterations : Training Loss =  4.7876764049909537e+253; Validation Loss = 3.9327737334272026e+253\n",
            "Cost after 137 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 138 iterations : Training Loss =  2500475817.072103; Validation Loss = 2094827171.6629388\n",
            "Cost after 139 iterations : Training Loss =  4.090502350311144e+51; Validation Loss = 3.3601185155481614e+51\n",
            "Cost after 140 iterations : Training Loss =  7.901639486277755e+304; Validation Loss = 6.49069759952842e+304\n",
            "Cost after 141 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 142 iterations : Training Loss =  1115.3403172596197; Validation Loss = 1583.9616252456872\n",
            "Cost after 143 iterations : Training Loss =  354265690177.05804; Validation Loss = 293512997143.93774\n",
            "Cost after 144 iterations : Training Loss =  3.326384281048042e+64; Validation Loss = 2.732429224431312e+64\n",
            "Cost after 145 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 146 iterations : Training Loss =  732127810.540857; Validation Loss = 611526928.4793512\n",
            "Cost after 147 iterations : Training Loss =  2.486114512103497e+48; Validation Loss = 2.042125278803651e+48\n",
            "Cost after 148 iterations : Training Loss =  3.982759356820247e+285; Validation Loss = 3.271584990910564e+285\n",
            "Cost after 149 iterations : Training Loss =  inf; Validation Loss = inf\n",
            "Cost after 150 iterations : Training Loss =  6.558055152492234; Validation Loss = 8.921877975595299\n",
            "Cost after 151 iterations : Training Loss =  3.749898356941735; Validation Loss = 4.907689647237281\n",
            "Cost after 152 iterations : Training Loss =  3.4902007890985085; Validation Loss = 4.7599065817028805\n",
            "Cost after 153 iterations : Training Loss =  3.45636614898124; Validation Loss = 4.754214087108452\n",
            "Cost after 154 iterations : Training Loss =  3.4266215892350296; Validation Loss = 4.713901069874333\n",
            "Cost after 155 iterations : Training Loss =  3.3973510266274225; Validation Loss = 4.674398329236565\n",
            "Cost after 156 iterations : Training Loss =  3.368543590382535; Validation Loss = 4.635510346644535\n",
            "Cost after 157 iterations : Training Loss =  3.3401887925128553; Validation Loss = 4.59722668547718\n",
            "Cost after 158 iterations : Training Loss =  3.3122764530782285; Validation Loss = 4.559533804398734\n",
            "Cost after 159 iterations : Training Loss =  3.2847966890755385; Validation Loss = 4.5224185995350785\n",
            "Cost after 160 iterations : Training Loss =  3.2577399038248345; Validation Loss = 4.485868344174755\n",
            "Cost after 161 iterations : Training Loss =  3.2310967768037426; Validation Loss = 4.4498706757694455\n",
            "Cost after 162 iterations : Training Loss =  3.204858253908134; Validation Loss = 4.414413583151228\n",
            "Cost after 163 iterations : Training Loss =  3.1790155381186844; Validation Loss = 4.379485394281128\n",
            "Cost after 164 iterations : Training Loss =  3.153560080553662; Validation Loss = 4.345074764502454\n",
            "Cost after 165 iterations : Training Loss =  3.128483571889713; Validation Loss = 4.3111706652754\n",
            "Cost after 166 iterations : Training Loss =  3.103777934133033; Validation Loss = 4.277762373369838\n",
            "Cost after 167 iterations : Training Loss =  3.0794353127246503; Validation Loss = 4.244839460495132\n",
            "Cost after 168 iterations : Training Loss =  3.0554480689639405; Validation Loss = 4.212391783346304\n",
            "Cost after 169 iterations : Training Loss =  3.0318087727359173; Validation Loss = 4.180409474047618\n",
            "Cost after 170 iterations : Training Loss =  3.008510195527985; Validation Loss = 4.148882930975047\n",
            "Cost after 171 iterations : Training Loss =  2.985545303722984; Validation Loss = 4.11780280994033\n",
            "Cost after 172 iterations : Training Loss =  2.9629072521560347; Validation Loss = 4.0871600157203885\n",
            "Cost after 173 iterations : Training Loss =  2.940589377923106; Validation Loss = 4.056945693916374\n",
            "Cost after 174 iterations : Training Loss =  2.9185851944299794; Validation Loss = 4.027151223127473\n",
            "Cost after 175 iterations : Training Loss =  2.896888385670968; Validation Loss = 3.997768207425626\n",
            "Cost after 176 iterations : Training Loss =  2.8754928007270197; Validation Loss = 3.9687884691177335\n",
            "Cost after 177 iterations : Training Loss =  2.854392448473638; Validation Loss = 3.940204041782676\n",
            "Cost after 178 iterations : Training Loss =  2.8335814924893974; Validation Loss = 3.9120071635712175\n",
            "Cost after 179 iterations : Training Loss =  2.8130542461561157; Validation Loss = 3.884190270757125\n",
            "Cost after 180 iterations : Training Loss =  2.792805167942667; Validation Loss = 3.8567459915289692\n",
            "Cost after 181 iterations : Training Loss =  2.7728288568641957; Validation Loss = 3.829667140011909\n",
            "Cost after 182 iterations : Training Loss =  2.753120048109501; Validation Loss = 3.8029467105100467\n",
            "Cost after 183 iterations : Training Loss =  2.7336736088291445; Validation Loss = 3.776577871959397\n",
            "Cost after 184 iterations : Training Loss =  2.7144845340777164; Validation Loss = 3.7505539625832363\n",
            "Cost after 185 iterations : Training Loss =  2.6955479429035605; Validation Loss = 3.724868484740852\n",
            "Cost after 186 iterations : Training Loss =  2.676859074579907; Validation Loss = 3.6995150999617206\n",
            "Cost after 187 iterations : Training Loss =  2.6584132849715; Validation Loss = 3.6744876241576514\n",
            "Cost after 188 iterations : Training Loss =  2.6402060430310352; Validation Loss = 3.649780023005157\n",
            "Cost after 189 iterations : Training Loss =  2.6222329274201472; Validation Loss = 3.625386407491457\n",
            "Cost after 190 iterations : Training Loss =  2.6044896232498047; Validation Loss = 3.6013010296171024\n",
            "Cost after 191 iterations : Training Loss =  2.5869719189352263; Validation Loss = 3.5775182782490944\n",
            "Cost after 192 iterations : Training Loss =  2.569675703160775; Validation Loss = 3.5540326751183398\n",
            "Cost after 193 iterations : Training Loss =  2.552596961950232; Validation Loss = 3.530838870955573\n",
            "Cost after 194 iterations : Training Loss =  2.5357317758384106; Validation Loss = 3.507931641760416\n",
            "Cost after 195 iterations : Training Loss =  2.5190763171399424; Validation Loss = 3.4853058851979917\n",
            "Cost after 196 iterations : Training Loss =  2.5026268473114714; Validation Loss = 3.462956617118446\n",
            "Cost after 197 iterations : Training Loss =  2.486379714403476; Validation Loss = 3.440878968194139\n",
            "Cost after 198 iterations : Training Loss =  2.470331350598332; Validation Loss = 3.419068180670213\n",
            "Cost after 199 iterations : Training Loss =  2.4544782698311454; Validation Loss = 3.397519605223993\n",
            "Cost after 200 iterations : Training Loss =  2.438817065490179; Validation Loss = 3.376228697929054\n",
            "Cost after 201 iterations : Training Loss =  2.4233444081938513; Validation Loss = 3.3551910173198793\n",
            "Cost after 202 iterations : Training Loss =  2.4080570436412985; Validation Loss = 3.334402221553354\n",
            "Cost after 203 iterations : Training Loss =  2.3929517905337057; Validation Loss = 3.313858065663372\n",
            "Cost after 204 iterations : Training Loss =  2.378025538563779; Validation Loss = 3.293554398904933\n",
            "Cost after 205 iterations : Training Loss =  2.3632752464706916; Validation Loss = 3.2734871621845736\n",
            "Cost after 206 iterations : Training Loss =  2.348697940158126; Validation Loss = 3.2536523855736803\n",
            "Cost after 207 iterations : Training Loss =  2.3342907108730224; Validation Loss = 3.234046185901724\n",
            "Cost after 208 iterations : Training Loss =  2.3200507134427277; Validation Loss = 3.2146647644264177\n",
            "Cost after 209 iterations : Training Loss =  2.305975164568459; Validation Loss = 3.1955044045779664\n",
            "Cost after 210 iterations : Training Loss =  2.292061341173017; Validation Loss = 3.176561469774723\n",
            "Cost after 211 iterations : Training Loss =  2.278306578800621; Validation Loss = 3.157832401307523\n",
            "Cost after 212 iterations : Training Loss =  2.2647082700671457; Validation Loss = 3.1393137162903386\n",
            "Cost after 213 iterations : Training Loss =  2.2512638631588486; Validation Loss = 3.1210020056748444\n",
            "Cost after 214 iterations : Training Loss =  2.237970860377778; Validation Loss = 3.102893932326461\n",
            "Cost after 215 iterations : Training Loss =  2.224826816732332; Validation Loss = 3.0849862291598935\n",
            "Cost after 216 iterations : Training Loss =  2.2118293385712127; Validation Loss = 3.0672756973317825\n",
            "Cost after 217 iterations : Training Loss =  2.1989760822592763; Validation Loss = 3.049759204488674\n",
            "Cost after 218 iterations : Training Loss =  2.186264752893916; Validation Loss = 3.032433683068298\n",
            "Cost after 219 iterations : Training Loss =  2.173693103060278; Validation Loss = 3.015296128652142\n",
            "Cost after 220 iterations : Training Loss =  2.161258931624279; Validation Loss = 2.998343598367755\n",
            "Cost after 221 iterations : Training Loss =  2.1489600825618327; Validation Loss = 2.9815732093388467\n",
            "Cost after 222 iterations : Training Loss =  2.1367944438231903; Validation Loss = 2.964982137181724\n",
            "Cost after 223 iterations : Training Loss =  2.1247599462311157; Validation Loss = 2.9485676145462207\n",
            "Cost after 224 iterations : Training Loss =  2.1128545624117505; Validation Loss = 2.932326929699891\n",
            "Cost after 225 iterations : Training Loss =  2.101076305757044; Validation Loss = 2.9162574251537685\n",
            "Cost after 226 iterations : Training Loss =  2.089423229417668; Validation Loss = 2.9003564963283277\n",
            "Cost after 227 iterations : Training Loss =  2.077893425325415; Validation Loss = 2.8846215902583543\n",
            "Cost after 228 iterations : Training Loss =  2.0664850232440135; Validation Loss = 2.869050204335384\n",
            "Cost after 229 iterations : Training Loss =  2.055196189847506; Validation Loss = 2.8536398850864026\n",
            "Cost after 230 iterations : Training Loss =  2.044025127825183; Validation Loss = 2.8383882269876755\n",
            "Cost after 231 iterations : Training Loss =  2.0329700750121806; Validation Loss = 2.8232928713124625\n",
            "Cost after 232 iterations : Training Loss =  2.0220293035450605; Validation Loss = 2.808351505011639\n",
            "Cost after 233 iterations : Training Loss =  2.0112011190412318; Validation Loss = 2.793561859625872\n",
            "Cost after 234 iterations : Training Loss =  2.000483859801777; Validation Loss = 2.7789217102286763\n",
            "Cost after 235 iterations : Training Loss =  1.9898758960366862; Validation Loss = 2.7644288743990586\n",
            "Cost after 236 iterations : Training Loss =  1.9793756291118423; Validation Loss = 2.7500812112229034\n",
            "Cost after 237 iterations : Training Loss =  1.968981490817061; Validation Loss = 2.735876620322132\n",
            "Cost after 238 iterations : Training Loss =  1.95869194265452; Validation Loss = 2.721813040910871\n",
            "Cost after 239 iterations : Training Loss =  1.948505475146848; Validation Loss = 2.7078884508775354\n",
            "Cost after 240 iterations : Training Loss =  1.9384206071643613; Validation Loss = 2.6941008658922567\n",
            "Cost after 241 iterations : Training Loss =  1.9284358852707102; Validation Loss = 2.6804483385385844\n",
            "Cost after 242 iterations : Training Loss =  1.9185498830864658; Validation Loss = 2.666928957468973\n",
            "Cost after 243 iterations : Training Loss =  1.908761200670004; Validation Loss = 2.6535408465830805\n",
            "Cost after 244 iterations : Training Loss =  1.8990684639151918; Validation Loss = 2.640282164228274\n",
            "Cost after 245 iterations : Training Loss =  1.8894703239652857; Validation Loss = 2.6271511024216396\n",
            "Cost after 246 iterations : Training Loss =  1.8799654566426431; Validation Loss = 2.6141458860928246\n",
            "Cost after 247 iterations : Training Loss =  1.8705525618936332; Validation Loss = 2.601264772347018\n",
            "Cost after 248 iterations : Training Loss =  1.86123036324837; Validation Loss = 2.5885060497475463\n",
            "Cost after 249 iterations : Training Loss =  1.8519976072947943; Validation Loss = 2.5758680376173686\n",
            "Cost after 250 iterations : Training Loss =  1.842853063166586; Validation Loss = 2.563349085359038\n",
            "Cost after 251 iterations : Training Loss =  1.8337955220446276; Validation Loss = 2.5509475717924013\n",
            "Cost after 252 iterations : Training Loss =  1.8248237966714929; Validation Loss = 2.5386619045096315\n",
            "Cost after 253 iterations : Training Loss =  1.8159367208785742; Validation Loss = 2.5264905192470084\n",
            "Cost after 254 iterations : Training Loss =  1.8071331491255505; Validation Loss = 2.514431879272945\n",
            "Cost after 255 iterations : Training Loss =  1.798411956051748; Validation Loss = 2.5024844747918555\n",
            "Cost after 256 iterations : Training Loss =  1.7897720360390335; Validation Loss = 2.4906468223632006\n",
            "Cost after 257 iterations : Training Loss =  1.781212302785984; Validation Loss = 2.4789174643355163\n",
            "Cost after 258 iterations : Training Loss =  1.7727316888928877; Validation Loss = 2.4672949682947585\n",
            "Cost after 259 iterations : Training Loss =  1.7643291454573324; Validation Loss = 2.455777926526704\n",
            "Cost after 260 iterations : Training Loss =  1.7560036416800566; Validation Loss = 2.444364955492882\n",
            "Cost after 261 iterations : Training Loss =  1.7477541644807177; Validation Loss = 2.433054695319682\n",
            "Cost after 262 iterations : Training Loss =  1.7395797181233643; Validation Loss = 2.4218458093003177\n",
            "Cost after 263 iterations : Training Loss =  1.7314793238512776; Validation Loss = 2.4107369834091883\n",
            "Cost after 264 iterations : Training Loss =  1.7234520195308844; Validation Loss = 2.3997269258282588\n",
            "Cost after 265 iterations : Training Loss =  1.7154968593045887; Validation Loss = 2.388814366485234\n",
            "Cost after 266 iterations : Training Loss =  1.7076129132521405; Validation Loss = 2.377998056603129\n",
            "Cost after 267 iterations : Training Loss =  1.6997992670603848; Validation Loss = 2.367276768260835\n",
            "Cost after 268 iterations : Training Loss =  1.6920550217010948; Validation Loss = 2.356649293964481\n",
            "Cost after 269 iterations : Training Loss =  1.6843792931167132; Validation Loss = 2.3461144462292287\n",
            "Cost after 270 iterations : Training Loss =  1.6767712119137321; Validation Loss = 2.335671057171236\n",
            "Cost after 271 iterations : Training Loss =  1.6692299230635381; Validation Loss = 2.3253179781094757\n",
            "Cost after 272 iterations : Training Loss =  1.6617545856104499; Validation Loss = 2.315054079177128\n",
            "Cost after 273 iterations : Training Loss =  1.6543443723868396; Validation Loss = 2.304878248942327\n",
            "Cost after 274 iterations : Training Loss =  1.6469984697350517; Validation Loss = 2.2947893940379736\n",
            "Cost after 275 iterations : Training Loss =  1.639716077235991; Validation Loss = 2.2847864388003396\n",
            "Cost after 276 iterations : Training Loss =  1.6324964074441517; Validation Loss = 2.2748683249162376\n",
            "Cost after 277 iterations : Training Loss =  1.625338685628958; Validation Loss = 2.2650340110785865\n",
            "Cost after 278 iterations : Training Loss =  1.6182421495222108; Validation Loss = 2.255282472650039\n",
            "Cost after 279 iterations : Training Loss =  1.6112060490714555; Validation Loss = 2.245612701334496\n",
            "Cost after 280 iterations : Training Loss =  1.6042296461991732; Validation Loss = 2.236023704856304\n",
            "Cost after 281 iterations : Training Loss =  1.5973122145675804; Validation Loss = 2.226514506646978\n",
            "Cost after 282 iterations : Training Loss =  1.590453039348891; Validation Loss = 2.217084145539069\n",
            "Cost after 283 iterations : Training Loss =  1.5836514170009408; Validation Loss = 2.207731675467249\n",
            "Cost after 284 iterations : Training Loss =  1.576906655047986; Validation Loss = 2.198456165176169\n",
            "Cost after 285 iterations : Training Loss =  1.5702180718665089; Validation Loss = 2.189256697935039\n",
            "Cost after 286 iterations : Training Loss =  1.5635849964760038; Validation Loss = 2.180132371258815\n",
            "Cost after 287 iterations : Training Loss =  1.557006768334498; Validation Loss = 2.1710822966356016\n",
            "Cost after 288 iterations : Training Loss =  1.5504827371387515; Validation Loss = 2.162105599260359\n",
            "Cost after 289 iterations : Training Loss =  1.5440122626289874; Validation Loss = 2.1532014177745578\n",
            "Cost after 290 iterations : Training Loss =  1.5375947143980262; Validation Loss = 2.1443689040117224\n",
            "Cost after 291 iterations : Training Loss =  1.5312294717047312; Validation Loss = 2.1356072227486997\n",
            "Cost after 292 iterations : Training Loss =  1.524915923291641; Validation Loss = 2.1269155514624534\n",
            "Cost after 293 iterations : Training Loss =  1.51865346720669; Validation Loss = 2.1182930800923305\n",
            "Cost after 294 iterations : Training Loss =  1.5124415106288709; Validation Loss = 2.109739010807549\n",
            "Cost after 295 iterations : Training Loss =  1.5062794696978201; Validation Loss = 2.101252557779886\n",
            "Cost after 296 iterations : Training Loss =  1.5001667693471155; Validation Loss = 2.0928329469613316\n",
            "Cost after 297 iterations : Training Loss =  1.4941028431413128; Validation Loss = 2.0844794158666624\n",
            "Cost after 298 iterations : Training Loss =  1.4880871331165082; Validation Loss = 2.0761912133607687\n",
            "Cost after 299 iterations : Training Loss =  1.482119089624438; Validation Loss = 2.0679675994506024\n",
            "Cost after 300 iterations : Training Loss =  1.4761981711799304; Validation Loss = 2.059807845081657\n",
            "Cost after 301 iterations : Training Loss =  1.4703238443117386; Validation Loss = 2.0517112319388806\n",
            "Cost after 302 iterations : Training Loss =  1.4644955834165583; Validation Loss = 2.043677052251889\n",
            "Cost after 303 iterations : Training Loss =  1.4587128706162065; Validation Loss = 2.035704608604284\n",
            "Cost after 304 iterations : Training Loss =  1.4529751956178871; Validation Loss = 2.027793213747202\n",
            "Cost after 305 iterations : Training Loss =  1.447282055577448; Validation Loss = 2.0199421904167405\n",
            "Cost after 306 iterations : Training Loss =  1.4416329549655686; Validation Loss = 2.012150871155304\n",
            "Cost after 307 iterations : Training Loss =  1.436027405436785; Validation Loss = 2.004418598136781\n",
            "Cost after 308 iterations : Training Loss =  1.4304649257012858; Validation Loss = 1.9967447229953366\n",
            "Cost after 309 iterations : Training Loss =  1.4249450413994762; Validation Loss = 1.9891286066578904\n",
            "Cost after 310 iterations : Training Loss =  1.41946728497911; Validation Loss = 1.981569619180077\n",
            "Cost after 311 iterations : Training Loss =  1.4140311955750766; Validation Loss = 1.9740671395856726\n",
            "Cost after 312 iterations : Training Loss =  1.408636318891647; Validation Loss = 1.966620555709333\n",
            "Cost after 313 iterations : Training Loss =  1.4032822070872155; Validation Loss = 1.9592292640426285\n",
            "Cost after 314 iterations : Training Loss =  1.3979684186614318; Validation Loss = 1.9518926695832415\n",
            "Cost after 315 iterations : Training Loss =  1.3926945183446702; Validation Loss = 1.9446101856873663\n",
            "Cost after 316 iterations : Training Loss =  1.3874600769897925; Validation Loss = 1.937381233925038\n",
            "Cost after 317 iterations : Training Loss =  1.3822646714661087; Validation Loss = 1.930205243938483\n",
            "Cost after 318 iterations : Training Loss =  1.3771078845555775; Validation Loss = 1.9230816533033888\n",
            "Cost after 319 iterations : Training Loss =  1.3719893048510639; Validation Loss = 1.9160099073929553\n",
            "Cost after 320 iterations : Training Loss =  1.3669085266567251; Validation Loss = 1.908989459244786\n",
            "Cost after 321 iterations : Training Loss =  1.3618651498903898; Validation Loss = 1.902019769430342\n",
            "Cost after 322 iterations : Training Loss =  1.3568587799879301; Validation Loss = 1.8951003059272091\n",
            "Cost after 323 iterations : Training Loss =  1.3518890278095756; Validation Loss = 1.8882305439937854\n",
            "Cost after 324 iterations : Training Loss =  1.346955509548081; Validation Loss = 1.8814099660465688\n",
            "Cost after 325 iterations : Training Loss =  1.3420578466387736; Validation Loss = 1.8746380615398497\n",
            "Cost after 326 iterations : Training Loss =  1.3371956656713908; Validation Loss = 1.8679143268478573\n",
            "Cost after 327 iterations : Training Loss =  1.3323685983036615; Validation Loss = 1.8612382651492068\n",
            "Cost after 328 iterations : Training Loss =  1.3275762811766214; Validation Loss = 1.8546093863136495\n",
            "Cost after 329 iterations : Training Loss =  1.3228183558316011; Validation Loss = 1.8480272067910386\n",
            "Cost after 330 iterations : Training Loss =  1.3180944686288631; Validation Loss = 1.8414912495025417\n",
            "Cost after 331 iterations : Training Loss =  1.3134042706678304; Validation Loss = 1.835001043733904\n",
            "Cost after 332 iterations : Training Loss =  1.308747417708874; Validation Loss = 1.8285561250308697\n",
            "Cost after 333 iterations : Training Loss =  1.3041235700966676; Validation Loss = 1.8221560350966193\n",
            "Cost after 334 iterations : Training Loss =  1.2995323926849938; Validation Loss = 1.8158003216911873\n",
            "Cost after 335 iterations : Training Loss =  1.2949735547630352; Validation Loss = 1.8094885385328476\n",
            "Cost after 336 iterations : Training Loss =  1.2904467299831108; Validation Loss = 1.8032202452013906\n",
            "Cost after 337 iterations : Training Loss =  1.2859515962897714; Validation Loss = 1.796995007043307\n",
            "Cost after 338 iterations : Training Loss =  1.281487835850294; Validation Loss = 1.7908123950787027\n",
            "Cost after 339 iterations : Training Loss =  1.2770551349864854; Validation Loss = 1.7846719859100888\n",
            "Cost after 340 iterations : Training Loss =  1.2726531841078275; Validation Loss = 1.7785733616328616\n",
            "Cost after 341 iterations : Training Loss =  1.2682816776458627; Validation Loss = 1.772516109747516\n",
            "Cost after 342 iterations : Training Loss =  1.2639403139898577; Validation Loss = 1.7664998230734912\n",
            "Cost after 343 iterations : Training Loss =  1.2596287954236574; Validation Loss = 1.76052409966467\n",
            "Cost after 344 iterations : Training Loss =  1.2553468280637758; Validation Loss = 1.7545885427264447\n",
            "Cost after 345 iterations : Training Loss =  1.2510941217986107; Validation Loss = 1.7486927605344114\n",
            "Cost after 346 iterations : Training Loss =  1.24687039022883; Validation Loss = 1.7428363663544877\n",
            "Cost after 347 iterations : Training Loss =  1.2426753506088744; Validation Loss = 1.7370189783646022\n",
            "Cost after 348 iterations : Training Loss =  1.238508723789541; Validation Loss = 1.7312402195778132\n",
            "Cost after 349 iterations : Training Loss =  1.234370234161648; Validation Loss = 1.7254997177668592\n",
            "Cost after 350 iterations : Training Loss =  1.2302596096007394; Validation Loss = 1.7197971053900893\n",
            "Cost after 351 iterations : Training Loss =  1.2261765814128287; Validation Loss = 1.7141320195188012\n",
            "Cost after 352 iterations : Training Loss =  1.2221208842811178; Validation Loss = 1.7085041017658917\n",
            "Cost after 353 iterations : Training Loss =  1.2180922562137357; Validation Loss = 1.7029129982158153\n",
            "Cost after 354 iterations : Training Loss =  1.2140904384923952; Validation Loss = 1.697358359355819\n",
            "Cost after 355 iterations : Training Loss =  1.210115175622029; Validation Loss = 1.6918398400084573\n",
            "Cost after 356 iterations : Training Loss =  1.2061662152813215; Validation Loss = 1.6863570992653343\n",
            "Cost after 357 iterations : Training Loss =  1.2022433082741422; Validation Loss = 1.680909800422\n",
            "Cost after 358 iterations : Training Loss =  1.1983462084818717; Validation Loss = 1.6754976109140898\n",
            "Cost after 359 iterations : Training Loss =  1.1944746728165934; Validation Loss = 1.6701202022545751\n",
            "Cost after 360 iterations : Training Loss =  1.1906284611751043; Validation Loss = 1.6647772499721532\n",
            "Cost after 361 iterations : Training Loss =  1.1868073363937905; Validation Loss = 1.6594684335507486\n",
            "Cost after 362 iterations : Training Loss =  1.1830110642042875; Validation Loss = 1.6541934363700892\n",
            "Cost after 363 iterations : Training Loss =  1.1792394131899306; Validation Loss = 1.6489519456473318\n",
            "Cost after 364 iterations : Training Loss =  1.1754921547430106; Validation Loss = 1.6437436523797644\n",
            "Cost after 365 iterations : Training Loss =  1.1717690630227602; Validation Loss = 1.6385682512884787\n",
            "Cost after 366 iterations : Training Loss =  1.1680699149141074; Validation Loss = 1.6334254407630604\n",
            "Cost after 367 iterations : Training Loss =  1.1643944899871392; Validation Loss = 1.628314922807271\n",
            "Cost after 368 iterations : Training Loss =  1.1607425704572953; Validation Loss = 1.6232364029856263\n",
            "Cost after 369 iterations : Training Loss =  1.1571139411462685; Validation Loss = 1.6181895903710128\n",
            "Cost after 370 iterations : Training Loss =  1.1535083894435556; Validation Loss = 1.6131741974930969\n",
            "Cost after 371 iterations : Training Loss =  1.1499257052687388; Validation Loss = 1.6081899402877509\n",
            "Cost after 372 iterations : Training Loss =  1.146365681034365; Validation Loss = 1.6032365380472533\n",
            "Cost after 373 iterations : Training Loss =  1.1428281116095131; Validation Loss = 1.5983137133714627\n",
            "Cost after 374 iterations : Training Loss =  1.1393127942839807; Validation Loss = 1.5934211921197163\n",
            "Cost after 375 iterations : Training Loss =  1.1358195287330786; Validation Loss = 1.5885587033636577\n",
            "Cost after 376 iterations : Training Loss =  1.1323481169830483; Validation Loss = 1.5837259793408272\n",
            "Cost after 377 iterations : Training Loss =  1.1288983633770797; Validation Loss = 1.5789227554090597\n",
            "Cost after 378 iterations : Training Loss =  1.1254700745418835; Validation Loss = 1.5741487700016492\n",
            "Cost after 379 iterations : Training Loss =  1.1220630593548744; Validation Loss = 1.5694037645832994\n",
            "Cost after 380 iterations : Training Loss =  1.118677128911871; Validation Loss = 1.564687483606835\n",
            "Cost after 381 iterations : Training Loss =  1.1153120964953924; Validation Loss = 1.5599996744705584\n",
            "Cost after 382 iterations : Training Loss =  1.1119677775434618; Validation Loss = 1.5553400874764947\n",
            "Cost after 383 iterations : Training Loss =  1.108643989618936; Validation Loss = 1.5507084757891534\n",
            "Cost after 384 iterations : Training Loss =  1.1053405523793705; Validation Loss = 1.546104595395123\n",
            "Cost after 385 iterations : Training Loss =  1.102057287547395; Validation Loss = 1.5415282050632704\n",
            "Cost after 386 iterations : Training Loss =  1.098794018881552; Validation Loss = 1.5369790663056564\n",
            "Cost after 387 iterations : Training Loss =  1.0955505721476717; Validation Loss = 1.53245694333904\n",
            "Cost after 388 iterations : Training Loss =  1.0923267750906878; Validation Loss = 1.527961603047096\n",
            "Cost after 389 iterations : Training Loss =  1.0891224574069371; Validation Loss = 1.5234928149431946\n",
            "Cost after 390 iterations : Training Loss =  1.0859374507169255; Validation Loss = 1.5190503511338624\n",
            "Cost after 391 iterations : Training Loss =  1.082771588538525; Validation Loss = 1.514633986282767\n",
            "Cost after 392 iterations : Training Loss =  1.0796247062606474; Validation Loss = 1.5102434975753838\n",
            "Cost after 393 iterations : Training Loss =  1.0764966411173087; Validation Loss = 1.5058786646841669\n",
            "Cost after 394 iterations : Training Loss =  1.0733872321621645; Validation Loss = 1.501539269734345\n",
            "Cost after 395 iterations : Training Loss =  1.0702963202434255; Validation Loss = 1.4972250972702386\n",
            "Cost after 396 iterations : Training Loss =  1.0672237479792028; Validation Loss = 1.4929359342221582\n",
            "Cost after 397 iterations : Training Loss =  1.064169359733255; Validation Loss = 1.4886715698738004\n",
            "Cost after 398 iterations : Training Loss =  1.0611330015911347; Validation Loss = 1.4844317958302324\n",
            "Cost after 399 iterations : Training Loss =  1.0581145213366887; Validation Loss = 1.4802164059863137\n",
            "Cost after 400 iterations : Training Loss =  1.0551137684289953; Validation Loss = 1.4760251964957198\n",
            "Cost after 401 iterations : Training Loss =  1.0521305939796182; Validation Loss = 1.4718579657403832\n",
            "Cost after 402 iterations : Training Loss =  1.049164850730267; Validation Loss = 1.4677145143004717\n",
            "Cost after 403 iterations : Training Loss =  1.0462163930307915; Validation Loss = 1.463594644924829\n",
            "Cost after 404 iterations : Training Loss =  1.0432850768175386; Validation Loss = 1.459498162501911\n",
            "Cost after 405 iterations : Training Loss =  1.0403707595920606; Validation Loss = 1.4554248740311306\n",
            "Cost after 406 iterations : Training Loss =  1.0374733004001515; Validation Loss = 1.4513745885947291\n",
            "Cost after 407 iterations : Training Loss =  1.0345925598112256; Validation Loss = 1.4473471173300425\n",
            "Cost after 408 iterations : Training Loss =  1.031728399898005; Validation Loss = 1.4433422734022157\n",
            "Cost after 409 iterations : Training Loss =  1.028880684216567; Validation Loss = 1.4393598719773681\n",
            "Cost after 410 iterations : Training Loss =  1.0260492777866634; Validation Loss = 1.435399730196159\n",
            "Cost after 411 iterations : Training Loss =  1.0232340470723718; Validation Loss = 1.431461667147762\n",
            "Cost after 412 iterations : Training Loss =  1.0204348599630486; Validation Loss = 1.4275455038442797\n",
            "Cost after 413 iterations : Training Loss =  1.0176515857545838; Validation Loss = 1.4236510631955375\n",
            "Cost after 414 iterations : Training Loss =  1.0148840951309337; Validation Loss = 1.4197781699842427\n",
            "Cost after 415 iterations : Training Loss =  1.0121322601459597; Validation Loss = 1.4159266508415889\n",
            "Cost after 416 iterations : Training Loss =  1.0093959542055297; Validation Loss = 1.4120963342231863\n",
            "Cost after 417 iterations : Training Loss =  1.0066750520499248; Validation Loss = 1.4082870503853886\n",
            "Cost after 418 iterations : Training Loss =  1.0039694297364874; Validation Loss = 1.4044986313619807\n",
            "Cost after 419 iterations : Training Loss =  1.0012789646225566; Validation Loss = 1.400730910941222\n",
            "Cost after 420 iterations : Training Loss =  0.9986035353486634; Validation Loss = 1.396983724643258\n",
            "Cost after 421 iterations : Training Loss =  0.9959430218219686; Validation Loss = 1.393256909697829\n",
            "Cost after 422 iterations : Training Loss =  0.9932973051999908; Validation Loss = 1.3895503050224032\n",
            "Cost after 423 iterations : Training Loss =  0.9906662678745322; Validation Loss = 1.3858637512005407\n",
            "Cost after 424 iterations : Training Loss =  0.9880497934558988; Validation Loss = 1.3821970904606775\n",
            "Cost after 425 iterations : Training Loss =  0.9854477667573208; Validation Loss = 1.3785501666551725\n",
            "Cost after 426 iterations : Training Loss =  0.9828600737796435; Validation Loss = 1.3749228252397114\n",
            "Cost after 427 iterations : Training Loss =  0.9802866016962156; Validation Loss = 1.3713149132529632\n",
            "Cost after 428 iterations : Training Loss =  0.9777272388380374; Validation Loss = 1.367726279296625\n",
            "Cost after 429 iterations : Training Loss =  0.9751818746791078; Validation Loss = 1.364156773515681\n",
            "Cost after 430 iterations : Training Loss =  0.9726503998220107; Validation Loss = 1.3606062475790404\n",
            "Cost after 431 iterations : Training Loss =  0.9701327059837026; Validation Loss = 1.357074554660374\n",
            "Cost after 432 iterations : Training Loss =  0.9676286859815219; Validation Loss = 1.3535615494193312\n",
            "Cost after 433 iterations : Training Loss =  0.9651382337193934; Validation Loss = 1.3500670879829506\n",
            "Cost after 434 iterations : Training Loss =  0.9626612441742693; Validation Loss = 1.3465910279274085\n",
            "Cost after 435 iterations : Training Loss =  0.960197613382737; Validation Loss = 1.3431332282600097\n",
            "Cost after 436 iterations : Training Loss =  0.9577472384278248; Validation Loss = 1.3396935494014286\n",
            "Cost after 437 iterations : Training Loss =  0.9553100174260513; Validation Loss = 1.3362718531682791\n",
            "Cost after 438 iterations : Training Loss =  0.9528858495145975; Validation Loss = 1.3328680027558315\n",
            "Cost after 439 iterations : Training Loss =  0.9504746348387164; Validation Loss = 1.329481862721092\n",
            "Cost after 440 iterations : Training Loss =  0.9480762745393108; Validation Loss = 1.3261132989660624\n",
            "Cost after 441 iterations : Training Loss =  0.9456906707406959; Validation Loss = 1.3227621787212624\n",
            "Cost after 442 iterations : Training Loss =  0.9433177265385263; Validation Loss = 1.3194283705295038\n",
            "Cost after 443 iterations : Training Loss =  0.9409573459879275; Validation Loss = 1.316111744229868\n",
            "Cost after 444 iterations : Training Loss =  0.9386094340917687; Validation Loss = 1.3128121709419622\n",
            "Cost after 445 iterations : Training Loss =  0.9362738967891318; Validation Loss = 1.309529523050373\n",
            "Cost after 446 iterations : Training Loss =  0.9339506409439281; Validation Loss = 1.3062636741893232\n",
            "Cost after 447 iterations : Training Loss =  0.9316395743336935; Validation Loss = 1.3030144992276085\n",
            "Cost after 448 iterations : Training Loss =  0.9293406056385284; Validation Loss = 1.2997818742537055\n",
            "Cost after 449 iterations : Training Loss =  0.9270536444302203; Validation Loss = 1.2965656765610893\n",
            "Cost after 450 iterations : Training Loss =  0.924778601161494; Validation Loss = 1.2933657846338034\n",
            "Cost after 451 iterations : Training Loss =  0.9225153871554447; Validation Loss = 1.2901820781321829\n",
            "Cost after 452 iterations : Training Loss =  0.9202639145950916; Validation Loss = 1.287014437878821\n",
            "Cost after 453 iterations : Training Loss =  0.9180240965131137; Validation Loss = 1.2838627458447216\n",
            "Cost after 454 iterations : Training Loss =  0.9157958467816926; Validation Loss = 1.2807268851356244\n",
            "Cost after 455 iterations : Training Loss =  0.9135790801025432; Validation Loss = 1.2776067399785773\n",
            "Cost after 456 iterations : Training Loss =  0.9113737119970482; Validation Loss = 1.2745021957086444\n",
            "Cost after 457 iterations : Training Loss =  0.909179658796539; Validation Loss = 1.2714131387558127\n",
            "Cost after 458 iterations : Training Loss =  0.9069968376327414; Validation Loss = 1.268339456632119\n",
            "Cost after 459 iterations : Training Loss =  0.9048251664283186; Validation Loss = 1.2652810379189157\n",
            "Cost after 460 iterations : Training Loss =  0.9026645638875681; Validation Loss = 1.2622377722543132\n",
            "Cost after 461 iterations : Training Loss =  0.9005149494872382; Validation Loss = 1.2592095503208502\n",
            "Cost after 462 iterations : Training Loss =  0.8983762434674881; Validation Loss = 1.2561962638332638\n",
            "Cost after 463 iterations : Training Loss =  0.8962483668229483; Validation Loss = 1.253197805526472\n",
            "Cost after 464 iterations : Training Loss =  0.894131241293937; Validation Loss = 1.250214069143723\n",
            "Cost after 465 iterations : Training Loss =  0.8920247893577655; Validation Loss = 1.2472449494249\n",
            "Cost after 466 iterations : Training Loss =  0.8899289342201897; Validation Loss = 1.2442903420949722\n",
            "Cost after 467 iterations : Training Loss =  0.8878435998069664; Validation Loss = 1.2413501438526242\n",
            "Cost after 468 iterations : Training Loss =  0.8857687107555319; Validation Loss = 1.238424252359046\n",
            "Cost after 469 iterations : Training Loss =  0.8837041924067824; Validation Loss = 1.2355125662268613\n",
            "Cost after 470 iterations : Training Loss =  0.881649970796989; Validation Loss = 1.232614985009207\n",
            "Cost after 471 iterations : Training Loss =  0.8796059726497998; Validation Loss = 1.2297314091889713\n",
            "Cost after 472 iterations : Training Loss =  0.8775721253683753; Validation Loss = 1.226861740168168\n",
            "Cost after 473 iterations : Training Loss =  0.8755483570275975; Validation Loss = 1.224005880257477\n",
            "Cost after 474 iterations : Training Loss =  0.873534596366433; Validation Loss = 1.2211637326658842\n",
            "Cost after 475 iterations : Training Loss =  0.8715307727803441; Validation Loss = 1.2183352014905249\n",
            "Cost after 476 iterations : Training Loss =  0.869536816313845; Validation Loss = 1.2155201917065852\n",
            "Cost after 477 iterations : Training Loss =  0.8675526576531434; Validation Loss = 1.2127186091574098\n",
            "Cost after 478 iterations : Training Loss =  0.8655782281188782; Validation Loss = 1.209930360544702\n",
            "Cost after 479 iterations : Training Loss =  0.8636134596589576; Validation Loss = 1.207155353418872\n",
            "Cost after 480 iterations : Training Loss =  0.8616582848414933; Validation Loss = 1.2043934961695038\n",
            "Cost after 481 iterations : Training Loss =  0.8597126368478353; Validation Loss = 1.2016446980159545\n",
            "Cost after 482 iterations : Training Loss =  0.8577764494656916; Validation Loss = 1.1989088689980942\n",
            "Cost after 483 iterations : Training Loss =  0.8558496570823326; Validation Loss = 1.1961859199671214\n",
            "Cost after 484 iterations : Training Loss =  0.8539321946779108; Validation Loss = 1.1934757625765677\n",
            "Cost after 485 iterations : Training Loss =  0.8520239978188429; Validation Loss = 1.1907783092733664\n",
            "Cost after 486 iterations : Training Loss =  0.8501250026512941; Validation Loss = 1.188093473289054\n",
            "Cost after 487 iterations : Training Loss =  0.84823514589475; Validation Loss = 1.1854211686311107\n",
            "Cost after 488 iterations : Training Loss =  0.8463543648356503; Validation Loss = 1.1827613100743792\n",
            "Cost after 489 iterations : Training Loss =  0.8444825973211476; Validation Loss = 1.1801138131526239\n",
            "Cost after 490 iterations : Training Loss =  0.8426197817529071; Validation Loss = 1.1774785941501853\n",
            "Cost after 491 iterations : Training Loss =  0.8407658570810118; Validation Loss = 1.1748555700937442\n",
            "Cost after 492 iterations : Training Loss =  0.8389207627979408; Validation Loss = 1.1722446587442097\n",
            "Cost after 493 iterations : Training Loss =  0.8370844389326187; Validation Loss = 1.1696457785886833\n",
            "Cost after 494 iterations : Training Loss =  0.8352568260445677; Validation Loss = 1.1670588488325608\n",
            "Cost after 495 iterations : Training Loss =  0.833437865218092; Validation Loss = 1.164483789391702\n",
            "Cost after 496 iterations : Training Loss =  0.8316274980565878; Validation Loss = 1.161920520884737\n",
            "Cost after 497 iterations : Training Loss =  0.8298256666768786; Validation Loss = 1.159368964625442\n",
            "Cost after 498 iterations : Training Loss =  0.8280323137036749; Validation Loss = 1.1568290426152246\n",
            "Cost after 499 iterations : Training Loss =  0.8262473822640484; Validation Loss = 1.154300677535722\n",
            "Cost after 500 iterations : Training Loss =  0.824470815982033; Validation Loss = 1.15178379274146\n",
            "Cost after 501 iterations : Training Loss =  0.8227025589732523; Validation Loss = 1.1492783122526313\n",
            "Cost after 502 iterations : Training Loss =  0.8209425558396363; Validation Loss = 1.1467841607479718\n",
            "Cost after 503 iterations : Training Loss =  0.8191907516642045; Validation Loss = 1.1443012635576801\n",
            "Cost after 504 iterations : Training Loss =  0.8174470920059111; Validation Loss = 1.1418295466565136\n",
            "Cost after 505 iterations : Training Loss =  0.8157115228945537; Validation Loss = 1.13936893665687\n",
            "Cost after 506 iterations : Training Loss =  0.8139839908257553; Validation Loss = 1.1369193608020336\n",
            "Cost after 507 iterations : Training Loss =  0.8122644427560064; Validation Loss = 1.1344807469594842\n",
            "Cost after 508 iterations : Training Loss =  0.8105528260977666; Validation Loss = 1.1320530236142772\n",
            "Cost after 509 iterations : Training Loss =  0.8088490887146195; Validation Loss = 1.1296361198624982\n",
            "Cost after 510 iterations : Training Loss =  0.8071531789165245; Validation Loss = 1.1272299654048705\n",
            "Cost after 511 iterations : Training Loss =  0.8054650454550671; Validation Loss = 1.1248344905403276\n",
            "Cost after 512 iterations : Training Loss =  0.8037846375188424; Validation Loss = 1.1224496261597756\n",
            "Cost after 513 iterations : Training Loss =  0.8021119047288291; Validation Loss = 1.1200753037398574\n",
            "Cost after 514 iterations : Training Loss =  0.8004467971338752; Validation Loss = 1.1177114553368532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in power\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in power\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in multiply\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in multiply\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Cost after 295002 iterations : Training Loss =  0.010270993572383851; Validation Loss = 0.019239949153560993\n",
            "Cost after 295003 iterations : Training Loss =  0.010270985297582046; Validation Loss = 0.019239946001279724\n",
            "Cost after 295004 iterations : Training Loss =  0.010270977022864433; Validation Loss = 0.019239942849044036\n",
            "Cost after 295005 iterations : Training Loss =  0.010270968748230894; Validation Loss = 0.019239939696853486\n",
            "Cost after 295006 iterations : Training Loss =  0.010270960473681574; Validation Loss = 0.01923993654470851\n",
            "Cost after 295007 iterations : Training Loss =  0.010270952199216478; Validation Loss = 0.01923993339260846\n",
            "Cost after 295008 iterations : Training Loss =  0.01027094392483547; Validation Loss = 0.019239930240553815\n",
            "Cost after 295009 iterations : Training Loss =  0.010270935650538668; Validation Loss = 0.019239927088544346\n",
            "Cost after 295010 iterations : Training Loss =  0.010270927376326129; Validation Loss = 0.01923992393658051\n",
            "Cost after 295011 iterations : Training Loss =  0.010270919102197585; Validation Loss = 0.019239920784661465\n",
            "Cost after 295012 iterations : Training Loss =  0.010270910828153361; Validation Loss = 0.01923991763278818\n",
            "Cost after 295013 iterations : Training Loss =  0.01027090255419322; Validation Loss = 0.019239914480960293\n",
            "Cost after 295014 iterations : Training Loss =  0.010270894280317315; Validation Loss = 0.01923991132917759\n",
            "Cost after 295015 iterations : Training Loss =  0.010270886006525459; Validation Loss = 0.01923990817743991\n",
            "Cost after 295016 iterations : Training Loss =  0.010270877732817815; Validation Loss = 0.019239905025747725\n",
            "Cost after 295017 iterations : Training Loss =  0.010270869459194441; Validation Loss = 0.01923990187410098\n",
            "Cost after 295018 iterations : Training Loss =  0.0102708611856551; Validation Loss = 0.019239898722499257\n",
            "Cost after 295019 iterations : Training Loss =  0.010270852912199948; Validation Loss = 0.019239895570942907\n",
            "Cost after 295020 iterations : Training Loss =  0.010270844638829024; Validation Loss = 0.019239892419432112\n",
            "Cost after 295021 iterations : Training Loss =  0.010270836365542255; Validation Loss = 0.019239889267966184\n",
            "Cost after 295022 iterations : Training Loss =  0.010270828092339606; Validation Loss = 0.019239886116545722\n",
            "Cost after 295023 iterations : Training Loss =  0.010270819819221096; Validation Loss = 0.019239882965170676\n",
            "Cost after 295024 iterations : Training Loss =  0.01027081154618681; Validation Loss = 0.019239879813840625\n",
            "Cost after 295025 iterations : Training Loss =  0.010270803273236616; Validation Loss = 0.019239876662556322\n",
            "Cost after 295026 iterations : Training Loss =  0.010270795000370517; Validation Loss = 0.0192398735113171\n",
            "Cost after 295027 iterations : Training Loss =  0.010270786727588705; Validation Loss = 0.01923987036012302\n",
            "Cost after 295028 iterations : Training Loss =  0.010270778454890963; Validation Loss = 0.01923986720897432\n",
            "Cost after 295029 iterations : Training Loss =  0.010270770182277377; Validation Loss = 0.019239864057871363\n",
            "Cost after 295030 iterations : Training Loss =  0.010270761909747926; Validation Loss = 0.01923986090681316\n",
            "Cost after 295031 iterations : Training Loss =  0.010270753637302689; Validation Loss = 0.019239857755800158\n",
            "Cost after 295032 iterations : Training Loss =  0.010270745364941608; Validation Loss = 0.01923985460483286\n",
            "Cost after 295033 iterations : Training Loss =  0.010270737092664529; Validation Loss = 0.01923985145391033\n",
            "Cost after 295034 iterations : Training Loss =  0.010270728820471711; Validation Loss = 0.019239848303033498\n",
            "Cost after 295035 iterations : Training Loss =  0.01027072054836303; Validation Loss = 0.019239845152201864\n",
            "Cost after 295036 iterations : Training Loss =  0.010270712276338483; Validation Loss = 0.019239842001415253\n",
            "Cost after 295037 iterations : Training Loss =  0.010270704004398083; Validation Loss = 0.019239838850674075\n",
            "Cost after 295038 iterations : Training Loss =  0.01027069573254181; Validation Loss = 0.019239835699978475\n",
            "Cost after 295039 iterations : Training Loss =  0.010270687460769697; Validation Loss = 0.01923983254932755\n",
            "Cost after 295040 iterations : Training Loss =  0.010270679189081682; Validation Loss = 0.019239829398722322\n",
            "Cost after 295041 iterations : Training Loss =  0.010270670917477834; Validation Loss = 0.019239826248162437\n",
            "Cost after 295042 iterations : Training Loss =  0.01027066264595804; Validation Loss = 0.019239823097647674\n",
            "Cost after 295043 iterations : Training Loss =  0.010270654374522532; Validation Loss = 0.01923981994717817\n",
            "Cost after 295044 iterations : Training Loss =  0.010270646103171084; Validation Loss = 0.01923981679675395\n",
            "Cost after 295045 iterations : Training Loss =  0.01027063783190374; Validation Loss = 0.019239813646375097\n",
            "Cost after 295046 iterations : Training Loss =  0.010270629560720548; Validation Loss = 0.01923981049604133\n",
            "Cost after 295047 iterations : Training Loss =  0.01027062128962151; Validation Loss = 0.019239807345753087\n",
            "Cost after 295048 iterations : Training Loss =  0.010270613018606599; Validation Loss = 0.019239804195509504\n",
            "Cost after 295049 iterations : Training Loss =  0.010270604747675823; Validation Loss = 0.019239801045312116\n",
            "Cost after 295050 iterations : Training Loss =  0.010270596476829128; Validation Loss = 0.019239797895159498\n",
            "Cost after 295051 iterations : Training Loss =  0.01027058820606663; Validation Loss = 0.01923979474505189\n",
            "Cost after 295052 iterations : Training Loss =  0.01027057993538825; Validation Loss = 0.019239791594989914\n",
            "Cost after 295053 iterations : Training Loss =  0.010270571664793907; Validation Loss = 0.019239788444972972\n",
            "Cost after 295054 iterations : Training Loss =  0.010270563394283698; Validation Loss = 0.019239785295001425\n",
            "Cost after 295055 iterations : Training Loss =  0.010270555123857678; Validation Loss = 0.019239782145075292\n",
            "Cost after 295056 iterations : Training Loss =  0.010270546853515779; Validation Loss = 0.019239778995194107\n",
            "Cost after 295057 iterations : Training Loss =  0.010270538583257924; Validation Loss = 0.01923977584535838\n",
            "Cost after 295058 iterations : Training Loss =  0.01027053031308431; Validation Loss = 0.019239772695567953\n",
            "Cost after 295059 iterations : Training Loss =  0.010270522042994681; Validation Loss = 0.0192397695458227\n",
            "Cost after 295060 iterations : Training Loss =  0.01027051377298931; Validation Loss = 0.01923976639612274\n",
            "Cost after 295061 iterations : Training Loss =  0.010270505503067948; Validation Loss = 0.01923976324646823\n",
            "Cost after 295062 iterations : Training Loss =  0.01027049723323071; Validation Loss = 0.01923976009685855\n",
            "Cost after 295063 iterations : Training Loss =  0.010270488963477549; Validation Loss = 0.01923975694729455\n",
            "Cost after 295064 iterations : Training Loss =  0.010270480693808585; Validation Loss = 0.019239753797775865\n",
            "Cost after 295065 iterations : Training Loss =  0.010270472424223673; Validation Loss = 0.019239750648302342\n",
            "Cost after 295066 iterations : Training Loss =  0.01027046415472288; Validation Loss = 0.019239747498873753\n",
            "Cost after 295067 iterations : Training Loss =  0.010270455885306229; Validation Loss = 0.019239744349490627\n",
            "Cost after 295068 iterations : Training Loss =  0.010270447615973738; Validation Loss = 0.01923974120015277\n",
            "Cost after 295069 iterations : Training Loss =  0.010270439346725254; Validation Loss = 0.0192397380508603\n",
            "Cost after 295070 iterations : Training Loss =  0.010270431077560878; Validation Loss = 0.0192397349016132\n",
            "Cost after 295071 iterations : Training Loss =  0.010270422808480664; Validation Loss = 0.01923973175241083\n",
            "Cost after 295072 iterations : Training Loss =  0.01027041453948451; Validation Loss = 0.01923972860325416\n",
            "Cost after 295073 iterations : Training Loss =  0.0102704062705725; Validation Loss = 0.019239725454142577\n",
            "Cost after 295074 iterations : Training Loss =  0.010270398001744514; Validation Loss = 0.019239722305076094\n",
            "Cost after 295075 iterations : Training Loss =  0.010270389733000674; Validation Loss = 0.019239719156055043\n",
            "Cost after 295076 iterations : Training Loss =  0.01027038146434088; Validation Loss = 0.019239716007079088\n",
            "Cost after 295077 iterations : Training Loss =  0.010270373195765294; Validation Loss = 0.019239712858148603\n",
            "Cost after 295078 iterations : Training Loss =  0.01027036492727375; Validation Loss = 0.019239709709263187\n",
            "Cost after 295079 iterations : Training Loss =  0.010270356658866355; Validation Loss = 0.01923970656042319\n",
            "Cost after 295080 iterations : Training Loss =  0.010270348390542968; Validation Loss = 0.01923970341162837\n",
            "Cost after 295081 iterations : Training Loss =  0.01027034012230372; Validation Loss = 0.019239700262878603\n",
            "Cost after 295082 iterations : Training Loss =  0.010270331854148497; Validation Loss = 0.01923969711417455\n",
            "Cost after 295083 iterations : Training Loss =  0.010270323586077467; Validation Loss = 0.01923969396551513\n",
            "Cost after 295084 iterations : Training Loss =  0.010270315318090402; Validation Loss = 0.019239690816901262\n",
            "Cost after 295085 iterations : Training Loss =  0.010270307050187633; Validation Loss = 0.019239687668332874\n",
            "Cost after 295086 iterations : Training Loss =  0.010270298782368776; Validation Loss = 0.019239684519809667\n",
            "Cost after 295087 iterations : Training Loss =  0.010270290514634095; Validation Loss = 0.01923968137133149\n",
            "Cost after 295088 iterations : Training Loss =  0.010270282246983372; Validation Loss = 0.019239678222898286\n",
            "Cost after 295089 iterations : Training Loss =  0.010270273979416851; Validation Loss = 0.019239675074510517\n",
            "Cost after 295090 iterations : Training Loss =  0.010270265711934375; Validation Loss = 0.019239671926168062\n",
            "Cost after 295091 iterations : Training Loss =  0.010270257444535918; Validation Loss = 0.019239668777870703\n",
            "Cost after 295092 iterations : Training Loss =  0.010270249177221653; Validation Loss = 0.01923966562961862\n",
            "Cost after 295093 iterations : Training Loss =  0.010270240909991454; Validation Loss = 0.01923966248141201\n",
            "Cost after 295094 iterations : Training Loss =  0.010270232642845328; Validation Loss = 0.01923965933325021\n",
            "Cost after 295095 iterations : Training Loss =  0.010270224375783286; Validation Loss = 0.019239656185133926\n",
            "Cost after 295096 iterations : Training Loss =  0.010270216108805241; Validation Loss = 0.019239653037063115\n",
            "Cost after 295097 iterations : Training Loss =  0.01027020784191135; Validation Loss = 0.019239649889037348\n",
            "Cost after 295098 iterations : Training Loss =  0.010270199575101495; Validation Loss = 0.0192396467410567\n",
            "Cost after 295099 iterations : Training Loss =  0.01027019130837573; Validation Loss = 0.019239643593121615\n",
            "Cost after 295100 iterations : Training Loss =  0.010270183041734058; Validation Loss = 0.01923964044523166\n",
            "Cost after 295101 iterations : Training Loss =  0.010270174775176414; Validation Loss = 0.019239637297386678\n",
            "Cost after 295102 iterations : Training Loss =  0.01027016650870289; Validation Loss = 0.019239634149586786\n",
            "Cost after 295103 iterations : Training Loss =  0.010270158242313386; Validation Loss = 0.019239631001832166\n",
            "Cost after 295104 iterations : Training Loss =  0.010270149976007925; Validation Loss = 0.019239627854123437\n",
            "Cost after 295105 iterations : Training Loss =  0.01027014170978659; Validation Loss = 0.01923962470645895\n",
            "Cost after 295106 iterations : Training Loss =  0.01027013344364928; Validation Loss = 0.019239621558840524\n",
            "Cost after 295107 iterations : Training Loss =  0.010270125177596045; Validation Loss = 0.019239618411267007\n",
            "Cost after 295108 iterations : Training Loss =  0.010270116911626954; Validation Loss = 0.01923961526373851\n",
            "Cost after 295109 iterations : Training Loss =  0.010270108645741809; Validation Loss = 0.019239612116255373\n",
            "Cost after 295110 iterations : Training Loss =  0.010270100379940873; Validation Loss = 0.01923960896881731\n",
            "Cost after 295111 iterations : Training Loss =  0.010270092114223814; Validation Loss = 0.01923960582142484\n",
            "Cost after 295112 iterations : Training Loss =  0.01027008384859091; Validation Loss = 0.019239602674077417\n",
            "Cost after 295113 iterations : Training Loss =  0.01027007558304203; Validation Loss = 0.019239599526775262\n",
            "Cost after 295114 iterations : Training Loss =  0.010270067317577243; Validation Loss = 0.01923959637951822\n",
            "Cost after 295115 iterations : Training Loss =  0.010270059052196507; Validation Loss = 0.019239593232306396\n",
            "Cost after 295116 iterations : Training Loss =  0.010270050786899854; Validation Loss = 0.019239590085139705\n",
            "Cost after 295117 iterations : Training Loss =  0.010270042521687242; Validation Loss = 0.019239586938018232\n",
            "Cost after 295118 iterations : Training Loss =  0.010270034256558693; Validation Loss = 0.01923958379094208\n",
            "Cost after 295119 iterations : Training Loss =  0.010270025991514125; Validation Loss = 0.019239580643910993\n",
            "Cost after 295120 iterations : Training Loss =  0.010270017726553664; Validation Loss = 0.019239577496925543\n",
            "Cost after 295121 iterations : Training Loss =  0.01027000946167723; Validation Loss = 0.01923957434998487\n",
            "Cost after 295122 iterations : Training Loss =  0.01027000119688484; Validation Loss = 0.01923957120308943\n",
            "Cost after 295123 iterations : Training Loss =  0.010269992932176541; Validation Loss = 0.019239568056239366\n",
            "Cost after 295124 iterations : Training Loss =  0.010269984667552223; Validation Loss = 0.019239564909434497\n",
            "Cost after 295125 iterations : Training Loss =  0.01026997640301201; Validation Loss = 0.019239561762674693\n",
            "Cost after 295126 iterations : Training Loss =  0.010269968138555737; Validation Loss = 0.019239558615960346\n",
            "Cost after 295127 iterations : Training Loss =  0.01026995987418364; Validation Loss = 0.01923955546929134\n",
            "Cost after 295128 iterations : Training Loss =  0.010269951609895503; Validation Loss = 0.01923955232266739\n",
            "Cost after 295129 iterations : Training Loss =  0.01026994334569139; Validation Loss = 0.019239549176088593\n",
            "Cost after 295130 iterations : Training Loss =  0.010269935081571447; Validation Loss = 0.0192395460295549\n",
            "Cost after 295131 iterations : Training Loss =  0.01026992681753545; Validation Loss = 0.019239542883066393\n",
            "Cost after 295132 iterations : Training Loss =  0.010269918553583429; Validation Loss = 0.01923953973662299\n",
            "Cost after 295133 iterations : Training Loss =  0.010269910289715547; Validation Loss = 0.01923953659022542\n",
            "Cost after 295134 iterations : Training Loss =  0.010269902025931645; Validation Loss = 0.019239533443872574\n",
            "Cost after 295135 iterations : Training Loss =  0.010269893762231816; Validation Loss = 0.019239530297564717\n",
            "Cost after 295136 iterations : Training Loss =  0.010269885498615978; Validation Loss = 0.0192395271513024\n",
            "Cost after 295137 iterations : Training Loss =  0.010269877235084253; Validation Loss = 0.019239524005084975\n",
            "Cost after 295138 iterations : Training Loss =  0.010269868971636486; Validation Loss = 0.019239520858912867\n",
            "Cost after 295139 iterations : Training Loss =  0.010269860708272762; Validation Loss = 0.01923951771278621\n",
            "Cost after 295140 iterations : Training Loss =  0.010269852444993023; Validation Loss = 0.01923951456670468\n",
            "Cost after 295141 iterations : Training Loss =  0.010269844181797435; Validation Loss = 0.019239511420668216\n",
            "Cost after 295142 iterations : Training Loss =  0.01026983591868578; Validation Loss = 0.01923950827467679\n",
            "Cost after 295143 iterations : Training Loss =  0.010269827655658122; Validation Loss = 0.01923950512873065\n",
            "Cost after 295144 iterations : Training Loss =  0.010269819392714522; Validation Loss = 0.019239501982829883\n",
            "Cost after 295145 iterations : Training Loss =  0.010269811129854919; Validation Loss = 0.01923949883697449\n",
            "Cost after 295146 iterations : Training Loss =  0.010269802867079424; Validation Loss = 0.01923949569116397\n",
            "Cost after 295147 iterations : Training Loss =  0.010269794604387879; Validation Loss = 0.019239492545398664\n",
            "Cost after 295148 iterations : Training Loss =  0.010269786341780347; Validation Loss = 0.019239489399678778\n",
            "Cost after 295149 iterations : Training Loss =  0.010269778079256891; Validation Loss = 0.019239486254003697\n",
            "Cost after 295150 iterations : Training Loss =  0.01026976981681738; Validation Loss = 0.01923948310837393\n",
            "Cost after 295151 iterations : Training Loss =  0.010269761554461987; Validation Loss = 0.01923947996278934\n",
            "Cost after 295152 iterations : Training Loss =  0.010269753292190516; Validation Loss = 0.019239476817250186\n",
            "Cost after 295153 iterations : Training Loss =  0.010269745030003089; Validation Loss = 0.01923947367175614\n",
            "Cost after 295154 iterations : Training Loss =  0.010269736767899669; Validation Loss = 0.01923947052630707\n",
            "Cost after 295155 iterations : Training Loss =  0.010269728505880338; Validation Loss = 0.019239467380903274\n",
            "Cost after 295156 iterations : Training Loss =  0.010269720243944963; Validation Loss = 0.01923946423554484\n",
            "Cost after 295157 iterations : Training Loss =  0.010269711982093538; Validation Loss = 0.019239461090231395\n",
            "Cost after 295158 iterations : Training Loss =  0.010269703720326165; Validation Loss = 0.01923945794496357\n",
            "Cost after 295159 iterations : Training Loss =  0.010269695458642831; Validation Loss = 0.019239454799740485\n",
            "Cost after 295160 iterations : Training Loss =  0.010269687197043485; Validation Loss = 0.019239451654562617\n",
            "Cost after 295161 iterations : Training Loss =  0.010269678935528064; Validation Loss = 0.019239448509429734\n",
            "Cost after 295162 iterations : Training Loss =  0.010269670674096757; Validation Loss = 0.0192394453643423\n",
            "Cost after 295163 iterations : Training Loss =  0.010269662412749386; Validation Loss = 0.019239442219299886\n",
            "Cost after 295164 iterations : Training Loss =  0.010269654151486117; Validation Loss = 0.019239439074302794\n",
            "Cost after 295165 iterations : Training Loss =  0.010269645890306614; Validation Loss = 0.019239435929350897\n",
            "Cost after 295166 iterations : Training Loss =  0.010269637629211365; Validation Loss = 0.019239432784444132\n",
            "Cost after 295167 iterations : Training Loss =  0.010269629368200053; Validation Loss = 0.01923942963958258\n",
            "Cost after 295168 iterations : Training Loss =  0.010269621107272755; Validation Loss = 0.01923942649476603\n",
            "Cost after 295169 iterations : Training Loss =  0.010269612846429353; Validation Loss = 0.019239423349994844\n",
            "Cost after 295170 iterations : Training Loss =  0.010269604585670025; Validation Loss = 0.01923942020526873\n",
            "Cost after 295171 iterations : Training Loss =  0.010269596324994723; Validation Loss = 0.019239417060587596\n",
            "Cost after 295172 iterations : Training Loss =  0.01026958806440333; Validation Loss = 0.01923941391595174\n",
            "Cost after 295173 iterations : Training Loss =  0.010269579803896017; Validation Loss = 0.019239410771361237\n",
            "Cost after 295174 iterations : Training Loss =  0.010269571543472569; Validation Loss = 0.019239407626815662\n",
            "Cost after 295175 iterations : Training Loss =  0.010269563283133226; Validation Loss = 0.019239404482315363\n",
            "Cost after 295176 iterations : Training Loss =  0.010269555022877814; Validation Loss = 0.019239401337860448\n",
            "Cost after 295177 iterations : Training Loss =  0.010269546762706368; Validation Loss = 0.01923939819345052\n",
            "Cost after 295178 iterations : Training Loss =  0.010269538502618887; Validation Loss = 0.01923939504908573\n",
            "Cost after 295179 iterations : Training Loss =  0.010269530242615552; Validation Loss = 0.019239391904766045\n",
            "Cost after 295180 iterations : Training Loss =  0.010269521982696084; Validation Loss = 0.019239388760491433\n",
            "Cost after 295181 iterations : Training Loss =  0.010269513722860597; Validation Loss = 0.01923938561626233\n",
            "Cost after 295182 iterations : Training Loss =  0.010269505463109112; Validation Loss = 0.019239382472078216\n",
            "Cost after 295183 iterations : Training Loss =  0.010269497203441589; Validation Loss = 0.019239379327939145\n",
            "Cost after 295184 iterations : Training Loss =  0.010269488943858061; Validation Loss = 0.01923937618384577\n",
            "Cost after 295185 iterations : Training Loss =  0.010269480684358534; Validation Loss = 0.01923937303979714\n",
            "Cost after 295186 iterations : Training Loss =  0.01026947242494293; Validation Loss = 0.019239369895793623\n",
            "Cost after 295187 iterations : Training Loss =  0.010269464165611418; Validation Loss = 0.019239366751835386\n",
            "Cost after 295188 iterations : Training Loss =  0.010269455906363715; Validation Loss = 0.019239363607921986\n",
            "Cost after 295189 iterations : Training Loss =  0.010269447647200105; Validation Loss = 0.019239360464054056\n",
            "Cost after 295190 iterations : Training Loss =  0.010269439388120477; Validation Loss = 0.019239357320231208\n",
            "Cost after 295191 iterations : Training Loss =  0.010269431129124774; Validation Loss = 0.019239354176453723\n",
            "Cost after 295192 iterations : Training Loss =  0.01026942287021299; Validation Loss = 0.01923935103272152\n",
            "Cost after 295193 iterations : Training Loss =  0.010269414611385237; Validation Loss = 0.01923934788903414\n",
            "Cost after 295194 iterations : Training Loss =  0.010269406352641489; Validation Loss = 0.019239344745391868\n",
            "Cost after 295195 iterations : Training Loss =  0.010269398093981717; Validation Loss = 0.019239341601795225\n",
            "Cost after 295196 iterations : Training Loss =  0.0102693898354058; Validation Loss = 0.019239338458243087\n",
            "Cost after 295197 iterations : Training Loss =  0.010269381576913981; Validation Loss = 0.01923933531473624\n",
            "Cost after 295198 iterations : Training Loss =  0.010269373318506083; Validation Loss = 0.01923933217127481\n",
            "Cost after 295199 iterations : Training Loss =  0.010269365060182077; Validation Loss = 0.01923932902785817\n",
            "Cost after 295200 iterations : Training Loss =  0.010269356801942157; Validation Loss = 0.01923932588448706\n",
            "Cost after 295201 iterations : Training Loss =  0.010269348543786066; Validation Loss = 0.0192393227411609\n",
            "Cost after 295202 iterations : Training Loss =  0.01026934028571402; Validation Loss = 0.019239319597879796\n",
            "Cost after 295203 iterations : Training Loss =  0.010269332027725921; Validation Loss = 0.019239316454644224\n",
            "Cost after 295204 iterations : Training Loss =  0.010269323769821771; Validation Loss = 0.019239313311453468\n",
            "Cost after 295205 iterations : Training Loss =  0.010269315512001529; Validation Loss = 0.019239310168308\n",
            "Cost after 295206 iterations : Training Loss =  0.010269307254265307; Validation Loss = 0.019239307025207656\n",
            "Cost after 295207 iterations : Training Loss =  0.01026929899661305; Validation Loss = 0.019239303882152267\n",
            "Cost after 295208 iterations : Training Loss =  0.0102692907390447; Validation Loss = 0.0192393007391423\n",
            "Cost after 295209 iterations : Training Loss =  0.01026928248156035; Validation Loss = 0.01923929759617726\n",
            "Cost after 295210 iterations : Training Loss =  0.010269274224159922; Validation Loss = 0.019239294453257325\n",
            "Cost after 295211 iterations : Training Loss =  0.010269265966843489; Validation Loss = 0.019239291310382794\n",
            "Cost after 295212 iterations : Training Loss =  0.010269257709610954; Validation Loss = 0.01923928816755331\n",
            "Cost after 295213 iterations : Training Loss =  0.010269249452462346; Validation Loss = 0.019239285024768977\n",
            "Cost after 295214 iterations : Training Loss =  0.010269241195397709; Validation Loss = 0.019239281882029505\n",
            "Cost after 295215 iterations : Training Loss =  0.010269232938417085; Validation Loss = 0.01923927873933532\n",
            "Cost after 295216 iterations : Training Loss =  0.010269224681520351; Validation Loss = 0.019239275596686523\n",
            "Cost after 295217 iterations : Training Loss =  0.010269216424707557; Validation Loss = 0.019239272454082697\n",
            "Cost after 295218 iterations : Training Loss =  0.010269208167978728; Validation Loss = 0.019239269311523716\n",
            "Cost after 295219 iterations : Training Loss =  0.010269199911333841; Validation Loss = 0.019239266169009978\n",
            "Cost after 295220 iterations : Training Loss =  0.010269191654772847; Validation Loss = 0.0192392630265417\n",
            "Cost after 295221 iterations : Training Loss =  0.010269183398295823; Validation Loss = 0.019239259884118676\n",
            "Cost after 295222 iterations : Training Loss =  0.010269175141902745; Validation Loss = 0.019239256741740148\n",
            "Cost after 295223 iterations : Training Loss =  0.010269166885593576; Validation Loss = 0.019239253599407275\n",
            "Cost after 295224 iterations : Training Loss =  0.010269158629368348; Validation Loss = 0.01923925045711925\n",
            "Cost after 295225 iterations : Training Loss =  0.010269150373227042; Validation Loss = 0.019239247314876563\n",
            "Cost after 295226 iterations : Training Loss =  0.010269142117169712; Validation Loss = 0.01923924417267869\n",
            "Cost after 295227 iterations : Training Loss =  0.010269133861196315; Validation Loss = 0.019239241030526415\n",
            "Cost after 295228 iterations : Training Loss =  0.010269125605306906; Validation Loss = 0.01923923788841894\n",
            "Cost after 295229 iterations : Training Loss =  0.010269117349501335; Validation Loss = 0.0192392347463565\n",
            "Cost after 295230 iterations : Training Loss =  0.010269109093779701; Validation Loss = 0.019239231604339588\n",
            "Cost after 295231 iterations : Training Loss =  0.010269100838142009; Validation Loss = 0.01923922846236788\n",
            "Cost after 295232 iterations : Training Loss =  0.01026909258258822; Validation Loss = 0.01923922532044125\n",
            "Cost after 295233 iterations : Training Loss =  0.010269084327118372; Validation Loss = 0.019239222178559272\n",
            "Cost after 295234 iterations : Training Loss =  0.010269076071732542; Validation Loss = 0.01923921903672276\n",
            "Cost after 295235 iterations : Training Loss =  0.01026906781643054; Validation Loss = 0.019239215894931244\n",
            "Cost after 295236 iterations : Training Loss =  0.010269059561212488; Validation Loss = 0.019239212753184593\n",
            "Cost after 295237 iterations : Training Loss =  0.01026905130607836; Validation Loss = 0.019239209611483513\n",
            "Cost after 295238 iterations : Training Loss =  0.01026904305102812; Validation Loss = 0.019239206469827116\n",
            "Cost after 295239 iterations : Training Loss =  0.010269034796061791; Validation Loss = 0.019239203328216183\n",
            "Cost after 295240 iterations : Training Loss =  0.010269026541179475; Validation Loss = 0.01923920018664994\n",
            "Cost after 295241 iterations : Training Loss =  0.010269018286380983; Validation Loss = 0.01923919704512937\n",
            "Cost after 295242 iterations : Training Loss =  0.01026901003166644; Validation Loss = 0.019239193903653872\n",
            "Cost after 295243 iterations : Training Loss =  0.010269001777035778; Validation Loss = 0.019239190762223312\n",
            "Cost after 295244 iterations : Training Loss =  0.010268993522489105; Validation Loss = 0.019239187620837785\n",
            "Cost after 295245 iterations : Training Loss =  0.010268985268026318; Validation Loss = 0.0192391844794974\n",
            "Cost after 295246 iterations : Training Loss =  0.010268977013647426; Validation Loss = 0.019239181338202006\n",
            "Cost after 295247 iterations : Training Loss =  0.010268968759352442; Validation Loss = 0.019239178196951565\n",
            "Cost after 295248 iterations : Training Loss =  0.01026896050514146; Validation Loss = 0.019239175055746877\n",
            "Cost after 295249 iterations : Training Loss =  0.01026895225101427; Validation Loss = 0.019239171914587284\n",
            "Cost after 295250 iterations : Training Loss =  0.010268943996970962; Validation Loss = 0.019239168773472246\n",
            "Cost after 295251 iterations : Training Loss =  0.010268935743011591; Validation Loss = 0.019239165632402466\n",
            "Cost after 295252 iterations : Training Loss =  0.010268927489136192; Validation Loss = 0.019239162491378116\n",
            "Cost after 295253 iterations : Training Loss =  0.010268919235344618; Validation Loss = 0.019239159350398664\n",
            "Cost after 295254 iterations : Training Loss =  0.010268910981637038; Validation Loss = 0.019239156209464335\n",
            "Cost after 295255 iterations : Training Loss =  0.010268902728013306; Validation Loss = 0.01923915306857507\n",
            "Cost after 295256 iterations : Training Loss =  0.010268894474473487; Validation Loss = 0.019239149927730997\n",
            "Cost after 295257 iterations : Training Loss =  0.010268886221017575; Validation Loss = 0.019239146786932115\n",
            "Cost after 295258 iterations : Training Loss =  0.010268877967645581; Validation Loss = 0.019239143646177858\n",
            "Cost after 295259 iterations : Training Loss =  0.010268869714357459; Validation Loss = 0.019239140505468922\n",
            "Cost after 295260 iterations : Training Loss =  0.010268861461153156; Validation Loss = 0.019239137364805082\n",
            "Cost after 295261 iterations : Training Loss =  0.010268853208032822; Validation Loss = 0.01923913422418664\n",
            "Cost after 295262 iterations : Training Loss =  0.01026884495499646; Validation Loss = 0.01923913108361284\n",
            "Cost after 295263 iterations : Training Loss =  0.010268836702043916; Validation Loss = 0.019239127943084435\n",
            "Cost after 295264 iterations : Training Loss =  0.010268828449175213; Validation Loss = 0.019239124802601287\n",
            "Cost after 295265 iterations : Training Loss =  0.010268820196390447; Validation Loss = 0.019239121662163003\n",
            "Cost after 295266 iterations : Training Loss =  0.010268811943689638; Validation Loss = 0.019239118521769714\n",
            "Cost after 295267 iterations : Training Loss =  0.010268803691072637; Validation Loss = 0.01923911538142173\n",
            "Cost after 295268 iterations : Training Loss =  0.010268795438539566; Validation Loss = 0.019239112241118565\n",
            "Cost after 295269 iterations : Training Loss =  0.010268787186090334; Validation Loss = 0.019239109100860748\n",
            "Cost after 295270 iterations : Training Loss =  0.010268778933724998; Validation Loss = 0.019239105960648102\n",
            "Cost after 295271 iterations : Training Loss =  0.010268770681443566; Validation Loss = 0.019239102820480306\n",
            "Cost after 295272 iterations : Training Loss =  0.01026876242924598; Validation Loss = 0.01923909968035764\n",
            "Cost after 295273 iterations : Training Loss =  0.01026875417713236; Validation Loss = 0.019239096540280397\n",
            "Cost after 295274 iterations : Training Loss =  0.010268745925102556; Validation Loss = 0.01923909340024786\n",
            "Cost after 295275 iterations : Training Loss =  0.010268737673156702; Validation Loss = 0.019239090260260616\n",
            "Cost after 295276 iterations : Training Loss =  0.01026872942129464; Validation Loss = 0.019239087120318615\n",
            "Cost after 295277 iterations : Training Loss =  0.010268721169516515; Validation Loss = 0.019239083980421533\n",
            "Cost after 295278 iterations : Training Loss =  0.010268712917822225; Validation Loss = 0.019239080840569706\n",
            "Cost after 295279 iterations : Training Loss =  0.010268704666211837; Validation Loss = 0.01923907770076277\n",
            "Cost after 295280 iterations : Training Loss =  0.010268696414685356; Validation Loss = 0.019239074561000578\n",
            "Cost after 295281 iterations : Training Loss =  0.010268688163242674; Validation Loss = 0.019239071421283938\n",
            "Cost after 295282 iterations : Training Loss =  0.01026867991188389; Validation Loss = 0.019239068281612207\n",
            "Cost after 295283 iterations : Training Loss =  0.010268671660609048; Validation Loss = 0.01923906514198581\n",
            "Cost after 295284 iterations : Training Loss =  0.010268663409417981; Validation Loss = 0.019239062002404102\n",
            "Cost after 295285 iterations : Training Loss =  0.010268655158310809; Validation Loss = 0.019239058862867548\n",
            "Cost after 295286 iterations : Training Loss =  0.010268646907287618; Validation Loss = 0.019239055723376586\n",
            "Cost after 295287 iterations : Training Loss =  0.010268638656348044; Validation Loss = 0.019239052583930074\n",
            "Cost after 295288 iterations : Training Loss =  0.010268630405492482; Validation Loss = 0.019239049444528936\n",
            "Cost after 295289 iterations : Training Loss =  0.010268622154720908; Validation Loss = 0.019239046305172695\n",
            "Cost after 295290 iterations : Training Loss =  0.010268613904033014; Validation Loss = 0.019239043165861617\n",
            "Cost after 295291 iterations : Training Loss =  0.01026860565342906; Validation Loss = 0.01923904002659577\n",
            "Cost after 295292 iterations : Training Loss =  0.010268597402908952; Validation Loss = 0.019239036887374657\n",
            "Cost after 295293 iterations : Training Loss =  0.010268589152472751; Validation Loss = 0.01923903374819905\n",
            "Cost after 295294 iterations : Training Loss =  0.010268580902120291; Validation Loss = 0.019239030609068244\n",
            "Cost after 295295 iterations : Training Loss =  0.010268572651851762; Validation Loss = 0.019239027469982736\n",
            "Cost after 295296 iterations : Training Loss =  0.010268564401667138; Validation Loss = 0.019239024330941743\n",
            "Cost after 295297 iterations : Training Loss =  0.010268556151566325; Validation Loss = 0.01923902119194653\n",
            "Cost after 295298 iterations : Training Loss =  0.010268547901549388; Validation Loss = 0.019239018052995792\n",
            "Cost after 295299 iterations : Training Loss =  0.010268539651616282; Validation Loss = 0.019239014914090583\n",
            "Cost after 295300 iterations : Training Loss =  0.010268531401767032; Validation Loss = 0.019239011775230058\n",
            "Cost after 295301 iterations : Training Loss =  0.010268523152001604; Validation Loss = 0.019239008636414708\n",
            "Cost after 295302 iterations : Training Loss =  0.01026851490232007; Validation Loss = 0.019239005497644433\n",
            "Cost after 295303 iterations : Training Loss =  0.010268506652722404; Validation Loss = 0.01923900235891947\n",
            "Cost after 295304 iterations : Training Loss =  0.010268498403208579; Validation Loss = 0.01923899922023915\n",
            "Cost after 295305 iterations : Training Loss =  0.010268490153778526; Validation Loss = 0.01923899608160451\n",
            "Cost after 295306 iterations : Training Loss =  0.010268481904432395; Validation Loss = 0.01923899294301472\n",
            "Cost after 295307 iterations : Training Loss =  0.010268473655170085; Validation Loss = 0.019238989804469794\n",
            "Cost after 295308 iterations : Training Loss =  0.010268465405991689; Validation Loss = 0.019238986665969873\n",
            "Cost after 295309 iterations : Training Loss =  0.010268457156896997; Validation Loss = 0.01923898352751529\n",
            "Cost after 295310 iterations : Training Loss =  0.01026844890788619; Validation Loss = 0.019238980389105584\n",
            "Cost after 295311 iterations : Training Loss =  0.010268440658959304; Validation Loss = 0.019238977250740644\n",
            "Cost after 295312 iterations : Training Loss =  0.010268432410116237; Validation Loss = 0.01923897411242131\n",
            "Cost after 295313 iterations : Training Loss =  0.010268424161356943; Validation Loss = 0.019238970974146927\n",
            "Cost after 295314 iterations : Training Loss =  0.010268415912681522; Validation Loss = 0.01923896783591758\n",
            "Cost after 295315 iterations : Training Loss =  0.010268407664089962; Validation Loss = 0.01923896469773298\n",
            "Cost after 295316 iterations : Training Loss =  0.010268399415582224; Validation Loss = 0.019238961559593686\n",
            "Cost after 295317 iterations : Training Loss =  0.010268391167158324; Validation Loss = 0.019238958421499357\n",
            "Cost after 295318 iterations : Training Loss =  0.010268382918818255; Validation Loss = 0.01923895528345047\n",
            "Cost after 295319 iterations : Training Loss =  0.010268374670562004; Validation Loss = 0.01923895214544642\n",
            "Cost after 295320 iterations : Training Loss =  0.010268366422389583; Validation Loss = 0.019238949007487356\n",
            "Cost after 295321 iterations : Training Loss =  0.010268358174301022; Validation Loss = 0.01923894586957336\n",
            "Cost after 295322 iterations : Training Loss =  0.010268349926296272; Validation Loss = 0.019238942731704397\n",
            "Cost after 295323 iterations : Training Loss =  0.010268341678375316; Validation Loss = 0.019238939593880313\n",
            "Cost after 295324 iterations : Training Loss =  0.010268333430538209; Validation Loss = 0.019238936456101342\n",
            "Cost after 295325 iterations : Training Loss =  0.01026832518278495; Validation Loss = 0.01923893331836756\n",
            "Cost after 295326 iterations : Training Loss =  0.010268316935115533; Validation Loss = 0.019238930180678883\n",
            "Cost after 295327 iterations : Training Loss =  0.010268308687529879; Validation Loss = 0.01923892704303522\n",
            "Cost after 295328 iterations : Training Loss =  0.01026830044002807; Validation Loss = 0.01923892390543662\n",
            "Cost after 295329 iterations : Training Loss =  0.010268292192610145; Validation Loss = 0.019238920767882688\n",
            "Cost after 295330 iterations : Training Loss =  0.010268283945275987; Validation Loss = 0.019238917630374254\n",
            "Cost after 295331 iterations : Training Loss =  0.010268275698025587; Validation Loss = 0.019238914492910722\n",
            "Cost after 295332 iterations : Training Loss =  0.010268267450859118; Validation Loss = 0.019238911355492116\n",
            "Cost after 295333 iterations : Training Loss =  0.0102682592037763; Validation Loss = 0.019238908218118658\n",
            "Cost after 295334 iterations : Training Loss =  0.010268250956777478; Validation Loss = 0.019238905080790406\n",
            "Cost after 295335 iterations : Training Loss =  0.010268242709862407; Validation Loss = 0.019238901943507126\n",
            "Cost after 295336 iterations : Training Loss =  0.010268234463031165; Validation Loss = 0.01923889880626858\n",
            "Cost after 295337 iterations : Training Loss =  0.01026822621628371; Validation Loss = 0.01923889566907525\n",
            "Cost after 295338 iterations : Training Loss =  0.010268217969620123; Validation Loss = 0.01923889253192705\n",
            "Cost after 295339 iterations : Training Loss =  0.010268209723040296; Validation Loss = 0.019238889394823697\n",
            "Cost after 295340 iterations : Training Loss =  0.010268201476544192; Validation Loss = 0.019238886257765417\n",
            "Cost after 295341 iterations : Training Loss =  0.010268193230132032; Validation Loss = 0.01923888312075256\n",
            "Cost after 295342 iterations : Training Loss =  0.010268184983803597; Validation Loss = 0.019238879983784398\n",
            "Cost after 295343 iterations : Training Loss =  0.010268176737559016; Validation Loss = 0.01923887684686126\n",
            "Cost after 295344 iterations : Training Loss =  0.010268168491398293; Validation Loss = 0.01923887370998333\n",
            "Cost after 295345 iterations : Training Loss =  0.010268160245321296; Validation Loss = 0.01923887057315039\n",
            "Cost after 295346 iterations : Training Loss =  0.01026815199932808; Validation Loss = 0.01923886743636243\n",
            "Cost after 295347 iterations : Training Loss =  0.010268143753418755; Validation Loss = 0.01923886429961956\n",
            "Cost after 295348 iterations : Training Loss =  0.010268135507593113; Validation Loss = 0.019238861162921676\n",
            "Cost after 295349 iterations : Training Loss =  0.010268127261851332; Validation Loss = 0.019238858026268617\n",
            "Cost after 295350 iterations : Training Loss =  0.010268119016193341; Validation Loss = 0.019238854889660883\n",
            "Cost after 295351 iterations : Training Loss =  0.010268110770619183; Validation Loss = 0.01923885175309807\n",
            "Cost after 295352 iterations : Training Loss =  0.010268102525128742; Validation Loss = 0.01923884861658045\n",
            "Cost after 295353 iterations : Training Loss =  0.010268094279722176; Validation Loss = 0.019238845480107514\n",
            "Cost after 295354 iterations : Training Loss =  0.010268086034399389; Validation Loss = 0.019238842343679702\n",
            "Cost after 295355 iterations : Training Loss =  0.010268077789160359; Validation Loss = 0.019238839207297086\n",
            "Cost after 295356 iterations : Training Loss =  0.01026806954400509; Validation Loss = 0.019238836070959515\n",
            "Cost after 295357 iterations : Training Loss =  0.010268061298933713; Validation Loss = 0.019238832934666904\n",
            "Cost after 295358 iterations : Training Loss =  0.010268053053946093; Validation Loss = 0.01923882979841908\n",
            "Cost after 295359 iterations : Training Loss =  0.010268044809042247; Validation Loss = 0.019238826662216574\n",
            "Cost after 295360 iterations : Training Loss =  0.010268036564222257; Validation Loss = 0.019238823526058848\n",
            "Cost after 295361 iterations : Training Loss =  0.010268028319485912; Validation Loss = 0.019238820389946343\n",
            "Cost after 295362 iterations : Training Loss =  0.01026802007483343; Validation Loss = 0.019238817253878705\n",
            "Cost after 295363 iterations : Training Loss =  0.010268011830264726; Validation Loss = 0.019238814117856044\n",
            "Cost after 295364 iterations : Training Loss =  0.010268003585779839; Validation Loss = 0.019238810981878463\n",
            "Cost after 295365 iterations : Training Loss =  0.010267995341378693; Validation Loss = 0.019238807845945952\n",
            "Cost after 295366 iterations : Training Loss =  0.010267987097061358; Validation Loss = 0.019238804710058295\n",
            "Cost after 295367 iterations : Training Loss =  0.010267978852827764; Validation Loss = 0.01923880157421609\n",
            "Cost after 295368 iterations : Training Loss =  0.010267970608677993; Validation Loss = 0.019238798438418542\n",
            "Cost after 295369 iterations : Training Loss =  0.010267962364611931; Validation Loss = 0.019238795302666253\n",
            "Cost after 295370 iterations : Training Loss =  0.010267954120629657; Validation Loss = 0.019238792166958864\n",
            "Cost after 295371 iterations : Training Loss =  0.010267945876731275; Validation Loss = 0.019238789031296295\n",
            "Cost after 295372 iterations : Training Loss =  0.010267937632916547; Validation Loss = 0.019238785895678935\n",
            "Cost after 295373 iterations : Training Loss =  0.010267929389185668; Validation Loss = 0.019238782760106633\n",
            "Cost after 295374 iterations : Training Loss =  0.010267921145538535; Validation Loss = 0.01923877962457917\n",
            "Cost after 295375 iterations : Training Loss =  0.010267912901975103; Validation Loss = 0.019238776489096887\n",
            "Cost after 295376 iterations : Training Loss =  0.010267904658495483; Validation Loss = 0.019238773353659543\n",
            "Cost after 295377 iterations : Training Loss =  0.010267896415099667; Validation Loss = 0.019238770218267167\n",
            "Cost after 295378 iterations : Training Loss =  0.010267888171787658; Validation Loss = 0.019238767082919776\n",
            "Cost after 295379 iterations : Training Loss =  0.010267879928559326; Validation Loss = 0.019238763947617636\n",
            "Cost after 295380 iterations : Training Loss =  0.010267871685414785; Validation Loss = 0.019238760812360294\n",
            "Cost after 295381 iterations : Training Loss =  0.010267863442353949; Validation Loss = 0.019238757677147885\n",
            "Cost after 295382 iterations : Training Loss =  0.010267855199376948; Validation Loss = 0.01923875454198067\n",
            "Cost after 295383 iterations : Training Loss =  0.010267846956483757; Validation Loss = 0.01923875140685848\n",
            "Cost after 295384 iterations : Training Loss =  0.010267838713674187; Validation Loss = 0.019238748271781358\n",
            "Cost after 295385 iterations : Training Loss =  0.010267830470948544; Validation Loss = 0.019238745136749117\n",
            "Cost after 295386 iterations : Training Loss =  0.010267822228306507; Validation Loss = 0.019238742001761903\n",
            "Cost after 295387 iterations : Training Loss =  0.010267813985748308; Validation Loss = 0.019238738866819777\n",
            "Cost after 295388 iterations : Training Loss =  0.01026780574327389; Validation Loss = 0.019238735731922443\n",
            "Cost after 295389 iterations : Training Loss =  0.010267797500883191; Validation Loss = 0.019238732597070252\n",
            "Cost after 295390 iterations : Training Loss =  0.01026778925857626; Validation Loss = 0.019238729462263175\n",
            "Cost after 295391 iterations : Training Loss =  0.010267781016353086; Validation Loss = 0.019238726327500694\n",
            "Cost after 295392 iterations : Training Loss =  0.010267772774213589; Validation Loss = 0.019238723192783386\n",
            "Cost after 295393 iterations : Training Loss =  0.010267764532157962; Validation Loss = 0.019238720058110878\n",
            "Cost after 295394 iterations : Training Loss =  0.010267756290186; Validation Loss = 0.019238716923483695\n",
            "Cost after 295395 iterations : Training Loss =  0.010267748048297878; Validation Loss = 0.01923871378890125\n",
            "Cost after 295396 iterations : Training Loss =  0.010267739806493415; Validation Loss = 0.019238710654364072\n",
            "Cost after 295397 iterations : Training Loss =  0.01026773156477273; Validation Loss = 0.019238707519871837\n",
            "Cost after 295398 iterations : Training Loss =  0.01026772332313581; Validation Loss = 0.019238704385424413\n",
            "Cost after 295399 iterations : Training Loss =  0.010267715081582637; Validation Loss = 0.019238701251021898\n",
            "Cost after 295400 iterations : Training Loss =  0.010267706840113114; Validation Loss = 0.01923869811666479\n",
            "Cost after 295401 iterations : Training Loss =  0.010267698598727422; Validation Loss = 0.019238694982352242\n",
            "Cost after 295402 iterations : Training Loss =  0.010267690357425474; Validation Loss = 0.01923869184808497\n",
            "Cost after 295403 iterations : Training Loss =  0.01026768211620728; Validation Loss = 0.019238688713862532\n",
            "Cost after 295404 iterations : Training Loss =  0.01026767387507277; Validation Loss = 0.019238685579685097\n",
            "Cost after 295405 iterations : Training Loss =  0.010267665634021934; Validation Loss = 0.019238682445552876\n",
            "Cost after 295406 iterations : Training Loss =  0.010267657393054968; Validation Loss = 0.01923867931146544\n",
            "Cost after 295407 iterations : Training Loss =  0.010267649152171779; Validation Loss = 0.019238676177423104\n",
            "Cost after 295408 iterations : Training Loss =  0.010267640911372165; Validation Loss = 0.019238673043425695\n",
            "Cost after 295409 iterations : Training Loss =  0.010267632670656408; Validation Loss = 0.019238669909473288\n",
            "Cost after 295410 iterations : Training Loss =  0.010267624430024299; Validation Loss = 0.019238666775565766\n",
            "Cost after 295411 iterations : Training Loss =  0.010267616189476015; Validation Loss = 0.019238663641703454\n",
            "Cost after 295412 iterations : Training Loss =  0.010267607949011362; Validation Loss = 0.019238660507885683\n",
            "Cost after 295413 iterations : Training Loss =  0.01026759970863045; Validation Loss = 0.01923865737411337\n",
            "Cost after 295414 iterations : Training Loss =  0.010267591468333288; Validation Loss = 0.01923865424038587\n",
            "Cost after 295415 iterations : Training Loss =  0.010267583228119898; Validation Loss = 0.01923865110670314\n",
            "Cost after 295416 iterations : Training Loss =  0.010267574987990125; Validation Loss = 0.019238647973065712\n",
            "Cost after 295417 iterations : Training Loss =  0.010267566747944268; Validation Loss = 0.019238644839473094\n",
            "Cost after 295418 iterations : Training Loss =  0.010267558507982033; Validation Loss = 0.019238641705925257\n",
            "Cost after 295419 iterations : Training Loss =  0.010267550268103404; Validation Loss = 0.01923863857242277\n",
            "Cost after 295420 iterations : Training Loss =  0.010267542028308594; Validation Loss = 0.019238635438965212\n",
            "Cost after 295421 iterations : Training Loss =  0.01026753378859757; Validation Loss = 0.01923863230555221\n",
            "Cost after 295422 iterations : Training Loss =  0.010267525548970199; Validation Loss = 0.019238629172184413\n",
            "Cost after 295423 iterations : Training Loss =  0.010267517309426535; Validation Loss = 0.019238626038861718\n",
            "Cost after 295424 iterations : Training Loss =  0.010267509069966593; Validation Loss = 0.019238622905583782\n",
            "Cost after 295425 iterations : Training Loss =  0.01026750083059031; Validation Loss = 0.019238619772350564\n",
            "Cost after 295426 iterations : Training Loss =  0.010267492591297896; Validation Loss = 0.019238616639162848\n",
            "Cost after 295427 iterations : Training Loss =  0.010267484352089074; Validation Loss = 0.019238613506020255\n",
            "Cost after 295428 iterations : Training Loss =  0.010267476112964082; Validation Loss = 0.01923861037292216\n",
            "Cost after 295429 iterations : Training Loss =  0.010267467873922723; Validation Loss = 0.01923860723986925\n",
            "Cost after 295430 iterations : Training Loss =  0.01026745963496505; Validation Loss = 0.019238604106861238\n",
            "Cost after 295431 iterations : Training Loss =  0.010267451396091021; Validation Loss = 0.019238600973898422\n",
            "Cost after 295432 iterations : Training Loss =  0.010267443157300873; Validation Loss = 0.019238597840980285\n",
            "Cost after 295433 iterations : Training Loss =  0.010267434918594394; Validation Loss = 0.01923859470810731\n",
            "Cost after 295434 iterations : Training Loss =  0.010267426679971568; Validation Loss = 0.01923859157527937\n",
            "Cost after 295435 iterations : Training Loss =  0.010267418441432405; Validation Loss = 0.01923858844249593\n",
            "Cost after 295436 iterations : Training Loss =  0.01026741020297709; Validation Loss = 0.019238585309757847\n",
            "Cost after 295437 iterations : Training Loss =  0.010267401964605318; Validation Loss = 0.019238582177064752\n",
            "Cost after 295438 iterations : Training Loss =  0.010267393726317315; Validation Loss = 0.019238579044416164\n",
            "Cost after 295439 iterations : Training Loss =  0.010267385488112978; Validation Loss = 0.019238575911812876\n",
            "Cost after 295440 iterations : Training Loss =  0.010267377249992419; Validation Loss = 0.019238572779254403\n",
            "Cost after 295441 iterations : Training Loss =  0.010267369011955532; Validation Loss = 0.01923856964674116\n",
            "Cost after 295442 iterations : Training Loss =  0.010267360774002362; Validation Loss = 0.019238566514272828\n",
            "Cost after 295443 iterations : Training Loss =  0.010267352536132849; Validation Loss = 0.01923856338184932\n",
            "Cost after 295444 iterations : Training Loss =  0.010267344298346996; Validation Loss = 0.019238560249470994\n",
            "Cost after 295445 iterations : Training Loss =  0.01026733606064492; Validation Loss = 0.01923855711713737\n",
            "Cost after 295446 iterations : Training Loss =  0.010267327823026487; Validation Loss = 0.019238553984848642\n",
            "Cost after 295447 iterations : Training Loss =  0.010267319585491748; Validation Loss = 0.019238550852604733\n",
            "Cost after 295448 iterations : Training Loss =  0.010267311348040785; Validation Loss = 0.019238547720406163\n",
            "Cost after 295449 iterations : Training Loss =  0.010267303110673483; Validation Loss = 0.01923854458825237\n",
            "Cost after 295450 iterations : Training Loss =  0.010267294873389685; Validation Loss = 0.019238541456143576\n",
            "Cost after 295451 iterations : Training Loss =  0.010267286636189706; Validation Loss = 0.019238538324079537\n",
            "Cost after 295452 iterations : Training Loss =  0.0102672783990735; Validation Loss = 0.019238535192060466\n",
            "Cost after 295453 iterations : Training Loss =  0.010267270162040928; Validation Loss = 0.01923853206008664\n",
            "Cost after 295454 iterations : Training Loss =  0.010267261925092; Validation Loss = 0.01923852892815722\n",
            "Cost after 295455 iterations : Training Loss =  0.010267253688226799; Validation Loss = 0.01923852579627307\n",
            "Cost after 295456 iterations : Training Loss =  0.01026724545144521; Validation Loss = 0.01923852266443398\n",
            "Cost after 295457 iterations : Training Loss =  0.010267237214747364; Validation Loss = 0.019238519532639838\n",
            "Cost after 295458 iterations : Training Loss =  0.010267228978133147; Validation Loss = 0.0192385164008905\n",
            "Cost after 295459 iterations : Training Loss =  0.010267220741602663; Validation Loss = 0.01923851326918625\n",
            "Cost after 295460 iterations : Training Loss =  0.010267212505155856; Validation Loss = 0.019238510137526685\n",
            "Cost after 295461 iterations : Training Loss =  0.010267204268792709; Validation Loss = 0.01923850700591256\n",
            "Cost after 295462 iterations : Training Loss =  0.010267196032513169; Validation Loss = 0.01923850387434296\n",
            "Cost after 295463 iterations : Training Loss =  0.010267187796317514; Validation Loss = 0.01923850074281824\n",
            "Cost after 295464 iterations : Training Loss =  0.010267179560205298; Validation Loss = 0.01923849761133887\n",
            "Cost after 295465 iterations : Training Loss =  0.010267171324176864; Validation Loss = 0.019238494479904274\n",
            "Cost after 295466 iterations : Training Loss =  0.010267163088232095; Validation Loss = 0.019238491348514703\n",
            "Cost after 295467 iterations : Training Loss =  0.010267154852370999; Validation Loss = 0.019238488217170193\n",
            "Cost after 295468 iterations : Training Loss =  0.010267146616593555; Validation Loss = 0.01923848508587039\n",
            "Cost after 295469 iterations : Training Loss =  0.010267138380899763; Validation Loss = 0.019238481954615652\n",
            "Cost after 295470 iterations : Training Loss =  0.010267130145289747; Validation Loss = 0.019238478823405535\n",
            "Cost after 295471 iterations : Training Loss =  0.010267121909763222; Validation Loss = 0.019238475692240492\n",
            "Cost after 295472 iterations : Training Loss =  0.01026711367432045; Validation Loss = 0.01923847256112039\n",
            "Cost after 295473 iterations : Training Loss =  0.010267105438961335; Validation Loss = 0.019238469430045292\n",
            "Cost after 295474 iterations : Training Loss =  0.010267097203685968; Validation Loss = 0.019238466299015194\n",
            "Cost after 295475 iterations : Training Loss =  0.010267088968494122; Validation Loss = 0.019238463168029744\n",
            "Cost after 295476 iterations : Training Loss =  0.010267080733386; Validation Loss = 0.019238460037089275\n",
            "Cost after 295477 iterations : Training Loss =  0.010267072498361578; Validation Loss = 0.019238456906193854\n",
            "Cost after 295478 iterations : Training Loss =  0.01026706426342073; Validation Loss = 0.019238453775343417\n",
            "Cost after 295479 iterations : Training Loss =  0.010267056028563528; Validation Loss = 0.019238450644537688\n",
            "Cost after 295480 iterations : Training Loss =  0.01026704779379018; Validation Loss = 0.019238447513776798\n",
            "Cost after 295481 iterations : Training Loss =  0.010267039559100277; Validation Loss = 0.019238444383061194\n",
            "Cost after 295482 iterations : Training Loss =  0.010267031324494177; Validation Loss = 0.01923844125239045\n",
            "Cost after 295483 iterations : Training Loss =  0.010267023089971558; Validation Loss = 0.019238438121764425\n",
            "Cost after 295484 iterations : Training Loss =  0.01026701485553279; Validation Loss = 0.019238434991183297\n",
            "Cost after 295485 iterations : Training Loss =  0.01026700662117751; Validation Loss = 0.019238431860647207\n",
            "Cost after 295486 iterations : Training Loss =  0.01026699838690592; Validation Loss = 0.019238428730156364\n",
            "Cost after 295487 iterations : Training Loss =  0.01026699015271797; Validation Loss = 0.019238425599710143\n",
            "Cost after 295488 iterations : Training Loss =  0.010266981918613723; Validation Loss = 0.019238422469308843\n",
            "Cost after 295489 iterations : Training Loss =  0.01026697368459303; Validation Loss = 0.019238419338952494\n",
            "Cost after 295490 iterations : Training Loss =  0.01026696545065605; Validation Loss = 0.01923841620864095\n",
            "Cost after 295491 iterations : Training Loss =  0.010266957216802755; Validation Loss = 0.019238413078374216\n",
            "Cost after 295492 iterations : Training Loss =  0.010266948983033018; Validation Loss = 0.019238409948153113\n",
            "Cost after 295493 iterations : Training Loss =  0.010266940749346996; Validation Loss = 0.019238406817976304\n",
            "Cost after 295494 iterations : Training Loss =  0.010266932515744573; Validation Loss = 0.01923840368784442\n",
            "Cost after 295495 iterations : Training Loss =  0.010266924282225838; Validation Loss = 0.019238400557757735\n",
            "Cost after 295496 iterations : Training Loss =  0.010266916048790698; Validation Loss = 0.019238397427715884\n",
            "Cost after 295497 iterations : Training Loss =  0.010266907815439175; Validation Loss = 0.01923839429771892\n",
            "Cost after 295498 iterations : Training Loss =  0.010266899582171307; Validation Loss = 0.019238391167766943\n",
            "Cost after 295499 iterations : Training Loss =  0.010266891348987117; Validation Loss = 0.019238388037859704\n",
            "Cost after 295500 iterations : Training Loss =  0.010266883115886557; Validation Loss = 0.019238384907997303\n",
            "Cost after 295501 iterations : Training Loss =  0.010266874882869639; Validation Loss = 0.019238381778179773\n",
            "Cost after 295502 iterations : Training Loss =  0.01026686664993628; Validation Loss = 0.019238378648407484\n",
            "Cost after 295503 iterations : Training Loss =  0.010266858417086562; Validation Loss = 0.019238375518679653\n",
            "Cost after 295504 iterations : Training Loss =  0.010266850184320496; Validation Loss = 0.019238372388997275\n",
            "Cost after 295505 iterations : Training Loss =  0.01026684195163807; Validation Loss = 0.01923836925935953\n",
            "Cost after 295506 iterations : Training Loss =  0.01026683371903927; Validation Loss = 0.01923836612976653\n",
            "Cost after 295507 iterations : Training Loss =  0.01026682548652404; Validation Loss = 0.01923836300021872\n",
            "Cost after 295508 iterations : Training Loss =  0.01026681725409252; Validation Loss = 0.0192383598707156\n",
            "Cost after 295509 iterations : Training Loss =  0.01026680902174459; Validation Loss = 0.01923835674125731\n",
            "Cost after 295510 iterations : Training Loss =  0.010266800789480347; Validation Loss = 0.019238353611844183\n",
            "Cost after 295511 iterations : Training Loss =  0.010266792557299632; Validation Loss = 0.01923835048247611\n",
            "Cost after 295512 iterations : Training Loss =  0.010266784325202572; Validation Loss = 0.019238347353152628\n",
            "Cost after 295513 iterations : Training Loss =  0.010266776093189173; Validation Loss = 0.019238344223874018\n",
            "Cost after 295514 iterations : Training Loss =  0.010266767861259341; Validation Loss = 0.01923834109464049\n",
            "Cost after 295515 iterations : Training Loss =  0.010266759629413069; Validation Loss = 0.01923833796545171\n",
            "Cost after 295516 iterations : Training Loss =  0.010266751397650488; Validation Loss = 0.019238334836307745\n",
            "Cost after 295517 iterations : Training Loss =  0.010266743165971537; Validation Loss = 0.01923833170720891\n",
            "Cost after 295518 iterations : Training Loss =  0.010266734934376211; Validation Loss = 0.019238328578154877\n",
            "Cost after 295519 iterations : Training Loss =  0.010266726702864477; Validation Loss = 0.01923832544914578\n",
            "Cost after 295520 iterations : Training Loss =  0.010266718471436244; Validation Loss = 0.01923832232018151\n",
            "Cost after 295521 iterations : Training Loss =  0.010266710240091759; Validation Loss = 0.019238319191262423\n",
            "Cost after 295522 iterations : Training Loss =  0.01026670200883093; Validation Loss = 0.019238316062388027\n",
            "Cost after 295523 iterations : Training Loss =  0.01026669377765352; Validation Loss = 0.019238312933558332\n",
            "Cost after 295524 iterations : Training Loss =  0.010266685546559923; Validation Loss = 0.01923830980477391\n",
            "Cost after 295525 iterations : Training Loss =  0.010266677315549809; Validation Loss = 0.0192383066760342\n",
            "Cost after 295526 iterations : Training Loss =  0.010266669084623366; Validation Loss = 0.019238303547339342\n",
            "Cost after 295527 iterations : Training Loss =  0.010266660853780534; Validation Loss = 0.0192383004186892\n",
            "Cost after 295528 iterations : Training Loss =  0.010266652623021226; Validation Loss = 0.019238297290084437\n",
            "Cost after 295529 iterations : Training Loss =  0.010266644392345568; Validation Loss = 0.01923829416152417\n",
            "Cost after 295530 iterations : Training Loss =  0.010266636161753518; Validation Loss = 0.019238291033008694\n",
            "Cost after 295531 iterations : Training Loss =  0.010266627931245078; Validation Loss = 0.019238287904538433\n",
            "Cost after 295532 iterations : Training Loss =  0.01026661970082015; Validation Loss = 0.019238284776112818\n",
            "Cost after 295533 iterations : Training Loss =  0.010266611470478922; Validation Loss = 0.01923828164773197\n",
            "Cost after 295534 iterations : Training Loss =  0.010266603240221253; Validation Loss = 0.01923827851939625\n",
            "Cost after 295535 iterations : Training Loss =  0.010266595010047213; Validation Loss = 0.019238275391105463\n",
            "Cost after 295536 iterations : Training Loss =  0.010266586779956679; Validation Loss = 0.019238272262859627\n",
            "Cost after 295537 iterations : Training Loss =  0.010266578549949919; Validation Loss = 0.0192382691346584\n",
            "Cost after 295538 iterations : Training Loss =  0.010266570320026612; Validation Loss = 0.019238266006502493\n",
            "Cost after 295539 iterations : Training Loss =  0.010266562090186858; Validation Loss = 0.019238262878390717\n",
            "Cost after 295540 iterations : Training Loss =  0.010266553860430724; Validation Loss = 0.019238259750324286\n",
            "Cost after 295541 iterations : Training Loss =  0.010266545630758264; Validation Loss = 0.01923825662230256\n",
            "Cost after 295542 iterations : Training Loss =  0.010266537401169342; Validation Loss = 0.019238253494325796\n",
            "Cost after 295543 iterations : Training Loss =  0.010266529171663997; Validation Loss = 0.019238250366393768\n",
            "Cost after 295544 iterations : Training Loss =  0.010266520942242207; Validation Loss = 0.01923824723850706\n",
            "Cost after 295545 iterations : Training Loss =  0.010266512712904066; Validation Loss = 0.019238244110664944\n",
            "Cost after 295546 iterations : Training Loss =  0.010266504483649477; Validation Loss = 0.01923824098286786\n",
            "Cost after 295547 iterations : Training Loss =  0.01026649625447855; Validation Loss = 0.019238237855115425\n",
            "Cost after 295548 iterations : Training Loss =  0.010266488025391105; Validation Loss = 0.01923823472740786\n",
            "Cost after 295549 iterations : Training Loss =  0.010266479796387284; Validation Loss = 0.01923823159974526\n",
            "Cost after 295550 iterations : Training Loss =  0.010266471567467019; Validation Loss = 0.019238228472127698\n",
            "Cost after 295551 iterations : Training Loss =  0.01026646333863035; Validation Loss = 0.01923822534455466\n",
            "Cost after 295552 iterations : Training Loss =  0.010266455109877275; Validation Loss = 0.019238222217026733\n",
            "Cost after 295553 iterations : Training Loss =  0.01026644688120776; Validation Loss = 0.019238219089543606\n",
            "Cost after 295554 iterations : Training Loss =  0.010266438652621843; Validation Loss = 0.019238215962105377\n",
            "Cost after 295555 iterations : Training Loss =  0.010266430424119436; Validation Loss = 0.019238212834712113\n",
            "Cost after 295556 iterations : Training Loss =  0.010266422195700635; Validation Loss = 0.01923820970736348\n",
            "Cost after 295557 iterations : Training Loss =  0.010266413967365457; Validation Loss = 0.019238206580059942\n",
            "Cost after 295558 iterations : Training Loss =  0.010266405739113775; Validation Loss = 0.019238203452801028\n",
            "Cost after 295559 iterations : Training Loss =  0.010266397510945722; Validation Loss = 0.01923820032558728\n",
            "Cost after 295560 iterations : Training Loss =  0.010266389282861211; Validation Loss = 0.01923819719841794\n",
            "Cost after 295561 iterations : Training Loss =  0.010266381054860186; Validation Loss = 0.019238194071293797\n",
            "Cost after 295562 iterations : Training Loss =  0.010266372826942873; Validation Loss = 0.01923819094421468\n",
            "Cost after 295563 iterations : Training Loss =  0.010266364599108934; Validation Loss = 0.01923818781718039\n",
            "Cost after 295564 iterations : Training Loss =  0.010266356371358755; Validation Loss = 0.019238184690190667\n",
            "Cost after 295565 iterations : Training Loss =  0.010266348143692101; Validation Loss = 0.019238181563245926\n",
            "Cost after 295566 iterations : Training Loss =  0.010266339916109028; Validation Loss = 0.01923817843634597\n",
            "Cost after 295567 iterations : Training Loss =  0.010266331688609355; Validation Loss = 0.019238175309490967\n",
            "Cost after 295568 iterations : Training Loss =  0.010266323461193454; Validation Loss = 0.01923817218268075\n",
            "Cost after 295569 iterations : Training Loss =  0.010266315233861024; Validation Loss = 0.019238169055915307\n",
            "Cost after 295570 iterations : Training Loss =  0.010266307006612137; Validation Loss = 0.019238165929195212\n",
            "Cost after 295571 iterations : Training Loss =  0.010266298779446782; Validation Loss = 0.019238162802519627\n",
            "Cost after 295572 iterations : Training Loss =  0.010266290552365014; Validation Loss = 0.019238159675889023\n",
            "Cost after 295573 iterations : Training Loss =  0.010266282325366813; Validation Loss = 0.019238156549302977\n",
            "Cost after 295574 iterations : Training Loss =  0.010266274098452105; Validation Loss = 0.01923815342276206\n",
            "Cost after 295575 iterations : Training Loss =  0.010266265871621015; Validation Loss = 0.01923815029626563\n",
            "Cost after 295576 iterations : Training Loss =  0.010266257644873449; Validation Loss = 0.019238147169814403\n",
            "Cost after 295577 iterations : Training Loss =  0.010266249418209524; Validation Loss = 0.01923814404340807\n",
            "Cost after 295578 iterations : Training Loss =  0.010266241191629055; Validation Loss = 0.01923814091704639\n",
            "Cost after 295579 iterations : Training Loss =  0.010266232965132103; Validation Loss = 0.019238137790729692\n",
            "Cost after 295580 iterations : Training Loss =  0.010266224738718749; Validation Loss = 0.019238134664457725\n",
            "Cost after 295581 iterations : Training Loss =  0.010266216512388935; Validation Loss = 0.019238131538230525\n",
            "Cost after 295582 iterations : Training Loss =  0.01026620828614271; Validation Loss = 0.019238128412048337\n",
            "Cost after 295583 iterations : Training Loss =  0.010266200059980005; Validation Loss = 0.01923812528591096\n",
            "Cost after 295584 iterations : Training Loss =  0.010266191833900755; Validation Loss = 0.01923812215981853\n",
            "Cost after 295585 iterations : Training Loss =  0.010266183607905092; Validation Loss = 0.0192381190337708\n",
            "Cost after 295586 iterations : Training Loss =  0.010266175381993014; Validation Loss = 0.019238115907768062\n",
            "Cost after 295587 iterations : Training Loss =  0.010266167156164518; Validation Loss = 0.01923811278181003\n",
            "Cost after 295588 iterations : Training Loss =  0.010266158930419402; Validation Loss = 0.019238109655897142\n",
            "Cost after 295589 iterations : Training Loss =  0.010266150704758042; Validation Loss = 0.01923810653002846\n",
            "Cost after 295590 iterations : Training Loss =  0.010266142479180033; Validation Loss = 0.01923810340420522\n",
            "Cost after 295591 iterations : Training Loss =  0.010266134253685617; Validation Loss = 0.019238100278426366\n",
            "Cost after 295592 iterations : Training Loss =  0.010266126028274711; Validation Loss = 0.01923809715269256\n",
            "Cost after 295593 iterations : Training Loss =  0.0102661178029474; Validation Loss = 0.0192380940270036\n",
            "Cost after 295594 iterations : Training Loss =  0.010266109577703622; Validation Loss = 0.019238090901359483\n",
            "Cost after 295595 iterations : Training Loss =  0.010266101352543348; Validation Loss = 0.019238087775760214\n",
            "Cost after 295596 iterations : Training Loss =  0.010266093127466666; Validation Loss = 0.019238084650205593\n",
            "Cost after 295597 iterations : Training Loss =  0.01026608490247334; Validation Loss = 0.01923808152469584\n",
            "Cost after 295598 iterations : Training Loss =  0.010266076677563627; Validation Loss = 0.01923807839923083\n",
            "Cost after 295599 iterations : Training Loss =  0.010266068452737457; Validation Loss = 0.01923807527381116\n",
            "Cost after 295600 iterations : Training Loss =  0.010266060227994873; Validation Loss = 0.019238072148435856\n",
            "Cost after 295601 iterations : Training Loss =  0.010266052003335772; Validation Loss = 0.019238069023105688\n",
            "Cost after 295602 iterations : Training Loss =  0.010266043778760153; Validation Loss = 0.01923806589782015\n",
            "Cost after 295603 iterations : Training Loss =  0.010266035554268048; Validation Loss = 0.019238062772579678\n",
            "Cost after 295604 iterations : Training Loss =  0.010266027329859602; Validation Loss = 0.019238059647383638\n",
            "Cost after 295605 iterations : Training Loss =  0.010266019105534468; Validation Loss = 0.019238056522232677\n",
            "Cost after 295606 iterations : Training Loss =  0.010266010881292977; Validation Loss = 0.019238053397126555\n",
            "Cost after 295607 iterations : Training Loss =  0.01026600265713497; Validation Loss = 0.019238050272065068\n",
            "Cost after 295608 iterations : Training Loss =  0.01026599443306055; Validation Loss = 0.01923804714704876\n",
            "Cost after 295609 iterations : Training Loss =  0.010265986209069522; Validation Loss = 0.01923804402207675\n",
            "Cost after 295610 iterations : Training Loss =  0.010265977985162097; Validation Loss = 0.01923804089714986\n",
            "Cost after 295611 iterations : Training Loss =  0.010265969761338143; Validation Loss = 0.019238037772267784\n",
            "Cost after 295612 iterations : Training Loss =  0.010265961537597798; Validation Loss = 0.019238034647430836\n",
            "Cost after 295613 iterations : Training Loss =  0.010265953313940814; Validation Loss = 0.019238031522638217\n",
            "Cost after 295614 iterations : Training Loss =  0.010265945090367481; Validation Loss = 0.019238028397890624\n",
            "Cost after 295615 iterations : Training Loss =  0.010265936866877578; Validation Loss = 0.019238025273187846\n",
            "Cost after 295616 iterations : Training Loss =  0.01026592864347119; Validation Loss = 0.019238022148529637\n",
            "Cost after 295617 iterations : Training Loss =  0.010265920420148313; Validation Loss = 0.019238019023916593\n",
            "Cost after 295618 iterations : Training Loss =  0.010265912196908977; Validation Loss = 0.019238015899348388\n",
            "Cost after 295619 iterations : Training Loss =  0.010265903973753035; Validation Loss = 0.019238012774824814\n",
            "Cost after 295620 iterations : Training Loss =  0.010265895750680761; Validation Loss = 0.019238009650346163\n",
            "Cost after 295621 iterations : Training Loss =  0.01026588752769191; Validation Loss = 0.019238006525912496\n",
            "Cost after 295622 iterations : Training Loss =  0.01026587930478656; Validation Loss = 0.01923800340152353\n",
            "Cost after 295623 iterations : Training Loss =  0.010265871081964651; Validation Loss = 0.019238000277178834\n",
            "Cost after 295624 iterations : Training Loss =  0.010265862859226307; Validation Loss = 0.019237997152879695\n",
            "Cost after 295625 iterations : Training Loss =  0.01026585463657145; Validation Loss = 0.019237994028625152\n",
            "Cost after 295626 iterations : Training Loss =  0.01026584641400004; Validation Loss = 0.01923799090441523\n",
            "Cost after 295627 iterations : Training Loss =  0.010265838191512236; Validation Loss = 0.019237987780250293\n",
            "Cost after 295628 iterations : Training Loss =  0.010265829969107852; Validation Loss = 0.019237984656130157\n",
            "Cost after 295629 iterations : Training Loss =  0.01026582174678703; Validation Loss = 0.019237981532054832\n",
            "Cost after 295630 iterations : Training Loss =  0.01026581352454958; Validation Loss = 0.01923797840802421\n",
            "Cost after 295631 iterations : Training Loss =  0.010265805302395705; Validation Loss = 0.019237975284038665\n",
            "Cost after 295632 iterations : Training Loss =  0.01026579708032529; Validation Loss = 0.019237972160097646\n",
            "Cost after 295633 iterations : Training Loss =  0.010265788858338423; Validation Loss = 0.01923796903620142\n",
            "Cost after 295634 iterations : Training Loss =  0.01026578063643498; Validation Loss = 0.019237965912350243\n",
            "Cost after 295635 iterations : Training Loss =  0.010265772414615053; Validation Loss = 0.019237962788543686\n",
            "Cost after 295636 iterations : Training Loss =  0.010265764192878595; Validation Loss = 0.019237959664782037\n",
            "Cost after 295637 iterations : Training Loss =  0.010265755971225657; Validation Loss = 0.019237956541065356\n",
            "Cost after 295638 iterations : Training Loss =  0.010265747749656131; Validation Loss = 0.019237953417393133\n",
            "Cost after 295639 iterations : Training Loss =  0.010265739528170151; Validation Loss = 0.01923795029376612\n",
            "Cost after 295640 iterations : Training Loss =  0.010265731306767594; Validation Loss = 0.01923794717018338\n",
            "Cost after 295641 iterations : Training Loss =  0.010265723085448618; Validation Loss = 0.019237944046645753\n",
            "Cost after 295642 iterations : Training Loss =  0.010265714864213038; Validation Loss = 0.019237940923152872\n",
            "Cost after 295643 iterations : Training Loss =  0.010265706643060929; Validation Loss = 0.01923793779970474\n",
            "Cost after 295644 iterations : Training Loss =  0.01026569842199243; Validation Loss = 0.01923793467630132\n",
            "Cost after 295645 iterations : Training Loss =  0.010265690201007238; Validation Loss = 0.019237931552943247\n",
            "Cost after 295646 iterations : Training Loss =  0.010265681980105634; Validation Loss = 0.019237928429629573\n",
            "Cost after 295647 iterations : Training Loss =  0.010265673759287448; Validation Loss = 0.01923792530636051\n",
            "Cost after 295648 iterations : Training Loss =  0.010265665538552795; Validation Loss = 0.019237922183136226\n",
            "Cost after 295649 iterations : Training Loss =  0.01026565731790146; Validation Loss = 0.019237919059956924\n",
            "Cost after 295650 iterations : Training Loss =  0.010265649097333766; Validation Loss = 0.019237915936822544\n",
            "Cost after 295651 iterations : Training Loss =  0.010265640876849487; Validation Loss = 0.01923791281373299\n",
            "Cost after 295652 iterations : Training Loss =  0.010265632656448616; Validation Loss = 0.019237909690688046\n",
            "Cost after 295653 iterations : Training Loss =  0.010265624436131302; Validation Loss = 0.019237906567687948\n",
            "Cost after 295654 iterations : Training Loss =  0.010265616215897404; Validation Loss = 0.01923790344473266\n",
            "Cost after 295655 iterations : Training Loss =  0.010265607995746944; Validation Loss = 0.019237900321821967\n",
            "Cost after 295656 iterations : Training Loss =  0.010265599775680034; Validation Loss = 0.01923789719895626\n",
            "Cost after 295657 iterations : Training Loss =  0.010265591555696556; Validation Loss = 0.0192378940761351\n",
            "Cost after 295658 iterations : Training Loss =  0.010265583335796481; Validation Loss = 0.019237890953359017\n",
            "Cost after 295659 iterations : Training Loss =  0.010265575115980015; Validation Loss = 0.01923788783062782\n",
            "Cost after 295660 iterations : Training Loss =  0.010265566896246865; Validation Loss = 0.01923788470794131\n",
            "Cost after 295661 iterations : Training Loss =  0.01026555867659727; Validation Loss = 0.019237881585299356\n",
            "Cost after 295662 iterations : Training Loss =  0.01026555045703106; Validation Loss = 0.019237878462702415\n",
            "Cost after 295663 iterations : Training Loss =  0.010265542237548296; Validation Loss = 0.01923787534015007\n",
            "Cost after 295664 iterations : Training Loss =  0.010265534018149045; Validation Loss = 0.019237872217642553\n",
            "Cost after 295665 iterations : Training Loss =  0.01026552579883326; Validation Loss = 0.01923786909517975\n",
            "Cost after 295666 iterations : Training Loss =  0.010265517579600847; Validation Loss = 0.01923786597276172\n",
            "Cost after 295667 iterations : Training Loss =  0.010265509360451936; Validation Loss = 0.019237862850388806\n",
            "Cost after 295668 iterations : Training Loss =  0.010265501141386427; Validation Loss = 0.019237859728060195\n",
            "Cost after 295669 iterations : Training Loss =  0.010265492922404404; Validation Loss = 0.019237856605776676\n",
            "Cost after 295670 iterations : Training Loss =  0.01026548470350587; Validation Loss = 0.01923785348353759\n",
            "Cost after 295671 iterations : Training Loss =  0.01026547648469078; Validation Loss = 0.01923785036134378\n",
            "Cost after 295672 iterations : Training Loss =  0.010265468265959105; Validation Loss = 0.019237847239194156\n",
            "Cost after 295673 iterations : Training Loss =  0.010265460047310878; Validation Loss = 0.019237844117089814\n",
            "Cost after 295674 iterations : Training Loss =  0.010265451828746138; Validation Loss = 0.019237840995030012\n",
            "Cost after 295675 iterations : Training Loss =  0.010265443610264793; Validation Loss = 0.019237837873015168\n",
            "Cost after 295676 iterations : Training Loss =  0.010265435391866917; Validation Loss = 0.019237834751045138\n",
            "Cost after 295677 iterations : Training Loss =  0.010265427173552414; Validation Loss = 0.01923783162911962\n",
            "Cost after 295678 iterations : Training Loss =  0.01026541895532148; Validation Loss = 0.019237828507238867\n",
            "Cost after 295679 iterations : Training Loss =  0.01026541073717386; Validation Loss = 0.019237825385402984\n",
            "Cost after 295680 iterations : Training Loss =  0.010265402519109765; Validation Loss = 0.019237822263612016\n",
            "Cost after 295681 iterations : Training Loss =  0.01026539430112903; Validation Loss = 0.019237819141865405\n",
            "Cost after 295682 iterations : Training Loss =  0.01026538608323179; Validation Loss = 0.01923781602016416\n",
            "Cost after 295683 iterations : Training Loss =  0.010265377865418018; Validation Loss = 0.019237812898506852\n",
            "Cost after 295684 iterations : Training Loss =  0.010265369647687615; Validation Loss = 0.019237809776894724\n",
            "Cost after 295685 iterations : Training Loss =  0.010265361430040658; Validation Loss = 0.01923780665532769\n",
            "Cost after 295686 iterations : Training Loss =  0.010265353212477075; Validation Loss = 0.01923780353380546\n",
            "Cost after 295687 iterations : Training Loss =  0.010265344994997053; Validation Loss = 0.019237800412327408\n",
            "Cost after 295688 iterations : Training Loss =  0.010265336777600338; Validation Loss = 0.019237797290894188\n",
            "Cost after 295689 iterations : Training Loss =  0.010265328560287145; Validation Loss = 0.019237794169505933\n",
            "Cost after 295690 iterations : Training Loss =  0.010265320343057303; Validation Loss = 0.019237791048162416\n",
            "Cost after 295691 iterations : Training Loss =  0.010265312125910937; Validation Loss = 0.019237787926863675\n",
            "Cost after 295692 iterations : Training Loss =  0.010265303908848042; Validation Loss = 0.019237784805609753\n",
            "Cost after 295693 iterations : Training Loss =  0.010265295691868449; Validation Loss = 0.019237781684400827\n",
            "Cost after 295694 iterations : Training Loss =  0.010265287474972367; Validation Loss = 0.019237778563236177\n",
            "Cost after 295695 iterations : Training Loss =  0.010265279258159626; Validation Loss = 0.019237775442116658\n",
            "Cost after 295696 iterations : Training Loss =  0.010265271041430378; Validation Loss = 0.019237772321041773\n",
            "Cost after 295697 iterations : Training Loss =  0.010265262824784571; Validation Loss = 0.019237769200011436\n",
            "Cost after 295698 iterations : Training Loss =  0.010265254608222092; Validation Loss = 0.019237766079025973\n",
            "Cost after 295699 iterations : Training Loss =  0.010265246391743105; Validation Loss = 0.01923776295808567\n",
            "Cost after 295700 iterations : Training Loss =  0.010265238175347573; Validation Loss = 0.019237759837189602\n",
            "Cost after 295701 iterations : Training Loss =  0.010265229959035345; Validation Loss = 0.019237756716338143\n",
            "Cost after 295702 iterations : Training Loss =  0.010265221742806618; Validation Loss = 0.01923775359553195\n",
            "Cost after 295703 iterations : Training Loss =  0.010265213526661302; Validation Loss = 0.01923775047477032\n",
            "Cost after 295704 iterations : Training Loss =  0.010265205310599322; Validation Loss = 0.01923774735405354\n",
            "Cost after 295705 iterations : Training Loss =  0.010265197094620758; Validation Loss = 0.01923774423338132\n",
            "Cost after 295706 iterations : Training Loss =  0.01026518887872565; Validation Loss = 0.019237741112754135\n",
            "Cost after 295707 iterations : Training Loss =  0.010265180662913986; Validation Loss = 0.01923773799217128\n",
            "Cost after 295708 iterations : Training Loss =  0.010265172447185708; Validation Loss = 0.019237734871633147\n",
            "Cost after 295709 iterations : Training Loss =  0.01026516423154073; Validation Loss = 0.019237731751140307\n",
            "Cost after 295710 iterations : Training Loss =  0.010265156015979304; Validation Loss = 0.01923772863069181\n",
            "Cost after 295711 iterations : Training Loss =  0.01026514780050122; Validation Loss = 0.019237725510287934\n",
            "Cost after 295712 iterations : Training Loss =  0.010265139585106539; Validation Loss = 0.019237722389929015\n",
            "Cost after 295713 iterations : Training Loss =  0.010265131369795297; Validation Loss = 0.019237719269614987\n",
            "Cost after 295714 iterations : Training Loss =  0.010265123154567457; Validation Loss = 0.0192377161493456\n",
            "Cost after 295715 iterations : Training Loss =  0.010265114939422934; Validation Loss = 0.019237713029120744\n",
            "Cost after 295716 iterations : Training Loss =  0.010265106724361896; Validation Loss = 0.01923770990894102\n",
            "Cost after 295717 iterations : Training Loss =  0.010265098509384205; Validation Loss = 0.019237706788805903\n",
            "Cost after 295718 iterations : Training Loss =  0.010265090294489902; Validation Loss = 0.019237703668715612\n",
            "Cost after 295719 iterations : Training Loss =  0.010265082079679; Validation Loss = 0.01923770054866964\n",
            "Cost after 295720 iterations : Training Loss =  0.010265073864951535; Validation Loss = 0.019237697428668942\n",
            "Cost after 295721 iterations : Training Loss =  0.01026506565030747; Validation Loss = 0.019237694308712732\n",
            "Cost after 295722 iterations : Training Loss =  0.010265057435746775; Validation Loss = 0.01923769118880127\n",
            "Cost after 295723 iterations : Training Loss =  0.010265049221269454; Validation Loss = 0.019237688068934194\n",
            "Cost after 295724 iterations : Training Loss =  0.010265041006875456; Validation Loss = 0.019237684949112176\n",
            "Cost after 295725 iterations : Training Loss =  0.01026503279256497; Validation Loss = 0.01923768182933468\n",
            "Cost after 295726 iterations : Training Loss =  0.010265024578337816; Validation Loss = 0.019237678709602163\n",
            "Cost after 295727 iterations : Training Loss =  0.010265016364194128; Validation Loss = 0.0192376755899141\n",
            "Cost after 295728 iterations : Training Loss =  0.010265008150133766; Validation Loss = 0.019237672470271\n",
            "Cost after 295729 iterations : Training Loss =  0.01026499993615669; Validation Loss = 0.019237669350672698\n",
            "Cost after 295730 iterations : Training Loss =  0.010264991722263098; Validation Loss = 0.01923766623111891\n",
            "Cost after 295731 iterations : Training Loss =  0.010264983508452903; Validation Loss = 0.01923766311161\n",
            "Cost after 295732 iterations : Training Loss =  0.010264975294726099; Validation Loss = 0.01923765999214587\n",
            "Cost after 295733 iterations : Training Loss =  0.010264967081082712; Validation Loss = 0.019237656872726154\n",
            "Cost after 295734 iterations : Training Loss =  0.010264958867522543; Validation Loss = 0.019237653753351312\n",
            "Cost after 295735 iterations : Training Loss =  0.010264950654045944; Validation Loss = 0.019237650634021188\n",
            "Cost after 295736 iterations : Training Loss =  0.010264942440652578; Validation Loss = 0.019237647514736014\n",
            "Cost after 295737 iterations : Training Loss =  0.010264934227342653; Validation Loss = 0.01923764439549565\n",
            "Cost after 295738 iterations : Training Loss =  0.010264926014116056; Validation Loss = 0.01923764127629973\n",
            "Cost after 295739 iterations : Training Loss =  0.010264917800972917; Validation Loss = 0.019237638157148275\n",
            "Cost after 295740 iterations : Training Loss =  0.010264909587913021; Validation Loss = 0.019237635038041937\n",
            "Cost after 295741 iterations : Training Loss =  0.010264901374936651; Validation Loss = 0.019237631918980324\n",
            "Cost after 295742 iterations : Training Loss =  0.010264893162043535; Validation Loss = 0.01923762879996325\n",
            "Cost after 295743 iterations : Training Loss =  0.010264884949233853; Validation Loss = 0.019237625680991074\n",
            "Cost after 295744 iterations : Training Loss =  0.010264876736507477; Validation Loss = 0.019237622562063635\n",
            "Cost after 295745 iterations : Training Loss =  0.010264868523864514; Validation Loss = 0.01923761944318074\n",
            "Cost after 295746 iterations : Training Loss =  0.01026486031130493; Validation Loss = 0.019237616324342816\n",
            "Cost after 295747 iterations : Training Loss =  0.010264852098828664; Validation Loss = 0.019237613205549647\n",
            "Cost after 295748 iterations : Training Loss =  0.010264843886435813; Validation Loss = 0.019237610086800944\n",
            "Cost after 295749 iterations : Training Loss =  0.010264835674126225; Validation Loss = 0.01923760696809711\n",
            "Cost after 295750 iterations : Training Loss =  0.01026482746190015; Validation Loss = 0.019237603849438237\n",
            "Cost after 295751 iterations : Training Loss =  0.010264819249757393; Validation Loss = 0.019237600730823718\n",
            "Cost after 295752 iterations : Training Loss =  0.010264811037697925; Validation Loss = 0.019237597612253736\n",
            "Cost after 295753 iterations : Training Loss =  0.010264802825721897; Validation Loss = 0.019237594493728513\n",
            "Cost after 295754 iterations : Training Loss =  0.010264794613829174; Validation Loss = 0.01923759137524819\n",
            "Cost after 295755 iterations : Training Loss =  0.010264786402019832; Validation Loss = 0.019237588256812613\n",
            "Cost after 295756 iterations : Training Loss =  0.0102647781902938; Validation Loss = 0.01923758513842185\n",
            "Cost after 295757 iterations : Training Loss =  0.010264769978651203; Validation Loss = 0.019237582020075448\n",
            "Cost after 295758 iterations : Training Loss =  0.010264761767091958; Validation Loss = 0.01923757890177396\n",
            "Cost after 295759 iterations : Training Loss =  0.010264753555615933; Validation Loss = 0.019237575783517095\n",
            "Cost after 295760 iterations : Training Loss =  0.01026474534422332; Validation Loss = 0.019237572665304944\n",
            "Cost after 295761 iterations : Training Loss =  0.01026473713291411; Validation Loss = 0.01923756954713752\n",
            "Cost after 295762 iterations : Training Loss =  0.010264728921688228; Validation Loss = 0.019237566429014807\n",
            "Cost after 295763 iterations : Training Loss =  0.010264720710545707; Validation Loss = 0.01923756331093687\n",
            "Cost after 295764 iterations : Training Loss =  0.010264712499486522; Validation Loss = 0.01923756019290338\n",
            "Cost after 295765 iterations : Training Loss =  0.010264704288510654; Validation Loss = 0.019237557074914924\n",
            "Cost after 295766 iterations : Training Loss =  0.010264696077618197; Validation Loss = 0.01923755395697109\n",
            "Cost after 295767 iterations : Training Loss =  0.010264687866808961; Validation Loss = 0.019237550839071655\n",
            "Cost after 295768 iterations : Training Loss =  0.010264679656083195; Validation Loss = 0.019237547721217253\n",
            "Cost after 295769 iterations : Training Loss =  0.010264671445440798; Validation Loss = 0.019237544603407382\n",
            "Cost after 295770 iterations : Training Loss =  0.010264663234881623; Validation Loss = 0.01923754148564213\n",
            "Cost after 295771 iterations : Training Loss =  0.01026465502440582; Validation Loss = 0.019237538367921667\n",
            "Cost after 295772 iterations : Training Loss =  0.010264646814013366; Validation Loss = 0.01923753525024583\n",
            "Cost after 295773 iterations : Training Loss =  0.01026463860370433; Validation Loss = 0.019237532132614695\n",
            "Cost after 295774 iterations : Training Loss =  0.010264630393478517; Validation Loss = 0.019237529015028627\n",
            "Cost after 295775 iterations : Training Loss =  0.010264622183335992; Validation Loss = 0.019237525897486882\n",
            "Cost after 295776 iterations : Training Loss =  0.010264613973276911; Validation Loss = 0.019237522779989948\n",
            "Cost after 295777 iterations : Training Loss =  0.010264605763301196; Validation Loss = 0.019237519662537592\n",
            "Cost after 295778 iterations : Training Loss =  0.010264597553408675; Validation Loss = 0.019237516545129812\n",
            "Cost after 295779 iterations : Training Loss =  0.010264589343599629; Validation Loss = 0.019237513427766553\n",
            "Cost after 295780 iterations : Training Loss =  0.010264581133873771; Validation Loss = 0.019237510310448572\n",
            "Cost after 295781 iterations : Training Loss =  0.010264572924231306; Validation Loss = 0.019237507193175005\n",
            "Cost after 295782 iterations : Training Loss =  0.01026456471467216; Validation Loss = 0.019237504075946165\n",
            "Cost after 295783 iterations : Training Loss =  0.010264556505196433; Validation Loss = 0.019237500958762085\n",
            "Cost after 295784 iterations : Training Loss =  0.010264548295803887; Validation Loss = 0.01923749784162268\n",
            "Cost after 295785 iterations : Training Loss =  0.010264540086494805; Validation Loss = 0.01923749472452797\n",
            "Cost after 295786 iterations : Training Loss =  0.01026453187726896; Validation Loss = 0.019237491607478004\n",
            "Cost after 295787 iterations : Training Loss =  0.010264523668126435; Validation Loss = 0.019237488490472573\n",
            "Cost after 295788 iterations : Training Loss =  0.010264515459067244; Validation Loss = 0.01923748537351192\n",
            "Cost after 295789 iterations : Training Loss =  0.010264507250091346; Validation Loss = 0.019237482256595877\n",
            "Cost after 295790 iterations : Training Loss =  0.010264499041198827; Validation Loss = 0.01923747913972456\n",
            "Cost after 295791 iterations : Training Loss =  0.010264490832389624; Validation Loss = 0.019237476022897854\n",
            "Cost after 295792 iterations : Training Loss =  0.010264482623663652; Validation Loss = 0.019237472906115653\n",
            "Cost after 295793 iterations : Training Loss =  0.010264474415021126; Validation Loss = 0.019237469789378434\n",
            "Cost after 295794 iterations : Training Loss =  0.010264466206461785; Validation Loss = 0.01923746667268596\n",
            "Cost after 295795 iterations : Training Loss =  0.01026445799798578; Validation Loss = 0.019237463556037768\n",
            "Cost after 295796 iterations : Training Loss =  0.010264449789593162; Validation Loss = 0.019237460439434535\n",
            "Cost after 295797 iterations : Training Loss =  0.010264441581283762; Validation Loss = 0.01923745732287603\n",
            "Cost after 295798 iterations : Training Loss =  0.01026443337305776; Validation Loss = 0.019237454206362272\n",
            "Cost after 295799 iterations : Training Loss =  0.010264425164915103; Validation Loss = 0.019237451089892763\n",
            "Cost after 295800 iterations : Training Loss =  0.010264416956855645; Validation Loss = 0.019237447973468314\n",
            "Cost after 295801 iterations : Training Loss =  0.010264408748879529; Validation Loss = 0.019237444857088275\n",
            "Cost after 295802 iterations : Training Loss =  0.010264400540986672; Validation Loss = 0.01923744174075331\n",
            "Cost after 295803 iterations : Training Loss =  0.010264392333177184; Validation Loss = 0.01923743862446255\n",
            "Cost after 295804 iterations : Training Loss =  0.010264384125451035; Validation Loss = 0.01923743550821664\n",
            "Cost after 295805 iterations : Training Loss =  0.010264375917808093; Validation Loss = 0.019237432392015627\n",
            "Cost after 295806 iterations : Training Loss =  0.010264367710248546; Validation Loss = 0.01923742927585901\n",
            "Cost after 295807 iterations : Training Loss =  0.010264359502772281; Validation Loss = 0.01923742615974729\n",
            "Cost after 295808 iterations : Training Loss =  0.010264351295379167; Validation Loss = 0.019237423043680195\n",
            "Cost after 295809 iterations : Training Loss =  0.01026434308806943; Validation Loss = 0.019237419927657654\n",
            "Cost after 295810 iterations : Training Loss =  0.010264334880843069; Validation Loss = 0.01923741681168002\n",
            "Cost after 295811 iterations : Training Loss =  0.01026432667370002; Validation Loss = 0.019237413695746896\n",
            "Cost after 295812 iterations : Training Loss =  0.010264318466640141; Validation Loss = 0.01923741057985834\n",
            "Cost after 295813 iterations : Training Loss =  0.010264310259663664; Validation Loss = 0.019237407464014304\n",
            "Cost after 295814 iterations : Training Loss =  0.010264302052770416; Validation Loss = 0.019237404348215276\n",
            "Cost after 295815 iterations : Training Loss =  0.010264293845960509; Validation Loss = 0.019237401232460708\n",
            "Cost after 295816 iterations : Training Loss =  0.010264285639233858; Validation Loss = 0.019237398116750796\n",
            "Cost after 295817 iterations : Training Loss =  0.01026427743259049; Validation Loss = 0.019237395001085823\n",
            "Cost after 295818 iterations : Training Loss =  0.010264269226030374; Validation Loss = 0.01923739188546508\n",
            "Cost after 295819 iterations : Training Loss =  0.010264261019553703; Validation Loss = 0.01923738876988933\n",
            "Cost after 295820 iterations : Training Loss =  0.010264252813160209; Validation Loss = 0.019237385654357918\n",
            "Cost after 295821 iterations : Training Loss =  0.010264244606850015; Validation Loss = 0.019237382538871556\n",
            "Cost after 295822 iterations : Training Loss =  0.010264236400623072; Validation Loss = 0.019237379423429686\n",
            "Cost after 295823 iterations : Training Loss =  0.010264228194479436; Validation Loss = 0.019237376308032312\n",
            "Cost after 295824 iterations : Training Loss =  0.010264219988419006; Validation Loss = 0.019237373192679874\n",
            "Cost after 295825 iterations : Training Loss =  0.010264211782441976; Validation Loss = 0.0192373700773719\n",
            "Cost after 295826 iterations : Training Loss =  0.010264203576548203; Validation Loss = 0.01923736696210859\n",
            "Cost after 295827 iterations : Training Loss =  0.01026419537073764; Validation Loss = 0.01923736384688976\n",
            "Cost after 295828 iterations : Training Loss =  0.010264187165010395; Validation Loss = 0.019237360731716006\n",
            "Cost after 295829 iterations : Training Loss =  0.010264178959366469; Validation Loss = 0.019237357616586535\n",
            "Cost after 295830 iterations : Training Loss =  0.010264170753805793; Validation Loss = 0.019237354501502112\n",
            "Cost after 295831 iterations : Training Loss =  0.010264162548328378; Validation Loss = 0.019237351386461772\n",
            "Cost after 295832 iterations : Training Loss =  0.010264154342934175; Validation Loss = 0.019237348271466562\n",
            "Cost after 295833 iterations : Training Loss =  0.010264146137623283; Validation Loss = 0.01923734515651586\n",
            "Cost after 295834 iterations : Training Loss =  0.010264137932395763; Validation Loss = 0.019237342041609692\n",
            "Cost after 295835 iterations : Training Loss =  0.010264129727251372; Validation Loss = 0.01923733892674837\n",
            "Cost after 295836 iterations : Training Loss =  0.010264121522190311; Validation Loss = 0.01923733581193178\n",
            "Cost after 295837 iterations : Training Loss =  0.010264113317212494; Validation Loss = 0.019237332697159583\n",
            "Cost after 295838 iterations : Training Loss =  0.010264105112318029; Validation Loss = 0.01923732958243202\n",
            "Cost after 295839 iterations : Training Loss =  0.010264096907506778; Validation Loss = 0.019237326467749293\n",
            "Cost after 295840 iterations : Training Loss =  0.010264088702778706; Validation Loss = 0.019237323353111024\n",
            "Cost after 295841 iterations : Training Loss =  0.010264080498134013; Validation Loss = 0.019237320238517378\n",
            "Cost after 295842 iterations : Training Loss =  0.01026407229357255; Validation Loss = 0.019237317123968357\n",
            "Cost after 295843 iterations : Training Loss =  0.01026406408909439; Validation Loss = 0.019237314009464306\n",
            "Cost after 295844 iterations : Training Loss =  0.010264055884699428; Validation Loss = 0.019237310895004644\n",
            "Cost after 295845 iterations : Training Loss =  0.010264047680387686; Validation Loss = 0.01923730778058929\n",
            "Cost after 295846 iterations : Training Loss =  0.010264039476159219; Validation Loss = 0.01923730466621905\n",
            "Cost after 295847 iterations : Training Loss =  0.010264031272014034; Validation Loss = 0.019237301551893383\n",
            "Cost after 295848 iterations : Training Loss =  0.010264023067952176; Validation Loss = 0.01923729843761219\n",
            "Cost after 295849 iterations : Training Loss =  0.01026401486397354; Validation Loss = 0.01923729532337592\n",
            "Cost after 295850 iterations : Training Loss =  0.010264006660078128; Validation Loss = 0.01923729220918429\n",
            "Cost after 295851 iterations : Training Loss =  0.01026399845626591; Validation Loss = 0.019237289095036882\n",
            "Cost after 295852 iterations : Training Loss =  0.010263990252537006; Validation Loss = 0.0192372859809344\n",
            "Cost after 295853 iterations : Training Loss =  0.010263982048891373; Validation Loss = 0.01923728286687646\n",
            "Cost after 295854 iterations : Training Loss =  0.010263973845329007; Validation Loss = 0.01923727975286301\n",
            "Cost after 295855 iterations : Training Loss =  0.010263965641849811; Validation Loss = 0.019237276638894088\n",
            "Cost after 295856 iterations : Training Loss =  0.0102639574384539; Validation Loss = 0.019237273524970188\n",
            "Cost after 295857 iterations : Training Loss =  0.010263949235141252; Validation Loss = 0.019237270411090947\n",
            "Cost after 295858 iterations : Training Loss =  0.01026394103191178; Validation Loss = 0.01923726729725598\n",
            "Cost after 295859 iterations : Training Loss =  0.01026393282876562; Validation Loss = 0.019237264183465753\n",
            "Cost after 295860 iterations : Training Loss =  0.010263924625702693; Validation Loss = 0.019237261069720263\n",
            "Cost after 295861 iterations : Training Loss =  0.010263916422722993; Validation Loss = 0.019237257956019486\n",
            "Cost after 295862 iterations : Training Loss =  0.010263908219826612; Validation Loss = 0.019237254842363487\n",
            "Cost after 295863 iterations : Training Loss =  0.010263900017013314; Validation Loss = 0.019237251728751778\n",
            "Cost after 295864 iterations : Training Loss =  0.010263891814283388; Validation Loss = 0.019237248615184863\n",
            "Cost after 295865 iterations : Training Loss =  0.010263883611636609; Validation Loss = 0.019237245501662163\n",
            "Cost after 295866 iterations : Training Loss =  0.010263875409073123; Validation Loss = 0.019237242388184746\n",
            "Cost after 295867 iterations : Training Loss =  0.010263867206592845; Validation Loss = 0.01923723927475141\n",
            "Cost after 295868 iterations : Training Loss =  0.010263859004195833; Validation Loss = 0.019237236161363212\n",
            "Cost after 295869 iterations : Training Loss =  0.010263850801881998; Validation Loss = 0.01923723304801947\n",
            "Cost after 295870 iterations : Training Loss =  0.010263842599651445; Validation Loss = 0.019237229934720105\n",
            "Cost after 295871 iterations : Training Loss =  0.010263834397504081; Validation Loss = 0.01923722682146547\n",
            "Cost after 295872 iterations : Training Loss =  0.010263826195439989; Validation Loss = 0.01923722370825545\n",
            "Cost after 295873 iterations : Training Loss =  0.01026381799345909; Validation Loss = 0.019237220595089904\n",
            "Cost after 295874 iterations : Training Loss =  0.01026380979156147; Validation Loss = 0.019237217481969028\n",
            "Cost after 295875 iterations : Training Loss =  0.010263801589747051; Validation Loss = 0.019237214368892863\n",
            "Cost after 295876 iterations : Training Loss =  0.010263793388015778; Validation Loss = 0.019237211255861225\n",
            "Cost after 295877 iterations : Training Loss =  0.010263785186367851; Validation Loss = 0.019237208142874364\n",
            "Cost after 295878 iterations : Training Loss =  0.010263776984803123; Validation Loss = 0.01923720502993198\n",
            "Cost after 295879 iterations : Training Loss =  0.01026376878332162; Validation Loss = 0.01923720191703434\n",
            "Cost after 295880 iterations : Training Loss =  0.01026376058192326; Validation Loss = 0.01923719880418112\n",
            "Cost after 295881 iterations : Training Loss =  0.010263752380608194; Validation Loss = 0.019237195691372746\n",
            "Cost after 295882 iterations : Training Loss =  0.01026374417937634; Validation Loss = 0.019237192578608495\n",
            "Cost after 295883 iterations : Training Loss =  0.010263735978227646; Validation Loss = 0.019237189465889228\n",
            "Cost after 295884 iterations : Training Loss =  0.010263727777162286; Validation Loss = 0.01923718635321443\n",
            "Cost after 295885 iterations : Training Loss =  0.010263719576179988; Validation Loss = 0.019237183240584505\n",
            "Cost after 295886 iterations : Training Loss =  0.010263711375280949; Validation Loss = 0.019237180127999135\n",
            "Cost after 295887 iterations : Training Loss =  0.010263703174465127; Validation Loss = 0.019237177015458275\n",
            "Cost after 295888 iterations : Training Loss =  0.010263694973732645; Validation Loss = 0.01923717390296213\n",
            "Cost after 295889 iterations : Training Loss =  0.010263686773083248; Validation Loss = 0.019237170790510183\n",
            "Cost after 295890 iterations : Training Loss =  0.01026367857251707; Validation Loss = 0.01923716767810332\n",
            "Cost after 295891 iterations : Training Loss =  0.01026367037203413; Validation Loss = 0.01923716456574101\n",
            "Cost after 295892 iterations : Training Loss =  0.010263662171634356; Validation Loss = 0.0192371614534232\n",
            "Cost after 295893 iterations : Training Loss =  0.010263653971317888; Validation Loss = 0.019237158341150067\n",
            "Cost after 295894 iterations : Training Loss =  0.010263645771084509; Validation Loss = 0.019237155228921138\n",
            "Cost after 295895 iterations : Training Loss =  0.010263637570934417; Validation Loss = 0.01923715211673707\n",
            "Cost after 295896 iterations : Training Loss =  0.0102636293708675; Validation Loss = 0.019237149004597467\n",
            "Cost after 295897 iterations : Training Loss =  0.010263621170883872; Validation Loss = 0.01923714589250284\n",
            "Cost after 295898 iterations : Training Loss =  0.010263612970983254; Validation Loss = 0.019237142780452552\n",
            "Cost after 295899 iterations : Training Loss =  0.010263604771165896; Validation Loss = 0.01923713966844667\n",
            "Cost after 295900 iterations : Training Loss =  0.010263596571431825; Validation Loss = 0.019237136556485426\n",
            "Cost after 295901 iterations : Training Loss =  0.010263588371780897; Validation Loss = 0.019237133444569163\n",
            "Cost after 295902 iterations : Training Loss =  0.010263580172213167; Validation Loss = 0.01923713033269752\n",
            "Cost after 295903 iterations : Training Loss =  0.01026357197272864; Validation Loss = 0.019237127220870058\n",
            "Cost after 295904 iterations : Training Loss =  0.01026356377332736; Validation Loss = 0.019237124109087546\n",
            "Cost after 295905 iterations : Training Loss =  0.01026355557400919; Validation Loss = 0.019237120997349526\n",
            "Cost after 295906 iterations : Training Loss =  0.010263547374774211; Validation Loss = 0.01923711788565562\n",
            "Cost after 295907 iterations : Training Loss =  0.010263539175622513; Validation Loss = 0.019237114774006768\n",
            "Cost after 295908 iterations : Training Loss =  0.010263530976553959; Validation Loss = 0.01923711166240247\n",
            "Cost after 295909 iterations : Training Loss =  0.010263522777568607; Validation Loss = 0.019237108550842465\n",
            "Cost after 295910 iterations : Training Loss =  0.010263514578666352; Validation Loss = 0.01923710543932732\n",
            "Cost after 295911 iterations : Training Loss =  0.01026350637984738; Validation Loss = 0.0192371023278569\n",
            "Cost after 295912 iterations : Training Loss =  0.010263498181111585; Validation Loss = 0.01923709921643087\n",
            "Cost after 295913 iterations : Training Loss =  0.010263489982458948; Validation Loss = 0.019237096105049322\n",
            "Cost after 295914 iterations : Training Loss =  0.010263481783889536; Validation Loss = 0.019237092993712554\n",
            "Cost after 295915 iterations : Training Loss =  0.01026347358540327; Validation Loss = 0.019237089882420316\n",
            "Cost after 295916 iterations : Training Loss =  0.010263465387000155; Validation Loss = 0.019237086771173\n",
            "Cost after 295917 iterations : Training Loss =  0.01026345718868023; Validation Loss = 0.01923708365997003\n",
            "Cost after 295918 iterations : Training Loss =  0.010263448990443595; Validation Loss = 0.019237080548811594\n",
            "Cost after 295919 iterations : Training Loss =  0.010263440792290041; Validation Loss = 0.01923707743769797\n",
            "Cost after 295920 iterations : Training Loss =  0.010263432594219633; Validation Loss = 0.019237074326628793\n",
            "Cost after 295921 iterations : Training Loss =  0.010263424396232498; Validation Loss = 0.019237071215603883\n",
            "Cost after 295922 iterations : Training Loss =  0.010263416198328488; Validation Loss = 0.019237068104623955\n",
            "Cost after 295923 iterations : Training Loss =  0.010263408000507585; Validation Loss = 0.019237064993688574\n",
            "Cost after 295924 iterations : Training Loss =  0.010263399802769958; Validation Loss = 0.019237061882797554\n",
            "Cost after 295925 iterations : Training Loss =  0.010263391605115513; Validation Loss = 0.01923705877195114\n",
            "Cost after 295926 iterations : Training Loss =  0.010263383407544235; Validation Loss = 0.01923705566114903\n",
            "Cost after 295927 iterations : Training Loss =  0.010263375210056033; Validation Loss = 0.01923705255039205\n",
            "Cost after 295928 iterations : Training Loss =  0.010263367012651054; Validation Loss = 0.019237049439679417\n",
            "Cost after 295929 iterations : Training Loss =  0.0102633588153293; Validation Loss = 0.01923704632901103\n",
            "Cost after 295930 iterations : Training Loss =  0.010263350618090639; Validation Loss = 0.019237043218387418\n",
            "Cost after 295931 iterations : Training Loss =  0.010263342420935165; Validation Loss = 0.019237040107808717\n",
            "Cost after 295932 iterations : Training Loss =  0.010263334223862773; Validation Loss = 0.019237036997274203\n",
            "Cost after 295933 iterations : Training Loss =  0.010263326026873644; Validation Loss = 0.019237033886784532\n",
            "Cost after 295934 iterations : Training Loss =  0.010263317829967726; Validation Loss = 0.019237030776339183\n",
            "Cost after 295935 iterations : Training Loss =  0.010263309633144915; Validation Loss = 0.019237027665938822\n",
            "Cost after 295936 iterations : Training Loss =  0.010263301436405284; Validation Loss = 0.019237024555582447\n",
            "Cost after 295937 iterations : Training Loss =  0.010263293239748702; Validation Loss = 0.019237021445270997\n",
            "Cost after 295938 iterations : Training Loss =  0.010263285043175419; Validation Loss = 0.019237018335004127\n",
            "Cost after 295939 iterations : Training Loss =  0.01026327684668516; Validation Loss = 0.019237015224781683\n",
            "Cost after 295940 iterations : Training Loss =  0.010263268650278174; Validation Loss = 0.019237012114603724\n",
            "Cost after 295941 iterations : Training Loss =  0.010263260453954306; Validation Loss = 0.019237009004470066\n",
            "Cost after 295942 iterations : Training Loss =  0.010263252257713533; Validation Loss = 0.019237005894381473\n",
            "Cost after 295943 iterations : Training Loss =  0.01026324406155598; Validation Loss = 0.019237002784337463\n",
            "Cost after 295944 iterations : Training Loss =  0.010263235865481564; Validation Loss = 0.019236999674337986\n",
            "Cost after 295945 iterations : Training Loss =  0.010263227669490307; Validation Loss = 0.019236996564382804\n",
            "Cost after 295946 iterations : Training Loss =  0.010263219473582164; Validation Loss = 0.019236993454472186\n",
            "Cost after 295947 iterations : Training Loss =  0.010263211277757166; Validation Loss = 0.019236990344606502\n",
            "Cost after 295948 iterations : Training Loss =  0.010263203082015327; Validation Loss = 0.019236987234784994\n",
            "Cost after 295949 iterations : Training Loss =  0.010263194886356708; Validation Loss = 0.019236984125008336\n",
            "Cost after 295950 iterations : Training Loss =  0.010263186690781157; Validation Loss = 0.019236981015275958\n",
            "Cost after 295951 iterations : Training Loss =  0.010263178495288823; Validation Loss = 0.019236977905588405\n",
            "Cost after 295952 iterations : Training Loss =  0.01026317029987956; Validation Loss = 0.019236974795945123\n",
            "Cost after 295953 iterations : Training Loss =  0.010263162104553468; Validation Loss = 0.01923697168634647\n",
            "Cost after 295954 iterations : Training Loss =  0.010263153909310482; Validation Loss = 0.01923696857679261\n",
            "Cost after 295955 iterations : Training Loss =  0.010263145714150643; Validation Loss = 0.0192369654672831\n",
            "Cost after 295956 iterations : Training Loss =  0.010263137519073928; Validation Loss = 0.019236962357818414\n",
            "Cost after 295957 iterations : Training Loss =  0.010263129324080434; Validation Loss = 0.01923695924839799\n",
            "Cost after 295958 iterations : Training Loss =  0.010263121129170003; Validation Loss = 0.01923695613902202\n",
            "Cost after 295959 iterations : Training Loss =  0.010263112934342719; Validation Loss = 0.01923695302969079\n",
            "Cost after 295960 iterations : Training Loss =  0.010263104739598626; Validation Loss = 0.01923694992040375\n",
            "Cost after 295961 iterations : Training Loss =  0.01026309654493762; Validation Loss = 0.019236946811161948\n",
            "Cost after 295962 iterations : Training Loss =  0.010263088350359658; Validation Loss = 0.01923694370196434\n",
            "Cost after 295963 iterations : Training Loss =  0.01026308015586497; Validation Loss = 0.01923694059281088\n",
            "Cost after 295964 iterations : Training Loss =  0.010263071961453357; Validation Loss = 0.019236937483702504\n",
            "Cost after 295965 iterations : Training Loss =  0.010263063767124847; Validation Loss = 0.019236934374638984\n",
            "Cost after 295966 iterations : Training Loss =  0.010263055572879513; Validation Loss = 0.01923693126561923\n",
            "Cost after 295967 iterations : Training Loss =  0.010263047378717246; Validation Loss = 0.01923692815664431\n",
            "Cost after 295968 iterations : Training Loss =  0.010263039184638166; Validation Loss = 0.019236925047714244\n",
            "Cost after 295969 iterations : Training Loss =  0.010263030990642092; Validation Loss = 0.019236921938828387\n",
            "Cost after 295970 iterations : Training Loss =  0.010263022796729351; Validation Loss = 0.019236918829986755\n",
            "Cost after 295971 iterations : Training Loss =  0.010263014602899568; Validation Loss = 0.019236915721190056\n",
            "Cost after 295972 iterations : Training Loss =  0.010263006409152887; Validation Loss = 0.019236912612437724\n",
            "Cost after 295973 iterations : Training Loss =  0.010262998215489408; Validation Loss = 0.01923690950372985\n",
            "Cost after 295974 iterations : Training Loss =  0.010262990021909058; Validation Loss = 0.01923690639506671\n",
            "Cost after 295975 iterations : Training Loss =  0.01026298182841184; Validation Loss = 0.019236903286448333\n",
            "Cost after 295976 iterations : Training Loss =  0.010262973634997648; Validation Loss = 0.019236900177874144\n",
            "Cost after 295977 iterations : Training Loss =  0.010262965441666638; Validation Loss = 0.019236897069344484\n",
            "Cost after 295978 iterations : Training Loss =  0.01026295724841873; Validation Loss = 0.01923689396085961\n",
            "Cost after 295979 iterations : Training Loss =  0.010262949055253885; Validation Loss = 0.019236890852419402\n",
            "Cost after 295980 iterations : Training Loss =  0.01026294086217231; Validation Loss = 0.01923688774402337\n",
            "Cost after 295981 iterations : Training Loss =  0.010262932669173693; Validation Loss = 0.01923688463567211\n",
            "Cost after 295982 iterations : Training Loss =  0.01026292447625818; Validation Loss = 0.019236881527365166\n",
            "Cost after 295983 iterations : Training Loss =  0.010262916283425894; Validation Loss = 0.01923687841910311\n",
            "Cost after 295984 iterations : Training Loss =  0.010262908090676646; Validation Loss = 0.019236875310885203\n",
            "Cost after 295985 iterations : Training Loss =  0.010262899898010476; Validation Loss = 0.019236872202711742\n",
            "Cost after 295986 iterations : Training Loss =  0.010262891705427483; Validation Loss = 0.01923686909458318\n",
            "Cost after 295987 iterations : Training Loss =  0.010262883512927564; Validation Loss = 0.01923686598649877\n",
            "Cost after 295988 iterations : Training Loss =  0.0102628753205107; Validation Loss = 0.019236862878459184\n",
            "Cost after 295989 iterations : Training Loss =  0.010262867128177035; Validation Loss = 0.019236859770463747\n",
            "Cost after 295990 iterations : Training Loss =  0.010262858935926327; Validation Loss = 0.019236856662513256\n",
            "Cost after 295991 iterations : Training Loss =  0.010262850743758834; Validation Loss = 0.019236853554606987\n",
            "Cost after 295992 iterations : Training Loss =  0.010262842551674414; Validation Loss = 0.019236850446745488\n",
            "Cost after 295993 iterations : Training Loss =  0.01026283435967312; Validation Loss = 0.0192368473389285\n",
            "Cost after 295994 iterations : Training Loss =  0.01026282616775494; Validation Loss = 0.019236844231155778\n",
            "Cost after 295995 iterations : Training Loss =  0.010262817975919726; Validation Loss = 0.019236841123427616\n",
            "Cost after 295996 iterations : Training Loss =  0.010262809784167737; Validation Loss = 0.019236838015744613\n",
            "Cost after 295997 iterations : Training Loss =  0.010262801592498873; Validation Loss = 0.01923683490810539\n",
            "Cost after 295998 iterations : Training Loss =  0.01026279340091297; Validation Loss = 0.019236831800510914\n",
            "Cost after 295999 iterations : Training Loss =  0.010262785209410255; Validation Loss = 0.019236828692960985\n",
            "Cost after 296000 iterations : Training Loss =  0.010262777017990506; Validation Loss = 0.0192368255854558\n",
            "Cost after 296001 iterations : Training Loss =  0.010262768826653999; Validation Loss = 0.0192368224779951\n",
            "Cost after 296002 iterations : Training Loss =  0.0102627606354005; Validation Loss = 0.019236819370578636\n",
            "Cost after 296003 iterations : Training Loss =  0.01026275244423008; Validation Loss = 0.019236816263206717\n",
            "Cost after 296004 iterations : Training Loss =  0.010262744253142811; Validation Loss = 0.019236813155879346\n",
            "Cost after 296005 iterations : Training Loss =  0.010262736062138553; Validation Loss = 0.019236810048596762\n",
            "Cost after 296006 iterations : Training Loss =  0.010262727871217394; Validation Loss = 0.019236806941358247\n",
            "Cost after 296007 iterations : Training Loss =  0.01026271968037936; Validation Loss = 0.01923680383416422\n",
            "Cost after 296008 iterations : Training Loss =  0.010262711489624446; Validation Loss = 0.019236800727015328\n",
            "Cost after 296009 iterations : Training Loss =  0.010262703298952479; Validation Loss = 0.01923679761991059\n",
            "Cost after 296010 iterations : Training Loss =  0.010262695108363721; Validation Loss = 0.01923679451285004\n",
            "Cost after 296011 iterations : Training Loss =  0.010262686917858047; Validation Loss = 0.01923679140583459\n",
            "Cost after 296012 iterations : Training Loss =  0.010262678727435297; Validation Loss = 0.019236788298862963\n",
            "Cost after 296013 iterations : Training Loss =  0.010262670537095757; Validation Loss = 0.019236785191936327\n",
            "Cost after 296014 iterations : Training Loss =  0.01026266234683929; Validation Loss = 0.019236782085053934\n",
            "Cost after 296015 iterations : Training Loss =  0.010262654156665845; Validation Loss = 0.01923677897821641\n",
            "Cost after 296016 iterations : Training Loss =  0.010262645966575485; Validation Loss = 0.019236775871423077\n",
            "Cost after 296017 iterations : Training Loss =  0.010262637776568174; Validation Loss = 0.01923677276467433\n",
            "Cost after 296018 iterations : Training Loss =  0.010262629586644017; Validation Loss = 0.01923676965797021\n",
            "Cost after 296019 iterations : Training Loss =  0.010262621396802852; Validation Loss = 0.01923676655131034\n",
            "Cost after 296020 iterations : Training Loss =  0.010262613207044798; Validation Loss = 0.019236763444695062\n",
            "Cost after 296021 iterations : Training Loss =  0.010262605017369765; Validation Loss = 0.019236760338124638\n",
            "Cost after 296022 iterations : Training Loss =  0.01026259682777784; Validation Loss = 0.01923675723159806\n",
            "Cost after 296023 iterations : Training Loss =  0.010262588638269003; Validation Loss = 0.019236754125116346\n",
            "Cost after 296024 iterations : Training Loss =  0.010262580448843166; Validation Loss = 0.019236751018679412\n",
            "Cost after 296025 iterations : Training Loss =  0.010262572259500438; Validation Loss = 0.019236747912286443\n",
            "Cost after 296026 iterations : Training Loss =  0.010262564070240749; Validation Loss = 0.01923674480593822\n",
            "Cost after 296027 iterations : Training Loss =  0.01026255588106411; Validation Loss = 0.019236741699634484\n",
            "Cost after 296028 iterations : Training Loss =  0.010262547691970552; Validation Loss = 0.01923673859337524\n",
            "Cost after 296029 iterations : Training Loss =  0.010262539502960103; Validation Loss = 0.0192367354871603\n",
            "Cost after 296030 iterations : Training Loss =  0.010262531314032654; Validation Loss = 0.01923673238099024\n",
            "Cost after 296031 iterations : Training Loss =  0.010262523125188319; Validation Loss = 0.01923672927486438\n",
            "Cost after 296032 iterations : Training Loss =  0.010262514936426962; Validation Loss = 0.019236726168783053\n",
            "Cost after 296033 iterations : Training Loss =  0.010262506747748732; Validation Loss = 0.019236723062746312\n",
            "Cost after 296034 iterations : Training Loss =  0.010262498559153559; Validation Loss = 0.01923671995675368\n",
            "Cost after 296035 iterations : Training Loss =  0.01026249037064129; Validation Loss = 0.01923671685080617\n",
            "Cost after 296036 iterations : Training Loss =  0.010262482182212207; Validation Loss = 0.01923671374490303\n",
            "Cost after 296037 iterations : Training Loss =  0.010262473993866157; Validation Loss = 0.019236710639044037\n",
            "Cost after 296038 iterations : Training Loss =  0.010262465805603128; Validation Loss = 0.019236707533229553\n",
            "Cost after 296039 iterations : Training Loss =  0.010262457617423218; Validation Loss = 0.019236704427459738\n",
            "Cost after 296040 iterations : Training Loss =  0.010262449429326262; Validation Loss = 0.019236701321734422\n",
            "Cost after 296041 iterations : Training Loss =  0.01026244124131241; Validation Loss = 0.019236698216053654\n",
            "Cost after 296042 iterations : Training Loss =  0.0102624330533816; Validation Loss = 0.019236695110417524\n",
            "Cost after 296043 iterations : Training Loss =  0.010262424865533793; Validation Loss = 0.019236692004825473\n",
            "Cost after 296044 iterations : Training Loss =  0.010262416677769035; Validation Loss = 0.019236688899277942\n",
            "Cost after 296045 iterations : Training Loss =  0.01026240849008744; Validation Loss = 0.019236685793775007\n",
            "Cost after 296046 iterations : Training Loss =  0.010262400302488732; Validation Loss = 0.01923668268831626\n",
            "Cost after 296047 iterations : Training Loss =  0.010262392114973148; Validation Loss = 0.019236679582902524\n",
            "Cost after 296048 iterations : Training Loss =  0.010262383927540527; Validation Loss = 0.019236676477533004\n",
            "Cost after 296049 iterations : Training Loss =  0.010262375740191; Validation Loss = 0.01923667337220809\n",
            "Cost after 296050 iterations : Training Loss =  0.010262367552924572; Validation Loss = 0.01923667026692763\n",
            "Cost after 296051 iterations : Training Loss =  0.010262359365741084; Validation Loss = 0.019236667161691614\n",
            "Cost after 296052 iterations : Training Loss =  0.010262351178640645; Validation Loss = 0.01923666405649999\n",
            "Cost after 296053 iterations : Training Loss =  0.010262342991623334; Validation Loss = 0.01923666095135273\n",
            "Cost after 296054 iterations : Training Loss =  0.010262334804688963; Validation Loss = 0.019236657846250182\n",
            "Cost after 296055 iterations : Training Loss =  0.010262326617837608; Validation Loss = 0.019236654741192412\n",
            "Cost after 296056 iterations : Training Loss =  0.010262318431069286; Validation Loss = 0.019236651636178798\n",
            "Cost after 296057 iterations : Training Loss =  0.010262310244384086; Validation Loss = 0.019236648531209464\n",
            "Cost after 296058 iterations : Training Loss =  0.010262302057781832; Validation Loss = 0.019236645426285028\n",
            "Cost after 296059 iterations : Training Loss =  0.010262293871262533; Validation Loss = 0.019236642321404786\n",
            "Cost after 296060 iterations : Training Loss =  0.010262285684826414; Validation Loss = 0.019236639216569144\n",
            "Cost after 296061 iterations : Training Loss =  0.010262277498473235; Validation Loss = 0.019236636111777877\n",
            "Cost after 296062 iterations : Training Loss =  0.010262269312203092; Validation Loss = 0.019236633007030993\n",
            "Cost after 296063 iterations : Training Loss =  0.010262261126015932; Validation Loss = 0.01923662990232877\n",
            "Cost after 296064 iterations : Training Loss =  0.010262252939911936; Validation Loss = 0.019236626797670672\n",
            "Cost after 296065 iterations : Training Loss =  0.010262244753890835; Validation Loss = 0.01923662369305718\n",
            "Cost after 296066 iterations : Training Loss =  0.010262236567952747; Validation Loss = 0.01923662058848791\n",
            "Cost after 296067 iterations : Training Loss =  0.010262228382097713; Validation Loss = 0.019236617483963516\n",
            "Cost after 296068 iterations : Training Loss =  0.010262220196325746; Validation Loss = 0.01923661437948336\n",
            "Cost after 296069 iterations : Training Loss =  0.010262212010636658; Validation Loss = 0.01923661127504776\n",
            "Cost after 296070 iterations : Training Loss =  0.010262203825030718; Validation Loss = 0.01923660817065663\n",
            "Cost after 296071 iterations : Training Loss =  0.010262195639507784; Validation Loss = 0.019236605066310097\n",
            "Cost after 296072 iterations : Training Loss =  0.010262187454067798; Validation Loss = 0.019236601962007757\n",
            "Cost after 296073 iterations : Training Loss =  0.010262179268710819; Validation Loss = 0.01923659885774975\n",
            "Cost after 296074 iterations : Training Loss =  0.010262171083436931; Validation Loss = 0.0192365957535364\n",
            "Cost after 296075 iterations : Training Loss =  0.010262162898246009; Validation Loss = 0.01923659264936758\n",
            "Cost after 296076 iterations : Training Loss =  0.010262154713138117; Validation Loss = 0.019236589545243033\n",
            "Cost after 296077 iterations : Training Loss =  0.010262146528113214; Validation Loss = 0.0192365864411631\n",
            "Cost after 296078 iterations : Training Loss =  0.01026213834317133; Validation Loss = 0.0192365833371276\n",
            "Cost after 296079 iterations : Training Loss =  0.010262130158312358; Validation Loss = 0.019236580233136638\n",
            "Cost after 296080 iterations : Training Loss =  0.0102621219735365; Validation Loss = 0.019236577129190163\n",
            "Cost after 296081 iterations : Training Loss =  0.010262113788843605; Validation Loss = 0.019236574025287955\n",
            "Cost after 296082 iterations : Training Loss =  0.01026210560423369; Validation Loss = 0.01923657092143039\n",
            "Cost after 296083 iterations : Training Loss =  0.010262097419706833; Validation Loss = 0.01923656781761701\n",
            "Cost after 296084 iterations : Training Loss =  0.010262089235262896; Validation Loss = 0.019236564713848294\n",
            "Cost after 296085 iterations : Training Loss =  0.01026208105090206; Validation Loss = 0.01923656161012425\n",
            "Cost after 296086 iterations : Training Loss =  0.010262072866624181; Validation Loss = 0.01923655850644451\n",
            "Cost after 296087 iterations : Training Loss =  0.010262064682429215; Validation Loss = 0.019236555402808865\n",
            "Cost after 296088 iterations : Training Loss =  0.01026205649831735; Validation Loss = 0.019236552299217963\n",
            "Cost after 296089 iterations : Training Loss =  0.010262048314288468; Validation Loss = 0.019236549195671696\n",
            "Cost after 296090 iterations : Training Loss =  0.010262040130342528; Validation Loss = 0.019236546092169533\n",
            "Cost after 296091 iterations : Training Loss =  0.010262031946479566; Validation Loss = 0.019236542988712222\n",
            "Cost after 296092 iterations : Training Loss =  0.010262023762699716; Validation Loss = 0.019236539885298772\n",
            "Cost after 296093 iterations : Training Loss =  0.010262015579002784; Validation Loss = 0.019236536781930238\n",
            "Cost after 296094 iterations : Training Loss =  0.010262007395388812; Validation Loss = 0.019236533678606112\n",
            "Cost after 296095 iterations : Training Loss =  0.01026199921185784; Validation Loss = 0.019236530575326167\n",
            "Cost after 296096 iterations : Training Loss =  0.010261991028409908; Validation Loss = 0.019236527472090852\n",
            "Cost after 296097 iterations : Training Loss =  0.010261982845044919; Validation Loss = 0.019236524368899874\n",
            "Cost after 296098 iterations : Training Loss =  0.01026197466176285; Validation Loss = 0.019236521265753703\n",
            "Cost after 296099 iterations : Training Loss =  0.010261966478563835; Validation Loss = 0.01923651816265148\n",
            "Cost after 296100 iterations : Training Loss =  0.010261958295447874; Validation Loss = 0.019236515059593645\n",
            "Cost after 296101 iterations : Training Loss =  0.010261950112414778; Validation Loss = 0.019236511956580486\n",
            "Cost after 296102 iterations : Training Loss =  0.010261941929464722; Validation Loss = 0.01923650885361178\n",
            "Cost after 296103 iterations : Training Loss =  0.010261933746597623; Validation Loss = 0.019236505750687704\n",
            "Cost after 296104 iterations : Training Loss =  0.010261925563813494; Validation Loss = 0.019236502647807838\n",
            "Cost after 296105 iterations : Training Loss =  0.010261917381112413; Validation Loss = 0.019236499544972478\n",
            "Cost after 296106 iterations : Training Loss =  0.010261909198494292; Validation Loss = 0.019236496442181714\n",
            "Cost after 296107 iterations : Training Loss =  0.01026190101595898; Validation Loss = 0.019236493339435203\n",
            "Cost after 296108 iterations : Training Loss =  0.010261892833506814; Validation Loss = 0.019236490236733267\n",
            "Cost after 296109 iterations : Training Loss =  0.010261884651137521; Validation Loss = 0.019236487134075567\n",
            "Cost after 296110 iterations : Training Loss =  0.010261876468851324; Validation Loss = 0.019236484031462057\n",
            "Cost after 296111 iterations : Training Loss =  0.010261868286647989; Validation Loss = 0.019236480928893265\n",
            "Cost after 296112 iterations : Training Loss =  0.010261860104527627; Validation Loss = 0.01923647782636866\n",
            "Cost after 296113 iterations : Training Loss =  0.01026185192249031; Validation Loss = 0.019236474723888685\n",
            "Cost after 296114 iterations : Training Loss =  0.01026184374053592; Validation Loss = 0.019236471621452807\n",
            "Cost after 296115 iterations : Training Loss =  0.010261835558664435; Validation Loss = 0.01923646851906204\n",
            "Cost after 296116 iterations : Training Loss =  0.010261827376876092; Validation Loss = 0.01923646541671536\n",
            "Cost after 296117 iterations : Training Loss =  0.010261819195170597; Validation Loss = 0.019236462314412874\n",
            "Cost after 296118 iterations : Training Loss =  0.010261811013548023; Validation Loss = 0.019236459212155256\n",
            "Cost after 296119 iterations : Training Loss =  0.010261802832008447; Validation Loss = 0.019236456109941798\n",
            "Cost after 296120 iterations : Training Loss =  0.010261794650551865; Validation Loss = 0.019236453007772738\n",
            "Cost after 296121 iterations : Training Loss =  0.010261786469178165; Validation Loss = 0.019236449905648195\n",
            "Cost after 296122 iterations : Training Loss =  0.010261778287887416; Validation Loss = 0.01923644680356804\n",
            "Cost after 296123 iterations : Training Loss =  0.010261770106679786; Validation Loss = 0.019236443701532255\n",
            "Cost after 296124 iterations : Training Loss =  0.010261761925554962; Validation Loss = 0.01923644059954098\n",
            "Cost after 296125 iterations : Training Loss =  0.010261753744513191; Validation Loss = 0.019236437497593996\n",
            "Cost after 296126 iterations : Training Loss =  0.010261745563554305; Validation Loss = 0.019236434395691497\n",
            "Cost after 296127 iterations : Training Loss =  0.010261737382678491; Validation Loss = 0.019236431293833366\n",
            "Cost after 296128 iterations : Training Loss =  0.010261729201885443; Validation Loss = 0.019236428192019775\n",
            "Cost after 296129 iterations : Training Loss =  0.010261721021175436; Validation Loss = 0.019236425090250524\n",
            "Cost after 296130 iterations : Training Loss =  0.010261712840548403; Validation Loss = 0.01923642198852575\n",
            "Cost after 296131 iterations : Training Loss =  0.010261704660004258; Validation Loss = 0.01923641888684541\n",
            "Cost after 296132 iterations : Training Loss =  0.010261696479543128; Validation Loss = 0.01923641578520944\n",
            "Cost after 296133 iterations : Training Loss =  0.010261688299164955; Validation Loss = 0.01923641268361802\n",
            "Cost after 296134 iterations : Training Loss =  0.010261680118869645; Validation Loss = 0.01923640958207056\n",
            "Cost after 296135 iterations : Training Loss =  0.010261671938657303; Validation Loss = 0.01923640648056777\n",
            "Cost after 296136 iterations : Training Loss =  0.010261663758527946; Validation Loss = 0.0192364033791095\n",
            "Cost after 296137 iterations : Training Loss =  0.01026165557848153; Validation Loss = 0.019236400277695874\n",
            "Cost after 296138 iterations : Training Loss =  0.010261647398518026; Validation Loss = 0.01923639717632619\n",
            "Cost after 296139 iterations : Training Loss =  0.010261639218637509; Validation Loss = 0.019236394075001315\n",
            "Cost after 296140 iterations : Training Loss =  0.010261631038839807; Validation Loss = 0.019236390973720582\n",
            "Cost after 296141 iterations : Training Loss =  0.010261622859125184; Validation Loss = 0.01923638787248429\n",
            "Cost after 296142 iterations : Training Loss =  0.010261614679493433; Validation Loss = 0.019236384771292355\n",
            "Cost after 296143 iterations : Training Loss =  0.010261606499944651; Validation Loss = 0.019236381670144694\n",
            "Cost after 296144 iterations : Training Loss =  0.010261598320478764; Validation Loss = 0.019236378569041826\n",
            "Cost after 296145 iterations : Training Loss =  0.01026159014109583; Validation Loss = 0.01923637546798303\n",
            "Cost after 296146 iterations : Training Loss =  0.010261581961795836; Validation Loss = 0.019236372366968676\n",
            "Cost after 296147 iterations : Training Loss =  0.010261573782578779; Validation Loss = 0.019236369265998848\n",
            "Cost after 296148 iterations : Training Loss =  0.010261565603444623; Validation Loss = 0.019236366165073487\n",
            "Cost after 296149 iterations : Training Loss =  0.010261557424393454; Validation Loss = 0.0192363630641926\n",
            "Cost after 296150 iterations : Training Loss =  0.01026154924542516; Validation Loss = 0.019236359963356153\n",
            "Cost after 296151 iterations : Training Loss =  0.010261541066539795; Validation Loss = 0.019236356862563826\n",
            "Cost after 296152 iterations : Training Loss =  0.01026153288773741; Validation Loss = 0.019236353761816188\n",
            "Cost after 296153 iterations : Training Loss =  0.010261524709017858; Validation Loss = 0.019236350661112615\n",
            "Cost after 296154 iterations : Training Loss =  0.01026151653038131; Validation Loss = 0.019236347560453382\n",
            "Cost after 296155 iterations : Training Loss =  0.01026150835182763; Validation Loss = 0.019236344459838836\n",
            "Cost after 296156 iterations : Training Loss =  0.010261500173356967; Validation Loss = 0.01923634135926851\n",
            "Cost after 296157 iterations : Training Loss =  0.010261491994969032; Validation Loss = 0.019236338258742627\n",
            "Cost after 296158 iterations : Training Loss =  0.010261483816664226; Validation Loss = 0.019236335158260995\n",
            "Cost after 296159 iterations : Training Loss =  0.01026147563844229; Validation Loss = 0.01923633205782412\n",
            "Cost after 296160 iterations : Training Loss =  0.010261467460303217; Validation Loss = 0.019236328957431\n",
            "Cost after 296161 iterations : Training Loss =  0.01026145928224703; Validation Loss = 0.019236325857082463\n",
            "Cost after 296162 iterations : Training Loss =  0.010261451104273902; Validation Loss = 0.01923632275677858\n",
            "Cost after 296163 iterations : Training Loss =  0.010261442926383505; Validation Loss = 0.019236319656519184\n",
            "Cost after 296164 iterations : Training Loss =  0.010261434748576215; Validation Loss = 0.01923631655630425\n",
            "Cost after 296165 iterations : Training Loss =  0.010261426570851735; Validation Loss = 0.019236313456133504\n",
            "Cost after 296166 iterations : Training Loss =  0.010261418393210105; Validation Loss = 0.019236310356007133\n",
            "Cost after 296167 iterations : Training Loss =  0.010261410215651474; Validation Loss = 0.019236307255925136\n",
            "Cost after 296168 iterations : Training Loss =  0.010261402038175686; Validation Loss = 0.019236304155887247\n",
            "Cost after 296169 iterations : Training Loss =  0.010261393860782904; Validation Loss = 0.01923630105589436\n",
            "Cost after 296170 iterations : Training Loss =  0.01026138568347289; Validation Loss = 0.01923629795594532\n",
            "Cost after 296171 iterations : Training Loss =  0.010261377506245888; Validation Loss = 0.01923629485604075\n",
            "Cost after 296172 iterations : Training Loss =  0.010261369329101804; Validation Loss = 0.01923629175618096\n",
            "Cost after 296173 iterations : Training Loss =  0.010261361152040521; Validation Loss = 0.019236288656365445\n",
            "Cost after 296174 iterations : Training Loss =  0.010261352975062261; Validation Loss = 0.01923628555659409\n",
            "Cost after 296175 iterations : Training Loss =  0.010261344798166832; Validation Loss = 0.01923628245686721\n",
            "Cost after 296176 iterations : Training Loss =  0.010261336621354302; Validation Loss = 0.019236279357184787\n",
            "Cost after 296177 iterations : Training Loss =  0.010261328444624716; Validation Loss = 0.019236276257546394\n",
            "Cost after 296178 iterations : Training Loss =  0.010261320267978028; Validation Loss = 0.01923627315795285\n",
            "Cost after 296179 iterations : Training Loss =  0.01026131209141423; Validation Loss = 0.01923627005840336\n",
            "Cost after 296180 iterations : Training Loss =  0.010261303914933318; Validation Loss = 0.019236266958898025\n",
            "Cost after 296181 iterations : Training Loss =  0.0102612957385353; Validation Loss = 0.01923626385943725\n",
            "Cost after 296182 iterations : Training Loss =  0.010261287562220082; Validation Loss = 0.0192362607600213\n",
            "Cost after 296183 iterations : Training Loss =  0.010261279385987901; Validation Loss = 0.019236257660649167\n",
            "Cost after 296184 iterations : Training Loss =  0.010261271209838494; Validation Loss = 0.019236254561321553\n",
            "Cost after 296185 iterations : Training Loss =  0.010261263033772073; Validation Loss = 0.01923625146203845\n",
            "Cost after 296186 iterations : Training Loss =  0.010261254857788551; Validation Loss = 0.019236248362799663\n",
            "Cost after 296187 iterations : Training Loss =  0.01026124668188783; Validation Loss = 0.019236245263605393\n",
            "Cost after 296188 iterations : Training Loss =  0.010261238506070025; Validation Loss = 0.019236242164455138\n",
            "Cost after 296189 iterations : Training Loss =  0.01026123033033516; Validation Loss = 0.019236239065349537\n",
            "Cost after 296190 iterations : Training Loss =  0.010261222154683131; Validation Loss = 0.01923623596628816\n",
            "Cost after 296191 iterations : Training Loss =  0.01026121397911397; Validation Loss = 0.019236232867271188\n",
            "Cost after 296192 iterations : Training Loss =  0.010261205803627724; Validation Loss = 0.019236229768298918\n",
            "Cost after 296193 iterations : Training Loss =  0.01026119762822436; Validation Loss = 0.019236226669370766\n",
            "Cost after 296194 iterations : Training Loss =  0.01026118945290386; Validation Loss = 0.019236223570487005\n",
            "Cost after 296195 iterations : Training Loss =  0.010261181277666198; Validation Loss = 0.019236220471647657\n",
            "Cost after 296196 iterations : Training Loss =  0.010261173102511558; Validation Loss = 0.019236217372852266\n",
            "Cost after 296197 iterations : Training Loss =  0.010261164927439715; Validation Loss = 0.019236214274101576\n",
            "Cost after 296198 iterations : Training Loss =  0.010261156752450746; Validation Loss = 0.019236211175395274\n",
            "Cost after 296199 iterations : Training Loss =  0.01026114857754463; Validation Loss = 0.01923620807673344\n",
            "Cost after 296200 iterations : Training Loss =  0.010261140402721374; Validation Loss = 0.019236204978115574\n",
            "Cost after 296201 iterations : Training Loss =  0.010261132227981043; Validation Loss = 0.019236201879542073\n",
            "Cost after 296202 iterations : Training Loss =  0.010261124053323547; Validation Loss = 0.019236198781013087\n",
            "Cost after 296203 iterations : Training Loss =  0.010261115878748958; Validation Loss = 0.01923619568252868\n",
            "Cost after 296204 iterations : Training Loss =  0.010261107704257253; Validation Loss = 0.019236192584088634\n",
            "Cost after 296205 iterations : Training Loss =  0.010261099529848333; Validation Loss = 0.019236189485692796\n",
            "Cost after 296206 iterations : Training Loss =  0.010261091355522401; Validation Loss = 0.019236186387341606\n",
            "Cost after 296207 iterations : Training Loss =  0.01026108318127918; Validation Loss = 0.019236183289034287\n",
            "Cost after 296208 iterations : Training Loss =  0.010261075007118902; Validation Loss = 0.01923618019077151\n",
            "Cost after 296209 iterations : Training Loss =  0.010261066833041543; Validation Loss = 0.01923617709255311\n",
            "Cost after 296210 iterations : Training Loss =  0.010261058659047011; Validation Loss = 0.01923617399437872\n",
            "Cost after 296211 iterations : Training Loss =  0.010261050485135351; Validation Loss = 0.019236170896249304\n",
            "Cost after 296212 iterations : Training Loss =  0.01026104231130657; Validation Loss = 0.019236167798164054\n",
            "Cost after 296213 iterations : Training Loss =  0.01026103413756055; Validation Loss = 0.01923616470012297\n",
            "Cost after 296214 iterations : Training Loss =  0.010261025963897503; Validation Loss = 0.019236161602126222\n",
            "Cost after 296215 iterations : Training Loss =  0.010261017790317236; Validation Loss = 0.019236158504174216\n",
            "Cost after 296216 iterations : Training Loss =  0.0102610096168199; Validation Loss = 0.019236155406265977\n",
            "Cost after 296217 iterations : Training Loss =  0.010261001443405393; Validation Loss = 0.019236152308402504\n",
            "Cost after 296218 iterations : Training Loss =  0.010260993270073694; Validation Loss = 0.019236149210583135\n",
            "Cost after 296219 iterations : Training Loss =  0.010260985096824911; Validation Loss = 0.019236146112808338\n",
            "Cost after 296220 iterations : Training Loss =  0.010260976923658903; Validation Loss = 0.019236143015077783\n",
            "Cost after 296221 iterations : Training Loss =  0.010260968750575845; Validation Loss = 0.019236139917391586\n",
            "Cost after 296222 iterations : Training Loss =  0.010260960577575605; Validation Loss = 0.019236136819749543\n",
            "Cost after 296223 iterations : Training Loss =  0.010260952404658139; Validation Loss = 0.01923613372215186\n",
            "Cost after 296224 iterations : Training Loss =  0.010260944231823532; Validation Loss = 0.019236130624598655\n",
            "Cost after 296225 iterations : Training Loss =  0.010260936059071914; Validation Loss = 0.019236127527089492\n",
            "Cost after 296226 iterations : Training Loss =  0.010260927886403015; Validation Loss = 0.01923612442962506\n",
            "Cost after 296227 iterations : Training Loss =  0.010260919713817056; Validation Loss = 0.019236121332204953\n",
            "Cost after 296228 iterations : Training Loss =  0.010260911541313844; Validation Loss = 0.019236118234829507\n",
            "Cost after 296229 iterations : Training Loss =  0.010260903368893482; Validation Loss = 0.019236115137497716\n",
            "Cost after 296230 iterations : Training Loss =  0.010260895196556; Validation Loss = 0.019236112040210657\n",
            "Cost after 296231 iterations : Training Loss =  0.01026088702430139; Validation Loss = 0.019236108942967577\n",
            "Cost after 296232 iterations : Training Loss =  0.0102608788521296; Validation Loss = 0.019236105845769253\n",
            "Cost after 296233 iterations : Training Loss =  0.010260870680040621; Validation Loss = 0.019236102748615064\n",
            "Cost after 296234 iterations : Training Loss =  0.01026086250803447; Validation Loss = 0.01923609965150549\n",
            "Cost after 296235 iterations : Training Loss =  0.010260854336111173; Validation Loss = 0.019236096554439835\n",
            "Cost after 296236 iterations : Training Loss =  0.010260846164270703; Validation Loss = 0.01923609345741845\n",
            "Cost after 296237 iterations : Training Loss =  0.010260837992513167; Validation Loss = 0.019236090360441688\n",
            "Cost after 296238 iterations : Training Loss =  0.010260829820838377; Validation Loss = 0.01923608726350913\n",
            "Cost after 296239 iterations : Training Loss =  0.010260821649246375; Validation Loss = 0.019236084166621033\n",
            "Cost after 296240 iterations : Training Loss =  0.010260813477737187; Validation Loss = 0.01923608106977705\n",
            "Cost after 296241 iterations : Training Loss =  0.010260805306310938; Validation Loss = 0.019236077972977696\n",
            "Cost after 296242 iterations : Training Loss =  0.010260797134967501; Validation Loss = 0.019236074876222347\n",
            "Cost after 296243 iterations : Training Loss =  0.010260788963706857; Validation Loss = 0.0192360717795115\n",
            "Cost after 296244 iterations : Training Loss =  0.010260780792529019; Validation Loss = 0.019236068682845255\n",
            "Cost after 296245 iterations : Training Loss =  0.010260772621434015; Validation Loss = 0.01923606558622262\n",
            "Cost after 296246 iterations : Training Loss =  0.010260764450421896; Validation Loss = 0.019236062489644754\n",
            "Cost after 296247 iterations : Training Loss =  0.010260756279492531; Validation Loss = 0.019236059393111196\n",
            "Cost after 296248 iterations : Training Loss =  0.010260748108646016; Validation Loss = 0.01923605629662208\n",
            "Cost after 296249 iterations : Training Loss =  0.010260739937882344; Validation Loss = 0.019236053200177402\n",
            "Cost after 296250 iterations : Training Loss =  0.010260731767201518; Validation Loss = 0.01923605010377651\n",
            "Cost after 296251 iterations : Training Loss =  0.010260723596603412; Validation Loss = 0.019236047007420428\n",
            "Cost after 296252 iterations : Training Loss =  0.010260715426088205; Validation Loss = 0.019236043911108378\n",
            "Cost after 296253 iterations : Training Loss =  0.010260707255655858; Validation Loss = 0.019236040814840878\n",
            "Cost after 296254 iterations : Training Loss =  0.01026069908530622; Validation Loss = 0.0192360377186175\n",
            "Cost after 296255 iterations : Training Loss =  0.010260690915039424; Validation Loss = 0.019236034622438673\n",
            "Cost after 296256 iterations : Training Loss =  0.010260682744855414; Validation Loss = 0.01923603152630391\n",
            "Cost after 296257 iterations : Training Loss =  0.010260674574754311; Validation Loss = 0.01923602843021355\n",
            "Cost after 296258 iterations : Training Loss =  0.01026066640473595; Validation Loss = 0.01923602533416743\n",
            "Cost after 296259 iterations : Training Loss =  0.010260658234800395; Validation Loss = 0.019236022238165763\n",
            "Cost after 296260 iterations : Training Loss =  0.010260650064947705; Validation Loss = 0.019236019142208474\n",
            "Cost after 296261 iterations : Training Loss =  0.010260641895177764; Validation Loss = 0.019236016046295178\n",
            "Cost after 296262 iterations : Training Loss =  0.010260633725490645; Validation Loss = 0.01923601295042652\n",
            "Cost after 296263 iterations : Training Loss =  0.010260625555886384; Validation Loss = 0.01923600985460196\n",
            "Cost after 296264 iterations : Training Loss =  0.010260617386364886; Validation Loss = 0.01923600675882182\n",
            "Cost after 296265 iterations : Training Loss =  0.010260609216926233; Validation Loss = 0.019236003663085866\n",
            "Cost after 296266 iterations : Training Loss =  0.010260601047570326; Validation Loss = 0.019236000567394575\n",
            "Cost after 296267 iterations : Training Loss =  0.010260592878297227; Validation Loss = 0.019235997471747188\n",
            "Cost after 296268 iterations : Training Loss =  0.010260584709106974; Validation Loss = 0.01923599437614439\n",
            "Cost after 296269 iterations : Training Loss =  0.010260576539999457; Validation Loss = 0.0192359912805858\n",
            "Cost after 296270 iterations : Training Loss =  0.010260568370974778; Validation Loss = 0.019235988185071672\n",
            "Cost after 296271 iterations : Training Loss =  0.010260560202032904; Validation Loss = 0.019235985089601483\n",
            "Cost after 296272 iterations : Training Loss =  0.010260552033173845; Validation Loss = 0.019235981994175495\n",
            "Cost after 296273 iterations : Training Loss =  0.01026054386439754; Validation Loss = 0.01923597889879441\n",
            "Cost after 296274 iterations : Training Loss =  0.01026053569570404; Validation Loss = 0.01923597580345744\n",
            "Cost after 296275 iterations : Training Loss =  0.01026052752709333; Validation Loss = 0.019235972708164573\n",
            "Cost after 296276 iterations : Training Loss =  0.010260519358565397; Validation Loss = 0.01923596961291601\n",
            "Cost after 296277 iterations : Training Loss =  0.010260511190120357; Validation Loss = 0.01923596651771175\n",
            "Cost after 296278 iterations : Training Loss =  0.010260503021757883; Validation Loss = 0.019235963422551846\n",
            "Cost after 296279 iterations : Training Loss =  0.010260494853478432; Validation Loss = 0.019235960327436418\n",
            "Cost after 296280 iterations : Training Loss =  0.010260486685281674; Validation Loss = 0.019235957232364913\n",
            "Cost after 296281 iterations : Training Loss =  0.010260478517167715; Validation Loss = 0.0192359541373378\n",
            "Cost after 296282 iterations : Training Loss =  0.010260470349136495; Validation Loss = 0.019235951042355284\n",
            "Cost after 296283 iterations : Training Loss =  0.010260462181188123; Validation Loss = 0.01923594794741666\n",
            "Cost after 296284 iterations : Training Loss =  0.010260454013322541; Validation Loss = 0.019235944852522637\n",
            "Cost after 296285 iterations : Training Loss =  0.010260445845539715; Validation Loss = 0.019235941757672872\n",
            "Cost after 296286 iterations : Training Loss =  0.010260437677839594; Validation Loss = 0.019235938662867387\n",
            "Cost after 296287 iterations : Training Loss =  0.010260429510222427; Validation Loss = 0.019235935568106315\n",
            "Cost after 296288 iterations : Training Loss =  0.010260421342687924; Validation Loss = 0.01923593247338929\n",
            "Cost after 296289 iterations : Training Loss =  0.01026041317523628; Validation Loss = 0.019235929378716828\n",
            "Cost after 296290 iterations : Training Loss =  0.010260405007867382; Validation Loss = 0.01923592628408827\n",
            "Cost after 296291 iterations : Training Loss =  0.010260396840581132; Validation Loss = 0.01923592318950437\n",
            "Cost after 296292 iterations : Training Loss =  0.01026038867337775; Validation Loss = 0.019235920094964724\n",
            "Cost after 296293 iterations : Training Loss =  0.010260380506257152; Validation Loss = 0.019235917000469237\n",
            "Cost after 296294 iterations : Training Loss =  0.01026037233921928; Validation Loss = 0.01923591390601802\n",
            "Cost after 296295 iterations : Training Loss =  0.010260364172264233; Validation Loss = 0.019235910811611166\n",
            "Cost after 296296 iterations : Training Loss =  0.010260356005392027; Validation Loss = 0.019235907717248528\n",
            "Cost after 296297 iterations : Training Loss =  0.010260347838602461; Validation Loss = 0.019235904622930036\n",
            "Cost after 296298 iterations : Training Loss =  0.010260339671895786; Validation Loss = 0.019235901528655962\n",
            "Cost after 296299 iterations : Training Loss =  0.010260331505271817; Validation Loss = 0.019235898434426052\n",
            "Cost after 296300 iterations : Training Loss =  0.01026032333873052; Validation Loss = 0.01923589534024059\n",
            "Cost after 296301 iterations : Training Loss =  0.010260315172272072; Validation Loss = 0.019235892246099253\n",
            "Cost after 296302 iterations : Training Loss =  0.010260307005896457; Validation Loss = 0.019235889152002632\n",
            "Cost after 296303 iterations : Training Loss =  0.01026029883960346; Validation Loss = 0.01923588605794973\n",
            "Cost after 296304 iterations : Training Loss =  0.010260290673393364; Validation Loss = 0.019235882963941485\n",
            "Cost after 296305 iterations : Training Loss =  0.01026028250726593; Validation Loss = 0.019235879869977535\n",
            "Cost after 296306 iterations : Training Loss =  0.010260274341221277; Validation Loss = 0.019235876776057266\n",
            "Cost after 296307 iterations : Training Loss =  0.01026026617525946; Validation Loss = 0.019235873682182162\n",
            "Cost after 296308 iterations : Training Loss =  0.010260258009380338; Validation Loss = 0.019235870588351047\n",
            "Cost after 296309 iterations : Training Loss =  0.01026024984358396; Validation Loss = 0.019235867494563942\n",
            "Cost after 296310 iterations : Training Loss =  0.01026024167787038; Validation Loss = 0.019235864400821135\n",
            "Cost after 296311 iterations : Training Loss =  0.010260233512239522; Validation Loss = 0.01923586130712278\n",
            "Cost after 296312 iterations : Training Loss =  0.01026022534669148; Validation Loss = 0.019235858213468914\n",
            "Cost after 296313 iterations : Training Loss =  0.010260217181226015; Validation Loss = 0.01923585511985904\n",
            "Cost after 296314 iterations : Training Loss =  0.010260209015843442; Validation Loss = 0.01923585202629332\n",
            "Cost after 296315 iterations : Training Loss =  0.010260200850543593; Validation Loss = 0.01923584893277237\n",
            "Cost after 296316 iterations : Training Loss =  0.010260192685326536; Validation Loss = 0.019235845839295156\n",
            "Cost after 296317 iterations : Training Loss =  0.01026018452019221; Validation Loss = 0.019235842745862423\n",
            "Cost after 296318 iterations : Training Loss =  0.01026017635514063; Validation Loss = 0.01923583965247402\n",
            "Cost after 296319 iterations : Training Loss =  0.010260168190171799; Validation Loss = 0.019235836559129613\n",
            "Cost after 296320 iterations : Training Loss =  0.010260160025285639; Validation Loss = 0.019235833465829622\n",
            "Cost after 296321 iterations : Training Loss =  0.010260151860482332; Validation Loss = 0.01923583037257377\n",
            "Cost after 296322 iterations : Training Loss =  0.010260143695761718; Validation Loss = 0.01923582727936228\n",
            "Cost after 296323 iterations : Training Loss =  0.010260135531123852; Validation Loss = 0.019235824186195145\n",
            "Cost after 296324 iterations : Training Loss =  0.010260127366568748; Validation Loss = 0.019235821093072467\n",
            "Cost after 296325 iterations : Training Loss =  0.010260119202096338; Validation Loss = 0.019235817999993816\n",
            "Cost after 296326 iterations : Training Loss =  0.010260111037706625; Validation Loss = 0.0192358149069594\n",
            "Cost after 296327 iterations : Training Loss =  0.010260102873399748; Validation Loss = 0.019235811813969328\n",
            "Cost after 296328 iterations : Training Loss =  0.010260094709175565; Validation Loss = 0.01923580872102352\n",
            "Cost after 296329 iterations : Training Loss =  0.0102600865450341; Validation Loss = 0.019235805628121893\n",
            "Cost after 296330 iterations : Training Loss =  0.010260078380975382; Validation Loss = 0.01923580253526441\n",
            "Cost after 296331 iterations : Training Loss =  0.010260070216999357; Validation Loss = 0.01923579944245109\n",
            "Cost after 296332 iterations : Training Loss =  0.010260062053106186; Validation Loss = 0.01923579634968242\n",
            "Cost after 296333 iterations : Training Loss =  0.01026005388929571; Validation Loss = 0.019235793256957838\n",
            "Cost after 296334 iterations : Training Loss =  0.010260045725567919; Validation Loss = 0.0192357901642775\n",
            "Cost after 296335 iterations : Training Loss =  0.010260037561922886; Validation Loss = 0.019235787071641773\n",
            "Cost after 296336 iterations : Training Loss =  0.010260029398360527; Validation Loss = 0.019235783979049827\n",
            "Cost after 296337 iterations : Training Loss =  0.01026002123488091; Validation Loss = 0.019235780886502518\n",
            "Cost after 296338 iterations : Training Loss =  0.010260013071484038; Validation Loss = 0.019235777793999122\n",
            "Cost after 296339 iterations : Training Loss =  0.0102600049081698; Validation Loss = 0.01923577470154006\n",
            "Cost after 296340 iterations : Training Loss =  0.010259996744938427; Validation Loss = 0.019235771609125273\n",
            "Cost after 296341 iterations : Training Loss =  0.010259988581789795; Validation Loss = 0.01923576851675475\n",
            "Cost after 296342 iterations : Training Loss =  0.01025998041872372; Validation Loss = 0.019235765424428633\n",
            "Cost after 296343 iterations : Training Loss =  0.010259972255740489; Validation Loss = 0.01923576233214686\n",
            "Cost after 296344 iterations : Training Loss =  0.010259964092839998; Validation Loss = 0.01923575923990901\n",
            "Cost after 296345 iterations : Training Loss =  0.010259955930022139; Validation Loss = 0.019235756147715825\n",
            "Cost after 296346 iterations : Training Loss =  0.01025994776728699; Validation Loss = 0.019235753055566504\n",
            "Cost after 296347 iterations : Training Loss =  0.010259939604634578; Validation Loss = 0.019235749963461297\n",
            "Cost after 296348 iterations : Training Loss =  0.010259931442064938; Validation Loss = 0.019235746871400408\n",
            "Cost after 296349 iterations : Training Loss =  0.010259923279577947; Validation Loss = 0.019235743779384185\n",
            "Cost after 296350 iterations : Training Loss =  0.010259915117173715; Validation Loss = 0.019235740687411847\n",
            "Cost after 296351 iterations : Training Loss =  0.010259906954852212; Validation Loss = 0.019235737595483623\n",
            "Cost after 296352 iterations : Training Loss =  0.010259898792613343; Validation Loss = 0.019235734503599902\n",
            "Cost after 296353 iterations : Training Loss =  0.010259890630457269; Validation Loss = 0.019235731411760167\n",
            "Cost after 296354 iterations : Training Loss =  0.010259882468383798; Validation Loss = 0.01923572831996494\n",
            "Cost after 296355 iterations : Training Loss =  0.010259874306393153; Validation Loss = 0.019235725228213745\n",
            "Cost after 296356 iterations : Training Loss =  0.010259866144485194; Validation Loss = 0.019235722136507007\n",
            "Cost after 296357 iterations : Training Loss =  0.010259857982659859; Validation Loss = 0.01923571904484429\n",
            "Cost after 296358 iterations : Training Loss =  0.010259849820917292; Validation Loss = 0.019235715953225804\n",
            "Cost after 296359 iterations : Training Loss =  0.010259841659257375; Validation Loss = 0.019235712861651762\n",
            "Cost after 296360 iterations : Training Loss =  0.010259833497680186; Validation Loss = 0.01923570977012167\n",
            "Cost after 296361 iterations : Training Loss =  0.010259825336185814; Validation Loss = 0.019235706678635882\n",
            "Cost after 296362 iterations : Training Loss =  0.010259817174773918; Validation Loss = 0.019235703587194473\n",
            "Cost after 296363 iterations : Training Loss =  0.01025980901344488; Validation Loss = 0.019235700495797193\n",
            "Cost after 296364 iterations : Training Loss =  0.010259800852198481; Validation Loss = 0.019235697404444082\n",
            "Cost after 296365 iterations : Training Loss =  0.010259792691034775; Validation Loss = 0.019235694313135203\n",
            "Cost after 296366 iterations : Training Loss =  0.010259784529953844; Validation Loss = 0.01923569122187065\n",
            "Cost after 296367 iterations : Training Loss =  0.010259776368955462; Validation Loss = 0.019235688130650554\n",
            "Cost after 296368 iterations : Training Loss =  0.010259768208039914; Validation Loss = 0.01923568503947454\n",
            "Cost after 296369 iterations : Training Loss =  0.010259760047207096; Validation Loss = 0.019235681948342583\n",
            "Cost after 296370 iterations : Training Loss =  0.010259751886456772; Validation Loss = 0.01923567885725494\n",
            "Cost after 296371 iterations : Training Loss =  0.010259743725789236; Validation Loss = 0.019235675766211855\n",
            "Cost after 296372 iterations : Training Loss =  0.01025973556520438; Validation Loss = 0.019235672675212485\n",
            "Cost after 296373 iterations : Training Loss =  0.010259727404702297; Validation Loss = 0.019235669584257774\n",
            "Cost after 296374 iterations : Training Loss =  0.010259719244282815; Validation Loss = 0.019235666493347153\n",
            "Cost after 296375 iterations : Training Loss =  0.010259711083945963; Validation Loss = 0.019235663402480545\n",
            "Cost after 296376 iterations : Training Loss =  0.01025970292369197; Validation Loss = 0.019235660311658343\n",
            "Cost after 296377 iterations : Training Loss =  0.010259694763520516; Validation Loss = 0.01923565722088033\n",
            "Cost after 296378 iterations : Training Loss =  0.010259686603431859; Validation Loss = 0.019235654130146326\n",
            "Cost after 296379 iterations : Training Loss =  0.010259678443425829; Validation Loss = 0.019235651039456604\n",
            "Cost after 296380 iterations : Training Loss =  0.010259670283502376; Validation Loss = 0.01923564794881153\n",
            "Cost after 296381 iterations : Training Loss =  0.010259662123661762; Validation Loss = 0.019235644858210273\n",
            "Cost after 296382 iterations : Training Loss =  0.010259653963903652; Validation Loss = 0.01923564176765352\n",
            "Cost after 296383 iterations : Training Loss =  0.010259645804228392; Validation Loss = 0.019235638677140857\n",
            "Cost after 296384 iterations : Training Loss =  0.010259637644635722; Validation Loss = 0.01923563558667241\n",
            "Cost after 296385 iterations : Training Loss =  0.010259629485125786; Validation Loss = 0.01923563249624833\n",
            "Cost after 296386 iterations : Training Loss =  0.010259621325698418; Validation Loss = 0.019235629405868142\n",
            "Cost after 296387 iterations : Training Loss =  0.010259613166353786; Validation Loss = 0.019235626315532647\n",
            "Cost after 296388 iterations : Training Loss =  0.010259605007091853; Validation Loss = 0.019235623225241014\n",
            "Cost after 296389 iterations : Training Loss =  0.010259596847912531; Validation Loss = 0.019235620134993737\n",
            "Cost after 296390 iterations : Training Loss =  0.010259588688815902; Validation Loss = 0.019235617044790602\n",
            "Cost after 296391 iterations : Training Loss =  0.010259580529801967; Validation Loss = 0.01923561395463147\n",
            "Cost after 296392 iterations : Training Loss =  0.010259572370870756; Validation Loss = 0.019235610864516643\n",
            "Cost after 296393 iterations : Training Loss =  0.010259564212022074; Validation Loss = 0.019235607774446387\n",
            "Cost after 296394 iterations : Training Loss =  0.010259556053256173; Validation Loss = 0.019235604684419896\n",
            "Cost after 296395 iterations : Training Loss =  0.010259547894572877; Validation Loss = 0.01923560159443777\n",
            "Cost after 296396 iterations : Training Loss =  0.010259539735972234; Validation Loss = 0.01923559850449978\n",
            "Cost after 296397 iterations : Training Loss =  0.010259531577454286; Validation Loss = 0.01923559541460622\n",
            "Cost after 296398 iterations : Training Loss =  0.010259523419018996; Validation Loss = 0.01923559232475651\n",
            "Cost after 296399 iterations : Training Loss =  0.010259515260666355; Validation Loss = 0.01923558923495144\n",
            "Cost after 296400 iterations : Training Loss =  0.010259507102396403; Validation Loss = 0.019235586145190033\n",
            "Cost after 296401 iterations : Training Loss =  0.01025949894420903; Validation Loss = 0.019235583055473425\n",
            "Cost after 296402 iterations : Training Loss =  0.010259490786104403; Validation Loss = 0.019235579965800442\n",
            "Cost after 296403 iterations : Training Loss =  0.010259482628082356; Validation Loss = 0.019235576876171747\n",
            "Cost after 296404 iterations : Training Loss =  0.010259474470143038; Validation Loss = 0.019235573786587672\n",
            "Cost after 296405 iterations : Training Loss =  0.010259466312286329; Validation Loss = 0.01923557069704764\n",
            "Cost after 296406 iterations : Training Loss =  0.010259458154512315; Validation Loss = 0.01923556760755175\n",
            "Cost after 296407 iterations : Training Loss =  0.010259449996820855; Validation Loss = 0.019235564518100018\n",
            "Cost after 296408 iterations : Training Loss =  0.010259441839212114; Validation Loss = 0.019235561428692437\n",
            "Cost after 296409 iterations : Training Loss =  0.010259433681686027; Validation Loss = 0.019235558339329193\n",
            "Cost after 296410 iterations : Training Loss =  0.010259425524242543; Validation Loss = 0.01923555525000994\n",
            "Cost after 296411 iterations : Training Loss =  0.010259417366881818; Validation Loss = 0.019235552160735133\n",
            "Cost after 296412 iterations : Training Loss =  0.010259409209603554; Validation Loss = 0.019235549071504283\n",
            "Cost after 296413 iterations : Training Loss =  0.010259401052408103; Validation Loss = 0.01923554598231787\n",
            "Cost after 296414 iterations : Training Loss =  0.010259392895295203; Validation Loss = 0.01923554289317572\n",
            "Cost after 296415 iterations : Training Loss =  0.010259384738265027; Validation Loss = 0.01923553980407745\n",
            "Cost after 296416 iterations : Training Loss =  0.01025937658131739; Validation Loss = 0.01923553671502385\n",
            "Cost after 296417 iterations : Training Loss =  0.010259368424452487; Validation Loss = 0.01923553362601376\n",
            "Cost after 296418 iterations : Training Loss =  0.010259360267670199; Validation Loss = 0.01923553053704807\n",
            "Cost after 296419 iterations : Training Loss =  0.010259352110970564; Validation Loss = 0.019235527448126837\n",
            "Cost after 296420 iterations : Training Loss =  0.010259343954353442; Validation Loss = 0.01923552435924985\n",
            "Cost after 296421 iterations : Training Loss =  0.010259335797819067; Validation Loss = 0.01923552127041695\n",
            "Cost after 296422 iterations : Training Loss =  0.010259327641367291; Validation Loss = 0.01923551818162812\n",
            "Cost after 296423 iterations : Training Loss =  0.01025931948499826; Validation Loss = 0.01923551509288348\n",
            "Cost after 296424 iterations : Training Loss =  0.010259311328711692; Validation Loss = 0.019235512004183053\n",
            "Cost after 296425 iterations : Training Loss =  0.010259303172507865; Validation Loss = 0.019235508915526573\n",
            "Cost after 296426 iterations : Training Loss =  0.0102592950163866; Validation Loss = 0.01923550582691466\n",
            "Cost after 296427 iterations : Training Loss =  0.010259286860348002; Validation Loss = 0.019235502738346594\n",
            "Cost after 296428 iterations : Training Loss =  0.010259278704392073; Validation Loss = 0.019235499649822917\n",
            "Cost after 296429 iterations : Training Loss =  0.01025927054851877; Validation Loss = 0.019235496561343563\n",
            "Cost after 296430 iterations : Training Loss =  0.010259262392728046; Validation Loss = 0.019235493472908077\n",
            "Cost after 296431 iterations : Training Loss =  0.010259254237019884; Validation Loss = 0.0192354903845172\n",
            "Cost after 296432 iterations : Training Loss =  0.01025924608139454; Validation Loss = 0.019235487296170122\n",
            "Cost after 296433 iterations : Training Loss =  0.010259237925851651; Validation Loss = 0.019235484207867293\n",
            "Cost after 296434 iterations : Training Loss =  0.010259229770391414; Validation Loss = 0.019235481119608842\n",
            "Cost after 296435 iterations : Training Loss =  0.010259221615013827; Validation Loss = 0.019235478031394374\n",
            "Cost after 296436 iterations : Training Loss =  0.01025921345971883; Validation Loss = 0.01923547494322448\n",
            "Cost after 296437 iterations : Training Loss =  0.010259205304506545; Validation Loss = 0.019235471855098135\n",
            "Cost after 296438 iterations : Training Loss =  0.010259197149376767; Validation Loss = 0.019235468767016428\n",
            "Cost after 296439 iterations : Training Loss =  0.010259188994329605; Validation Loss = 0.019235465678978977\n",
            "Cost after 296440 iterations : Training Loss =  0.01025918083936516; Validation Loss = 0.019235462590985023\n",
            "Cost after 296441 iterations : Training Loss =  0.010259172684483233; Validation Loss = 0.01923545950303606\n",
            "Cost after 296442 iterations : Training Loss =  0.010259164529684008; Validation Loss = 0.019235456415130716\n",
            "Cost after 296443 iterations : Training Loss =  0.010259156374967278; Validation Loss = 0.019235453327269718\n",
            "Cost after 296444 iterations : Training Loss =  0.010259148220333289; Validation Loss = 0.019235450239452792\n",
            "Cost after 296445 iterations : Training Loss =  0.010259140065781746; Validation Loss = 0.019235447151680275\n",
            "Cost after 296446 iterations : Training Loss =  0.010259131911313004; Validation Loss = 0.019235444063951966\n",
            "Cost after 296447 iterations : Training Loss =  0.010259123756926747; Validation Loss = 0.01923544097626791\n",
            "Cost after 296448 iterations : Training Loss =  0.010259115602623133; Validation Loss = 0.01923543788862776\n",
            "Cost after 296449 iterations : Training Loss =  0.010259107448402168; Validation Loss = 0.01923543480103172\n",
            "Cost after 296450 iterations : Training Loss =  0.010259099294263795; Validation Loss = 0.019235431713480133\n",
            "Cost after 296451 iterations : Training Loss =  0.010259091140207953; Validation Loss = 0.01923542862597224\n",
            "Cost after 296452 iterations : Training Loss =  0.010259082986234862; Validation Loss = 0.019235425538508447\n",
            "Cost after 296453 iterations : Training Loss =  0.010259074832344162; Validation Loss = 0.019235422451089237\n",
            "Cost after 296454 iterations : Training Loss =  0.01025906667853618; Validation Loss = 0.01923541936371417\n",
            "Cost after 296455 iterations : Training Loss =  0.010259058524810771; Validation Loss = 0.019235416276382998\n",
            "Cost after 296456 iterations : Training Loss =  0.010259050371167918; Validation Loss = 0.019235413189096162\n",
            "Cost after 296457 iterations : Training Loss =  0.010259042217607815; Validation Loss = 0.019235410101853465\n",
            "Cost after 296458 iterations : Training Loss =  0.010259034064130177; Validation Loss = 0.019235407014654697\n",
            "Cost after 296459 iterations : Training Loss =  0.010259025910735171; Validation Loss = 0.01923540392750039\n",
            "Cost after 296460 iterations : Training Loss =  0.01025901775742277; Validation Loss = 0.019235400840390274\n",
            "Cost after 296461 iterations : Training Loss =  0.010259009604192921; Validation Loss = 0.019235397753324196\n",
            "Cost after 296462 iterations : Training Loss =  0.010259001451045788; Validation Loss = 0.019235394666302356\n",
            "Cost after 296463 iterations : Training Loss =  0.010258993297981111; Validation Loss = 0.019235391579324756\n",
            "Cost after 296464 iterations : Training Loss =  0.01025898514499904; Validation Loss = 0.019235388492391047\n",
            "Cost after 296465 iterations : Training Loss =  0.01025897699209957; Validation Loss = 0.019235385405501695\n",
            "Cost after 296466 iterations : Training Loss =  0.01025896883928269; Validation Loss = 0.019235382318656194\n",
            "Cost after 296467 iterations : Training Loss =  0.010258960686548397; Validation Loss = 0.019235379231854942\n",
            "Cost after 296468 iterations : Training Loss =  0.01025895253389678; Validation Loss = 0.019235376145097956\n",
            "Cost after 296469 iterations : Training Loss =  0.010258944381327684; Validation Loss = 0.0192353730583853\n",
            "Cost after 296470 iterations : Training Loss =  0.010258936228841165; Validation Loss = 0.019235369971716564\n",
            "Cost after 296471 iterations : Training Loss =  0.010258928076437192; Validation Loss = 0.019235366885091986\n",
            "Cost after 296472 iterations : Training Loss =  0.010258919924115864; Validation Loss = 0.019235363798511703\n",
            "Cost after 296473 iterations : Training Loss =  0.01025891177187702; Validation Loss = 0.01923536071197566\n",
            "Cost after 296474 iterations : Training Loss =  0.010258903619720887; Validation Loss = 0.019235357625483354\n",
            "Cost after 296475 iterations : Training Loss =  0.01025889546764725; Validation Loss = 0.019235354539035295\n",
            "Cost after 296476 iterations : Training Loss =  0.010258887315656136; Validation Loss = 0.019235351452631756\n",
            "Cost after 296477 iterations : Training Loss =  0.01025887916374771; Validation Loss = 0.01923534836627209\n",
            "Cost after 296478 iterations : Training Loss =  0.010258871011921777; Validation Loss = 0.019235345279956732\n",
            "Cost after 296479 iterations : Training Loss =  0.010258862860178462; Validation Loss = 0.019235342193685157\n",
            "Cost after 296480 iterations : Training Loss =  0.010258854708517746; Validation Loss = 0.019235339107457863\n",
            "Cost after 296481 iterations : Training Loss =  0.010258846556939578; Validation Loss = 0.019235336021274484\n",
            "Cost after 296482 iterations : Training Loss =  0.010258838405443949; Validation Loss = 0.01923533293513583\n",
            "Cost after 296483 iterations : Training Loss =  0.010258830254030925; Validation Loss = 0.019235329849040975\n",
            "Cost after 296484 iterations : Training Loss =  0.010258822102700398; Validation Loss = 0.019235326762990175\n",
            "Cost after 296485 iterations : Training Loss =  0.010258813951452563; Validation Loss = 0.019235323676983964\n",
            "Cost after 296486 iterations : Training Loss =  0.010258805800287233; Validation Loss = 0.01923532059102136\n",
            "Cost after 296487 iterations : Training Loss =  0.01025879764920438; Validation Loss = 0.019235317505103253\n",
            "Cost after 296488 iterations : Training Loss =  0.010258789498204263; Validation Loss = 0.019235314419229134\n",
            "Cost after 296489 iterations : Training Loss =  0.010258781347286616; Validation Loss = 0.019235311333399313\n",
            "Cost after 296490 iterations : Training Loss =  0.010258773196451541; Validation Loss = 0.019235308247613384\n",
            "Cost after 296491 iterations : Training Loss =  0.010258765045699033; Validation Loss = 0.019235305161871556\n",
            "Cost after 296492 iterations : Training Loss =  0.010258756895029052; Validation Loss = 0.01923530207617385\n",
            "Cost after 296493 iterations : Training Loss =  0.01025874874444164; Validation Loss = 0.019235298990520223\n",
            "Cost after 296494 iterations : Training Loss =  0.010258740593936767; Validation Loss = 0.019235295904910805\n",
            "Cost after 296495 iterations : Training Loss =  0.010258732443514603; Validation Loss = 0.019235292819346012\n",
            "Cost after 296496 iterations : Training Loss =  0.010258724293174822; Validation Loss = 0.01923528973382496\n",
            "Cost after 296497 iterations : Training Loss =  0.010258716142917665; Validation Loss = 0.01923528664834811\n",
            "Cost after 296498 iterations : Training Loss =  0.010258707992742973; Validation Loss = 0.019235283562915345\n",
            "Cost after 296499 iterations : Training Loss =  0.010258699842650949; Validation Loss = 0.0192352804775267\n",
            "Cost after 296500 iterations : Training Loss =  0.010258691692641438; Validation Loss = 0.019235277392181935\n",
            "Cost after 296501 iterations : Training Loss =  0.010258683542714498; Validation Loss = 0.0192352743068816\n",
            "Cost after 296502 iterations : Training Loss =  0.010258675392870049; Validation Loss = 0.019235271221625515\n",
            "Cost after 296503 iterations : Training Loss =  0.01025866724310818; Validation Loss = 0.019235268136413245\n",
            "Cost after 296504 iterations : Training Loss =  0.010258659093428817; Validation Loss = 0.0192352650512451\n",
            "Cost after 296505 iterations : Training Loss =  0.010258650943832062; Validation Loss = 0.01923526196612142\n",
            "Cost after 296506 iterations : Training Loss =  0.010258642794317865; Validation Loss = 0.01923525888104177\n",
            "Cost after 296507 iterations : Training Loss =  0.010258634644886204; Validation Loss = 0.019235255796005865\n",
            "Cost after 296508 iterations : Training Loss =  0.010258626495537061; Validation Loss = 0.01923525271101489\n",
            "Cost after 296509 iterations : Training Loss =  0.01025861834627041; Validation Loss = 0.019235249626067433\n",
            "Cost after 296510 iterations : Training Loss =  0.010258610197086349; Validation Loss = 0.01923524654116412\n",
            "Cost after 296511 iterations : Training Loss =  0.010258602047984884; Validation Loss = 0.019235243456304986\n",
            "Cost after 296512 iterations : Training Loss =  0.010258593898965858; Validation Loss = 0.019235240371490065\n",
            "Cost after 296513 iterations : Training Loss =  0.010258585750029406; Validation Loss = 0.019235237286718932\n",
            "Cost after 296514 iterations : Training Loss =  0.010258577601175486; Validation Loss = 0.01923523420199233\n",
            "Cost after 296515 iterations : Training Loss =  0.010258569452404177; Validation Loss = 0.019235231117309525\n",
            "Cost after 296516 iterations : Training Loss =  0.010258561303715292; Validation Loss = 0.01923522803267096\n",
            "Cost after 296517 iterations : Training Loss =  0.010258553155108971; Validation Loss = 0.019235224948076477\n",
            "Cost after 296518 iterations : Training Loss =  0.010258545006585225; Validation Loss = 0.019235221863526275\n",
            "Cost after 296519 iterations : Training Loss =  0.010258536858143997; Validation Loss = 0.019235218779019937\n",
            "Cost after 296520 iterations : Training Loss =  0.010258528709785343; Validation Loss = 0.019235215694557765\n",
            "Cost after 296521 iterations : Training Loss =  0.010258520561509143; Validation Loss = 0.019235212610139617\n",
            "Cost after 296522 iterations : Training Loss =  0.010258512413315466; Validation Loss = 0.019235209525765636\n",
            "Cost after 296523 iterations : Training Loss =  0.010258504265204306; Validation Loss = 0.019235206441435622\n",
            "Cost after 296524 iterations : Training Loss =  0.01025849611717565; Validation Loss = 0.01923520335714998\n",
            "Cost after 296525 iterations : Training Loss =  0.010258487969229594; Validation Loss = 0.019235200272908634\n",
            "Cost after 296526 iterations : Training Loss =  0.010258479821366044; Validation Loss = 0.019235197188711056\n",
            "Cost after 296527 iterations : Training Loss =  0.01025847167358503; Validation Loss = 0.019235194104557984\n",
            "Cost after 296528 iterations : Training Loss =  0.010258463525886545; Validation Loss = 0.01923519102044864\n",
            "Cost after 296529 iterations : Training Loss =  0.010258455378270541; Validation Loss = 0.019235187936383548\n",
            "Cost after 296530 iterations : Training Loss =  0.010258447230737117; Validation Loss = 0.01923518485236232\n",
            "Cost after 296531 iterations : Training Loss =  0.01025843908328613; Validation Loss = 0.019235181768385116\n",
            "Cost after 296532 iterations : Training Loss =  0.010258430935917698; Validation Loss = 0.01923517868445273\n",
            "Cost after 296533 iterations : Training Loss =  0.010258422788631722; Validation Loss = 0.019235175600563897\n",
            "Cost after 296534 iterations : Training Loss =  0.010258414641428331; Validation Loss = 0.019235172516719547\n",
            "Cost after 296535 iterations : Training Loss =  0.010258406494307551; Validation Loss = 0.019235169432918867\n",
            "Cost after 296536 iterations : Training Loss =  0.010258398347269134; Validation Loss = 0.019235166349162554\n",
            "Cost after 296537 iterations : Training Loss =  0.010258390200313224; Validation Loss = 0.01923516326545035\n",
            "Cost after 296538 iterations : Training Loss =  0.010258382053439855; Validation Loss = 0.01923516018178208\n",
            "Cost after 296539 iterations : Training Loss =  0.010258373906648988; Validation Loss = 0.019235157098158057\n",
            "Cost after 296540 iterations : Training Loss =  0.010258365759940644; Validation Loss = 0.019235154014578086\n",
            "Cost after 296541 iterations : Training Loss =  0.010258357613314902; Validation Loss = 0.0192351509310422\n",
            "Cost after 296542 iterations : Training Loss =  0.010258349466771432; Validation Loss = 0.019235147847550368\n",
            "Cost after 296543 iterations : Training Loss =  0.01025834132031067; Validation Loss = 0.019235144764102687\n",
            "Cost after 296544 iterations : Training Loss =  0.010258333173932445; Validation Loss = 0.019235141680699053\n",
            "Cost after 296545 iterations : Training Loss =  0.010258325027636577; Validation Loss = 0.019235138597339333\n",
            "Cost after 296546 iterations : Training Loss =  0.010258316881423262; Validation Loss = 0.019235135514024088\n",
            "Cost after 296547 iterations : Training Loss =  0.010258308735292425; Validation Loss = 0.019235132430752505\n",
            "Cost after 296548 iterations : Training Loss =  0.010258300589244138; Validation Loss = 0.01923512934752519\n",
            "Cost after 296549 iterations : Training Loss =  0.010258292443278328; Validation Loss = 0.019235126264342123\n",
            "Cost after 296550 iterations : Training Loss =  0.010258284297394966; Validation Loss = 0.019235123181202903\n",
            "Cost after 296551 iterations : Training Loss =  0.01025827615159419; Validation Loss = 0.01923512009810759\n",
            "Cost after 296552 iterations : Training Loss =  0.010258268005875908; Validation Loss = 0.01923511701505681\n",
            "Cost after 296553 iterations : Training Loss =  0.010258259860240045; Validation Loss = 0.019235113932049786\n",
            "Cost after 296554 iterations : Training Loss =  0.010258251714686688; Validation Loss = 0.01923511084908695\n",
            "Cost after 296555 iterations : Training Loss =  0.010258243569215878; Validation Loss = 0.01923510776616837\n",
            "Cost after 296556 iterations : Training Loss =  0.010258235423827582; Validation Loss = 0.019235104683293754\n",
            "Cost after 296557 iterations : Training Loss =  0.01025822727852163; Validation Loss = 0.01923510160046334\n",
            "Cost after 296558 iterations : Training Loss =  0.010258219133298288; Validation Loss = 0.019235098517676824\n",
            "Cost after 296559 iterations : Training Loss =  0.01025821098815736; Validation Loss = 0.01923509543493425\n",
            "Cost after 296560 iterations : Training Loss =  0.01025820284309897; Validation Loss = 0.019235092352235916\n",
            "Cost after 296561 iterations : Training Loss =  0.01025819469812304; Validation Loss = 0.01923508926958179\n",
            "Cost after 296562 iterations : Training Loss =  0.010258186553229647; Validation Loss = 0.019235086186971943\n",
            "Cost after 296563 iterations : Training Loss =  0.010258178408418717; Validation Loss = 0.01923508310440579\n",
            "Cost after 296564 iterations : Training Loss =  0.010258170263690185; Validation Loss = 0.019235080021883757\n",
            "Cost after 296565 iterations : Training Loss =  0.010258162119044305; Validation Loss = 0.019235076939405907\n",
            "Cost after 296566 iterations : Training Loss =  0.010258153974480733; Validation Loss = 0.019235073856972148\n",
            "Cost after 296567 iterations : Training Loss =  0.010258145829999756; Validation Loss = 0.01923507077458234\n",
            "Cost after 296568 iterations : Training Loss =  0.01025813768560122; Validation Loss = 0.01923506769223676\n",
            "Cost after 296569 iterations : Training Loss =  0.010258129541285188; Validation Loss = 0.01923506460993523\n",
            "Cost after 296570 iterations : Training Loss =  0.010258121397051645; Validation Loss = 0.019235061527677903\n",
            "Cost after 296571 iterations : Training Loss =  0.010258113252900422; Validation Loss = 0.01923505844546425\n",
            "Cost after 296572 iterations : Training Loss =  0.010258105108831867; Validation Loss = 0.019235055363294794\n",
            "Cost after 296573 iterations : Training Loss =  0.010258096964845654; Validation Loss = 0.0192350522811695\n",
            "Cost after 296574 iterations : Training Loss =  0.010258088820941971; Validation Loss = 0.01923504919908838\n",
            "Cost after 296575 iterations : Training Loss =  0.010258080677120727; Validation Loss = 0.019235046117050913\n",
            "Cost after 296576 iterations : Training Loss =  0.010258072533381981; Validation Loss = 0.019235043035058103\n",
            "Cost after 296577 iterations : Training Loss =  0.010258064389725693; Validation Loss = 0.019235039953109125\n",
            "Cost after 296578 iterations : Training Loss =  0.01025805624615186; Validation Loss = 0.019235036871204033\n",
            "Cost after 296579 iterations : Training Loss =  0.010258048102660535; Validation Loss = 0.01923503378934325\n",
            "Cost after 296580 iterations : Training Loss =  0.010258039959251656; Validation Loss = 0.019235030707526468\n",
            "Cost after 296581 iterations : Training Loss =  0.010258031815925297; Validation Loss = 0.019235027625753794\n",
            "Cost after 296582 iterations : Training Loss =  0.010258023672681367; Validation Loss = 0.019235024544025106\n",
            "Cost after 296583 iterations : Training Loss =  0.0102580155295198; Validation Loss = 0.019235021462340823\n",
            "Cost after 296584 iterations : Training Loss =  0.010258007386440783; Validation Loss = 0.019235018380700172\n",
            "Cost after 296585 iterations : Training Loss =  0.0102579992434442; Validation Loss = 0.019235015299103837\n",
            "Cost after 296586 iterations : Training Loss =  0.010257991100530188; Validation Loss = 0.019235012217551306\n",
            "Cost after 296587 iterations : Training Loss =  0.010257982957698442; Validation Loss = 0.01923500913604264\n",
            "Cost after 296588 iterations : Training Loss =  0.010257974814949293; Validation Loss = 0.019235006054578455\n",
            "Cost after 296589 iterations : Training Loss =  0.010257966672282428; Validation Loss = 0.019235002973157986\n",
            "Cost after 296590 iterations : Training Loss =  0.010257958529698241; Validation Loss = 0.019234999891781818\n",
            "Cost after 296591 iterations : Training Loss =  0.010257950387196348; Validation Loss = 0.019234996810449614\n",
            "Cost after 296592 iterations : Training Loss =  0.010257942244777041; Validation Loss = 0.01923499372916149\n",
            "Cost after 296593 iterations : Training Loss =  0.010257934102440046; Validation Loss = 0.019234990647917553\n",
            "Cost after 296594 iterations : Training Loss =  0.010257925960185573; Validation Loss = 0.01923498756671764\n",
            "Cost after 296595 iterations : Training Loss =  0.010257917818013608; Validation Loss = 0.019234984485561563\n",
            "Cost after 296596 iterations : Training Loss =  0.010257909675923927; Validation Loss = 0.019234981404449826\n",
            "Cost after 296597 iterations : Training Loss =  0.010257901533916886; Validation Loss = 0.01923497832338156\n",
            "Cost after 296598 iterations : Training Loss =  0.010257893391992163; Validation Loss = 0.01923497524235772\n",
            "Cost after 296599 iterations : Training Loss =  0.010257885250149914; Validation Loss = 0.019234972161378313\n",
            "Cost after 296600 iterations : Training Loss =  0.010257877108390106; Validation Loss = 0.019234969080442713\n",
            "Cost after 296601 iterations : Training Loss =  0.0102578689667128; Validation Loss = 0.01923496599955119\n",
            "Cost after 296602 iterations : Training Loss =  0.010257860825117872; Validation Loss = 0.01923496291870336\n",
            "Cost after 296603 iterations : Training Loss =  0.010257852683605405; Validation Loss = 0.019234959837900186\n",
            "Cost after 296604 iterations : Training Loss =  0.010257844542175439; Validation Loss = 0.019234956757140625\n",
            "Cost after 296605 iterations : Training Loss =  0.010257836400827773; Validation Loss = 0.01923495367642523\n",
            "Cost after 296606 iterations : Training Loss =  0.010257828259562627; Validation Loss = 0.019234950595753596\n",
            "Cost after 296607 iterations : Training Loss =  0.01025782011837986; Validation Loss = 0.019234947515126468\n",
            "Cost after 296608 iterations : Training Loss =  0.010257811977279598; Validation Loss = 0.019234944434543343\n",
            "Cost after 296609 iterations : Training Loss =  0.01025780383626177; Validation Loss = 0.019234941354003755\n",
            "Cost after 296610 iterations : Training Loss =  0.010257795695326373; Validation Loss = 0.019234938273508497\n",
            "Cost after 296611 iterations : Training Loss =  0.010257787554473343; Validation Loss = 0.019234935193057464\n",
            "Cost after 296612 iterations : Training Loss =  0.010257779413702817; Validation Loss = 0.01923493211264993\n",
            "Cost after 296613 iterations : Training Loss =  0.010257771273014634; Validation Loss = 0.01923492903228701\n",
            "Cost after 296614 iterations : Training Loss =  0.010257763132408945; Validation Loss = 0.019234925951967703\n",
            "Cost after 296615 iterations : Training Loss =  0.010257754991885673; Validation Loss = 0.01923492287169303\n",
            "Cost after 296616 iterations : Training Loss =  0.010257746851444852; Validation Loss = 0.019234919791461705\n",
            "Cost after 296617 iterations : Training Loss =  0.010257738711086446; Validation Loss = 0.019234916711275127\n",
            "Cost after 296618 iterations : Training Loss =  0.010257730570810444; Validation Loss = 0.01923491363113201\n",
            "Cost after 296619 iterations : Training Loss =  0.0102577224306169; Validation Loss = 0.0192349105510331\n",
            "Cost after 296620 iterations : Training Loss =  0.010257714290505735; Validation Loss = 0.01923490747097826\n",
            "Cost after 296621 iterations : Training Loss =  0.010257706150476983; Validation Loss = 0.01923490439096741\n",
            "Cost after 296622 iterations : Training Loss =  0.010257698010530739; Validation Loss = 0.019234901311000627\n",
            "Cost after 296623 iterations : Training Loss =  0.010257689870666898; Validation Loss = 0.019234898231077876\n",
            "Cost after 296624 iterations : Training Loss =  0.010257681730885395; Validation Loss = 0.019234895151199013\n",
            "Cost after 296625 iterations : Training Loss =  0.010257673591186288; Validation Loss = 0.01923489207136436\n",
            "Cost after 296626 iterations : Training Loss =  0.010257665451569723; Validation Loss = 0.019234888991573655\n",
            "Cost after 296627 iterations : Training Loss =  0.010257657312035477; Validation Loss = 0.019234885911827197\n",
            "Cost after 296628 iterations : Training Loss =  0.010257649172583698; Validation Loss = 0.01923488283212458\n",
            "Cost after 296629 iterations : Training Loss =  0.010257641033214271; Validation Loss = 0.01923487975246594\n",
            "Cost after 296630 iterations : Training Loss =  0.010257632893927279; Validation Loss = 0.019234876672851384\n",
            "Cost after 296631 iterations : Training Loss =  0.010257624754722796; Validation Loss = 0.01923487359328115\n",
            "Cost after 296632 iterations : Training Loss =  0.010257616615600632; Validation Loss = 0.01923487051375446\n",
            "Cost after 296633 iterations : Training Loss =  0.010257608476560869; Validation Loss = 0.01923486743427195\n",
            "Cost after 296634 iterations : Training Loss =  0.01025760033760356; Validation Loss = 0.019234864354833518\n",
            "Cost after 296635 iterations : Training Loss =  0.010257592198728583; Validation Loss = 0.019234861275438837\n",
            "Cost after 296636 iterations : Training Loss =  0.010257584059936067; Validation Loss = 0.01923485819608869\n",
            "Cost after 296637 iterations : Training Loss =  0.010257575921225939; Validation Loss = 0.01923485511678251\n",
            "Cost after 296638 iterations : Training Loss =  0.010257567782598218; Validation Loss = 0.019234852037519977\n",
            "Cost after 296639 iterations : Training Loss =  0.010257559644052911; Validation Loss = 0.019234848958301534\n",
            "Cost after 296640 iterations : Training Loss =  0.010257551505589959; Validation Loss = 0.019234845879127313\n",
            "Cost after 296641 iterations : Training Loss =  0.010257543367209432; Validation Loss = 0.01923484279999689\n",
            "Cost after 296642 iterations : Training Loss =  0.010257535228911287; Validation Loss = 0.019234839720910887\n",
            "Cost after 296643 iterations : Training Loss =  0.010257527090695596; Validation Loss = 0.019234836641868557\n",
            "Cost after 296644 iterations : Training Loss =  0.010257518952562281; Validation Loss = 0.01923483356287017\n",
            "Cost after 296645 iterations : Training Loss =  0.010257510814511381; Validation Loss = 0.01923483048391599\n",
            "Cost after 296646 iterations : Training Loss =  0.010257502676542808; Validation Loss = 0.019234827405005817\n",
            "Cost after 296647 iterations : Training Loss =  0.010257494538656667; Validation Loss = 0.01923482432613976\n",
            "Cost after 296648 iterations : Training Loss =  0.01025748640085296; Validation Loss = 0.019234821247317346\n",
            "Cost after 296649 iterations : Training Loss =  0.010257478263131623; Validation Loss = 0.019234818168539242\n",
            "Cost after 296650 iterations : Training Loss =  0.010257470125492583; Validation Loss = 0.019234815089804905\n",
            "Cost after 296651 iterations : Training Loss =  0.010257461987936036; Validation Loss = 0.019234812011114797\n",
            "Cost after 296652 iterations : Training Loss =  0.010257453850461853; Validation Loss = 0.019234808932468938\n",
            "Cost after 296653 iterations : Training Loss =  0.010257445713070105; Validation Loss = 0.019234805853866794\n",
            "Cost after 296654 iterations : Training Loss =  0.010257437575760617; Validation Loss = 0.019234802775308528\n",
            "Cost after 296655 iterations : Training Loss =  0.01025742943853362; Validation Loss = 0.019234799696794536\n",
            "Cost after 296656 iterations : Training Loss =  0.010257421301388975; Validation Loss = 0.019234796618324387\n",
            "Cost after 296657 iterations : Training Loss =  0.010257413164326701; Validation Loss = 0.01923479353989832\n",
            "Cost after 296658 iterations : Training Loss =  0.01025740502734673; Validation Loss = 0.019234790461516216\n",
            "Cost after 296659 iterations : Training Loss =  0.010257396890449324; Validation Loss = 0.019234787383178145\n",
            "Cost after 296660 iterations : Training Loss =  0.010257388753634142; Validation Loss = 0.019234784304884102\n",
            "Cost after 296661 iterations : Training Loss =  0.010257380616901462; Validation Loss = 0.019234781226633687\n",
            "Cost after 296662 iterations : Training Loss =  0.010257372480251095; Validation Loss = 0.019234778148427868\n",
            "Cost after 296663 iterations : Training Loss =  0.010257364343683057; Validation Loss = 0.019234775070265657\n",
            "Cost after 296664 iterations : Training Loss =  0.010257356207197378; Validation Loss = 0.019234771992147598\n",
            "Cost after 296665 iterations : Training Loss =  0.010257348070794267; Validation Loss = 0.019234768914073472\n",
            "Cost after 296666 iterations : Training Loss =  0.010257339934473381; Validation Loss = 0.019234765836043537\n",
            "Cost after 296667 iterations : Training Loss =  0.010257331798234862; Validation Loss = 0.019234762758057328\n",
            "Cost after 296668 iterations : Training Loss =  0.010257323662078746; Validation Loss = 0.019234759680115152\n",
            "Cost after 296669 iterations : Training Loss =  0.01025731552600504; Validation Loss = 0.01923475660221729\n",
            "Cost after 296670 iterations : Training Loss =  0.010257307390013659; Validation Loss = 0.019234753524363105\n",
            "Cost after 296671 iterations : Training Loss =  0.010257299254104575; Validation Loss = 0.01923475044655305\n",
            "Cost after 296672 iterations : Training Loss =  0.01025729111827796; Validation Loss = 0.01923474736878711\n",
            "Cost after 296673 iterations : Training Loss =  0.010257282982533684; Validation Loss = 0.019234744291064962\n",
            "Cost after 296674 iterations : Training Loss =  0.010257274846871792; Validation Loss = 0.01923474121338681\n",
            "Cost after 296675 iterations : Training Loss =  0.010257266711292195; Validation Loss = 0.019234738135752646\n",
            "Cost after 296676 iterations : Training Loss =  0.010257258575795054; Validation Loss = 0.01923473505816226\n",
            "Cost after 296677 iterations : Training Loss =  0.01025725044038024; Validation Loss = 0.019234731980615954\n",
            "Cost after 296678 iterations : Training Loss =  0.010257242305047777; Validation Loss = 0.01923472890311392\n",
            "Cost after 296679 iterations : Training Loss =  0.010257234169797555; Validation Loss = 0.019234725825655916\n",
            "Cost after 296680 iterations : Training Loss =  0.010257226034629887; Validation Loss = 0.01923472274824185\n",
            "Cost after 296681 iterations : Training Loss =  0.010257217899544408; Validation Loss = 0.019234719670871286\n",
            "Cost after 296682 iterations : Training Loss =  0.010257209764541416; Validation Loss = 0.019234716593545555\n",
            "Cost after 296683 iterations : Training Loss =  0.01025720162962069; Validation Loss = 0.01923471351626314\n",
            "Cost after 296684 iterations : Training Loss =  0.010257193494782366; Validation Loss = 0.01923471043902477\n",
            "Cost after 296685 iterations : Training Loss =  0.0102571853600264; Validation Loss = 0.019234707361830507\n",
            "Cost after 296686 iterations : Training Loss =  0.010257177225352784; Validation Loss = 0.019234704284680357\n",
            "Cost after 296687 iterations : Training Loss =  0.010257169090761497; Validation Loss = 0.019234701207573958\n",
            "Cost after 296688 iterations : Training Loss =  0.01025716095625261; Validation Loss = 0.019234698130511665\n",
            "Cost after 296689 iterations : Training Loss =  0.010257152821826017; Validation Loss = 0.01923469505349348\n",
            "Cost after 296690 iterations : Training Loss =  0.010257144687481807; Validation Loss = 0.019234691976519065\n",
            "Cost after 296691 iterations : Training Loss =  0.010257136553219843; Validation Loss = 0.019234688899588983\n",
            "Cost after 296692 iterations : Training Loss =  0.010257128419040275; Validation Loss = 0.019234685822702498\n",
            "Cost after 296693 iterations : Training Loss =  0.010257120284943211; Validation Loss = 0.01923468274585988\n",
            "Cost after 296694 iterations : Training Loss =  0.010257112150928284; Validation Loss = 0.019234679669061786\n",
            "Cost after 296695 iterations : Training Loss =  0.010257104016995788; Validation Loss = 0.019234676592307116\n",
            "Cost after 296696 iterations : Training Loss =  0.010257095883145603; Validation Loss = 0.01923467351559653\n",
            "Cost after 296697 iterations : Training Loss =  0.010257087749377837; Validation Loss = 0.0192346704389302\n",
            "Cost after 296698 iterations : Training Loss =  0.010257079615692274; Validation Loss = 0.019234667362307615\n",
            "Cost after 296699 iterations : Training Loss =  0.010257071482089182; Validation Loss = 0.019234664285729106\n",
            "Cost after 296700 iterations : Training Loss =  0.010257063348568317; Validation Loss = 0.019234661209194405\n",
            "Cost after 296701 iterations : Training Loss =  0.010257055215129818; Validation Loss = 0.019234658132704054\n",
            "Cost after 296702 iterations : Training Loss =  0.010257047081773702; Validation Loss = 0.019234655056257426\n",
            "Cost after 296703 iterations : Training Loss =  0.010257038948499901; Validation Loss = 0.019234651979854668\n",
            "Cost after 296704 iterations : Training Loss =  0.010257030815308401; Validation Loss = 0.019234648903496056\n",
            "Cost after 296705 iterations : Training Loss =  0.010257022682199222; Validation Loss = 0.019234645827181224\n",
            "Cost after 296706 iterations : Training Loss =  0.010257014549172517; Validation Loss = 0.019234642750910583\n",
            "Cost after 296707 iterations : Training Loss =  0.010257006416228025; Validation Loss = 0.019234639674683758\n",
            "Cost after 296708 iterations : Training Loss =  0.010256998283365813; Validation Loss = 0.019234636598500898\n",
            "Cost after 296709 iterations : Training Loss =  0.01025699015058597; Validation Loss = 0.01923463352236216\n",
            "Cost after 296710 iterations : Training Loss =  0.010256982017888423; Validation Loss = 0.019234630446267012\n",
            "Cost after 296711 iterations : Training Loss =  0.010256973885273343; Validation Loss = 0.019234627370215914\n",
            "Cost after 296712 iterations : Training Loss =  0.010256965752740457; Validation Loss = 0.01923462429420924\n",
            "Cost after 296713 iterations : Training Loss =  0.010256957620289909; Validation Loss = 0.019234621218245963\n",
            "Cost after 296714 iterations : Training Loss =  0.010256949487921716; Validation Loss = 0.019234618142326878\n",
            "Cost after 296715 iterations : Training Loss =  0.01025694135563583; Validation Loss = 0.019234615066451754\n",
            "Cost after 296716 iterations : Training Loss =  0.010256933223432187; Validation Loss = 0.01923461199062083\n",
            "Cost after 296717 iterations : Training Loss =  0.010256925091311007; Validation Loss = 0.019234608914833525\n",
            "Cost after 296718 iterations : Training Loss =  0.010256916959272064; Validation Loss = 0.019234605839090268\n",
            "Cost after 296719 iterations : Training Loss =  0.010256908827315434; Validation Loss = 0.019234602763390594\n",
            "Cost after 296720 iterations : Training Loss =  0.010256900695441171; Validation Loss = 0.019234599687735214\n",
            "Cost after 296721 iterations : Training Loss =  0.010256892563649167; Validation Loss = 0.01923459661212409\n",
            "Cost after 296722 iterations : Training Loss =  0.010256884431939532; Validation Loss = 0.019234593536556807\n",
            "Cost after 296723 iterations : Training Loss =  0.010256876300312096; Validation Loss = 0.019234590461033162\n",
            "Cost after 296724 iterations : Training Loss =  0.010256868168767091; Validation Loss = 0.019234587385553743\n",
            "Cost after 296725 iterations : Training Loss =  0.01025686003730433; Validation Loss = 0.01923458431011798\n",
            "Cost after 296726 iterations : Training Loss =  0.010256851905923832; Validation Loss = 0.019234581234726354\n",
            "Cost after 296727 iterations : Training Loss =  0.010256843774625749; Validation Loss = 0.019234578159378925\n",
            "Cost after 296728 iterations : Training Loss =  0.010256835643409939; Validation Loss = 0.019234575084075615\n",
            "Cost after 296729 iterations : Training Loss =  0.010256827512276436; Validation Loss = 0.019234572008815624\n",
            "Cost after 296730 iterations : Training Loss =  0.010256819381225255; Validation Loss = 0.01923456893359981\n",
            "Cost after 296731 iterations : Training Loss =  0.010256811250256338; Validation Loss = 0.019234565858428098\n",
            "Cost after 296732 iterations : Training Loss =  0.010256803119369677; Validation Loss = 0.019234562783300015\n",
            "Cost after 296733 iterations : Training Loss =  0.010256794988565372; Validation Loss = 0.019234559708216165\n",
            "Cost after 296734 iterations : Training Loss =  0.010256786857843437; Validation Loss = 0.019234556633176216\n",
            "Cost after 296735 iterations : Training Loss =  0.010256778727203682; Validation Loss = 0.019234553558180278\n",
            "Cost after 296736 iterations : Training Loss =  0.01025677059664631; Validation Loss = 0.01923455048322806\n",
            "Cost after 296737 iterations : Training Loss =  0.010256762466171248; Validation Loss = 0.019234547408319935\n",
            "Cost after 296738 iterations : Training Loss =  0.010256754335778337; Validation Loss = 0.019234544333455974\n",
            "Cost after 296739 iterations : Training Loss =  0.0102567462054679; Validation Loss = 0.01923454125863569\n",
            "Cost after 296740 iterations : Training Loss =  0.01025673807523966; Validation Loss = 0.01923453818385922\n",
            "Cost after 296741 iterations : Training Loss =  0.010256729945093674; Validation Loss = 0.01923453510912689\n",
            "Cost after 296742 iterations : Training Loss =  0.01025672181503007; Validation Loss = 0.019234532034438145\n",
            "Cost after 296743 iterations : Training Loss =  0.01025671368504867; Validation Loss = 0.019234528959793912\n",
            "Cost after 296744 iterations : Training Loss =  0.010256705555149638; Validation Loss = 0.01923452588519305\n",
            "Cost after 296745 iterations : Training Loss =  0.010256697425332924; Validation Loss = 0.01923452281063643\n",
            "Cost after 296746 iterations : Training Loss =  0.010256689295598394; Validation Loss = 0.019234519736123692\n",
            "Cost after 296747 iterations : Training Loss =  0.010256681165946224; Validation Loss = 0.019234516661655134\n",
            "Cost after 296748 iterations : Training Loss =  0.010256673036376309; Validation Loss = 0.01923451358723001\n",
            "Cost after 296749 iterations : Training Loss =  0.01025666490688864; Validation Loss = 0.01923451051284941\n",
            "Cost after 296750 iterations : Training Loss =  0.010256656777483403; Validation Loss = 0.019234507438512436\n",
            "Cost after 296751 iterations : Training Loss =  0.01025664864816025; Validation Loss = 0.01923450436421947\n",
            "Cost after 296752 iterations : Training Loss =  0.010256640518919507; Validation Loss = 0.019234501289970226\n",
            "Cost after 296753 iterations : Training Loss =  0.01025663238976097; Validation Loss = 0.019234498215765113\n",
            "Cost after 296754 iterations : Training Loss =  0.010256624260684799; Validation Loss = 0.01923449514160379\n",
            "Cost after 296755 iterations : Training Loss =  0.010256616131690837; Validation Loss = 0.01923449206748668\n",
            "Cost after 296756 iterations : Training Loss =  0.010256608002779187; Validation Loss = 0.019234488993413172\n",
            "Cost after 296757 iterations : Training Loss =  0.010256599873949796; Validation Loss = 0.019234485919383763\n",
            "Cost after 296758 iterations : Training Loss =  0.010256591745202671; Validation Loss = 0.019234482845398523\n",
            "Cost after 296759 iterations : Training Loss =  0.010256583616537791; Validation Loss = 0.01923447977145674\n",
            "Cost after 296760 iterations : Training Loss =  0.010256575487955359; Validation Loss = 0.019234476697559386\n",
            "Cost after 296761 iterations : Training Loss =  0.010256567359455036; Validation Loss = 0.019234473623705787\n",
            "Cost after 296762 iterations : Training Loss =  0.010256559231036924; Validation Loss = 0.01923447054989642\n",
            "Cost after 296763 iterations : Training Loss =  0.010256551102701213; Validation Loss = 0.01923446747613058\n",
            "Cost after 296764 iterations : Training Loss =  0.010256542974447674; Validation Loss = 0.019234464402408624\n",
            "Cost after 296765 iterations : Training Loss =  0.010256534846276468; Validation Loss = 0.01923446132873065\n",
            "Cost after 296766 iterations : Training Loss =  0.010256526718187553; Validation Loss = 0.01923445825509655\n",
            "Cost after 296767 iterations : Training Loss =  0.010256518590180792; Validation Loss = 0.019234455181506314\n",
            "Cost after 296768 iterations : Training Loss =  0.010256510462256426; Validation Loss = 0.01923445210795989\n",
            "Cost after 296769 iterations : Training Loss =  0.010256502334414256; Validation Loss = 0.01923444903445755\n",
            "Cost after 296770 iterations : Training Loss =  0.010256494206654306; Validation Loss = 0.019234445960998936\n",
            "Cost after 296771 iterations : Training Loss =  0.010256486078976675; Validation Loss = 0.01923444288758459\n",
            "Cost after 296772 iterations : Training Loss =  0.010256477951381338; Validation Loss = 0.019234439814213983\n",
            "Cost after 296773 iterations : Training Loss =  0.010256469823868169; Validation Loss = 0.019234436740887213\n",
            "Cost after 296774 iterations : Training Loss =  0.010256461696437322; Validation Loss = 0.019234433667604668\n",
            "Cost after 296775 iterations : Training Loss =  0.010256453569088783; Validation Loss = 0.019234430594365838\n",
            "Cost after 296776 iterations : Training Loss =  0.01025644544182232; Validation Loss = 0.019234427521171008\n",
            "Cost after 296777 iterations : Training Loss =  0.010256437314638284; Validation Loss = 0.019234424448019986\n",
            "Cost after 296778 iterations : Training Loss =  0.010256429187536465; Validation Loss = 0.019234421374913033\n",
            "Cost after 296779 iterations : Training Loss =  0.010256421060516776; Validation Loss = 0.01923441830184979\n",
            "Cost after 296780 iterations : Training Loss =  0.010256412933579498; Validation Loss = 0.019234415228830442\n",
            "Cost after 296781 iterations : Training Loss =  0.01025640480672438; Validation Loss = 0.019234412155855026\n",
            "Cost after 296782 iterations : Training Loss =  0.010256396679951527; Validation Loss = 0.019234409082923714\n",
            "Cost after 296783 iterations : Training Loss =  0.010256388553261027; Validation Loss = 0.019234406010035978\n",
            "Cost after 296784 iterations : Training Loss =  0.010256380426652632; Validation Loss = 0.019234402937192457\n",
            "Cost after 296785 iterations : Training Loss =  0.010256372300126451; Validation Loss = 0.019234399864392567\n",
            "Cost after 296786 iterations : Training Loss =  0.010256364173682643; Validation Loss = 0.01923439679163673\n",
            "Cost after 296787 iterations : Training Loss =  0.010256356047321004; Validation Loss = 0.01923439371892493\n",
            "Cost after 296788 iterations : Training Loss =  0.0102563479210417; Validation Loss = 0.019234390646256653\n",
            "Cost after 296789 iterations : Training Loss =  0.010256339794844492; Validation Loss = 0.019234387573632574\n",
            "Cost after 296790 iterations : Training Loss =  0.010256331668729635; Validation Loss = 0.0192343845010523\n",
            "Cost after 296791 iterations : Training Loss =  0.010256323542696988; Validation Loss = 0.019234381428516155\n",
            "Cost after 296792 iterations : Training Loss =  0.010256315416746516; Validation Loss = 0.019234378356023835\n",
            "Cost after 296793 iterations : Training Loss =  0.010256307290878374; Validation Loss = 0.01923437528357527\n",
            "Cost after 296794 iterations : Training Loss =  0.010256299165092422; Validation Loss = 0.019234372211170532\n",
            "Cost after 296795 iterations : Training Loss =  0.010256291039388755; Validation Loss = 0.019234369138809874\n",
            "Cost after 296796 iterations : Training Loss =  0.010256282913767291; Validation Loss = 0.019234366066493043\n",
            "Cost after 296797 iterations : Training Loss =  0.01025627478822813; Validation Loss = 0.019234362994220013\n",
            "Cost after 296798 iterations : Training Loss =  0.01025626666277104; Validation Loss = 0.019234359921990775\n",
            "Cost after 296799 iterations : Training Loss =  0.010256258537396222; Validation Loss = 0.019234356849805786\n",
            "Cost after 296800 iterations : Training Loss =  0.010256250412103658; Validation Loss = 0.0192343537776646\n",
            "Cost after 296801 iterations : Training Loss =  0.010256242286893362; Validation Loss = 0.019234350705567158\n",
            "Cost after 296802 iterations : Training Loss =  0.010256234161765332; Validation Loss = 0.019234347633513765\n",
            "Cost after 296803 iterations : Training Loss =  0.010256226036719445; Validation Loss = 0.019234344561504343\n",
            "Cost after 296804 iterations : Training Loss =  0.010256217911755808; Validation Loss = 0.019234341489538526\n",
            "Cost after 296805 iterations : Training Loss =  0.01025620978687431; Validation Loss = 0.01923433841761677\n",
            "Cost after 296806 iterations : Training Loss =  0.010256201662075139; Validation Loss = 0.019234335345738925\n",
            "Cost after 296807 iterations : Training Loss =  0.010256193537358235; Validation Loss = 0.01923433227390495\n",
            "Cost after 296808 iterations : Training Loss =  0.010256185412723433; Validation Loss = 0.019234329202114625\n",
            "Cost after 296809 iterations : Training Loss =  0.010256177288170939; Validation Loss = 0.019234326130368306\n",
            "Cost after 296810 iterations : Training Loss =  0.010256169163700672; Validation Loss = 0.01923432305866604\n",
            "Cost after 296811 iterations : Training Loss =  0.01025616103931253; Validation Loss = 0.019234319987007684\n",
            "Cost after 296812 iterations : Training Loss =  0.01025615291500665; Validation Loss = 0.019234316915392874\n",
            "Cost after 296813 iterations : Training Loss =  0.010256144790783043; Validation Loss = 0.01923431384382239\n",
            "Cost after 296814 iterations : Training Loss =  0.010256136666641553; Validation Loss = 0.019234310772295666\n",
            "Cost after 296815 iterations : Training Loss =  0.010256128542582376; Validation Loss = 0.019234307700812796\n",
            "Cost after 296816 iterations : Training Loss =  0.010256120418605314; Validation Loss = 0.01923430462937343\n",
            "Cost after 296817 iterations : Training Loss =  0.010256112294710486; Validation Loss = 0.019234301557978405\n",
            "Cost after 296818 iterations : Training Loss =  0.010256104170897949; Validation Loss = 0.019234298486627426\n",
            "Cost after 296819 iterations : Training Loss =  0.010256096047167493; Validation Loss = 0.019234295415319992\n",
            "Cost after 296820 iterations : Training Loss =  0.010256087923519364; Validation Loss = 0.01923429234405664\n",
            "Cost after 296821 iterations : Training Loss =  0.010256079799953386; Validation Loss = 0.019234289272837272\n",
            "Cost after 296822 iterations : Training Loss =  0.010256071676469605; Validation Loss = 0.019234286201661552\n",
            "Cost after 296823 iterations : Training Loss =  0.010256063553068044; Validation Loss = 0.01923428313052973\n",
            "Cost after 296824 iterations : Training Loss =  0.01025605542974874; Validation Loss = 0.019234280059441874\n",
            "Cost after 296825 iterations : Training Loss =  0.01025604730651157; Validation Loss = 0.019234276988397848\n",
            "Cost after 296826 iterations : Training Loss =  0.010256039183356559; Validation Loss = 0.019234273917397533\n",
            "Cost after 296827 iterations : Training Loss =  0.010256031060283909; Validation Loss = 0.01923427084644124\n",
            "Cost after 296828 iterations : Training Loss =  0.010256022937293307; Validation Loss = 0.01923426777552891\n",
            "Cost after 296829 iterations : Training Loss =  0.010256014814384961; Validation Loss = 0.01923426470465999\n",
            "Cost after 296830 iterations : Training Loss =  0.010256006691558772; Validation Loss = 0.019234261633835303\n",
            "Cost after 296831 iterations : Training Loss =  0.010255998568814878; Validation Loss = 0.01923425856305466\n",
            "Cost after 296832 iterations : Training Loss =  0.010255990446153069; Validation Loss = 0.01923425549231751\n",
            "Cost after 296833 iterations : Training Loss =  0.010255982323573554; Validation Loss = 0.019234252421624392\n",
            "Cost after 296834 iterations : Training Loss =  0.010255974201076181; Validation Loss = 0.019234249350975213\n",
            "Cost after 296835 iterations : Training Loss =  0.010255966078661036; Validation Loss = 0.01923424628036981\n",
            "Cost after 296836 iterations : Training Loss =  0.010255957956327983; Validation Loss = 0.019234243209808512\n",
            "Cost after 296837 iterations : Training Loss =  0.010255949834077188; Validation Loss = 0.019234240139290832\n",
            "Cost after 296838 iterations : Training Loss =  0.01025594171190866; Validation Loss = 0.0192342370688168\n",
            "Cost after 296839 iterations : Training Loss =  0.010255933589822104; Validation Loss = 0.01923423399838706\n",
            "Cost after 296840 iterations : Training Loss =  0.010255925467817907; Validation Loss = 0.019234230928000664\n",
            "Cost after 296841 iterations : Training Loss =  0.010255917345895963; Validation Loss = 0.019234227857658704\n",
            "Cost after 296842 iterations : Training Loss =  0.010255909224055981; Validation Loss = 0.01923422478736056\n",
            "Cost after 296843 iterations : Training Loss =  0.010255901102298435; Validation Loss = 0.019234221717105687\n",
            "Cost after 296844 iterations : Training Loss =  0.01025589298062282; Validation Loss = 0.01923421864689502\n",
            "Cost after 296845 iterations : Training Loss =  0.01025588485902954; Validation Loss = 0.01923421557672845\n",
            "Cost after 296846 iterations : Training Loss =  0.010255876737518335; Validation Loss = 0.019234212506605487\n",
            "Cost after 296847 iterations : Training Loss =  0.010255868616089429; Validation Loss = 0.01923420943652645\n",
            "Cost after 296848 iterations : Training Loss =  0.010255860494742598; Validation Loss = 0.019234206366491192\n",
            "Cost after 296849 iterations : Training Loss =  0.01025585237347805; Validation Loss = 0.019234203296499636\n",
            "Cost after 296850 iterations : Training Loss =  0.0102558442522956; Validation Loss = 0.01923420022655225\n",
            "Cost after 296851 iterations : Training Loss =  0.010255836131195321; Validation Loss = 0.019234197156648775\n",
            "Cost after 296852 iterations : Training Loss =  0.010255828010177256; Validation Loss = 0.01923419408678872\n",
            "Cost after 296853 iterations : Training Loss =  0.010255819889241376; Validation Loss = 0.01923419101697278\n",
            "Cost after 296854 iterations : Training Loss =  0.010255811768387578; Validation Loss = 0.01923418794720077\n",
            "Cost after 296855 iterations : Training Loss =  0.010255803647616078; Validation Loss = 0.01923418487747228\n",
            "Cost after 296856 iterations : Training Loss =  0.010255795526926726; Validation Loss = 0.01923418180778828\n",
            "Cost after 296857 iterations : Training Loss =  0.010255787406319512; Validation Loss = 0.019234178738147773\n",
            "Cost after 296858 iterations : Training Loss =  0.010255779285794448; Validation Loss = 0.019234175668551075\n",
            "Cost after 296859 iterations : Training Loss =  0.010255771165351589; Validation Loss = 0.01923417259899833\n",
            "Cost after 296860 iterations : Training Loss =  0.010255763044990825; Validation Loss = 0.019234169529489413\n",
            "Cost after 296861 iterations : Training Loss =  0.01025575492471227; Validation Loss = 0.019234166460024498\n",
            "Cost after 296862 iterations : Training Loss =  0.010255746804515865; Validation Loss = 0.019234163390603264\n",
            "Cost after 296863 iterations : Training Loss =  0.010255738684401626; Validation Loss = 0.019234160321225928\n",
            "Cost after 296864 iterations : Training Loss =  0.010255730564369583; Validation Loss = 0.01923415725189235\n",
            "Cost after 296865 iterations : Training Loss =  0.010255722444419649; Validation Loss = 0.019234154182602475\n",
            "Cost after 296866 iterations : Training Loss =  0.010255714324551872; Validation Loss = 0.019234151113356905\n",
            "Cost after 296867 iterations : Training Loss =  0.010255706204766303; Validation Loss = 0.019234148044154492\n",
            "Cost after 296868 iterations : Training Loss =  0.010255698085062928; Validation Loss = 0.019234144974996523\n",
            "Cost after 296869 iterations : Training Loss =  0.010255689965441626; Validation Loss = 0.019234141905882324\n",
            "Cost after 296870 iterations : Training Loss =  0.010255681845902469; Validation Loss = 0.019234138836811906\n",
            "Cost after 296871 iterations : Training Loss =  0.010255673726445485; Validation Loss = 0.01923413576778524\n",
            "Cost after 296872 iterations : Training Loss =  0.010255665607070634; Validation Loss = 0.019234132698802406\n",
            "Cost after 296873 iterations : Training Loss =  0.010255657487778005; Validation Loss = 0.019234129629863476\n",
            "Cost after 296874 iterations : Training Loss =  0.010255649368567439; Validation Loss = 0.019234126560968404\n",
            "Cost after 296875 iterations : Training Loss =  0.010255641249439134; Validation Loss = 0.019234123492117183\n",
            "Cost after 296876 iterations : Training Loss =  0.010255633130392912; Validation Loss = 0.019234120423309655\n",
            "Cost after 296877 iterations : Training Loss =  0.010255625011428822; Validation Loss = 0.019234117354545985\n",
            "Cost after 296878 iterations : Training Loss =  0.010255616892546894; Validation Loss = 0.019234114285826002\n",
            "Cost after 296879 iterations : Training Loss =  0.010255608773747125; Validation Loss = 0.019234111217150206\n",
            "Cost after 296880 iterations : Training Loss =  0.010255600655029475; Validation Loss = 0.019234108148518194\n",
            "Cost after 296881 iterations : Training Loss =  0.010255592536394013; Validation Loss = 0.019234105079929845\n",
            "Cost after 296882 iterations : Training Loss =  0.010255584417840644; Validation Loss = 0.019234102011385357\n",
            "Cost after 296883 iterations : Training Loss =  0.010255576299369436; Validation Loss = 0.01923409894288446\n",
            "Cost after 296884 iterations : Training Loss =  0.01025556818098039; Validation Loss = 0.01923409587442809\n",
            "Cost after 296885 iterations : Training Loss =  0.010255560062673431; Validation Loss = 0.01923409280601493\n",
            "Cost after 296886 iterations : Training Loss =  0.010255551944448665; Validation Loss = 0.019234089737645822\n",
            "Cost after 296887 iterations : Training Loss =  0.010255543826305984; Validation Loss = 0.019234086669320628\n",
            "Cost after 296888 iterations : Training Loss =  0.010255535708245465; Validation Loss = 0.019234083601038853\n",
            "Cost after 296889 iterations : Training Loss =  0.01025552759026708; Validation Loss = 0.019234080532801474\n",
            "Cost after 296890 iterations : Training Loss =  0.010255519472370857; Validation Loss = 0.019234077464607435\n",
            "Cost after 296891 iterations : Training Loss =  0.010255511354556683; Validation Loss = 0.019234074396457242\n",
            "Cost after 296892 iterations : Training Loss =  0.01025550323682475; Validation Loss = 0.01923407132835127\n",
            "Cost after 296893 iterations : Training Loss =  0.010255495119174849; Validation Loss = 0.019234068260288936\n",
            "Cost after 296894 iterations : Training Loss =  0.010255487001607127; Validation Loss = 0.019234065192270482\n",
            "Cost after 296895 iterations : Training Loss =  0.010255478884121544; Validation Loss = 0.01923406212429555\n",
            "Cost after 296896 iterations : Training Loss =  0.010255470766718023; Validation Loss = 0.019234059056364577\n",
            "Cost after 296897 iterations : Training Loss =  0.010255462649396719; Validation Loss = 0.019234055988477505\n",
            "Cost after 296898 iterations : Training Loss =  0.010255454532157478; Validation Loss = 0.01923405292063439\n",
            "Cost after 296899 iterations : Training Loss =  0.010255446415000343; Validation Loss = 0.0192340498528348\n",
            "Cost after 296900 iterations : Training Loss =  0.01025543829792545; Validation Loss = 0.0192340467850793\n",
            "Cost after 296901 iterations : Training Loss =  0.010255430180932555; Validation Loss = 0.01923404371736758\n",
            "Cost after 296902 iterations : Training Loss =  0.010255422064021864; Validation Loss = 0.01923404064969955\n",
            "Cost after 296903 iterations : Training Loss =  0.010255413947193181; Validation Loss = 0.019234037582075238\n",
            "Cost after 296904 iterations : Training Loss =  0.010255405830446696; Validation Loss = 0.019234034514494902\n",
            "Cost after 296905 iterations : Training Loss =  0.01025539771378231; Validation Loss = 0.019234031446958424\n",
            "Cost after 296906 iterations : Training Loss =  0.010255389597200022; Validation Loss = 0.019234028379465796\n",
            "Cost after 296907 iterations : Training Loss =  0.010255381480699929; Validation Loss = 0.019234025312016904\n",
            "Cost after 296908 iterations : Training Loss =  0.010255373364281891; Validation Loss = 0.01923402224461166\n",
            "Cost after 296909 iterations : Training Loss =  0.01025536524794596; Validation Loss = 0.019234019177250566\n",
            "Cost after 296910 iterations : Training Loss =  0.010255357131692195; Validation Loss = 0.0192340161099333\n",
            "Cost after 296911 iterations : Training Loss =  0.010255349015520467; Validation Loss = 0.01923401304265958\n",
            "Cost after 296912 iterations : Training Loss =  0.01025534089943087; Validation Loss = 0.019234009975430046\n",
            "Cost after 296913 iterations : Training Loss =  0.010255332783423384; Validation Loss = 0.01923400690824417\n",
            "Cost after 296914 iterations : Training Loss =  0.010255324667498083; Validation Loss = 0.019234003841101858\n",
            "Cost after 296915 iterations : Training Loss =  0.010255316551654767; Validation Loss = 0.01923400077400352\n",
            "Cost after 296916 iterations : Training Loss =  0.010255308435893567; Validation Loss = 0.01923399770694894\n",
            "Cost after 296917 iterations : Training Loss =  0.010255300320214577; Validation Loss = 0.01923399463993817\n",
            "Cost after 296918 iterations : Training Loss =  0.010255292204617657; Validation Loss = 0.019233991572970992\n",
            "Cost after 296919 iterations : Training Loss =  0.010255284089102692; Validation Loss = 0.019233988506048058\n",
            "Cost after 296920 iterations : Training Loss =  0.01025527597367001; Validation Loss = 0.01923398543916856\n",
            "Cost after 296921 iterations : Training Loss =  0.010255267858319261; Validation Loss = 0.01923398237233285\n",
            "Cost after 296922 iterations : Training Loss =  0.010255259743050828; Validation Loss = 0.019233979305541077\n",
            "Cost after 296923 iterations : Training Loss =  0.01025525162786432; Validation Loss = 0.01923397623879291\n",
            "Cost after 296924 iterations : Training Loss =  0.010255243512760071; Validation Loss = 0.019233973172088595\n",
            "Cost after 296925 iterations : Training Loss =  0.010255235397737825; Validation Loss = 0.019233970105428207\n",
            "Cost after 296926 iterations : Training Loss =  0.010255227282797562; Validation Loss = 0.01923396703881177\n",
            "Cost after 296927 iterations : Training Loss =  0.01025521916793958; Validation Loss = 0.019233963972238997\n",
            "Cost after 296928 iterations : Training Loss =  0.010255211053163541; Validation Loss = 0.019233960905709665\n",
            "Cost after 296929 iterations : Training Loss =  0.010255202938469671; Validation Loss = 0.019233957839224592\n",
            "Cost after 296930 iterations : Training Loss =  0.0102551948238579; Validation Loss = 0.019233954772783057\n",
            "Cost after 296931 iterations : Training Loss =  0.010255186709328231; Validation Loss = 0.019233951706385487\n",
            "Cost after 296932 iterations : Training Loss =  0.010255178594880525; Validation Loss = 0.01923394864003158\n",
            "Cost after 296933 iterations : Training Loss =  0.010255170480515033; Validation Loss = 0.01923394557372184\n",
            "Cost after 296934 iterations : Training Loss =  0.010255162366231642; Validation Loss = 0.019233942507455273\n",
            "Cost after 296935 iterations : Training Loss =  0.010255154252030241; Validation Loss = 0.01923393944123307\n",
            "Cost after 296936 iterations : Training Loss =  0.010255146137910967; Validation Loss = 0.01923393637505415\n",
            "Cost after 296937 iterations : Training Loss =  0.010255138023873771; Validation Loss = 0.019233933308919288\n",
            "Cost after 296938 iterations : Training Loss =  0.0102551299099187; Validation Loss = 0.019233930242828316\n",
            "Cost after 296939 iterations : Training Loss =  0.010255121796045633; Validation Loss = 0.019233927176781115\n",
            "Cost after 296940 iterations : Training Loss =  0.010255113682254737; Validation Loss = 0.01923392411077719\n",
            "Cost after 296941 iterations : Training Loss =  0.010255105568545793; Validation Loss = 0.01923392104481767\n",
            "Cost after 296942 iterations : Training Loss =  0.010255097454919117; Validation Loss = 0.019233917978901338\n",
            "Cost after 296943 iterations : Training Loss =  0.010255089341374386; Validation Loss = 0.01923391491302941\n",
            "Cost after 296944 iterations : Training Loss =  0.010255081227911718; Validation Loss = 0.019233911847201038\n",
            "Cost after 296945 iterations : Training Loss =  0.010255073114531115; Validation Loss = 0.01923390878141654\n",
            "Cost after 296946 iterations : Training Loss =  0.010255065001232685; Validation Loss = 0.019233905715675723\n",
            "Cost after 296947 iterations : Training Loss =  0.010255056888016274; Validation Loss = 0.019233902649978617\n",
            "Cost after 296948 iterations : Training Loss =  0.010255048774881917; Validation Loss = 0.019233899584325396\n",
            "Cost after 296949 iterations : Training Loss =  0.010255040661829647; Validation Loss = 0.019233896518715682\n",
            "Cost after 296950 iterations : Training Loss =  0.010255032548859537; Validation Loss = 0.019233893453150127\n",
            "Cost after 296951 iterations : Training Loss =  0.010255024435971342; Validation Loss = 0.019233890387628114\n",
            "Cost after 296952 iterations : Training Loss =  0.01025501632316526; Validation Loss = 0.019233887322150235\n",
            "Cost after 296953 iterations : Training Loss =  0.010255008210441214; Validation Loss = 0.01923388425671581\n",
            "Cost after 296954 iterations : Training Loss =  0.01025500009779933; Validation Loss = 0.01923388119132509\n",
            "Cost after 296955 iterations : Training Loss =  0.010254991985239477; Validation Loss = 0.01923387812597849\n",
            "Cost after 296956 iterations : Training Loss =  0.010254983872761716; Validation Loss = 0.019233875060675552\n",
            "Cost after 296957 iterations : Training Loss =  0.010254975760365857; Validation Loss = 0.01923387199541635\n",
            "Cost after 296958 iterations : Training Loss =  0.010254967648052189; Validation Loss = 0.019233868930200997\n",
            "Cost after 296959 iterations : Training Loss =  0.010254959535820587; Validation Loss = 0.01923386586502935\n",
            "Cost after 296960 iterations : Training Loss =  0.0102549514236711; Validation Loss = 0.019233862799901413\n",
            "Cost after 296961 iterations : Training Loss =  0.010254943311603497; Validation Loss = 0.019233859734817515\n",
            "Cost after 296962 iterations : Training Loss =  0.010254935199618107; Validation Loss = 0.01923385666977707\n",
            "Cost after 296963 iterations : Training Loss =  0.01025492708771464; Validation Loss = 0.019233853604780516\n",
            "Cost after 296964 iterations : Training Loss =  0.010254918975893413; Validation Loss = 0.01923385053982748\n",
            "Cost after 296965 iterations : Training Loss =  0.010254910864154117; Validation Loss = 0.019233847474918568\n",
            "Cost after 296966 iterations : Training Loss =  0.010254902752496842; Validation Loss = 0.019233844410053356\n",
            "Cost after 296967 iterations : Training Loss =  0.010254894640921657; Validation Loss = 0.019233841345231758\n",
            "Cost after 296968 iterations : Training Loss =  0.010254886529428568; Validation Loss = 0.019233838280453833\n",
            "Cost after 296969 iterations : Training Loss =  0.010254878418017477; Validation Loss = 0.019233835215719856\n",
            "Cost after 296970 iterations : Training Loss =  0.010254870306688401; Validation Loss = 0.019233832151029372\n",
            "Cost after 296971 iterations : Training Loss =  0.010254862195441387; Validation Loss = 0.019233829086382853\n",
            "Cost after 296972 iterations : Training Loss =  0.010254854084276439; Validation Loss = 0.01923382602178008\n",
            "Cost after 296973 iterations : Training Loss =  0.01025484597319358; Validation Loss = 0.019233822957221137\n",
            "Cost after 296974 iterations : Training Loss =  0.010254837862192685; Validation Loss = 0.019233819892705766\n",
            "Cost after 296975 iterations : Training Loss =  0.010254829751273922; Validation Loss = 0.019233816828234253\n",
            "Cost after 296976 iterations : Training Loss =  0.010254821640437093; Validation Loss = 0.019233813763806847\n",
            "Cost after 296977 iterations : Training Loss =  0.01025481352968236; Validation Loss = 0.019233810699422555\n",
            "Cost after 296978 iterations : Training Loss =  0.010254805419009715; Validation Loss = 0.019233807635082898\n",
            "Cost after 296979 iterations : Training Loss =  0.010254797308419102; Validation Loss = 0.0192338045707864\n",
            "Cost after 296980 iterations : Training Loss =  0.010254789197910485; Validation Loss = 0.019233801506533855\n",
            "Cost after 296981 iterations : Training Loss =  0.01025478108748385; Validation Loss = 0.019233798442324892\n",
            "Cost after 296982 iterations : Training Loss =  0.010254772977139304; Validation Loss = 0.019233795378159727\n",
            "Cost after 296983 iterations : Training Loss =  0.01025476486687677; Validation Loss = 0.01923379231403845\n",
            "Cost after 296984 iterations : Training Loss =  0.010254756756696307; Validation Loss = 0.01923378924996118\n",
            "Cost after 296985 iterations : Training Loss =  0.010254748646597919; Validation Loss = 0.01923378618592707\n",
            "Cost after 296986 iterations : Training Loss =  0.01025474053658147; Validation Loss = 0.019233783121936736\n",
            "Cost after 296987 iterations : Training Loss =  0.010254732426647057; Validation Loss = 0.019233780057990247\n",
            "Cost after 296988 iterations : Training Loss =  0.01025472431679473; Validation Loss = 0.019233776994087758\n",
            "Cost after 296989 iterations : Training Loss =  0.01025471620702442; Validation Loss = 0.019233773930228914\n",
            "Cost after 296990 iterations : Training Loss =  0.010254708097336107; Validation Loss = 0.019233770866414018\n",
            "Cost after 296991 iterations : Training Loss =  0.010254699987729857; Validation Loss = 0.019233767802642392\n",
            "Cost after 296992 iterations : Training Loss =  0.010254691878205588; Validation Loss = 0.019233764738914777\n",
            "Cost after 296993 iterations : Training Loss =  0.010254683768763354; Validation Loss = 0.019233761675230814\n",
            "Cost after 296994 iterations : Training Loss =  0.010254675659403092; Validation Loss = 0.01923375861159096\n",
            "Cost after 296995 iterations : Training Loss =  0.010254667550124977; Validation Loss = 0.019233755547994374\n",
            "Cost after 296996 iterations : Training Loss =  0.010254659440928805; Validation Loss = 0.01923375248444191\n",
            "Cost after 296997 iterations : Training Loss =  0.010254651331814631; Validation Loss = 0.019233749420932894\n",
            "Cost after 296998 iterations : Training Loss =  0.010254643222782522; Validation Loss = 0.019233746357467624\n",
            "Cost after 296999 iterations : Training Loss =  0.010254635113832391; Validation Loss = 0.019233743294046306\n",
            "Cost after 297000 iterations : Training Loss =  0.010254627004964356; Validation Loss = 0.019233740230668348\n",
            "Cost after 297001 iterations : Training Loss =  0.010254618896178304; Validation Loss = 0.0192337371673347\n",
            "Cost after 297002 iterations : Training Loss =  0.010254610787474163; Validation Loss = 0.0192337341040445\n",
            "Cost after 297003 iterations : Training Loss =  0.010254602678852155; Validation Loss = 0.01923373104079821\n",
            "Cost after 297004 iterations : Training Loss =  0.010254594570312038; Validation Loss = 0.019233727977595462\n",
            "Cost after 297005 iterations : Training Loss =  0.01025458646185405; Validation Loss = 0.019233724914436492\n",
            "Cost after 297006 iterations : Training Loss =  0.010254578353478096; Validation Loss = 0.01923372185132135\n",
            "Cost after 297007 iterations : Training Loss =  0.010254570245184056; Validation Loss = 0.019233718788249816\n",
            "Cost after 297008 iterations : Training Loss =  0.010254562136972054; Validation Loss = 0.019233715725221867\n",
            "Cost after 297009 iterations : Training Loss =  0.010254554028841994; Validation Loss = 0.01923371266223777\n",
            "Cost after 297010 iterations : Training Loss =  0.010254545920794021; Validation Loss = 0.019233709599297162\n",
            "Cost after 297011 iterations : Training Loss =  0.01025453781282808; Validation Loss = 0.01923370653640077\n",
            "Cost after 297012 iterations : Training Loss =  0.010254529704944105; Validation Loss = 0.019233703473548042\n",
            "Cost after 297013 iterations : Training Loss =  0.010254521597142083; Validation Loss = 0.019233700410739244\n",
            "Cost after 297014 iterations : Training Loss =  0.010254513489422142; Validation Loss = 0.019233697347973665\n",
            "Cost after 297015 iterations : Training Loss =  0.010254505381784159; Validation Loss = 0.019233694285251884\n",
            "Cost after 297016 iterations : Training Loss =  0.010254497274228185; Validation Loss = 0.019233691222573932\n",
            "Cost after 297017 iterations : Training Loss =  0.010254489166754198; Validation Loss = 0.019233688159939845\n",
            "Cost after 297018 iterations : Training Loss =  0.010254481059362233; Validation Loss = 0.01923368509734922\n",
            "Cost after 297019 iterations : Training Loss =  0.01025447295205227; Validation Loss = 0.01923368203480218\n",
            "Cost after 297020 iterations : Training Loss =  0.010254464844824165; Validation Loss = 0.01923367897229949\n",
            "Cost after 297021 iterations : Training Loss =  0.010254456737678228; Validation Loss = 0.01923367590984017\n",
            "Cost after 297022 iterations : Training Loss =  0.010254448630614211; Validation Loss = 0.01923367284742459\n",
            "Cost after 297023 iterations : Training Loss =  0.010254440523632237; Validation Loss = 0.019233669785052653\n",
            "Cost after 297024 iterations : Training Loss =  0.010254432416732136; Validation Loss = 0.01923366672272471\n",
            "Cost after 297025 iterations : Training Loss =  0.010254424309914075; Validation Loss = 0.01923366366044033\n",
            "Cost after 297026 iterations : Training Loss =  0.01025441620317812; Validation Loss = 0.01923366059819949\n",
            "Cost after 297027 iterations : Training Loss =  0.010254408096524009; Validation Loss = 0.01923365753600286\n",
            "Cost after 297028 iterations : Training Loss =  0.010254399989951956; Validation Loss = 0.019233654473849646\n",
            "Cost after 297029 iterations : Training Loss =  0.010254391883461873; Validation Loss = 0.0192336514117403\n",
            "Cost after 297030 iterations : Training Loss =  0.010254383777053733; Validation Loss = 0.019233648349674733\n",
            "Cost after 297031 iterations : Training Loss =  0.010254375670727634; Validation Loss = 0.01923364528765252\n",
            "Cost after 297032 iterations : Training Loss =  0.010254367564483446; Validation Loss = 0.019233642225674245\n",
            "Cost after 297033 iterations : Training Loss =  0.010254359458321313; Validation Loss = 0.01923363916373966\n",
            "Cost after 297034 iterations : Training Loss =  0.010254351352241097; Validation Loss = 0.01923363610184905\n",
            "Cost after 297035 iterations : Training Loss =  0.010254343246242841; Validation Loss = 0.019233633040001898\n",
            "Cost after 297036 iterations : Training Loss =  0.010254335140326636; Validation Loss = 0.019233629978198167\n",
            "Cost after 297037 iterations : Training Loss =  0.010254327034492472; Validation Loss = 0.019233626916438446\n",
            "Cost after 297038 iterations : Training Loss =  0.010254318928740142; Validation Loss = 0.019233623854722503\n",
            "Cost after 297039 iterations : Training Loss =  0.010254310823069825; Validation Loss = 0.01923362079305038\n",
            "Cost after 297040 iterations : Training Loss =  0.010254302717481557; Validation Loss = 0.019233617731421765\n",
            "Cost after 297041 iterations : Training Loss =  0.01025429461197518; Validation Loss = 0.01923361466983679\n",
            "Cost after 297042 iterations : Training Loss =  0.010254286506550882; Validation Loss = 0.019233611608295526\n",
            "Cost after 297043 iterations : Training Loss =  0.010254278401208418; Validation Loss = 0.01923360854679781\n",
            "Cost after 297044 iterations : Training Loss =  0.01025427029594798; Validation Loss = 0.019233605485344057\n",
            "Cost after 297045 iterations : Training Loss =  0.010254262190769494; Validation Loss = 0.019233602423934015\n",
            "Cost after 297046 iterations : Training Loss =  0.010254254085673016; Validation Loss = 0.019233599362567598\n",
            "Cost after 297047 iterations : Training Loss =  0.010254245980658437; Validation Loss = 0.01923359630124507\n",
            "Cost after 297048 iterations : Training Loss =  0.010254237875725823; Validation Loss = 0.01923359323996609\n",
            "Cost after 297049 iterations : Training Loss =  0.010254229770875287; Validation Loss = 0.019233590178730536\n",
            "Cost after 297050 iterations : Training Loss =  0.010254221666106596; Validation Loss = 0.019233587117538944\n",
            "Cost after 297051 iterations : Training Loss =  0.01025421356141997; Validation Loss = 0.01923358405639118\n",
            "Cost after 297052 iterations : Training Loss =  0.010254205456815166; Validation Loss = 0.019233580995287297\n",
            "Cost after 297053 iterations : Training Loss =  0.010254197352292374; Validation Loss = 0.019233577934226503\n",
            "Cost after 297054 iterations : Training Loss =  0.010254189247851589; Validation Loss = 0.01923357487320981\n",
            "Cost after 297055 iterations : Training Loss =  0.010254181143492736; Validation Loss = 0.01923357181223643\n",
            "Cost after 297056 iterations : Training Loss =  0.010254173039215875; Validation Loss = 0.01923356875130709\n",
            "Cost after 297057 iterations : Training Loss =  0.010254164935020878; Validation Loss = 0.019233565690421513\n",
            "Cost after 297058 iterations : Training Loss =  0.010254156830907975; Validation Loss = 0.019233562629579586\n",
            "Cost after 297059 iterations : Training Loss =  0.010254148726876935; Validation Loss = 0.019233559568781047\n",
            "Cost after 297060 iterations : Training Loss =  0.010254140622927843; Validation Loss = 0.019233556508026307\n",
            "Cost after 297061 iterations : Training Loss =  0.010254132519060651; Validation Loss = 0.019233553447315446\n",
            "Cost after 297062 iterations : Training Loss =  0.010254124415275508; Validation Loss = 0.01923355038664813\n",
            "Cost after 297063 iterations : Training Loss =  0.010254116311572244; Validation Loss = 0.019233547326024406\n",
            "Cost after 297064 iterations : Training Loss =  0.010254108207951026; Validation Loss = 0.019233544265444363\n",
            "Cost after 297065 iterations : Training Loss =  0.010254100104411646; Validation Loss = 0.019233541204908385\n",
            "Cost after 297066 iterations : Training Loss =  0.010254092000954219; Validation Loss = 0.019233538144415942\n",
            "Cost after 297067 iterations : Training Loss =  0.010254083897578767; Validation Loss = 0.01923353508396717\n",
            "Cost after 297068 iterations : Training Loss =  0.01025407579428527; Validation Loss = 0.019233532023562167\n",
            "Cost after 297069 iterations : Training Loss =  0.010254067691073709; Validation Loss = 0.0192335289632005\n",
            "Cost after 297070 iterations : Training Loss =  0.010254059587944142; Validation Loss = 0.019233525902882832\n",
            "Cost after 297071 iterations : Training Loss =  0.010254051484896434; Validation Loss = 0.01923352284260875\n",
            "Cost after 297072 iterations : Training Loss =  0.010254043381930733; Validation Loss = 0.019233519782378476\n",
            "Cost after 297073 iterations : Training Loss =  0.010254035279046869; Validation Loss = 0.019233516722191847\n",
            "Cost after 297074 iterations : Training Loss =  0.010254027176245057; Validation Loss = 0.01923351366204873\n",
            "Cost after 297075 iterations : Training Loss =  0.01025401907352519; Validation Loss = 0.01923351060194976\n",
            "Cost after 297076 iterations : Training Loss =  0.010254010970887044; Validation Loss = 0.019233507541894176\n",
            "Cost after 297077 iterations : Training Loss =  0.01025400286833102; Validation Loss = 0.01923350448188201\n",
            "Cost after 297078 iterations : Training Loss =  0.010253994765856918; Validation Loss = 0.01923350142191384\n",
            "Cost after 297079 iterations : Training Loss =  0.010253986663464701; Validation Loss = 0.019233498361989544\n",
            "Cost after 297080 iterations : Training Loss =  0.010253978561154438; Validation Loss = 0.019233495302109004\n",
            "Cost after 297081 iterations : Training Loss =  0.010253970458926091; Validation Loss = 0.019233492242271655\n",
            "Cost after 297082 iterations : Training Loss =  0.01025396235677974; Validation Loss = 0.019233489182478244\n",
            "Cost after 297083 iterations : Training Loss =  0.010253954254715265; Validation Loss = 0.01923348612272825\n",
            "Cost after 297084 iterations : Training Loss =  0.010253946152732703; Validation Loss = 0.019233483063022024\n",
            "Cost after 297085 iterations : Training Loss =  0.010253938050832105; Validation Loss = 0.019233480003359647\n",
            "Cost after 297086 iterations : Training Loss =  0.010253929949013278; Validation Loss = 0.019233476943740786\n",
            "Cost after 297087 iterations : Training Loss =  0.01025392184727662; Validation Loss = 0.01923347388416569\n",
            "Cost after 297088 iterations : Training Loss =  0.010253913745621724; Validation Loss = 0.019233470824633975\n",
            "Cost after 297089 iterations : Training Loss =  0.010253905644048763; Validation Loss = 0.019233467765146398\n",
            "Cost after 297090 iterations : Training Loss =  0.010253897542557788; Validation Loss = 0.019233464705702454\n",
            "Cost after 297091 iterations : Training Loss =  0.010253889441148672; Validation Loss = 0.019233461646301967\n",
            "Cost after 297092 iterations : Training Loss =  0.010253881339821434; Validation Loss = 0.019233458586945033\n",
            "Cost after 297093 iterations : Training Loss =  0.010253873238576284; Validation Loss = 0.019233455527632115\n",
            "Cost after 297094 iterations : Training Loss =  0.010253865137412877; Validation Loss = 0.019233452468362867\n",
            "Cost after 297095 iterations : Training Loss =  0.010253857036331455; Validation Loss = 0.019233449409136988\n",
            "Cost after 297096 iterations : Training Loss =  0.010253848935331948; Validation Loss = 0.019233446349954525\n",
            "Cost after 297097 iterations : Training Loss =  0.010253840834414307; Validation Loss = 0.019233443290816284\n",
            "Cost after 297098 iterations : Training Loss =  0.010253832733578674; Validation Loss = 0.019233440231721313\n",
            "Cost after 297099 iterations : Training Loss =  0.010253824632824883; Validation Loss = 0.019233437172670353\n",
            "Cost after 297100 iterations : Training Loss =  0.010253816532152977; Validation Loss = 0.0192334341136629\n",
            "Cost after 297101 iterations : Training Loss =  0.01025380843156298; Validation Loss = 0.019233431054699074\n",
            "Cost after 297102 iterations : Training Loss =  0.010253800331054967; Validation Loss = 0.019233427995779068\n",
            "Cost after 297103 iterations : Training Loss =  0.01025379223062879; Validation Loss = 0.019233424936902714\n",
            "Cost after 297104 iterations : Training Loss =  0.010253784130284582; Validation Loss = 0.019233421878069805\n",
            "Cost after 297105 iterations : Training Loss =  0.010253776030022215; Validation Loss = 0.01923341881928063\n",
            "Cost after 297106 iterations : Training Loss =  0.010253767929841704; Validation Loss = 0.01923341576053521\n",
            "Cost after 297107 iterations : Training Loss =  0.010253759829743208; Validation Loss = 0.019233412701833243\n",
            "Cost after 297108 iterations : Training Loss =  0.01025375172972662; Validation Loss = 0.019233409643174996\n",
            "Cost after 297109 iterations : Training Loss =  0.010253743629791836; Validation Loss = 0.019233406584560436\n",
            "Cost after 297110 iterations : Training Loss =  0.010253735529939075; Validation Loss = 0.019233403525989352\n",
            "Cost after 297111 iterations : Training Loss =  0.010253727430168032; Validation Loss = 0.019233400467462254\n",
            "Cost after 297112 iterations : Training Loss =  0.010253719330479054; Validation Loss = 0.01923339740897882\n",
            "Cost after 297113 iterations : Training Loss =  0.010253711230871881; Validation Loss = 0.019233394350538988\n",
            "Cost after 297114 iterations : Training Loss =  0.010253703131346645; Validation Loss = 0.019233391292142802\n",
            "Cost after 297115 iterations : Training Loss =  0.010253695031903235; Validation Loss = 0.019233388233790602\n",
            "Cost after 297116 iterations : Training Loss =  0.010253686932541764; Validation Loss = 0.019233385175481656\n",
            "Cost after 297117 iterations : Training Loss =  0.010253678833262305; Validation Loss = 0.01923338211721637\n",
            "Cost after 297118 iterations : Training Loss =  0.010253670734064553; Validation Loss = 0.019233379058994988\n",
            "Cost after 297119 iterations : Training Loss =  0.010253662634948719; Validation Loss = 0.01923337600081672\n",
            "Cost after 297120 iterations : Training Loss =  0.010253654535914852; Validation Loss = 0.019233372942682552\n",
            "Cost after 297121 iterations : Training Loss =  0.01025364643696284; Validation Loss = 0.01923336988459172\n",
            "Cost after 297122 iterations : Training Loss =  0.010253638338092695; Validation Loss = 0.01923336682654454\n",
            "Cost after 297123 iterations : Training Loss =  0.010253630239304399; Validation Loss = 0.019233363768540977\n",
            "Cost after 297124 iterations : Training Loss =  0.01025362214059802; Validation Loss = 0.01923336071058154\n",
            "Cost after 297125 iterations : Training Loss =  0.010253614041973527; Validation Loss = 0.019233357652665595\n",
            "Cost after 297126 iterations : Training Loss =  0.01025360594343099; Validation Loss = 0.019233354594792834\n",
            "Cost after 297127 iterations : Training Loss =  0.010253597844970245; Validation Loss = 0.019233351536964233\n",
            "Cost after 297128 iterations : Training Loss =  0.010253589746591421; Validation Loss = 0.019233348479178982\n",
            "Cost after 297129 iterations : Training Loss =  0.010253581648294456; Validation Loss = 0.01923334542143763\n",
            "Cost after 297130 iterations : Training Loss =  0.010253573550079368; Validation Loss = 0.01923334236373998\n",
            "Cost after 297131 iterations : Training Loss =  0.010253565451946112; Validation Loss = 0.019233339306085596\n",
            "Cost after 297132 iterations : Training Loss =  0.010253557353894794; Validation Loss = 0.019233336248474946\n",
            "Cost after 297133 iterations : Training Loss =  0.010253549255925403; Validation Loss = 0.019233333190908027\n",
            "Cost after 297134 iterations : Training Loss =  0.01025354115803784; Validation Loss = 0.01923333013338503\n",
            "Cost after 297135 iterations : Training Loss =  0.010253533060232065; Validation Loss = 0.019233327075905245\n",
            "Cost after 297136 iterations : Training Loss =  0.010253524962508172; Validation Loss = 0.01923332401846906\n",
            "Cost after 297137 iterations : Training Loss =  0.010253516864866264; Validation Loss = 0.01923332096107671\n",
            "Cost after 297138 iterations : Training Loss =  0.010253508767306182; Validation Loss = 0.019233317903728283\n",
            "Cost after 297139 iterations : Training Loss =  0.010253500669827904; Validation Loss = 0.019233314846423172\n",
            "Cost after 297140 iterations : Training Loss =  0.010253492572431463; Validation Loss = 0.019233311789161666\n",
            "Cost after 297141 iterations : Training Loss =  0.010253484475116993; Validation Loss = 0.019233308731944038\n",
            "Cost after 297142 iterations : Training Loss =  0.010253476377884423; Validation Loss = 0.019233305674769757\n",
            "Cost after 297143 iterations : Training Loss =  0.010253468280733615; Validation Loss = 0.019233302617639045\n",
            "Cost after 297144 iterations : Training Loss =  0.010253460183664654; Validation Loss = 0.019233299560552135\n",
            "Cost after 297145 iterations : Training Loss =  0.010253452086677583; Validation Loss = 0.019233296503508777\n",
            "Cost after 297146 iterations : Training Loss =  0.010253443989772444; Validation Loss = 0.019233293446509175\n",
            "Cost after 297147 iterations : Training Loss =  0.010253435892949126; Validation Loss = 0.019233290389553237\n",
            "Cost after 297148 iterations : Training Loss =  0.010253427796207715; Validation Loss = 0.019233287332640618\n",
            "Cost after 297149 iterations : Training Loss =  0.010253419699548063; Validation Loss = 0.019233284275771693\n",
            "Cost after 297150 iterations : Training Loss =  0.010253411602970197; Validation Loss = 0.01923328121894681\n",
            "Cost after 297151 iterations : Training Loss =  0.010253403506474313; Validation Loss = 0.0192332781621654\n",
            "Cost after 297152 iterations : Training Loss =  0.01025339541006035; Validation Loss = 0.019233275105427626\n",
            "Cost after 297153 iterations : Training Loss =  0.010253387313728124; Validation Loss = 0.019233272048733173\n",
            "Cost after 297154 iterations : Training Loss =  0.010253379217477834; Validation Loss = 0.01923326899208247\n",
            "Cost after 297155 iterations : Training Loss =  0.010253371121309312; Validation Loss = 0.019233265935475523\n",
            "Cost after 297156 iterations : Training Loss =  0.010253363025222695; Validation Loss = 0.0192332628789116\n",
            "Cost after 297157 iterations : Training Loss =  0.010253354929217827; Validation Loss = 0.01923325982239221\n",
            "Cost after 297158 iterations : Training Loss =  0.01025334683329484; Validation Loss = 0.01923325676591583\n",
            "Cost after 297159 iterations : Training Loss =  0.010253338737453786; Validation Loss = 0.019233253709483297\n",
            "Cost after 297160 iterations : Training Loss =  0.010253330641694487; Validation Loss = 0.019233250653094395\n",
            "Cost after 297161 iterations : Training Loss =  0.010253322546017095; Validation Loss = 0.01923324759674901\n",
            "Cost after 297162 iterations : Training Loss =  0.010253314450421564; Validation Loss = 0.019233244540447086\n",
            "Cost after 297163 iterations : Training Loss =  0.010253306354907833; Validation Loss = 0.01923324148418916\n",
            "Cost after 297164 iterations : Training Loss =  0.010253298259475928; Validation Loss = 0.01923323842797448\n",
            "Cost after 297165 iterations : Training Loss =  0.010253290164125935; Validation Loss = 0.019233235371803773\n",
            "Cost after 297166 iterations : Training Loss =  0.010253282068857719; Validation Loss = 0.01923323231567649\n",
            "Cost after 297167 iterations : Training Loss =  0.010253273973671338; Validation Loss = 0.01923322925959295\n",
            "Cost after 297168 iterations : Training Loss =  0.010253265878566816; Validation Loss = 0.019233226203552837\n",
            "Cost after 297169 iterations : Training Loss =  0.01025325778354411; Validation Loss = 0.019233223147556617\n",
            "Cost after 297170 iterations : Training Loss =  0.010253249688603186; Validation Loss = 0.01923322009160351\n",
            "Cost after 297171 iterations : Training Loss =  0.010253241593744225; Validation Loss = 0.019233217035694222\n",
            "Cost after 297172 iterations : Training Loss =  0.010253233498967033; Validation Loss = 0.019233213979828526\n",
            "Cost after 297173 iterations : Training Loss =  0.010253225404271647; Validation Loss = 0.019233210924006514\n",
            "Cost after 297174 iterations : Training Loss =  0.010253217309658143; Validation Loss = 0.01923320786822832\n",
            "Cost after 297175 iterations : Training Loss =  0.010253209215126487; Validation Loss = 0.019233204812493514\n",
            "Cost after 297176 iterations : Training Loss =  0.01025320112067657; Validation Loss = 0.01923320175680214\n",
            "Cost after 297177 iterations : Training Loss =  0.010253193026308479; Validation Loss = 0.019233198701154728\n",
            "Cost after 297178 iterations : Training Loss =  0.010253184932022331; Validation Loss = 0.019233195645550618\n",
            "Cost after 297179 iterations : Training Loss =  0.010253176837817922; Validation Loss = 0.01923319258999022\n",
            "Cost after 297180 iterations : Training Loss =  0.010253168743695324; Validation Loss = 0.01923318953447349\n",
            "Cost after 297181 iterations : Training Loss =  0.010253160649654576; Validation Loss = 0.019233186479000686\n",
            "Cost after 297182 iterations : Training Loss =  0.010253152555695685; Validation Loss = 0.019233183423570846\n",
            "Cost after 297183 iterations : Training Loss =  0.010253144461818542; Validation Loss = 0.01923318036818493\n",
            "Cost after 297184 iterations : Training Loss =  0.010253136368023308; Validation Loss = 0.019233177312842468\n",
            "Cost after 297185 iterations : Training Loss =  0.01025312827430981; Validation Loss = 0.019233174257543475\n",
            "Cost after 297186 iterations : Training Loss =  0.010253120180678157; Validation Loss = 0.019233171202288468\n",
            "Cost after 297187 iterations : Training Loss =  0.010253112087128327; Validation Loss = 0.01923316814707701\n",
            "Cost after 297188 iterations : Training Loss =  0.010253103993660305; Validation Loss = 0.0192331650919089\n",
            "Cost after 297189 iterations : Training Loss =  0.010253095900274085; Validation Loss = 0.01923316203678471\n",
            "Cost after 297190 iterations : Training Loss =  0.01025308780696977; Validation Loss = 0.019233158981703753\n",
            "Cost after 297191 iterations : Training Loss =  0.010253079713747133; Validation Loss = 0.01923315592666678\n",
            "Cost after 297192 iterations : Training Loss =  0.01025307162060641; Validation Loss = 0.019233152871673324\n",
            "Cost after 297193 iterations : Training Loss =  0.010253063527547417; Validation Loss = 0.019233149816723322\n",
            "Cost after 297194 iterations : Training Loss =  0.010253055434570377; Validation Loss = 0.019233146761816804\n",
            "Cost after 297195 iterations : Training Loss =  0.010253047341675037; Validation Loss = 0.019233143706953965\n",
            "Cost after 297196 iterations : Training Loss =  0.010253039248861527; Validation Loss = 0.019233140652134537\n",
            "Cost after 297197 iterations : Training Loss =  0.01025303115612978; Validation Loss = 0.019233137597358882\n",
            "Cost after 297198 iterations : Training Loss =  0.010253023063479891; Validation Loss = 0.019233134542626994\n",
            "Cost after 297199 iterations : Training Loss =  0.010253014970911774; Validation Loss = 0.019233131487938253\n",
            "Cost after 297200 iterations : Training Loss =  0.010253006878425402; Validation Loss = 0.01923312843329324\n",
            "Cost after 297201 iterations : Training Loss =  0.010252998786020897; Validation Loss = 0.01923312537869223\n",
            "Cost after 297202 iterations : Training Loss =  0.010252990693698168; Validation Loss = 0.01923312232413442\n",
            "Cost after 297203 iterations : Training Loss =  0.01025298260145732; Validation Loss = 0.019233119269620312\n",
            "Cost after 297204 iterations : Training Loss =  0.010252974509298216; Validation Loss = 0.019233116215149792\n",
            "Cost after 297205 iterations : Training Loss =  0.010252966417220951; Validation Loss = 0.019233113160722647\n",
            "Cost after 297206 iterations : Training Loss =  0.01025295832522538; Validation Loss = 0.019233110106339314\n",
            "Cost after 297207 iterations : Training Loss =  0.0102529502333117; Validation Loss = 0.01923310705199957\n",
            "Cost after 297208 iterations : Training Loss =  0.010252942141479695; Validation Loss = 0.019233103997703373\n",
            "Cost after 297209 iterations : Training Loss =  0.010252934049729594; Validation Loss = 0.019233100943450602\n",
            "Cost after 297210 iterations : Training Loss =  0.01025292595806129; Validation Loss = 0.019233097889241588\n",
            "Cost after 297211 iterations : Training Loss =  0.01025291786647472; Validation Loss = 0.019233094835076014\n",
            "Cost after 297212 iterations : Training Loss =  0.010252909774969985; Validation Loss = 0.019233091780954278\n",
            "Cost after 297213 iterations : Training Loss =  0.010252901683547026; Validation Loss = 0.019233088726876138\n",
            "Cost after 297214 iterations : Training Loss =  0.01025289359220588; Validation Loss = 0.019233085672841137\n",
            "Cost after 297215 iterations : Training Loss =  0.010252885500946482; Validation Loss = 0.019233082618849845\n",
            "Cost after 297216 iterations : Training Loss =  0.01025287740976887; Validation Loss = 0.019233079564902528\n",
            "Cost after 297217 iterations : Training Loss =  0.01025286931867306; Validation Loss = 0.019233076510998263\n",
            "Cost after 297218 iterations : Training Loss =  0.010252861227659085; Validation Loss = 0.019233073457137834\n",
            "Cost after 297219 iterations : Training Loss =  0.010252853136726834; Validation Loss = 0.019233070403320767\n",
            "Cost after 297220 iterations : Training Loss =  0.010252845045876334; Validation Loss = 0.019233067349547578\n",
            "Cost after 297221 iterations : Training Loss =  0.010252836955107638; Validation Loss = 0.019233064295817785\n",
            "Cost after 297222 iterations : Training Loss =  0.010252828864420667; Validation Loss = 0.019233061242131585\n",
            "Cost after 297223 iterations : Training Loss =  0.010252820773815547; Validation Loss = 0.019233058188489013\n",
            "Cost after 297224 iterations : Training Loss =  0.010252812683292246; Validation Loss = 0.019233055134889824\n",
            "Cost after 297225 iterations : Training Loss =  0.010252804592850686; Validation Loss = 0.01923305208133452\n",
            "Cost after 297226 iterations : Training Loss =  0.010252796502490903; Validation Loss = 0.019233049027822694\n",
            "Cost after 297227 iterations : Training Loss =  0.010252788412212856; Validation Loss = 0.019233045974354334\n",
            "Cost after 297228 iterations : Training Loss =  0.010252780322016573; Validation Loss = 0.019233042920929748\n",
            "Cost after 297229 iterations : Training Loss =  0.010252772231902116; Validation Loss = 0.01923303986754846\n",
            "Cost after 297230 iterations : Training Loss =  0.010252764141869373; Validation Loss = 0.019233036814210826\n",
            "Cost after 297231 iterations : Training Loss =  0.010252756051918414; Validation Loss = 0.019233033760916868\n",
            "Cost after 297232 iterations : Training Loss =  0.010252747962049232; Validation Loss = 0.019233030707666417\n",
            "Cost after 297233 iterations : Training Loss =  0.010252739872261846; Validation Loss = 0.01923302765445934\n",
            "Cost after 297234 iterations : Training Loss =  0.010252731782556262; Validation Loss = 0.019233024601295994\n",
            "Cost after 297235 iterations : Training Loss =  0.010252723692932286; Validation Loss = 0.01923302154817631\n",
            "Cost after 297236 iterations : Training Loss =  0.010252715603390274; Validation Loss = 0.019233018495099827\n",
            "Cost after 297237 iterations : Training Loss =  0.010252707513929841; Validation Loss = 0.019233015442067274\n",
            "Cost after 297238 iterations : Training Loss =  0.010252699424551244; Validation Loss = 0.019233012389078163\n",
            "Cost after 297239 iterations : Training Loss =  0.010252691335254473; Validation Loss = 0.019233009336132617\n",
            "Cost after 297240 iterations : Training Loss =  0.01025268324603945; Validation Loss = 0.019233006283230738\n",
            "Cost after 297241 iterations : Training Loss =  0.010252675156906128; Validation Loss = 0.019233003230372137\n",
            "Cost after 297242 iterations : Training Loss =  0.010252667067854604; Validation Loss = 0.019233000177557132\n",
            "Cost after 297243 iterations : Training Loss =  0.010252658978884801; Validation Loss = 0.019232997124785895\n",
            "Cost after 297244 iterations : Training Loss =  0.01025265088999681; Validation Loss = 0.019232994072058075\n",
            "Cost after 297245 iterations : Training Loss =  0.01025264280119052; Validation Loss = 0.019232991019373983\n",
            "Cost after 297246 iterations : Training Loss =  0.01025263471246606; Validation Loss = 0.019232987966733124\n",
            "Cost after 297247 iterations : Training Loss =  0.010252626623823194; Validation Loss = 0.019232984914136195\n",
            "Cost after 297248 iterations : Training Loss =  0.010252618535262246; Validation Loss = 0.019232981861582583\n",
            "Cost after 297249 iterations : Training Loss =  0.010252610446782976; Validation Loss = 0.01923297880907248\n",
            "Cost after 297250 iterations : Training Loss =  0.010252602358385484; Validation Loss = 0.01923297575660592\n",
            "Cost after 297251 iterations : Training Loss =  0.010252594270069654; Validation Loss = 0.019232972704183043\n",
            "Cost after 297252 iterations : Training Loss =  0.010252586181835673; Validation Loss = 0.019232969651803968\n",
            "Cost after 297253 iterations : Training Loss =  0.010252578093683438; Validation Loss = 0.019232966599468094\n",
            "Cost after 297254 iterations : Training Loss =  0.010252570005612933; Validation Loss = 0.019232963547175876\n",
            "Cost after 297255 iterations : Training Loss =  0.01025256191762412; Validation Loss = 0.019232960494927034\n",
            "Cost after 297256 iterations : Training Loss =  0.010252553829717093; Validation Loss = 0.019232957442721847\n",
            "Cost after 297257 iterations : Training Loss =  0.010252545741891772; Validation Loss = 0.01923295439056039\n",
            "Cost after 297258 iterations : Training Loss =  0.010252537654148235; Validation Loss = 0.019232951338441953\n",
            "Cost after 297259 iterations : Training Loss =  0.010252529566486462; Validation Loss = 0.019232948286367738\n",
            "Cost after 297260 iterations : Training Loss =  0.010252521478906316; Validation Loss = 0.019232945234336413\n",
            "Cost after 297261 iterations : Training Loss =  0.010252513391408036; Validation Loss = 0.01923294218234916\n",
            "Cost after 297262 iterations : Training Loss =  0.01025250530399144; Validation Loss = 0.019232939130405222\n",
            "Cost after 297263 iterations : Training Loss =  0.010252497216656524; Validation Loss = 0.019232936078504875\n",
            "Cost after 297264 iterations : Training Loss =  0.01025248912940343; Validation Loss = 0.019232933026648105\n",
            "Cost after 297265 iterations : Training Loss =  0.010252481042232033; Validation Loss = 0.019232929974834792\n",
            "Cost after 297266 iterations : Training Loss =  0.010252472955142446; Validation Loss = 0.019232926923065115\n",
            "Cost after 297267 iterations : Training Loss =  0.010252464868134472; Validation Loss = 0.019232923871338965\n",
            "Cost after 297268 iterations : Training Loss =  0.010252456781208284; Validation Loss = 0.019232920819656243\n",
            "Cost after 297269 iterations : Training Loss =  0.010252448694363772; Validation Loss = 0.019232917768017138\n",
            "Cost after 297270 iterations : Training Loss =  0.010252440607601011; Validation Loss = 0.019232914716421554\n",
            "Cost after 297271 iterations : Training Loss =  0.010252432520920089; Validation Loss = 0.01923291166486934\n",
            "Cost after 297272 iterations : Training Loss =  0.010252424434320735; Validation Loss = 0.019232908613360983\n",
            "Cost after 297273 iterations : Training Loss =  0.010252416347803224; Validation Loss = 0.019232905561896024\n",
            "Cost after 297274 iterations : Training Loss =  0.010252408261367389; Validation Loss = 0.019232902510474464\n",
            "Cost after 297275 iterations : Training Loss =  0.01025240017501323; Validation Loss = 0.01923289945909626\n",
            "Cost after 297276 iterations : Training Loss =  0.01025239208874091; Validation Loss = 0.019232896407761994\n",
            "Cost after 297277 iterations : Training Loss =  0.010252384002550241; Validation Loss = 0.019232893356470986\n",
            "Cost after 297278 iterations : Training Loss =  0.010252375916441304; Validation Loss = 0.019232890305223783\n",
            "Cost after 297279 iterations : Training Loss =  0.01025236783041403; Validation Loss = 0.019232887254019816\n",
            "Cost after 297280 iterations : Training Loss =  0.01025235974446854; Validation Loss = 0.01923288420285937\n",
            "Cost after 297281 iterations : Training Loss =  0.01025235165860472; Validation Loss = 0.019232881151742886\n",
            "Cost after 297282 iterations : Training Loss =  0.010252343572822668; Validation Loss = 0.01923287810066938\n",
            "Cost after 297283 iterations : Training Loss =  0.010252335487122346; Validation Loss = 0.01923287504963958\n",
            "Cost after 297284 iterations : Training Loss =  0.010252327401503631; Validation Loss = 0.01923287199865349\n",
            "Cost after 297285 iterations : Training Loss =  0.010252319315966733; Validation Loss = 0.019232868947710747\n",
            "Cost after 297286 iterations : Training Loss =  0.010252311230511535; Validation Loss = 0.019232865896812116\n",
            "Cost after 297287 iterations : Training Loss =  0.010252303145137996; Validation Loss = 0.01923286284595665\n",
            "Cost after 297288 iterations : Training Loss =  0.01025229505984618; Validation Loss = 0.0192328597951447\n",
            "Cost after 297289 iterations : Training Loss =  0.010252286974636165; Validation Loss = 0.01923285674437587\n",
            "Cost after 297290 iterations : Training Loss =  0.010252278889507664; Validation Loss = 0.01923285369365085\n",
            "Cost after 297291 iterations : Training Loss =  0.010252270804461006; Validation Loss = 0.019232850642969288\n",
            "Cost after 297292 iterations : Training Loss =  0.010252262719495996; Validation Loss = 0.019232847592331462\n",
            "Cost after 297293 iterations : Training Loss =  0.010252254634612792; Validation Loss = 0.019232844541736724\n",
            "Cost after 297294 iterations : Training Loss =  0.010252246549811127; Validation Loss = 0.019232841491185645\n",
            "Cost after 297295 iterations : Training Loss =  0.01025223846509132; Validation Loss = 0.01923283844067813\n",
            "Cost after 297296 iterations : Training Loss =  0.01025223038045315; Validation Loss = 0.019232835390213792\n",
            "Cost after 297297 iterations : Training Loss =  0.010252222295896755; Validation Loss = 0.019232832339793334\n",
            "Cost after 297298 iterations : Training Loss =  0.010252214211421886; Validation Loss = 0.01923282928941638\n",
            "Cost after 297299 iterations : Training Loss =  0.01025220612702886; Validation Loss = 0.019232826239083064\n",
            "Cost after 297300 iterations : Training Loss =  0.010252198042717488; Validation Loss = 0.019232823188792898\n",
            "Cost after 297301 iterations : Training Loss =  0.010252189958487779; Validation Loss = 0.019232820138546318\n",
            "Cost after 297302 iterations : Training Loss =  0.010252181874339876; Validation Loss = 0.01923281708834339\n",
            "Cost after 297303 iterations : Training Loss =  0.010252173790273515; Validation Loss = 0.019232814038183978\n",
            "Cost after 297304 iterations : Training Loss =  0.010252165706288948; Validation Loss = 0.019232810988068085\n",
            "Cost after 297305 iterations : Training Loss =  0.010252157622386012; Validation Loss = 0.019232807937995477\n",
            "Cost after 297306 iterations : Training Loss =  0.010252149538564825; Validation Loss = 0.019232804887966724\n",
            "Cost after 297307 iterations : Training Loss =  0.010252141454825317; Validation Loss = 0.01923280183798125\n",
            "Cost after 297308 iterations : Training Loss =  0.010252133371167522; Validation Loss = 0.019232798788039396\n",
            "Cost after 297309 iterations : Training Loss =  0.010252125287591283; Validation Loss = 0.019232795738141174\n",
            "Cost after 297310 iterations : Training Loss =  0.010252117204096847; Validation Loss = 0.019232792688286385\n",
            "Cost after 297311 iterations : Training Loss =  0.010252109120684064; Validation Loss = 0.019232789638474884\n",
            "Cost after 297312 iterations : Training Loss =  0.010252101037352952; Validation Loss = 0.019232786588707113\n",
            "Cost after 297313 iterations : Training Loss =  0.010252092954103518; Validation Loss = 0.019232783538982594\n",
            "Cost after 297314 iterations : Training Loss =  0.010252084870935783; Validation Loss = 0.01923278048930203\n",
            "Cost after 297315 iterations : Training Loss =  0.010252076787849836; Validation Loss = 0.01923277743966464\n",
            "Cost after 297316 iterations : Training Loss =  0.010252068704845437; Validation Loss = 0.019232774390070585\n",
            "Cost after 297317 iterations : Training Loss =  0.010252060621922673; Validation Loss = 0.01923277134051997\n",
            "Cost after 297318 iterations : Training Loss =  0.010252052539081692; Validation Loss = 0.019232768291013197\n",
            "Cost after 297319 iterations : Training Loss =  0.010252044456322381; Validation Loss = 0.01923276524154979\n",
            "Cost after 297320 iterations : Training Loss =  0.010252036373644642; Validation Loss = 0.01923276219212973\n",
            "Cost after 297321 iterations : Training Loss =  0.010252028291048676; Validation Loss = 0.019232759142753294\n",
            "Cost after 297322 iterations : Training Loss =  0.010252020208534406; Validation Loss = 0.019232756093420643\n",
            "Cost after 297323 iterations : Training Loss =  0.010252012126101771; Validation Loss = 0.01923275304413076\n",
            "Cost after 297324 iterations : Training Loss =  0.01025200404375076; Validation Loss = 0.01923274999488487\n",
            "Cost after 297325 iterations : Training Loss =  0.010251995961481455; Validation Loss = 0.019232746945682475\n",
            "Cost after 297326 iterations : Training Loss =  0.010251987879293809; Validation Loss = 0.019232743896523623\n",
            "Cost after 297327 iterations : Training Loss =  0.010251979797187895; Validation Loss = 0.01923274084740844\n",
            "Cost after 297328 iterations : Training Loss =  0.010251971715163635; Validation Loss = 0.01923273779833641\n",
            "Cost after 297329 iterations : Training Loss =  0.010251963633220958; Validation Loss = 0.019232734749307815\n",
            "Cost after 297330 iterations : Training Loss =  0.010251955551360048; Validation Loss = 0.019232731700322605\n",
            "Cost after 297331 iterations : Training Loss =  0.01025194746958069; Validation Loss = 0.01923272865138134\n",
            "Cost after 297332 iterations : Training Loss =  0.010251939387883059; Validation Loss = 0.01923272560248358\n",
            "Cost after 297333 iterations : Training Loss =  0.010251931306267136; Validation Loss = 0.019232722553629067\n",
            "Cost after 297334 iterations : Training Loss =  0.010251923224732734; Validation Loss = 0.01923271950481817\n",
            "Cost after 297335 iterations : Training Loss =  0.010251915143280102; Validation Loss = 0.019232716456050517\n",
            "Cost after 297336 iterations : Training Loss =  0.010251907061909054; Validation Loss = 0.019232713407326497\n",
            "Cost after 297337 iterations : Training Loss =  0.010251898980619786; Validation Loss = 0.019232710358645853\n",
            "Cost after 297338 iterations : Training Loss =  0.010251890899411998; Validation Loss = 0.01923270731000895\n",
            "Cost after 297339 iterations : Training Loss =  0.010251882818286047; Validation Loss = 0.019232704261415195\n",
            "Cost after 297340 iterations : Training Loss =  0.01025187473724166; Validation Loss = 0.019232701212864908\n",
            "Cost after 297341 iterations : Training Loss =  0.010251866656278998; Validation Loss = 0.019232698164358568\n",
            "Cost after 297342 iterations : Training Loss =  0.010251858575397914; Validation Loss = 0.019232695115895277\n",
            "Cost after 297343 iterations : Training Loss =  0.01025185049459848; Validation Loss = 0.019232692067475833\n",
            "Cost after 297344 iterations : Training Loss =  0.01025184241388072; Validation Loss = 0.01923268901909956\n",
            "Cost after 297345 iterations : Training Loss =  0.010251834333244643; Validation Loss = 0.01923268597076669\n",
            "Cost after 297346 iterations : Training Loss =  0.010251826252690131; Validation Loss = 0.019232682922477522\n",
            "Cost after 297347 iterations : Training Loss =  0.010251818172217238; Validation Loss = 0.019232679874231648\n",
            "Cost after 297348 iterations : Training Loss =  0.0102518100918261; Validation Loss = 0.019232676826029152\n",
            "Cost after 297349 iterations : Training Loss =  0.010251802011516563; Validation Loss = 0.01923267377787048\n",
            "Cost after 297350 iterations : Training Loss =  0.010251793931288755; Validation Loss = 0.019232670729755017\n",
            "Cost after 297351 iterations : Training Loss =  0.010251785851142394; Validation Loss = 0.019232667681683288\n",
            "Cost after 297352 iterations : Training Loss =  0.010251777771077823; Validation Loss = 0.019232664633654722\n",
            "Cost after 297353 iterations : Training Loss =  0.010251769691094892; Validation Loss = 0.01923266158567002\n",
            "Cost after 297354 iterations : Training Loss =  0.010251761611193547; Validation Loss = 0.019232658537728545\n",
            "Cost after 297355 iterations : Training Loss =  0.010251753531373814; Validation Loss = 0.01923265548983077\n",
            "Cost after 297356 iterations : Training Loss =  0.010251745451635883; Validation Loss = 0.0192326524419762\n",
            "Cost after 297357 iterations : Training Loss =  0.010251737371979383; Validation Loss = 0.019232649394165455\n",
            "Cost after 297358 iterations : Training Loss =  0.010251729292404614; Validation Loss = 0.01923264634639759\n",
            "Cost after 297359 iterations : Training Loss =  0.0102517212129115; Validation Loss = 0.01923264329867329\n",
            "Cost after 297360 iterations : Training Loss =  0.010251713133499925; Validation Loss = 0.019232640250992635\n",
            "Cost after 297361 iterations : Training Loss =  0.010251705054170095; Validation Loss = 0.019232637203355458\n",
            "Cost after 297362 iterations : Training Loss =  0.010251696974921804; Validation Loss = 0.019232634155761527\n",
            "Cost after 297363 iterations : Training Loss =  0.01025168889575523; Validation Loss = 0.019232631108211377\n",
            "Cost after 297364 iterations : Training Loss =  0.010251680816670248; Validation Loss = 0.019232628060704637\n",
            "Cost after 297365 iterations : Training Loss =  0.010251672737666879; Validation Loss = 0.01923262501324092\n",
            "Cost after 297366 iterations : Training Loss =  0.01025166465874514; Validation Loss = 0.01923262196582112\n",
            "Cost after 297367 iterations : Training Loss =  0.01025165657990502; Validation Loss = 0.01923261891844483\n",
            "Cost after 297368 iterations : Training Loss =  0.010251648501146567; Validation Loss = 0.019232615871111743\n",
            "Cost after 297369 iterations : Training Loss =  0.010251640422469701; Validation Loss = 0.01923261282382224\n",
            "Cost after 297370 iterations : Training Loss =  0.010251632343874502; Validation Loss = 0.019232609776576096\n",
            "Cost after 297371 iterations : Training Loss =  0.010251624265360835; Validation Loss = 0.01923260672937367\n",
            "Cost after 297372 iterations : Training Loss =  0.010251616186928898; Validation Loss = 0.019232603682214706\n",
            "Cost after 297373 iterations : Training Loss =  0.010251608108578465; Validation Loss = 0.019232600635098757\n",
            "Cost after 297374 iterations : Training Loss =  0.010251600030309713; Validation Loss = 0.019232597588026694\n",
            "Cost after 297375 iterations : Training Loss =  0.010251591952122514; Validation Loss = 0.01923259454099799\n",
            "Cost after 297376 iterations : Training Loss =  0.010251583874017003; Validation Loss = 0.019232591494012234\n",
            "Cost after 297377 iterations : Training Loss =  0.010251575795993138; Validation Loss = 0.019232588447070344\n",
            "Cost after 297378 iterations : Training Loss =  0.01025156771805082; Validation Loss = 0.01923258540017177\n",
            "Cost after 297379 iterations : Training Loss =  0.010251559640190205; Validation Loss = 0.019232582353316884\n",
            "Cost after 297380 iterations : Training Loss =  0.010251551562411023; Validation Loss = 0.01923257930650579\n",
            "Cost after 297381 iterations : Training Loss =  0.010251543484713603; Validation Loss = 0.019232576259737608\n",
            "Cost after 297382 iterations : Training Loss =  0.010251535407097775; Validation Loss = 0.019232573213012746\n",
            "Cost after 297383 iterations : Training Loss =  0.01025152732956355; Validation Loss = 0.01923257016633184\n",
            "Cost after 297384 iterations : Training Loss =  0.0102515192521108; Validation Loss = 0.01923256711969401\n",
            "Cost after 297385 iterations : Training Loss =  0.010251511174739826; Validation Loss = 0.01923256407309978\n",
            "Cost after 297386 iterations : Training Loss =  0.010251503097450397; Validation Loss = 0.019232561026548824\n",
            "Cost after 297387 iterations : Training Loss =  0.010251495020242567; Validation Loss = 0.0192325579800414\n",
            "Cost after 297388 iterations : Training Loss =  0.010251486943116379; Validation Loss = 0.019232554933577523\n",
            "Cost after 297389 iterations : Training Loss =  0.010251478866071747; Validation Loss = 0.019232551887156998\n",
            "Cost after 297390 iterations : Training Loss =  0.010251470789108756; Validation Loss = 0.019232548840779875\n",
            "Cost after 297391 iterations : Training Loss =  0.010251462712227355; Validation Loss = 0.01923254579444648\n",
            "Cost after 297392 iterations : Training Loss =  0.010251454635427551; Validation Loss = 0.019232542748156275\n",
            "Cost after 297393 iterations : Training Loss =  0.010251446558709297; Validation Loss = 0.019232539701909662\n",
            "Cost after 297394 iterations : Training Loss =  0.010251438482072652; Validation Loss = 0.01923253665570633\n",
            "Cost after 297395 iterations : Training Loss =  0.010251430405517698; Validation Loss = 0.01923253360954638\n",
            "Cost after 297396 iterations : Training Loss =  0.01025142232904418; Validation Loss = 0.019232530563429894\n",
            "Cost after 297397 iterations : Training Loss =  0.01025141425265244; Validation Loss = 0.019232527517356962\n",
            "Cost after 297398 iterations : Training Loss =  0.010251406176342138; Validation Loss = 0.01923252447132764\n",
            "Cost after 297399 iterations : Training Loss =  0.010251398100113544; Validation Loss = 0.019232521425341223\n",
            "Cost after 297400 iterations : Training Loss =  0.010251390023966488; Validation Loss = 0.019232518379398652\n",
            "Cost after 297401 iterations : Training Loss =  0.010251381947901005; Validation Loss = 0.019232515333499272\n",
            "Cost after 297402 iterations : Training Loss =  0.010251373871917116; Validation Loss = 0.019232512287643586\n",
            "Cost after 297403 iterations : Training Loss =  0.010251365796014803; Validation Loss = 0.019232509241831182\n",
            "Cost after 297404 iterations : Training Loss =  0.010251357720194114; Validation Loss = 0.019232506196061993\n",
            "Cost after 297405 iterations : Training Loss =  0.010251349644454996; Validation Loss = 0.019232503150336585\n",
            "Cost after 297406 iterations : Training Loss =  0.010251341568797477; Validation Loss = 0.019232500104654496\n",
            "Cost after 297407 iterations : Training Loss =  0.010251333493221522; Validation Loss = 0.019232497059015655\n",
            "Cost after 297408 iterations : Training Loss =  0.010251325417727135; Validation Loss = 0.01923249401342028\n",
            "Cost after 297409 iterations : Training Loss =  0.010251317342314386; Validation Loss = 0.01923249096786851\n",
            "Cost after 297410 iterations : Training Loss =  0.010251309266983134; Validation Loss = 0.01923248792236002\n",
            "Cost after 297411 iterations : Training Loss =  0.01025130119173356; Validation Loss = 0.019232484876895186\n",
            "Cost after 297412 iterations : Training Loss =  0.010251293116565481; Validation Loss = 0.019232481831473486\n",
            "Cost after 297413 iterations : Training Loss =  0.010251285041478967; Validation Loss = 0.019232478786095564\n",
            "Cost after 297414 iterations : Training Loss =  0.010251276966474055; Validation Loss = 0.019232475740760865\n",
            "Cost after 297415 iterations : Training Loss =  0.01025126889155074; Validation Loss = 0.019232472695469308\n",
            "Cost after 297416 iterations : Training Loss =  0.010251260816709035; Validation Loss = 0.01923246965022173\n",
            "Cost after 297417 iterations : Training Loss =  0.010251252741948751; Validation Loss = 0.01923246660501724\n",
            "Cost after 297418 iterations : Training Loss =  0.010251244667270263; Validation Loss = 0.019232463559856423\n",
            "Cost after 297419 iterations : Training Loss =  0.01025123659267316; Validation Loss = 0.01923246051473873\n",
            "Cost after 297420 iterations : Training Loss =  0.010251228518157627; Validation Loss = 0.01923245746966474\n",
            "Cost after 297421 iterations : Training Loss =  0.010251220443723785; Validation Loss = 0.019232454424633644\n",
            "Cost after 297422 iterations : Training Loss =  0.010251212369371437; Validation Loss = 0.01923245137964645\n",
            "Cost after 297423 iterations : Training Loss =  0.010251204295100704; Validation Loss = 0.019232448334702338\n",
            "Cost after 297424 iterations : Training Loss =  0.01025119622091151; Validation Loss = 0.01923244528980207\n",
            "Cost after 297425 iterations : Training Loss =  0.01025118814680381; Validation Loss = 0.019232442244945094\n",
            "Cost after 297426 iterations : Training Loss =  0.010251180072777824; Validation Loss = 0.019232439200131426\n",
            "Cost after 297427 iterations : Training Loss =  0.010251171998833266; Validation Loss = 0.01923243615536129\n",
            "Cost after 297428 iterations : Training Loss =  0.010251163924970323; Validation Loss = 0.0192324331106342\n",
            "Cost after 297429 iterations : Training Loss =  0.010251155851188886; Validation Loss = 0.01923243006595092\n",
            "Cost after 297430 iterations : Training Loss =  0.010251147777489067; Validation Loss = 0.019232427021310762\n",
            "Cost after 297431 iterations : Training Loss =  0.010251139703870829; Validation Loss = 0.019232423976714076\n",
            "Cost after 297432 iterations : Training Loss =  0.010251131630334103; Validation Loss = 0.019232420932161078\n",
            "Cost after 297433 iterations : Training Loss =  0.010251123556878976; Validation Loss = 0.01923241788765149\n",
            "Cost after 297434 iterations : Training Loss =  0.010251115483505371; Validation Loss = 0.019232414843185063\n",
            "Cost after 297435 iterations : Training Loss =  0.010251107410213334; Validation Loss = 0.01923241179876231\n",
            "Cost after 297436 iterations : Training Loss =  0.010251099337002783; Validation Loss = 0.019232408754382698\n",
            "Cost after 297437 iterations : Training Loss =  0.010251091263873878; Validation Loss = 0.0192324057100467\n",
            "Cost after 297438 iterations : Training Loss =  0.010251083190826356; Validation Loss = 0.019232402665754086\n",
            "Cost after 297439 iterations : Training Loss =  0.010251075117860546; Validation Loss = 0.019232399621504956\n",
            "Cost after 297440 iterations : Training Loss =  0.010251067044976253; Validation Loss = 0.019232396577298864\n",
            "Cost after 297441 iterations : Training Loss =  0.010251058972173533; Validation Loss = 0.019232393533136386\n",
            "Cost after 297442 iterations : Training Loss =  0.01025105089945233; Validation Loss = 0.019232390489017304\n",
            "Cost after 297443 iterations : Training Loss =  0.0102510428268126; Validation Loss = 0.01923238744494171\n",
            "Cost after 297444 iterations : Training Loss =  0.01025103475425454; Validation Loss = 0.01923238440090935\n",
            "Cost after 297445 iterations : Training Loss =  0.010251026681777948; Validation Loss = 0.019232381356920594\n",
            "Cost after 297446 iterations : Training Loss =  0.010251018609382939; Validation Loss = 0.01923237831297499\n",
            "Cost after 297447 iterations : Training Loss =  0.010251010537069463; Validation Loss = 0.019232375269072937\n",
            "Cost after 297448 iterations : Training Loss =  0.010251002464837495; Validation Loss = 0.019232372225214157\n",
            "Cost after 297449 iterations : Training Loss =  0.010250994392687096; Validation Loss = 0.019232369181398902\n",
            "Cost after 297450 iterations : Training Loss =  0.010250986320618197; Validation Loss = 0.01923236613762699\n",
            "Cost after 297451 iterations : Training Loss =  0.010250978248630825; Validation Loss = 0.019232363093898662\n",
            "Cost after 297452 iterations : Training Loss =  0.010250970176725058; Validation Loss = 0.01923236005021344\n",
            "Cost after 297453 iterations : Training Loss =  0.010250962104900747; Validation Loss = 0.019232357006571915\n",
            "Cost after 297454 iterations : Training Loss =  0.01025095403315802; Validation Loss = 0.01923235396297342\n",
            "Cost after 297455 iterations : Training Loss =  0.010250945961496824; Validation Loss = 0.019232350919418734\n",
            "Cost after 297456 iterations : Training Loss =  0.010250937889917097; Validation Loss = 0.0192323478759073\n",
            "Cost after 297457 iterations : Training Loss =  0.010250929818419026; Validation Loss = 0.019232344832438627\n",
            "Cost after 297458 iterations : Training Loss =  0.010250921747002454; Validation Loss = 0.019232341789013962\n",
            "Cost after 297459 iterations : Training Loss =  0.01025091367566735; Validation Loss = 0.019232338745632887\n",
            "Cost after 297460 iterations : Training Loss =  0.010250905604413779; Validation Loss = 0.019232335702295048\n",
            "Cost after 297461 iterations : Training Loss =  0.01025089753324171; Validation Loss = 0.01923233265900012\n",
            "Cost after 297462 iterations : Training Loss =  0.010250889462151205; Validation Loss = 0.019232329615749017\n",
            "Cost after 297463 iterations : Training Loss =  0.01025088139114224; Validation Loss = 0.01923232657254107\n",
            "Cost after 297464 iterations : Training Loss =  0.010250873320214744; Validation Loss = 0.01923232352937661\n",
            "Cost after 297465 iterations : Training Loss =  0.010250865249368826; Validation Loss = 0.019232320486255804\n",
            "Cost after 297466 iterations : Training Loss =  0.010250857178604425; Validation Loss = 0.01923231744317798\n",
            "Cost after 297467 iterations : Training Loss =  0.010250849107921536; Validation Loss = 0.019232314400143804\n",
            "Cost after 297468 iterations : Training Loss =  0.010250841037320059; Validation Loss = 0.019232311357153083\n",
            "Cost after 297469 iterations : Training Loss =  0.010250832966800285; Validation Loss = 0.019232308314205516\n",
            "Cost after 297470 iterations : Training Loss =  0.010250824896361891; Validation Loss = 0.019232305271301386\n",
            "Cost after 297471 iterations : Training Loss =  0.010250816826005062; Validation Loss = 0.019232302228440218\n",
            "Cost after 297472 iterations : Training Loss =  0.010250808755729748; Validation Loss = 0.019232299185622952\n",
            "Cost after 297473 iterations : Training Loss =  0.010250800685535973; Validation Loss = 0.019232296142848892\n",
            "Cost after 297474 iterations : Training Loss =  0.010250792615423698; Validation Loss = 0.019232293100118224\n",
            "Cost after 297475 iterations : Training Loss =  0.010250784545392903; Validation Loss = 0.01923229005743105\n",
            "Cost after 297476 iterations : Training Loss =  0.010250776475443575; Validation Loss = 0.019232287014787006\n",
            "Cost after 297477 iterations : Training Loss =  0.01025076840557581; Validation Loss = 0.019232283972186518\n",
            "Cost after 297478 iterations : Training Loss =  0.010250760335789614; Validation Loss = 0.019232280929629218\n",
            "Cost after 297479 iterations : Training Loss =  0.01025075226608478; Validation Loss = 0.019232277887115762\n",
            "Cost after 297480 iterations : Training Loss =  0.01025074419646158; Validation Loss = 0.0192322748446452\n",
            "Cost after 297481 iterations : Training Loss =  0.010250736126919853; Validation Loss = 0.019232271802218377\n",
            "Cost after 297482 iterations : Training Loss =  0.010250728057459649; Validation Loss = 0.01923226875983502\n",
            "Cost after 297483 iterations : Training Loss =  0.01025071998808096; Validation Loss = 0.01923226571749471\n",
            "Cost after 297484 iterations : Training Loss =  0.010250711918783692; Validation Loss = 0.019232262675197617\n",
            "Cost after 297485 iterations : Training Loss =  0.010250703849568008; Validation Loss = 0.01923225963294435\n",
            "Cost after 297486 iterations : Training Loss =  0.010250695780433725; Validation Loss = 0.01923225659073408\n",
            "Cost after 297487 iterations : Training Loss =  0.010250687711380968; Validation Loss = 0.019232253548567505\n",
            "Cost after 297488 iterations : Training Loss =  0.01025067964240973; Validation Loss = 0.019232250506443855\n",
            "Cost after 297489 iterations : Training Loss =  0.010250671573520014; Validation Loss = 0.019232247464363743\n",
            "Cost after 297490 iterations : Training Loss =  0.01025066350471178; Validation Loss = 0.019232244422327304\n",
            "Cost after 297491 iterations : Training Loss =  0.010250655435985048; Validation Loss = 0.01923224138033384\n",
            "Cost after 297492 iterations : Training Loss =  0.010250647367339763; Validation Loss = 0.01923223833838393\n",
            "Cost after 297493 iterations : Training Loss =  0.010250639298776057; Validation Loss = 0.01923223529647723\n",
            "Cost after 297494 iterations : Training Loss =  0.010250631230293792; Validation Loss = 0.01923223225461403\n",
            "Cost after 297495 iterations : Training Loss =  0.01025062316189302; Validation Loss = 0.01923222921279419\n",
            "Cost after 297496 iterations : Training Loss =  0.010250615093573707; Validation Loss = 0.01923222617101777\n",
            "Cost after 297497 iterations : Training Loss =  0.010250607025335956; Validation Loss = 0.019232223129284878\n",
            "Cost after 297498 iterations : Training Loss =  0.010250598957179611; Validation Loss = 0.01923222008759536\n",
            "Cost after 297499 iterations : Training Loss =  0.010250590889104785; Validation Loss = 0.019232217045948904\n",
            "Cost after 297500 iterations : Training Loss =  0.010250582821111465; Validation Loss = 0.01923221400434588\n",
            "Cost after 297501 iterations : Training Loss =  0.010250574753199578; Validation Loss = 0.019232210962786347\n",
            "Cost after 297502 iterations : Training Loss =  0.010250566685369174; Validation Loss = 0.01923220792126979\n",
            "Cost after 297503 iterations : Training Loss =  0.010250558617620224; Validation Loss = 0.019232204879796978\n",
            "Cost after 297504 iterations : Training Loss =  0.010250550549952882; Validation Loss = 0.019232201838367182\n",
            "Cost after 297505 iterations : Training Loss =  0.010250542482366945; Validation Loss = 0.01923219879698099\n",
            "Cost after 297506 iterations : Training Loss =  0.01025053441486246; Validation Loss = 0.019232195755637966\n",
            "Cost after 297507 iterations : Training Loss =  0.01025052634743947; Validation Loss = 0.019232192714338417\n",
            "Cost after 297508 iterations : Training Loss =  0.010250518280097938; Validation Loss = 0.0192321896730821\n",
            "Cost after 297509 iterations : Training Loss =  0.010250510212837925; Validation Loss = 0.019232186631869267\n",
            "Cost after 297510 iterations : Training Loss =  0.010250502145659408; Validation Loss = 0.019232183590699847\n",
            "Cost after 297511 iterations : Training Loss =  0.010250494078562253; Validation Loss = 0.019232180549573485\n",
            "Cost after 297512 iterations : Training Loss =  0.010250486011546708; Validation Loss = 0.019232177508490298\n",
            "Cost after 297513 iterations : Training Loss =  0.010250477944612516; Validation Loss = 0.019232174467450742\n",
            "Cost after 297514 iterations : Training Loss =  0.01025046987775988; Validation Loss = 0.019232171426454368\n",
            "Cost after 297515 iterations : Training Loss =  0.010250461810988695; Validation Loss = 0.019232168385501767\n",
            "Cost after 297516 iterations : Training Loss =  0.010250453744298883; Validation Loss = 0.019232165344592565\n",
            "Cost after 297517 iterations : Training Loss =  0.01025044567769072; Validation Loss = 0.019232162303726114\n",
            "Cost after 297518 iterations : Training Loss =  0.01025043761116392; Validation Loss = 0.019232159262903312\n",
            "Cost after 297519 iterations : Training Loss =  0.010250429544718536; Validation Loss = 0.019232156222124014\n",
            "Cost after 297520 iterations : Training Loss =  0.010250421478354655; Validation Loss = 0.019232153181387785\n",
            "Cost after 297521 iterations : Training Loss =  0.010250413412072197; Validation Loss = 0.0192321501406951\n",
            "Cost after 297522 iterations : Training Loss =  0.010250405345871223; Validation Loss = 0.01923214710004579\n",
            "Cost after 297523 iterations : Training Loss =  0.010250397279751822; Validation Loss = 0.019232144059439765\n",
            "Cost after 297524 iterations : Training Loss =  0.010250389213713747; Validation Loss = 0.019232141018877203\n",
            "Cost after 297525 iterations : Training Loss =  0.010250381147757226; Validation Loss = 0.01923213797835764\n",
            "Cost after 297526 iterations : Training Loss =  0.010250373081882033; Validation Loss = 0.019232134937881697\n",
            "Cost after 297527 iterations : Training Loss =  0.010250365016088388; Validation Loss = 0.019232131897449052\n",
            "Cost after 297528 iterations : Training Loss =  0.010250356950376231; Validation Loss = 0.019232128857059695\n",
            "Cost after 297529 iterations : Training Loss =  0.010250348884745445; Validation Loss = 0.01923212581671372\n",
            "Cost after 297530 iterations : Training Loss =  0.010250340819196194; Validation Loss = 0.019232122776410846\n",
            "Cost after 297531 iterations : Training Loss =  0.010250332753728353; Validation Loss = 0.019232119736151708\n",
            "Cost after 297532 iterations : Training Loss =  0.01025032468834198; Validation Loss = 0.019232116695935667\n",
            "Cost after 297533 iterations : Training Loss =  0.01025031662303701; Validation Loss = 0.019232113655762974\n",
            "Cost after 297534 iterations : Training Loss =  0.010250308557813496; Validation Loss = 0.019232110615633385\n",
            "Cost after 297535 iterations : Training Loss =  0.010250300492671427; Validation Loss = 0.019232107575547493\n",
            "Cost after 297536 iterations : Training Loss =  0.010250292427610878; Validation Loss = 0.01923210453550486\n",
            "Cost after 297537 iterations : Training Loss =  0.010250284362631637; Validation Loss = 0.019232101495505687\n",
            "Cost after 297538 iterations : Training Loss =  0.010250276297734007; Validation Loss = 0.019232098455549677\n",
            "Cost after 297539 iterations : Training Loss =  0.010250268232917673; Validation Loss = 0.019232095415637144\n",
            "Cost after 297540 iterations : Training Loss =  0.010250260168182854; Validation Loss = 0.019232092375767493\n",
            "Cost after 297541 iterations : Training Loss =  0.010250252103529462; Validation Loss = 0.01923208933594143\n",
            "Cost after 297542 iterations : Training Loss =  0.010250244038957588; Validation Loss = 0.019232086296158895\n",
            "Cost after 297543 iterations : Training Loss =  0.010250235974467104; Validation Loss = 0.01923208325641959\n",
            "Cost after 297544 iterations : Training Loss =  0.010250227910057955; Validation Loss = 0.019232080216723783\n",
            "Cost after 297545 iterations : Training Loss =  0.01025021984573033; Validation Loss = 0.019232077177070945\n",
            "Cost after 297546 iterations : Training Loss =  0.010250211781484149; Validation Loss = 0.019232074137461447\n",
            "Cost after 297547 iterations : Training Loss =  0.01025020371731941; Validation Loss = 0.019232071097895504\n",
            "Cost after 297548 iterations : Training Loss =  0.010250195653236071; Validation Loss = 0.019232068058372905\n",
            "Cost after 297549 iterations : Training Loss =  0.01025018758923417; Validation Loss = 0.01923206501889344\n",
            "Cost after 297550 iterations : Training Loss =  0.01025017952531378; Validation Loss = 0.019232061979457208\n",
            "Cost after 297551 iterations : Training Loss =  0.010250171461474682; Validation Loss = 0.019232058940064533\n",
            "Cost after 297552 iterations : Training Loss =  0.010250163397717137; Validation Loss = 0.019232055900715157\n",
            "Cost after 297553 iterations : Training Loss =  0.01025015533404087; Validation Loss = 0.019232052861408718\n",
            "Cost after 297554 iterations : Training Loss =  0.010250147270446175; Validation Loss = 0.01923204982214606\n",
            "Cost after 297555 iterations : Training Loss =  0.010250139206932943; Validation Loss = 0.01923204678292676\n",
            "Cost after 297556 iterations : Training Loss =  0.01025013114350103; Validation Loss = 0.01923204374375056\n",
            "Cost after 297557 iterations : Training Loss =  0.01025012308015059; Validation Loss = 0.019232040704617547\n",
            "Cost after 297558 iterations : Training Loss =  0.010250115016881481; Validation Loss = 0.019232037665527963\n",
            "Cost after 297559 iterations : Training Loss =  0.010250106953693851; Validation Loss = 0.019232034626481705\n",
            "Cost after 297560 iterations : Training Loss =  0.010250098890587558; Validation Loss = 0.019232031587478805\n",
            "Cost after 297561 iterations : Training Loss =  0.010250090827562865; Validation Loss = 0.01923202854851923\n",
            "Cost after 297562 iterations : Training Loss =  0.010250082764619485; Validation Loss = 0.019232025509602818\n",
            "Cost after 297563 iterations : Training Loss =  0.01025007470175753; Validation Loss = 0.019232022470730012\n",
            "Cost after 297564 iterations : Training Loss =  0.010250066638977026; Validation Loss = 0.019232019431900515\n",
            "Cost after 297565 iterations : Training Loss =  0.010250058576277886; Validation Loss = 0.01923201639311425\n",
            "Cost after 297566 iterations : Training Loss =  0.010250050513660243; Validation Loss = 0.019232013354371018\n",
            "Cost after 297567 iterations : Training Loss =  0.010250042451123955; Validation Loss = 0.01923201031567103\n",
            "Cost after 297568 iterations : Training Loss =  0.010250034388668975; Validation Loss = 0.01923200727701479\n",
            "Cost after 297569 iterations : Training Loss =  0.010250026326295599; Validation Loss = 0.019232004238401476\n",
            "Cost after 297570 iterations : Training Loss =  0.010250018264003562; Validation Loss = 0.019232001199831854\n",
            "Cost after 297571 iterations : Training Loss =  0.010250010201792965; Validation Loss = 0.01923199816130517\n",
            "Cost after 297572 iterations : Training Loss =  0.010250002139663681; Validation Loss = 0.019231995122822084\n",
            "Cost after 297573 iterations : Training Loss =  0.010249994077615869; Validation Loss = 0.019231992084382036\n",
            "Cost after 297574 iterations : Training Loss =  0.0102499860156495; Validation Loss = 0.019231989045985706\n",
            "Cost after 297575 iterations : Training Loss =  0.010249977953764495; Validation Loss = 0.01923198600763221\n",
            "Cost after 297576 iterations : Training Loss =  0.010249969891960897; Validation Loss = 0.019231982969322065\n",
            "Cost after 297577 iterations : Training Loss =  0.010249961830238671; Validation Loss = 0.019231979931055358\n",
            "Cost after 297578 iterations : Training Loss =  0.010249953768597862; Validation Loss = 0.019231976892832108\n",
            "Cost after 297579 iterations : Training Loss =  0.010249945707038506; Validation Loss = 0.019231973854652126\n",
            "Cost after 297580 iterations : Training Loss =  0.010249937645560455; Validation Loss = 0.01923197081651517\n",
            "Cost after 297581 iterations : Training Loss =  0.010249929584163897; Validation Loss = 0.01923196777842138\n",
            "Cost after 297582 iterations : Training Loss =  0.010249921522848695; Validation Loss = 0.019231964740371022\n",
            "Cost after 297583 iterations : Training Loss =  0.010249913461614853; Validation Loss = 0.01923196170236419\n",
            "Cost after 297584 iterations : Training Loss =  0.010249905400462415; Validation Loss = 0.019231958664400466\n",
            "Cost after 297585 iterations : Training Loss =  0.010249897339391398; Validation Loss = 0.019231955626480276\n",
            "Cost after 297586 iterations : Training Loss =  0.010249889278401812; Validation Loss = 0.019231952588603198\n",
            "Cost after 297587 iterations : Training Loss =  0.010249881217493578; Validation Loss = 0.019231949550769383\n",
            "Cost after 297588 iterations : Training Loss =  0.010249873156666776; Validation Loss = 0.019231946512978906\n",
            "Cost after 297589 iterations : Training Loss =  0.010249865095921252; Validation Loss = 0.019231943475231765\n",
            "Cost after 297590 iterations : Training Loss =  0.010249857035257293; Validation Loss = 0.019231940437527656\n",
            "Cost after 297591 iterations : Training Loss =  0.01024984897467455; Validation Loss = 0.019231937399866988\n",
            "Cost after 297592 iterations : Training Loss =  0.010249840914173304; Validation Loss = 0.01923193436224991\n",
            "Cost after 297593 iterations : Training Loss =  0.010249832853753353; Validation Loss = 0.01923193132467581\n",
            "Cost after 297594 iterations : Training Loss =  0.010249824793414929; Validation Loss = 0.01923192828714494\n",
            "Cost after 297595 iterations : Training Loss =  0.010249816733157818; Validation Loss = 0.01923192524965782\n",
            "Cost after 297596 iterations : Training Loss =  0.010249808672981971; Validation Loss = 0.019231922212213697\n",
            "Cost after 297597 iterations : Training Loss =  0.010249800612887619; Validation Loss = 0.01923191917481249\n",
            "Cost after 297598 iterations : Training Loss =  0.010249792552874682; Validation Loss = 0.019231916137454955\n",
            "Cost after 297599 iterations : Training Loss =  0.01024978449294309; Validation Loss = 0.01923191310014055\n",
            "Cost after 297600 iterations : Training Loss =  0.010249776433092874; Validation Loss = 0.019231910062869673\n",
            "Cost after 297601 iterations : Training Loss =  0.010249768373324048; Validation Loss = 0.019231907025642193\n",
            "Cost after 297602 iterations : Training Loss =  0.010249760313636695; Validation Loss = 0.019231903988457432\n",
            "Cost after 297603 iterations : Training Loss =  0.010249752254030528; Validation Loss = 0.019231900951316043\n",
            "Cost after 297604 iterations : Training Loss =  0.010249744194505847; Validation Loss = 0.019231897914217897\n",
            "Cost after 297605 iterations : Training Loss =  0.010249736135062477; Validation Loss = 0.019231894877163473\n",
            "Cost after 297606 iterations : Training Loss =  0.010249728075700468; Validation Loss = 0.019231891840152313\n",
            "Cost after 297607 iterations : Training Loss =  0.01024972001641992; Validation Loss = 0.019231888803184355\n",
            "Cost after 297608 iterations : Training Loss =  0.010249711957220646; Validation Loss = 0.019231885766259362\n",
            "Cost after 297609 iterations : Training Loss =  0.010249703898102749; Validation Loss = 0.019231882729377862\n",
            "Cost after 297610 iterations : Training Loss =  0.010249695839066325; Validation Loss = 0.01923187969253978\n",
            "Cost after 297611 iterations : Training Loss =  0.01024968778011122; Validation Loss = 0.019231876655744717\n",
            "Cost after 297612 iterations : Training Loss =  0.010249679721237486; Validation Loss = 0.019231873618992826\n",
            "Cost after 297613 iterations : Training Loss =  0.010249671662445056; Validation Loss = 0.01923187058228471\n",
            "Cost after 297614 iterations : Training Loss =  0.010249663603734088; Validation Loss = 0.019231867545619424\n",
            "Cost after 297615 iterations : Training Loss =  0.010249655545104399; Validation Loss = 0.019231864508997575\n",
            "Cost after 297616 iterations : Training Loss =  0.01024964748655602; Validation Loss = 0.019231861472419034\n",
            "Cost after 297617 iterations : Training Loss =  0.01024963942808913; Validation Loss = 0.019231858435883757\n",
            "Cost after 297618 iterations : Training Loss =  0.010249631369703563; Validation Loss = 0.01923185539939182\n",
            "Cost after 297619 iterations : Training Loss =  0.010249623311399258; Validation Loss = 0.019231852362943273\n",
            "Cost after 297620 iterations : Training Loss =  0.01024961525317647; Validation Loss = 0.0192318493265376\n",
            "Cost after 297621 iterations : Training Loss =  0.010249607195034989; Validation Loss = 0.01923184629017528\n",
            "Cost after 297622 iterations : Training Loss =  0.010249599136974775; Validation Loss = 0.019231843253856563\n",
            "Cost after 297623 iterations : Training Loss =  0.010249591078995987; Validation Loss = 0.01923184021758073\n",
            "Cost after 297624 iterations : Training Loss =  0.010249583021098586; Validation Loss = 0.01923183718134808\n",
            "Cost after 297625 iterations : Training Loss =  0.01024957496328249; Validation Loss = 0.01923183414515889\n",
            "Cost after 297626 iterations : Training Loss =  0.010249566905547734; Validation Loss = 0.019231831109012706\n",
            "Cost after 297627 iterations : Training Loss =  0.010249558847894398; Validation Loss = 0.019231828072910163\n",
            "Cost after 297628 iterations : Training Loss =  0.010249550790322309; Validation Loss = 0.01923182503685065\n",
            "Cost after 297629 iterations : Training Loss =  0.010249542732831607; Validation Loss = 0.019231822000834557\n",
            "Cost after 297630 iterations : Training Loss =  0.010249534675422356; Validation Loss = 0.019231818964861588\n",
            "Cost after 297631 iterations : Training Loss =  0.010249526618094338; Validation Loss = 0.019231815928931778\n",
            "Cost after 297632 iterations : Training Loss =  0.010249518560847656; Validation Loss = 0.019231812893045742\n",
            "Cost after 297633 iterations : Training Loss =  0.010249510503682356; Validation Loss = 0.01923180985720276\n",
            "Cost after 297634 iterations : Training Loss =  0.010249502446598375; Validation Loss = 0.019231806821402818\n",
            "Cost after 297635 iterations : Training Loss =  0.010249494389595696; Validation Loss = 0.019231803785646342\n",
            "Cost after 297636 iterations : Training Loss =  0.010249486332674498; Validation Loss = 0.019231800749932897\n",
            "Cost after 297637 iterations : Training Loss =  0.010249478275834539; Validation Loss = 0.01923179771426266\n",
            "Cost after 297638 iterations : Training Loss =  0.010249470219075952; Validation Loss = 0.019231794678635762\n",
            "Cost after 297639 iterations : Training Loss =  0.010249462162398623; Validation Loss = 0.019231791643052106\n",
            "Cost after 297640 iterations : Training Loss =  0.010249454105802764; Validation Loss = 0.019231788607511773\n",
            "Cost after 297641 iterations : Training Loss =  0.010249446049288171; Validation Loss = 0.01923178557201475\n",
            "Cost after 297642 iterations : Training Loss =  0.01024943799285484; Validation Loss = 0.019231782536560785\n",
            "Cost after 297643 iterations : Training Loss =  0.010249429936503008; Validation Loss = 0.01923177950115024\n",
            "Cost after 297644 iterations : Training Loss =  0.010249421880232394; Validation Loss = 0.01923177646578295\n",
            "Cost after 297645 iterations : Training Loss =  0.010249413824043149; Validation Loss = 0.019231773430458735\n",
            "Cost after 297646 iterations : Training Loss =  0.010249405767935146; Validation Loss = 0.01923177039517786\n",
            "Cost after 297647 iterations : Training Loss =  0.010249397711908626; Validation Loss = 0.019231767359940446\n",
            "Cost after 297648 iterations : Training Loss =  0.010249389655963233; Validation Loss = 0.019231764324745913\n",
            "Cost after 297649 iterations : Training Loss =  0.010249381600099406; Validation Loss = 0.019231761289594952\n",
            "Cost after 297650 iterations : Training Loss =  0.010249373544316721; Validation Loss = 0.01923175825448718\n",
            "Cost after 297651 iterations : Training Loss =  0.010249365488615351; Validation Loss = 0.019231755219422546\n",
            "Cost after 297652 iterations : Training Loss =  0.0102493574329955; Validation Loss = 0.019231752184401138\n",
            "Cost after 297653 iterations : Training Loss =  0.010249349377456838; Validation Loss = 0.019231749149423022\n",
            "Cost after 297654 iterations : Training Loss =  0.010249341321999512; Validation Loss = 0.019231746114488194\n",
            "Cost after 297655 iterations : Training Loss =  0.010249333266623499; Validation Loss = 0.019231743079596506\n",
            "Cost after 297656 iterations : Training Loss =  0.010249325211328706; Validation Loss = 0.019231740044747918\n",
            "Cost after 297657 iterations : Training Loss =  0.010249317156115354; Validation Loss = 0.019231737009942764\n",
            "Cost after 297658 iterations : Training Loss =  0.010249309100983399; Validation Loss = 0.01923173397518121\n",
            "Cost after 297659 iterations : Training Loss =  0.010249301045932665; Validation Loss = 0.019231730940462422\n",
            "Cost after 297660 iterations : Training Loss =  0.010249292990963228; Validation Loss = 0.01923172790578691\n",
            "Cost after 297661 iterations : Training Loss =  0.010249284936075168; Validation Loss = 0.019231724871154505\n",
            "Cost after 297662 iterations : Training Loss =  0.010249276881268327; Validation Loss = 0.019231721836565876\n",
            "Cost after 297663 iterations : Training Loss =  0.01024926882654285; Validation Loss = 0.019231718802019807\n",
            "Cost after 297664 iterations : Training Loss =  0.01024926077189869; Validation Loss = 0.019231715767517008\n",
            "Cost after 297665 iterations : Training Loss =  0.010249252717335866; Validation Loss = 0.019231712733057717\n",
            "Cost after 297666 iterations : Training Loss =  0.010249244662854307; Validation Loss = 0.01923170969864172\n",
            "Cost after 297667 iterations : Training Loss =  0.010249236608454034; Validation Loss = 0.019231706664269\n",
            "Cost after 297668 iterations : Training Loss =  0.010249228554135103; Validation Loss = 0.019231703629938982\n",
            "Cost after 297669 iterations : Training Loss =  0.010249220499897547; Validation Loss = 0.019231700595652605\n",
            "Cost after 297670 iterations : Training Loss =  0.01024921244574118; Validation Loss = 0.019231697561409567\n",
            "Cost after 297671 iterations : Training Loss =  0.010249204391666208; Validation Loss = 0.01923169452720972\n",
            "Cost after 297672 iterations : Training Loss =  0.010249196337672474; Validation Loss = 0.019231691493052906\n",
            "Cost after 297673 iterations : Training Loss =  0.01024918828376002; Validation Loss = 0.01923168845893938\n",
            "Cost after 297674 iterations : Training Loss =  0.010249180229928965; Validation Loss = 0.01923168542486897\n",
            "Cost after 297675 iterations : Training Loss =  0.010249172176179138; Validation Loss = 0.01923168239084192\n",
            "Cost after 297676 iterations : Training Loss =  0.010249164122510581; Validation Loss = 0.01923167935685803\n",
            "Cost after 297677 iterations : Training Loss =  0.010249156068923446; Validation Loss = 0.01923167632291732\n",
            "Cost after 297678 iterations : Training Loss =  0.01024914801541747; Validation Loss = 0.01923167328901994\n",
            "Cost after 297679 iterations : Training Loss =  0.010249139961992833; Validation Loss = 0.019231670255165745\n",
            "Cost after 297680 iterations : Training Loss =  0.010249131908649525; Validation Loss = 0.019231667221354797\n",
            "Cost after 297681 iterations : Training Loss =  0.01024912385538759; Validation Loss = 0.01923166418758708\n",
            "Cost after 297682 iterations : Training Loss =  0.010249115802206782; Validation Loss = 0.01923166115386289\n",
            "Cost after 297683 iterations : Training Loss =  0.010249107749107311; Validation Loss = 0.019231658120181752\n",
            "Cost after 297684 iterations : Training Loss =  0.010249099696089175; Validation Loss = 0.01923165508654354\n",
            "Cost after 297685 iterations : Training Loss =  0.010249091643152283; Validation Loss = 0.01923165205294908\n",
            "Cost after 297686 iterations : Training Loss =  0.010249083590296673; Validation Loss = 0.019231649019397376\n",
            "Cost after 297687 iterations : Training Loss =  0.010249075537522323; Validation Loss = 0.019231645985889025\n",
            "Cost after 297688 iterations : Training Loss =  0.01024906748482937; Validation Loss = 0.019231642952423866\n",
            "Cost after 297689 iterations : Training Loss =  0.010249059432217695; Validation Loss = 0.01923163991900211\n",
            "Cost after 297690 iterations : Training Loss =  0.010249051379687193; Validation Loss = 0.019231636885623203\n",
            "Cost after 297691 iterations : Training Loss =  0.01024904332723805; Validation Loss = 0.019231633852287582\n",
            "Cost after 297692 iterations : Training Loss =  0.010249035274870213; Validation Loss = 0.019231630818995343\n",
            "Cost after 297693 iterations : Training Loss =  0.010249027222583593; Validation Loss = 0.01923162778574602\n",
            "Cost after 297694 iterations : Training Loss =  0.010249019170378236; Validation Loss = 0.01923162475254029\n",
            "Cost after 297695 iterations : Training Loss =  0.010249011118254195; Validation Loss = 0.019231621719377624\n",
            "Cost after 297696 iterations : Training Loss =  0.010249003066211475; Validation Loss = 0.0192316186862582\n",
            "Cost after 297697 iterations : Training Loss =  0.010248995014249933; Validation Loss = 0.019231615653181898\n",
            "Cost after 297698 iterations : Training Loss =  0.01024898696236977; Validation Loss = 0.019231612620148934\n",
            "Cost after 297699 iterations : Training Loss =  0.010248978910570803; Validation Loss = 0.01923160958715876\n",
            "Cost after 297700 iterations : Training Loss =  0.010248970858853195; Validation Loss = 0.01923160655421226\n",
            "Cost after 297701 iterations : Training Loss =  0.010248962807216789; Validation Loss = 0.01923160352130882\n",
            "Cost after 297702 iterations : Training Loss =  0.010248954755661618; Validation Loss = 0.019231600488448852\n",
            "Cost after 297703 iterations : Training Loss =  0.010248946704187802; Validation Loss = 0.019231597455631914\n",
            "Cost after 297704 iterations : Training Loss =  0.010248938652795225; Validation Loss = 0.019231594422858017\n",
            "Cost after 297705 iterations : Training Loss =  0.010248930601483883; Validation Loss = 0.01923159139012748\n",
            "Cost after 297706 iterations : Training Loss =  0.010248922550253852; Validation Loss = 0.019231588357440207\n",
            "Cost after 297707 iterations : Training Loss =  0.010248914499105057; Validation Loss = 0.019231585324795877\n",
            "Cost after 297708 iterations : Training Loss =  0.010248906448037587; Validation Loss = 0.019231582292195116\n",
            "Cost after 297709 iterations : Training Loss =  0.010248898397051313; Validation Loss = 0.019231579259637296\n",
            "Cost after 297710 iterations : Training Loss =  0.010248890346146376; Validation Loss = 0.019231576227122543\n",
            "Cost after 297711 iterations : Training Loss =  0.010248882295322661; Validation Loss = 0.019231573194651047\n",
            "Cost after 297712 iterations : Training Loss =  0.010248874244580113; Validation Loss = 0.019231570162223068\n",
            "Cost after 297713 iterations : Training Loss =  0.010248866193918989; Validation Loss = 0.01923156712983823\n",
            "Cost after 297714 iterations : Training Loss =  0.010248858143339; Validation Loss = 0.01923156409749626\n",
            "Cost after 297715 iterations : Training Loss =  0.01024885009284034; Validation Loss = 0.019231561065197786\n",
            "Cost after 297716 iterations : Training Loss =  0.010248842042422952; Validation Loss = 0.019231558032942384\n",
            "Cost after 297717 iterations : Training Loss =  0.01024883399208669; Validation Loss = 0.019231555000730177\n",
            "Cost after 297718 iterations : Training Loss =  0.010248825941831808; Validation Loss = 0.019231551968561307\n",
            "Cost after 297719 iterations : Training Loss =  0.010248817891658173; Validation Loss = 0.019231548936435333\n",
            "Cost after 297720 iterations : Training Loss =  0.010248809841565678; Validation Loss = 0.019231545904352713\n",
            "Cost after 297721 iterations : Training Loss =  0.010248801791554503; Validation Loss = 0.01923154287231316\n",
            "Cost after 297722 iterations : Training Loss =  0.010248793741624625; Validation Loss = 0.019231539840316815\n",
            "Cost after 297723 iterations : Training Loss =  0.010248785691775945; Validation Loss = 0.01923153680836383\n",
            "Cost after 297724 iterations : Training Loss =  0.010248777642008567; Validation Loss = 0.019231533776453966\n",
            "Cost after 297725 iterations : Training Loss =  0.010248769592322366; Validation Loss = 0.01923153074458719\n",
            "Cost after 297726 iterations : Training Loss =  0.01024876154271743; Validation Loss = 0.019231527712763683\n",
            "Cost after 297727 iterations : Training Loss =  0.010248753493193749; Validation Loss = 0.019231524680983107\n",
            "Cost after 297728 iterations : Training Loss =  0.010248745443751277; Validation Loss = 0.019231521649245877\n",
            "Cost after 297729 iterations : Training Loss =  0.010248737394390176; Validation Loss = 0.019231518617552366\n",
            "Cost after 297730 iterations : Training Loss =  0.01024872934511023; Validation Loss = 0.019231515585901516\n",
            "Cost after 297731 iterations : Training Loss =  0.010248721295911475; Validation Loss = 0.019231512554293697\n",
            "Cost after 297732 iterations : Training Loss =  0.01024871324679396; Validation Loss = 0.019231509522729343\n",
            "Cost after 297733 iterations : Training Loss =  0.010248705197757765; Validation Loss = 0.019231506491208153\n",
            "Cost after 297734 iterations : Training Loss =  0.010248697148802768; Validation Loss = 0.019231503459730067\n",
            "Cost after 297735 iterations : Training Loss =  0.010248689099929018; Validation Loss = 0.019231500428295325\n",
            "Cost after 297736 iterations : Training Loss =  0.01024868105113648; Validation Loss = 0.019231497396903302\n",
            "Cost after 297737 iterations : Training Loss =  0.01024867300242517; Validation Loss = 0.019231494365554873\n",
            "Cost after 297738 iterations : Training Loss =  0.010248664953795113; Validation Loss = 0.019231491334249555\n",
            "Cost after 297739 iterations : Training Loss =  0.010248656905246381; Validation Loss = 0.019231488302987324\n",
            "Cost after 297740 iterations : Training Loss =  0.0102486488567787; Validation Loss = 0.019231485271768493\n",
            "Cost after 297741 iterations : Training Loss =  0.010248640808392311; Validation Loss = 0.019231482240592474\n",
            "Cost after 297742 iterations : Training Loss =  0.010248632760087202; Validation Loss = 0.019231479209459803\n",
            "Cost after 297743 iterations : Training Loss =  0.01024862471186335; Validation Loss = 0.01923147617837023\n",
            "Cost after 297744 iterations : Training Loss =  0.010248616663720622; Validation Loss = 0.019231473147324287\n",
            "Cost after 297745 iterations : Training Loss =  0.010248608615659216; Validation Loss = 0.019231470116321165\n",
            "Cost after 297746 iterations : Training Loss =  0.010248600567678906; Validation Loss = 0.019231467085361116\n",
            "Cost after 297747 iterations : Training Loss =  0.010248592519779918; Validation Loss = 0.01923146405444429\n",
            "Cost after 297748 iterations : Training Loss =  0.010248584471962198; Validation Loss = 0.01923146102357072\n",
            "Cost after 297749 iterations : Training Loss =  0.010248576424225555; Validation Loss = 0.019231457992740536\n",
            "Cost after 297750 iterations : Training Loss =  0.010248568376570257; Validation Loss = 0.019231454961953207\n",
            "Cost after 297751 iterations : Training Loss =  0.010248560328996162; Validation Loss = 0.019231451931209106\n",
            "Cost after 297752 iterations : Training Loss =  0.01024855228150328; Validation Loss = 0.019231448900508416\n",
            "Cost after 297753 iterations : Training Loss =  0.01024854423409155; Validation Loss = 0.019231445869850494\n",
            "Cost after 297754 iterations : Training Loss =  0.010248536186761064; Validation Loss = 0.019231442839235988\n",
            "Cost after 297755 iterations : Training Loss =  0.01024852813951177; Validation Loss = 0.019231439808664565\n",
            "Cost after 297756 iterations : Training Loss =  0.010248520092343739; Validation Loss = 0.019231436778136046\n",
            "Cost after 297757 iterations : Training Loss =  0.010248512045256923; Validation Loss = 0.019231433747651155\n",
            "Cost after 297758 iterations : Training Loss =  0.010248503998251344; Validation Loss = 0.019231430717209164\n",
            "Cost after 297759 iterations : Training Loss =  0.01024849595132685; Validation Loss = 0.01923142768681047\n",
            "Cost after 297760 iterations : Training Loss =  0.01024848790448375; Validation Loss = 0.019231424656454714\n",
            "Cost after 297761 iterations : Training Loss =  0.010248479857721718; Validation Loss = 0.019231421626142366\n",
            "Cost after 297762 iterations : Training Loss =  0.010248471811040964; Validation Loss = 0.01923141859587302\n",
            "Cost after 297763 iterations : Training Loss =  0.010248463764441376; Validation Loss = 0.01923141556564705\n",
            "Cost after 297764 iterations : Training Loss =  0.01024845571792303; Validation Loss = 0.019231412535463864\n",
            "Cost after 297765 iterations : Training Loss =  0.010248447671485863; Validation Loss = 0.019231409505324037\n",
            "Cost after 297766 iterations : Training Loss =  0.010248439625129972; Validation Loss = 0.019231406475227423\n",
            "Cost after 297767 iterations : Training Loss =  0.010248431578855093; Validation Loss = 0.019231403445174277\n",
            "Cost after 297768 iterations : Training Loss =  0.010248423532661578; Validation Loss = 0.019231400415163462\n",
            "Cost after 297769 iterations : Training Loss =  0.010248415486549213; Validation Loss = 0.019231397385196504\n",
            "Cost after 297770 iterations : Training Loss =  0.010248407440518088; Validation Loss = 0.019231394355272383\n",
            "Cost after 297771 iterations : Training Loss =  0.010248399394568153; Validation Loss = 0.019231391325391506\n",
            "Cost after 297772 iterations : Training Loss =  0.010248391348699412; Validation Loss = 0.019231388295553785\n",
            "Cost after 297773 iterations : Training Loss =  0.010248383302911848; Validation Loss = 0.019231385265759033\n",
            "Cost after 297774 iterations : Training Loss =  0.010248375257205442; Validation Loss = 0.01923138223600776\n",
            "Cost after 297775 iterations : Training Loss =  0.010248367211580248; Validation Loss = 0.019231379206299543\n",
            "Cost after 297776 iterations : Training Loss =  0.01024835916603628; Validation Loss = 0.01923137617663431\n",
            "Cost after 297777 iterations : Training Loss =  0.010248351120573526; Validation Loss = 0.019231373147012506\n",
            "Cost after 297778 iterations : Training Loss =  0.010248343075191995; Validation Loss = 0.01923137011743366\n",
            "Cost after 297779 iterations : Training Loss =  0.01024833502989148; Validation Loss = 0.019231367087898025\n",
            "Cost after 297780 iterations : Training Loss =  0.01024832698467228; Validation Loss = 0.019231364058405233\n",
            "Cost after 297781 iterations : Training Loss =  0.010248318939534274; Validation Loss = 0.01923136102895595\n",
            "Cost after 297782 iterations : Training Loss =  0.010248310894477444; Validation Loss = 0.01923135799954979\n",
            "Cost after 297783 iterations : Training Loss =  0.01024830284950179; Validation Loss = 0.019231354970186593\n",
            "Cost after 297784 iterations : Training Loss =  0.010248294804607308; Validation Loss = 0.01923135194086633\n",
            "Cost after 297785 iterations : Training Loss =  0.010248286759794055; Validation Loss = 0.019231348911589636\n",
            "Cost after 297786 iterations : Training Loss =  0.010248278715061931; Validation Loss = 0.019231345882355765\n",
            "Cost after 297787 iterations : Training Loss =  0.010248270670410994; Validation Loss = 0.01923134285316562\n",
            "Cost after 297788 iterations : Training Loss =  0.010248262625841296; Validation Loss = 0.01923133982401801\n",
            "Cost after 297789 iterations : Training Loss =  0.010248254581352794; Validation Loss = 0.019231336794913654\n",
            "Cost after 297790 iterations : Training Loss =  0.01024824653694535; Validation Loss = 0.019231333765852558\n",
            "Cost after 297791 iterations : Training Loss =  0.01024823849261913; Validation Loss = 0.019231330736835008\n",
            "Cost after 297792 iterations : Training Loss =  0.010248230448374038; Validation Loss = 0.019231327707860048\n",
            "Cost after 297793 iterations : Training Loss =  0.010248222404210276; Validation Loss = 0.01923132467892827\n",
            "Cost after 297794 iterations : Training Loss =  0.010248214360127518; Validation Loss = 0.019231321650039858\n",
            "Cost after 297795 iterations : Training Loss =  0.010248206316126043; Validation Loss = 0.01923131862119464\n",
            "Cost after 297796 iterations : Training Loss =  0.010248198272205675; Validation Loss = 0.019231315592392165\n",
            "Cost after 297797 iterations : Training Loss =  0.010248190228366512; Validation Loss = 0.019231312563633102\n",
            "Cost after 297798 iterations : Training Loss =  0.010248182184608547; Validation Loss = 0.019231309534916922\n",
            "Cost after 297799 iterations : Training Loss =  0.010248174140931736; Validation Loss = 0.019231306506243852\n",
            "Cost after 297800 iterations : Training Loss =  0.010248166097336057; Validation Loss = 0.0192313034776142\n",
            "Cost after 297801 iterations : Training Loss =  0.010248158053821512; Validation Loss = 0.01923130044902754\n",
            "Cost after 297802 iterations : Training Loss =  0.010248150010388257; Validation Loss = 0.01923129742048361\n",
            "Cost after 297803 iterations : Training Loss =  0.010248141967036058; Validation Loss = 0.01923129439198339\n",
            "Cost after 297804 iterations : Training Loss =  0.010248133923765097; Validation Loss = 0.01923129136352619\n",
            "Cost after 297805 iterations : Training Loss =  0.010248125880575292; Validation Loss = 0.019231288335112114\n",
            "Cost after 297806 iterations : Training Loss =  0.010248117837466583; Validation Loss = 0.019231285306740942\n",
            "Cost after 297807 iterations : Training Loss =  0.010248109794439096; Validation Loss = 0.01923128227841296\n",
            "Cost after 297808 iterations : Training Loss =  0.010248101751492728; Validation Loss = 0.019231279250128266\n",
            "Cost after 297809 iterations : Training Loss =  0.010248093708627497; Validation Loss = 0.0192312762218867\n",
            "Cost after 297810 iterations : Training Loss =  0.010248085665843485; Validation Loss = 0.01923127319368807\n",
            "Cost after 297811 iterations : Training Loss =  0.010248077623140554; Validation Loss = 0.01923127016553285\n",
            "Cost after 297812 iterations : Training Loss =  0.01024806958051895; Validation Loss = 0.01923126713742048\n",
            "Cost after 297813 iterations : Training Loss =  0.010248061537978312; Validation Loss = 0.019231264109351522\n",
            "Cost after 297814 iterations : Training Loss =  0.010248053495518826; Validation Loss = 0.019231261081325314\n",
            "Cost after 297815 iterations : Training Loss =  0.01024804545314065; Validation Loss = 0.01923125805334274\n",
            "Cost after 297816 iterations : Training Loss =  0.010248037410843496; Validation Loss = 0.01923125502540297\n",
            "Cost after 297817 iterations : Training Loss =  0.010248029368627542; Validation Loss = 0.01923125199750633\n",
            "Cost after 297818 iterations : Training Loss =  0.010248021326492726; Validation Loss = 0.019231248969652674\n",
            "Cost after 297819 iterations : Training Loss =  0.010248013284439004; Validation Loss = 0.019231245941842282\n",
            "Cost after 297820 iterations : Training Loss =  0.010248005242466508; Validation Loss = 0.019231242914074933\n",
            "Cost after 297821 iterations : Training Loss =  0.010247997200575162; Validation Loss = 0.019231239886350986\n",
            "Cost after 297822 iterations : Training Loss =  0.010247989158764948; Validation Loss = 0.0192312368586698\n",
            "Cost after 297823 iterations : Training Loss =  0.010247981117035803; Validation Loss = 0.019231233831031785\n",
            "Cost after 297824 iterations : Training Loss =  0.0102479730753879; Validation Loss = 0.019231230803437076\n",
            "Cost after 297825 iterations : Training Loss =  0.01024796503382112; Validation Loss = 0.01923122777588539\n",
            "Cost after 297826 iterations : Training Loss =  0.010247956992335383; Validation Loss = 0.019231224748376793\n",
            "Cost after 297827 iterations : Training Loss =  0.01024794895093087; Validation Loss = 0.019231221720911292\n",
            "Cost after 297828 iterations : Training Loss =  0.01024794090960755; Validation Loss = 0.019231218693488896\n",
            "Cost after 297829 iterations : Training Loss =  0.010247932868365315; Validation Loss = 0.019231215666109292\n",
            "Cost after 297830 iterations : Training Loss =  0.010247924827204196; Validation Loss = 0.019231212638773053\n",
            "Cost after 297831 iterations : Training Loss =  0.01024791678612414; Validation Loss = 0.019231209611479852\n",
            "Cost after 297832 iterations : Training Loss =  0.010247908745125312; Validation Loss = 0.019231206584229582\n",
            "Cost after 297833 iterations : Training Loss =  0.010247900704207606; Validation Loss = 0.019231203557023\n",
            "Cost after 297834 iterations : Training Loss =  0.010247892663371078; Validation Loss = 0.01923120052985917\n",
            "Cost after 297835 iterations : Training Loss =  0.010247884622615604; Validation Loss = 0.019231197502738537\n",
            "Cost after 297836 iterations : Training Loss =  0.010247876581941302; Validation Loss = 0.019231194475661007\n",
            "Cost after 297837 iterations : Training Loss =  0.010247868541348036; Validation Loss = 0.01923119144862645\n",
            "Cost after 297838 iterations : Training Loss =  0.01024786050083599; Validation Loss = 0.01923118842163487\n",
            "Cost after 297839 iterations : Training Loss =  0.010247852460405112; Validation Loss = 0.01923118539468671\n",
            "Cost after 297840 iterations : Training Loss =  0.01024784442005529; Validation Loss = 0.019231182367781628\n",
            "Cost after 297841 iterations : Training Loss =  0.01024783637978661; Validation Loss = 0.01923117934091943\n",
            "Cost after 297842 iterations : Training Loss =  0.010247828339599054; Validation Loss = 0.01923117631410069\n",
            "Cost after 297843 iterations : Training Loss =  0.010247820299492656; Validation Loss = 0.019231173287324918\n",
            "Cost after 297844 iterations : Training Loss =  0.010247812259467288; Validation Loss = 0.01923117026059211\n",
            "Cost after 297845 iterations : Training Loss =  0.01024780421952306; Validation Loss = 0.019231167233902554\n",
            "Cost after 297846 iterations : Training Loss =  0.010247796179659992; Validation Loss = 0.019231164207256093\n",
            "Cost after 297847 iterations : Training Loss =  0.010247788139878056; Validation Loss = 0.019231161180652514\n",
            "Cost after 297848 iterations : Training Loss =  0.010247780100177217; Validation Loss = 0.019231158154092275\n",
            "Cost after 297849 iterations : Training Loss =  0.010247772060557452; Validation Loss = 0.01923115512757477\n",
            "Cost after 297850 iterations : Training Loss =  0.010247764021018877; Validation Loss = 0.01923115210110023\n",
            "Cost after 297851 iterations : Training Loss =  0.010247755981561394; Validation Loss = 0.01923114907466926\n",
            "Cost after 297852 iterations : Training Loss =  0.010247747942184983; Validation Loss = 0.019231146048281387\n",
            "Cost after 297853 iterations : Training Loss =  0.010247739902889732; Validation Loss = 0.01923114302193649\n",
            "Cost after 297854 iterations : Training Loss =  0.010247731863675549; Validation Loss = 0.019231139995634597\n",
            "Cost after 297855 iterations : Training Loss =  0.010247723824542521; Validation Loss = 0.019231136969375905\n",
            "Cost after 297856 iterations : Training Loss =  0.010247715785490644; Validation Loss = 0.019231133943160308\n",
            "Cost after 297857 iterations : Training Loss =  0.010247707746519761; Validation Loss = 0.019231130916987478\n",
            "Cost after 297858 iterations : Training Loss =  0.010247699707629987; Validation Loss = 0.019231127890858186\n",
            "Cost after 297859 iterations : Training Loss =  0.010247691668821413; Validation Loss = 0.01923112486477185\n",
            "Cost after 297860 iterations : Training Loss =  0.01024768363009384; Validation Loss = 0.01923112183872853\n",
            "Cost after 297861 iterations : Training Loss =  0.010247675591447448; Validation Loss = 0.0192311188127283\n",
            "Cost after 297862 iterations : Training Loss =  0.010247667552882167; Validation Loss = 0.01923111578677099\n",
            "Cost after 297863 iterations : Training Loss =  0.01024765951439796; Validation Loss = 0.019231112760856615\n",
            "Cost after 297864 iterations : Training Loss =  0.01024765147599482; Validation Loss = 0.019231109734985694\n",
            "Cost after 297865 iterations : Training Loss =  0.01024764343767284; Validation Loss = 0.019231106709157966\n",
            "Cost after 297866 iterations : Training Loss =  0.010247635399431935; Validation Loss = 0.019231103683372993\n",
            "Cost after 297867 iterations : Training Loss =  0.010247627361272103; Validation Loss = 0.019231100657631166\n",
            "Cost after 297868 iterations : Training Loss =  0.010247619323193425; Validation Loss = 0.01923109763193245\n",
            "Cost after 297869 iterations : Training Loss =  0.010247611285195803; Validation Loss = 0.019231094606277185\n",
            "Cost after 297870 iterations : Training Loss =  0.010247603247279222; Validation Loss = 0.019231091580664602\n",
            "Cost after 297871 iterations : Training Loss =  0.010247595209443863; Validation Loss = 0.019231088555095185\n",
            "Cost after 297872 iterations : Training Loss =  0.01024758717168953; Validation Loss = 0.019231085529569134\n",
            "Cost after 297873 iterations : Training Loss =  0.010247579134016287; Validation Loss = 0.019231082504085745\n",
            "Cost after 297874 iterations : Training Loss =  0.010247571096424117; Validation Loss = 0.01923107947864547\n",
            "Cost after 297875 iterations : Training Loss =  0.010247563058913094; Validation Loss = 0.019231076453248012\n",
            "Cost after 297876 iterations : Training Loss =  0.010247555021483019; Validation Loss = 0.01923107342789387\n",
            "Cost after 297877 iterations : Training Loss =  0.010247546984134126; Validation Loss = 0.019231070402582998\n",
            "Cost after 297878 iterations : Training Loss =  0.010247538946866227; Validation Loss = 0.019231067377315135\n",
            "Cost after 297879 iterations : Training Loss =  0.010247530909679574; Validation Loss = 0.019231064352090425\n",
            "Cost after 297880 iterations : Training Loss =  0.010247522872573953; Validation Loss = 0.019231061326908626\n",
            "Cost after 297881 iterations : Training Loss =  0.010247514835549351; Validation Loss = 0.019231058301770056\n",
            "Cost after 297882 iterations : Training Loss =  0.010247506798605937; Validation Loss = 0.019231055276674635\n",
            "Cost after 297883 iterations : Training Loss =  0.010247498761743523; Validation Loss = 0.019231052251622024\n",
            "Cost after 297884 iterations : Training Loss =  0.010247490724962179; Validation Loss = 0.019231049226612756\n",
            "Cost after 297885 iterations : Training Loss =  0.010247482688261967; Validation Loss = 0.019231046201646274\n",
            "Cost after 297886 iterations : Training Loss =  0.010247474651642774; Validation Loss = 0.019231043176722983\n",
            "Cost after 297887 iterations : Training Loss =  0.010247466615104677; Validation Loss = 0.019231040151842703\n",
            "Cost after 297888 iterations : Training Loss =  0.01024745857864767; Validation Loss = 0.019231037127005583\n",
            "Cost after 297889 iterations : Training Loss =  0.010247450542271766; Validation Loss = 0.019231034102211247\n",
            "Cost after 297890 iterations : Training Loss =  0.010247442505976916; Validation Loss = 0.019231031077459992\n",
            "Cost after 297891 iterations : Training Loss =  0.010247434469763068; Validation Loss = 0.01923102805275204\n",
            "Cost after 297892 iterations : Training Loss =  0.010247426433630354; Validation Loss = 0.019231025028086875\n",
            "Cost after 297893 iterations : Training Loss =  0.010247418397578666; Validation Loss = 0.019231022003464934\n",
            "Cost after 297894 iterations : Training Loss =  0.010247410361608122; Validation Loss = 0.019231018978885923\n",
            "Cost after 297895 iterations : Training Loss =  0.01024740232571858; Validation Loss = 0.019231015954350374\n",
            "Cost after 297896 iterations : Training Loss =  0.010247394289910186; Validation Loss = 0.019231012929857603\n",
            "Cost after 297897 iterations : Training Loss =  0.010247386254182776; Validation Loss = 0.019231009905407877\n",
            "Cost after 297898 iterations : Training Loss =  0.010247378218536494; Validation Loss = 0.019231006881000993\n",
            "Cost after 297899 iterations : Training Loss =  0.010247370182971224; Validation Loss = 0.01923100385663724\n",
            "Cost after 297900 iterations : Training Loss =  0.010247362147486998; Validation Loss = 0.019231000832317144\n",
            "Cost after 297901 iterations : Training Loss =  0.010247354112083929; Validation Loss = 0.0192309978080396\n",
            "Cost after 297902 iterations : Training Loss =  0.010247346076761839; Validation Loss = 0.019230994783805168\n",
            "Cost after 297903 iterations : Training Loss =  0.010247338041520813; Validation Loss = 0.019230991759613767\n",
            "Cost after 297904 iterations : Training Loss =  0.010247330006360869; Validation Loss = 0.019230988735465526\n",
            "Cost after 297905 iterations : Training Loss =  0.010247321971281968; Validation Loss = 0.019230985711360112\n",
            "Cost after 297906 iterations : Training Loss =  0.010247313936284124; Validation Loss = 0.019230982687297872\n",
            "Cost after 297907 iterations : Training Loss =  0.010247305901367266; Validation Loss = 0.01923097966327889\n",
            "Cost after 297908 iterations : Training Loss =  0.01024729786653159; Validation Loss = 0.01923097663930283\n",
            "Cost after 297909 iterations : Training Loss =  0.01024728983177694; Validation Loss = 0.01923097361536958\n",
            "Cost after 297910 iterations : Training Loss =  0.010247281797103307; Validation Loss = 0.01923097059147984\n",
            "Cost after 297911 iterations : Training Loss =  0.01024727376251074; Validation Loss = 0.019230967567632935\n",
            "Cost after 297912 iterations : Training Loss =  0.010247265727999215; Validation Loss = 0.019230964543828694\n",
            "Cost after 297913 iterations : Training Loss =  0.010247257693568674; Validation Loss = 0.019230961520067976\n",
            "Cost after 297914 iterations : Training Loss =  0.01024724965921932; Validation Loss = 0.019230958496350318\n",
            "Cost after 297915 iterations : Training Loss =  0.010247241624950932; Validation Loss = 0.01923095547267564\n",
            "Cost after 297916 iterations : Training Loss =  0.010247233590763545; Validation Loss = 0.019230952449043708\n",
            "Cost after 297917 iterations : Training Loss =  0.010247225556657332; Validation Loss = 0.01923094942545497\n",
            "Cost after 297918 iterations : Training Loss =  0.010247217522632013; Validation Loss = 0.019230946401909487\n",
            "Cost after 297919 iterations : Training Loss =  0.010247209488687915; Validation Loss = 0.019230943378406997\n",
            "Cost after 297920 iterations : Training Loss =  0.010247201454824691; Validation Loss = 0.01923094035494726\n",
            "Cost after 297921 iterations : Training Loss =  0.010247193421042583; Validation Loss = 0.01923093733153099\n",
            "Cost after 297922 iterations : Training Loss =  0.010247185387341502; Validation Loss = 0.019230934308157557\n",
            "Cost after 297923 iterations : Training Loss =  0.010247177353721453; Validation Loss = 0.01923093128482729\n",
            "Cost after 297924 iterations : Training Loss =  0.010247169320182452; Validation Loss = 0.019230928261539812\n",
            "Cost after 297925 iterations : Training Loss =  0.010247161286724432; Validation Loss = 0.01923092523829518\n",
            "Cost after 297926 iterations : Training Loss =  0.010247153253347564; Validation Loss = 0.01923092221509407\n",
            "Cost after 297927 iterations : Training Loss =  0.01024714522005164; Validation Loss = 0.019230919191935772\n",
            "Cost after 297928 iterations : Training Loss =  0.01024713718683676; Validation Loss = 0.01923091616882048\n",
            "Cost after 297929 iterations : Training Loss =  0.010247129153702947; Validation Loss = 0.019230913145748287\n",
            "Cost after 297930 iterations : Training Loss =  0.010247121120650108; Validation Loss = 0.019230910122718884\n",
            "Cost after 297931 iterations : Training Loss =  0.010247113087678351; Validation Loss = 0.01923090709973262\n",
            "Cost after 297932 iterations : Training Loss =  0.010247105054787622; Validation Loss = 0.019230904076789488\n",
            "Cost after 297933 iterations : Training Loss =  0.010247097021977895; Validation Loss = 0.01923090105388966\n",
            "Cost after 297934 iterations : Training Loss =  0.010247088989249165; Validation Loss = 0.01923089803103228\n",
            "Cost after 297935 iterations : Training Loss =  0.01024708095660149; Validation Loss = 0.01923089500821804\n",
            "Cost after 297936 iterations : Training Loss =  0.010247072924034858; Validation Loss = 0.019230891985447016\n",
            "Cost after 297937 iterations : Training Loss =  0.010247064891549265; Validation Loss = 0.019230888962718976\n",
            "Cost after 297938 iterations : Training Loss =  0.010247056859144637; Validation Loss = 0.019230885940033877\n",
            "Cost after 297939 iterations : Training Loss =  0.010247048826821084; Validation Loss = 0.01923088291739204\n",
            "Cost after 297940 iterations : Training Loss =  0.010247040794578562; Validation Loss = 0.019230879894793197\n",
            "Cost after 297941 iterations : Training Loss =  0.010247032762417056; Validation Loss = 0.019230876872237127\n",
            "Cost after 297942 iterations : Training Loss =  0.010247024730336481; Validation Loss = 0.019230873849724228\n",
            "Cost after 297943 iterations : Training Loss =  0.01024701669833694; Validation Loss = 0.019230870827254384\n",
            "Cost after 297944 iterations : Training Loss =  0.010247008666418504; Validation Loss = 0.01923086780482754\n",
            "Cost after 297945 iterations : Training Loss =  0.010247000634581052; Validation Loss = 0.019230864782443704\n",
            "Cost after 297946 iterations : Training Loss =  0.010246992602824594; Validation Loss = 0.01923086176010311\n",
            "Cost after 297947 iterations : Training Loss =  0.010246984571149205; Validation Loss = 0.01923085873780526\n",
            "Cost after 297948 iterations : Training Loss =  0.010246976539554723; Validation Loss = 0.01923085571555061\n",
            "Cost after 297949 iterations : Training Loss =  0.010246968508041375; Validation Loss = 0.01923085269333893\n",
            "Cost after 297950 iterations : Training Loss =  0.010246960476608936; Validation Loss = 0.01923084967117004\n",
            "Cost after 297951 iterations : Training Loss =  0.010246952445257545; Validation Loss = 0.019230846649044644\n",
            "Cost after 297952 iterations : Training Loss =  0.010246944413987142; Validation Loss = 0.01923084362696206\n",
            "Cost after 297953 iterations : Training Loss =  0.010246936382797816; Validation Loss = 0.019230840604922562\n",
            "Cost after 297954 iterations : Training Loss =  0.010246928351689355; Validation Loss = 0.019230837582926003\n",
            "Cost after 297955 iterations : Training Loss =  0.010246920320662007; Validation Loss = 0.019230834560972253\n",
            "Cost after 297956 iterations : Training Loss =  0.010246912289715608; Validation Loss = 0.019230831539061334\n",
            "Cost after 297957 iterations : Training Loss =  0.010246904258850316; Validation Loss = 0.019230828517193866\n",
            "Cost after 297958 iterations : Training Loss =  0.010246896228065914; Validation Loss = 0.019230825495369454\n",
            "Cost after 297959 iterations : Training Loss =  0.01024688819736253; Validation Loss = 0.019230822473587657\n",
            "Cost after 297960 iterations : Training Loss =  0.010246880166740191; Validation Loss = 0.019230819451848895\n",
            "Cost after 297961 iterations : Training Loss =  0.010246872136198766; Validation Loss = 0.019230816430153515\n",
            "Cost after 297962 iterations : Training Loss =  0.010246864105738461; Validation Loss = 0.019230813408501236\n",
            "Cost after 297963 iterations : Training Loss =  0.01024685607535909; Validation Loss = 0.019230810386891548\n",
            "Cost after 297964 iterations : Training Loss =  0.01024684804506074; Validation Loss = 0.019230807365324978\n",
            "Cost after 297965 iterations : Training Loss =  0.010246840014843318; Validation Loss = 0.019230804343801228\n",
            "Cost after 297966 iterations : Training Loss =  0.01024683198470691; Validation Loss = 0.019230801322320936\n",
            "Cost after 297967 iterations : Training Loss =  0.010246823954651546; Validation Loss = 0.019230798300883346\n",
            "Cost after 297968 iterations : Training Loss =  0.010246815924677131; Validation Loss = 0.01923079527948874\n",
            "Cost after 297969 iterations : Training Loss =  0.010246807894783794; Validation Loss = 0.019230792258137036\n",
            "Cost after 297970 iterations : Training Loss =  0.010246799864971283; Validation Loss = 0.019230789236828482\n",
            "Cost after 297971 iterations : Training Loss =  0.010246791835239823; Validation Loss = 0.01923078621556336\n",
            "Cost after 297972 iterations : Training Loss =  0.01024678380558945; Validation Loss = 0.019230783194340663\n",
            "Cost after 297973 iterations : Training Loss =  0.010246775776019932; Validation Loss = 0.019230780173161207\n",
            "Cost after 297974 iterations : Training Loss =  0.010246767746531464; Validation Loss = 0.01923077715202451\n",
            "Cost after 297975 iterations : Training Loss =  0.010246759717123941; Validation Loss = 0.01923077413093123\n",
            "Cost after 297976 iterations : Training Loss =  0.010246751687797446; Validation Loss = 0.01923077110988049\n",
            "Cost after 297977 iterations : Training Loss =  0.010246743658551912; Validation Loss = 0.019230768088873064\n",
            "Cost after 297978 iterations : Training Loss =  0.010246735629387405; Validation Loss = 0.019230765067908544\n",
            "Cost after 297979 iterations : Training Loss =  0.010246727600303833; Validation Loss = 0.019230762046986982\n",
            "Cost after 297980 iterations : Training Loss =  0.010246719571301122; Validation Loss = 0.019230759026108525\n",
            "Cost after 297981 iterations : Training Loss =  0.010246711542379548; Validation Loss = 0.01923075600527273\n",
            "Cost after 297982 iterations : Training Loss =  0.010246703513538928; Validation Loss = 0.01923075298448035\n",
            "Cost after 297983 iterations : Training Loss =  0.010246695484779265; Validation Loss = 0.019230749963730503\n",
            "Cost after 297984 iterations : Training Loss =  0.010246687456100577; Validation Loss = 0.019230746943024166\n",
            "Cost after 297985 iterations : Training Loss =  0.010246679427502865; Validation Loss = 0.019230743922360193\n",
            "Cost after 297986 iterations : Training Loss =  0.010246671398986093; Validation Loss = 0.01923074090173977\n",
            "Cost after 297987 iterations : Training Loss =  0.010246663370550328; Validation Loss = 0.0192307378811622\n",
            "Cost after 297988 iterations : Training Loss =  0.01024665534219548; Validation Loss = 0.01923073486062728\n",
            "Cost after 297989 iterations : Training Loss =  0.01024664731392161; Validation Loss = 0.019230731840135833\n",
            "Cost after 297990 iterations : Training Loss =  0.010246639285728772; Validation Loss = 0.019230728819687192\n",
            "Cost after 297991 iterations : Training Loss =  0.010246631257616878; Validation Loss = 0.019230725799281333\n",
            "Cost after 297992 iterations : Training Loss =  0.010246623229585973; Validation Loss = 0.019230722778918945\n",
            "Cost after 297993 iterations : Training Loss =  0.010246615201635967; Validation Loss = 0.019230719758599222\n",
            "Cost after 297994 iterations : Training Loss =  0.010246607173766964; Validation Loss = 0.019230716738322468\n",
            "Cost after 297995 iterations : Training Loss =  0.010246599145978925; Validation Loss = 0.01923071371808872\n",
            "Cost after 297996 iterations : Training Loss =  0.010246591118271801; Validation Loss = 0.019230710697898036\n",
            "Cost after 297997 iterations : Training Loss =  0.010246583090645722; Validation Loss = 0.01923070767775046\n",
            "Cost after 297998 iterations : Training Loss =  0.010246575063100578; Validation Loss = 0.019230704657645533\n",
            "Cost after 297999 iterations : Training Loss =  0.010246567035636383; Validation Loss = 0.01923070163758373\n",
            "Cost after 298000 iterations : Training Loss =  0.010246559008253091; Validation Loss = 0.019230698617564834\n",
            "Cost after 298001 iterations : Training Loss =  0.010246550980950795; Validation Loss = 0.01923069559758911\n",
            "Cost after 298002 iterations : Training Loss =  0.010246542953729494; Validation Loss = 0.019230692577656378\n",
            "Cost after 298003 iterations : Training Loss =  0.010246534926589071; Validation Loss = 0.01923068955776645\n",
            "Cost after 298004 iterations : Training Loss =  0.010246526899529705; Validation Loss = 0.01923068653791946\n",
            "Cost after 298005 iterations : Training Loss =  0.010246518872551172; Validation Loss = 0.019230683518115493\n",
            "Cost after 298006 iterations : Training Loss =  0.010246510845653641; Validation Loss = 0.019230680498354465\n",
            "Cost after 298007 iterations : Training Loss =  0.010246502818837105; Validation Loss = 0.01923067747863674\n",
            "Cost after 298008 iterations : Training Loss =  0.010246494792101412; Validation Loss = 0.019230674458961573\n",
            "Cost after 298009 iterations : Training Loss =  0.010246486765446792; Validation Loss = 0.01923067143932951\n",
            "Cost after 298010 iterations : Training Loss =  0.010246478738873055; Validation Loss = 0.019230668419740683\n",
            "Cost after 298011 iterations : Training Loss =  0.010246470712380294; Validation Loss = 0.019230665400194708\n",
            "Cost after 298012 iterations : Training Loss =  0.010246462685968395; Validation Loss = 0.019230662380691706\n",
            "Cost after 298013 iterations : Training Loss =  0.010246454659637558; Validation Loss = 0.019230659361231674\n",
            "Cost after 298014 iterations : Training Loss =  0.010246446633387585; Validation Loss = 0.019230656341814374\n",
            "Cost after 298015 iterations : Training Loss =  0.010246438607218574; Validation Loss = 0.019230653322440117\n",
            "Cost after 298016 iterations : Training Loss =  0.010246430581130487; Validation Loss = 0.01923065030310891\n",
            "Cost after 298017 iterations : Training Loss =  0.010246422555123319; Validation Loss = 0.01923064728382073\n",
            "Cost after 298018 iterations : Training Loss =  0.010246414529197206; Validation Loss = 0.019230644264575367\n",
            "Cost after 298019 iterations : Training Loss =  0.01024640650335191; Validation Loss = 0.019230641245372968\n",
            "Cost after 298020 iterations : Training Loss =  0.010246398477587553; Validation Loss = 0.019230638226213556\n",
            "Cost after 298021 iterations : Training Loss =  0.010246390451904204; Validation Loss = 0.01923063520709719\n",
            "Cost after 298022 iterations : Training Loss =  0.010246382426301706; Validation Loss = 0.0192306321880236\n",
            "Cost after 298023 iterations : Training Loss =  0.010246374400780293; Validation Loss = 0.019230629168993336\n",
            "Cost after 298024 iterations : Training Loss =  0.01024636637533963; Validation Loss = 0.0192306261500059\n",
            "Cost after 298025 iterations : Training Loss =  0.010246358349979997; Validation Loss = 0.019230623131061397\n",
            "Cost after 298026 iterations : Training Loss =  0.010246350324701312; Validation Loss = 0.01923062011215937\n",
            "Cost after 298027 iterations : Training Loss =  0.010246342299503499; Validation Loss = 0.0192306170933009\n",
            "Cost after 298028 iterations : Training Loss =  0.010246334274386586; Validation Loss = 0.01923061407448501\n",
            "Cost after 298029 iterations : Training Loss =  0.010246326249350665; Validation Loss = 0.019230611055712526\n",
            "Cost after 298030 iterations : Training Loss =  0.010246318224395638; Validation Loss = 0.01923060803698274\n",
            "Cost after 298031 iterations : Training Loss =  0.010246310199521583; Validation Loss = 0.01923060501829572\n",
            "Cost after 298032 iterations : Training Loss =  0.010246302174728446; Validation Loss = 0.01923060199965201\n",
            "Cost after 298033 iterations : Training Loss =  0.010246294150016145; Validation Loss = 0.019230598981051072\n",
            "Cost after 298034 iterations : Training Loss =  0.010246286125384878; Validation Loss = 0.019230595962493473\n",
            "Cost after 298035 iterations : Training Loss =  0.010246278100834465; Validation Loss = 0.019230592943978168\n",
            "Cost after 298036 iterations : Training Loss =  0.0102462700763649; Validation Loss = 0.019230589925506095\n",
            "Cost after 298037 iterations : Training Loss =  0.010246262051976394; Validation Loss = 0.019230586907077147\n",
            "Cost after 298038 iterations : Training Loss =  0.010246254027668707; Validation Loss = 0.01923058388869081\n",
            "Cost after 298039 iterations : Training Loss =  0.010246246003442035; Validation Loss = 0.019230580870347822\n",
            "Cost after 298040 iterations : Training Loss =  0.010246237979296255; Validation Loss = 0.019230577852047327\n",
            "Cost after 298041 iterations : Training Loss =  0.010246229955231272; Validation Loss = 0.019230574833790214\n",
            "Cost after 298042 iterations : Training Loss =  0.010246221931247263; Validation Loss = 0.019230571815575994\n",
            "Cost after 298043 iterations : Training Loss =  0.01024621390734423; Validation Loss = 0.0192305687974048\n",
            "Cost after 298044 iterations : Training Loss =  0.010246205883522068; Validation Loss = 0.019230565779276298\n",
            "Cost after 298045 iterations : Training Loss =  0.010246197859780879; Validation Loss = 0.019230562761190652\n",
            "Cost after 298046 iterations : Training Loss =  0.010246189836120514; Validation Loss = 0.019230559743147895\n",
            "Cost after 298047 iterations : Training Loss =  0.010246181812541133; Validation Loss = 0.019230556725148413\n",
            "Cost after 298048 iterations : Training Loss =  0.01024617378904248; Validation Loss = 0.019230553707191688\n",
            "Cost after 298049 iterations : Training Loss =  0.010246165765624923; Validation Loss = 0.019230550689277814\n",
            "Cost after 298050 iterations : Training Loss =  0.010246157742288174; Validation Loss = 0.019230547671407278\n",
            "Cost after 298051 iterations : Training Loss =  0.010246149719032418; Validation Loss = 0.019230544653579255\n",
            "Cost after 298052 iterations : Training Loss =  0.010246141695857485; Validation Loss = 0.019230541635794116\n",
            "Cost after 298053 iterations : Training Loss =  0.010246133672763467; Validation Loss = 0.019230538618052517\n",
            "Cost after 298054 iterations : Training Loss =  0.01024612564975036; Validation Loss = 0.0192305356003532\n",
            "Cost after 298055 iterations : Training Loss =  0.010246117626818192; Validation Loss = 0.019230532582697107\n",
            "Cost after 298056 iterations : Training Loss =  0.010246109603966854; Validation Loss = 0.01923052956508377\n",
            "Cost after 298057 iterations : Training Loss =  0.010246101581196437; Validation Loss = 0.019230526547513456\n",
            "Cost after 298058 iterations : Training Loss =  0.010246093558506897; Validation Loss = 0.019230523529986186\n",
            "Cost after 298059 iterations : Training Loss =  0.010246085535898331; Validation Loss = 0.01923052051250154\n",
            "Cost after 298060 iterations : Training Loss =  0.010246077513370613; Validation Loss = 0.019230517495060134\n",
            "Cost after 298061 iterations : Training Loss =  0.01024606949092372; Validation Loss = 0.01923051447766208\n",
            "Cost after 298062 iterations : Training Loss =  0.010246061468557829; Validation Loss = 0.01923051146030616\n",
            "Cost after 298063 iterations : Training Loss =  0.01024605344627282; Validation Loss = 0.019230508442993573\n",
            "Cost after 298064 iterations : Training Loss =  0.010246045424068624; Validation Loss = 0.019230505425724152\n",
            "Cost after 298065 iterations : Training Loss =  0.010246037401945376; Validation Loss = 0.019230502408497198\n",
            "Cost after 298066 iterations : Training Loss =  0.010246029379903005; Validation Loss = 0.01923049939131342\n",
            "Cost after 298067 iterations : Training Loss =  0.01024602135794148; Validation Loss = 0.019230496374172557\n",
            "Cost after 298068 iterations : Training Loss =  0.010246013336060842; Validation Loss = 0.019230493357074576\n",
            "Cost after 298069 iterations : Training Loss =  0.010246005314261137; Validation Loss = 0.01923049034001987\n",
            "Cost after 298070 iterations : Training Loss =  0.01024599729254226; Validation Loss = 0.019230487323007976\n",
            "Cost after 298071 iterations : Training Loss =  0.010245989270904312; Validation Loss = 0.019230484306038534\n",
            "Cost after 298072 iterations : Training Loss =  0.010245981249347301; Validation Loss = 0.019230481289112478\n",
            "Cost after 298073 iterations : Training Loss =  0.010245973227871064; Validation Loss = 0.019230478272228926\n",
            "Cost after 298074 iterations : Training Loss =  0.010245965206475782; Validation Loss = 0.01923047525538829\n",
            "Cost after 298075 iterations : Training Loss =  0.010245957185161296; Validation Loss = 0.01923047223859111\n",
            "Cost after 298076 iterations : Training Loss =  0.010245949163927735; Validation Loss = 0.01923046922183674\n",
            "Cost after 298077 iterations : Training Loss =  0.010245941142775095; Validation Loss = 0.019230466205125214\n",
            "Cost after 298078 iterations : Training Loss =  0.010245933121703275; Validation Loss = 0.019230463188456306\n",
            "Cost after 298079 iterations : Training Loss =  0.010245925100712358; Validation Loss = 0.019230460171830794\n",
            "Cost after 298080 iterations : Training Loss =  0.0102459170798023; Validation Loss = 0.019230457155247863\n",
            "Cost after 298081 iterations : Training Loss =  0.010245909058973098; Validation Loss = 0.019230454138707997\n",
            "Cost after 298082 iterations : Training Loss =  0.010245901038224795; Validation Loss = 0.01923045112221089\n",
            "Cost after 298083 iterations : Training Loss =  0.010245893017557328; Validation Loss = 0.019230448105756744\n",
            "Cost after 298084 iterations : Training Loss =  0.010245884996970806; Validation Loss = 0.0192304450893456\n",
            "Cost after 298085 iterations : Training Loss =  0.010245876976465054; Validation Loss = 0.019230442072977305\n",
            "Cost after 298086 iterations : Training Loss =  0.010245868956040221; Validation Loss = 0.019230439056651734\n",
            "Cost after 298087 iterations : Training Loss =  0.010245860935696263; Validation Loss = 0.019230436040369243\n",
            "Cost after 298088 iterations : Training Loss =  0.010245852915433142; Validation Loss = 0.01923043302412949\n",
            "Cost after 298089 iterations : Training Loss =  0.010245844895250922; Validation Loss = 0.019230430007932925\n",
            "Cost after 298090 iterations : Training Loss =  0.010245836875149556; Validation Loss = 0.019230426991779185\n",
            "Cost after 298091 iterations : Training Loss =  0.010245828855129013; Validation Loss = 0.01923042397566814\n",
            "Cost after 298092 iterations : Training Loss =  0.010245820835189332; Validation Loss = 0.019230420959600578\n",
            "Cost after 298093 iterations : Training Loss =  0.010245812815330567; Validation Loss = 0.01923041794357552\n",
            "Cost after 298094 iterations : Training Loss =  0.010245804795552614; Validation Loss = 0.019230414927593587\n",
            "Cost after 298095 iterations : Training Loss =  0.010245796775855498; Validation Loss = 0.019230411911654408\n",
            "Cost after 298096 iterations : Training Loss =  0.010245788756239285; Validation Loss = 0.019230408895758194\n",
            "Cost after 298097 iterations : Training Loss =  0.010245780736703923; Validation Loss = 0.01923040587990501\n",
            "Cost after 298098 iterations : Training Loss =  0.010245772717249425; Validation Loss = 0.01923040286409472\n",
            "Cost after 298099 iterations : Training Loss =  0.01024576469787582; Validation Loss = 0.019230399848326824\n",
            "Cost after 298100 iterations : Training Loss =  0.01024575667858297; Validation Loss = 0.01923039683260226\n",
            "Cost after 298101 iterations : Training Loss =  0.010245748659371024; Validation Loss = 0.019230393816920648\n",
            "Cost after 298102 iterations : Training Loss =  0.010245740640239937; Validation Loss = 0.019230390801281635\n",
            "Cost after 298103 iterations : Training Loss =  0.010245732621189624; Validation Loss = 0.01923038778568567\n",
            "Cost after 298104 iterations : Training Loss =  0.010245724602220192; Validation Loss = 0.019230384770132825\n",
            "Cost after 298105 iterations : Training Loss =  0.01024571658333162; Validation Loss = 0.01923038175462251\n",
            "Cost after 298106 iterations : Training Loss =  0.010245708564523968; Validation Loss = 0.019230378739155018\n",
            "Cost after 298107 iterations : Training Loss =  0.010245700545797124; Validation Loss = 0.019230375723731093\n",
            "Cost after 298108 iterations : Training Loss =  0.010245692527151113; Validation Loss = 0.01923037270834971\n",
            "Cost after 298109 iterations : Training Loss =  0.010245684508585842; Validation Loss = 0.01923036969301101\n",
            "Cost after 298110 iterations : Training Loss =  0.010245676490101541; Validation Loss = 0.019230366677715452\n",
            "Cost after 298111 iterations : Training Loss =  0.010245668471698052; Validation Loss = 0.01923036366246263\n",
            "Cost after 298112 iterations : Training Loss =  0.010245660453375327; Validation Loss = 0.01923036064725278\n",
            "Cost after 298113 iterations : Training Loss =  0.010245652435133555; Validation Loss = 0.019230357632085592\n",
            "Cost after 298114 iterations : Training Loss =  0.010245644416972531; Validation Loss = 0.01923035461696159\n",
            "Cost after 298115 iterations : Training Loss =  0.010245636398892387; Validation Loss = 0.019230351601880524\n",
            "Cost after 298116 iterations : Training Loss =  0.01024562838089317; Validation Loss = 0.019230348586842274\n",
            "Cost after 298117 iterations : Training Loss =  0.010245620362974646; Validation Loss = 0.019230345571846823\n",
            "Cost after 298118 iterations : Training Loss =  0.010245612345136967; Validation Loss = 0.01923034255689451\n",
            "Cost after 298119 iterations : Training Loss =  0.010245604327380156; Validation Loss = 0.019230339541984835\n",
            "Cost after 298120 iterations : Training Loss =  0.01024559630970419; Validation Loss = 0.019230336527118475\n",
            "Cost after 298121 iterations : Training Loss =  0.010245588292109038; Validation Loss = 0.01923033351229449\n",
            "Cost after 298122 iterations : Training Loss =  0.010245580274594735; Validation Loss = 0.019230330497513482\n",
            "Cost after 298123 iterations : Training Loss =  0.010245572257161193; Validation Loss = 0.01923032748277534\n",
            "Cost after 298124 iterations : Training Loss =  0.0102455642398086; Validation Loss = 0.019230324468080415\n",
            "Cost after 298125 iterations : Training Loss =  0.010245556222536714; Validation Loss = 0.019230321453427953\n",
            "Cost after 298126 iterations : Training Loss =  0.010245548205345685; Validation Loss = 0.01923031843881885\n",
            "Cost after 298127 iterations : Training Loss =  0.010245540188235487; Validation Loss = 0.01923031542425215\n",
            "Cost after 298128 iterations : Training Loss =  0.01024553217120612; Validation Loss = 0.019230312409728376\n",
            "Cost after 298129 iterations : Training Loss =  0.010245524154257566; Validation Loss = 0.019230309395247766\n",
            "Cost after 298130 iterations : Training Loss =  0.01024551613738985; Validation Loss = 0.01923030638080989\n",
            "Cost after 298131 iterations : Training Loss =  0.010245508120602929; Validation Loss = 0.01923030336641481\n",
            "Cost after 298132 iterations : Training Loss =  0.010245500103896887; Validation Loss = 0.01923030035206283\n",
            "Cost after 298133 iterations : Training Loss =  0.010245492087271674; Validation Loss = 0.019230297337753492\n",
            "Cost after 298134 iterations : Training Loss =  0.010245484070727185; Validation Loss = 0.019230294323487208\n",
            "Cost after 298135 iterations : Training Loss =  0.010245476054263621; Validation Loss = 0.019230291309263653\n",
            "Cost after 298136 iterations : Training Loss =  0.01024546803788075; Validation Loss = 0.019230288295082856\n",
            "Cost after 298137 iterations : Training Loss =  0.010245460021578681; Validation Loss = 0.019230285280945402\n",
            "Cost after 298138 iterations : Training Loss =  0.01024545200535752; Validation Loss = 0.01923028226685071\n",
            "Cost after 298139 iterations : Training Loss =  0.01024544398921713; Validation Loss = 0.019230279252798688\n",
            "Cost after 298140 iterations : Training Loss =  0.010245435973157573; Validation Loss = 0.01923027623878982\n",
            "Cost after 298141 iterations : Training Loss =  0.010245427957178801; Validation Loss = 0.019230273224823593\n",
            "Cost after 298142 iterations : Training Loss =  0.010245419941280894; Validation Loss = 0.019230270210900374\n",
            "Cost after 298143 iterations : Training Loss =  0.01024541192546377; Validation Loss = 0.019230267197019698\n",
            "Cost after 298144 iterations : Training Loss =  0.010245403909727313; Validation Loss = 0.019230264183182164\n",
            "Cost after 298145 iterations : Training Loss =  0.010245395894071843; Validation Loss = 0.019230261169387516\n",
            "Cost after 298146 iterations : Training Loss =  0.01024538787849711; Validation Loss = 0.019230258155636098\n",
            "Cost after 298147 iterations : Training Loss =  0.010245379863003185; Validation Loss = 0.019230255141926868\n",
            "Cost after 298148 iterations : Training Loss =  0.010245371847590034; Validation Loss = 0.01923025212826068\n",
            "Cost after 298149 iterations : Training Loss =  0.010245363832257685; Validation Loss = 0.019230249114637308\n",
            "Cost after 298150 iterations : Training Loss =  0.01024535581700621; Validation Loss = 0.019230246101056822\n",
            "Cost after 298151 iterations : Training Loss =  0.010245347801835487; Validation Loss = 0.01923024308751949\n",
            "Cost after 298152 iterations : Training Loss =  0.01024533978674557; Validation Loss = 0.019230240074025146\n",
            "Cost after 298153 iterations : Training Loss =  0.01024533177173639; Validation Loss = 0.019230237060573335\n",
            "Cost after 298154 iterations : Training Loss =  0.010245323756808112; Validation Loss = 0.019230234047164237\n",
            "Cost after 298155 iterations : Training Loss =  0.010245315741960559; Validation Loss = 0.01923023103379811\n",
            "Cost after 298156 iterations : Training Loss =  0.010245307727193823; Validation Loss = 0.019230228020474967\n",
            "Cost after 298157 iterations : Training Loss =  0.010245299712507816; Validation Loss = 0.019230225007194523\n",
            "Cost after 298158 iterations : Training Loss =  0.01024529169790267; Validation Loss = 0.01923022199395711\n",
            "Cost after 298159 iterations : Training Loss =  0.010245283683378322; Validation Loss = 0.01923021898076233\n",
            "Cost after 298160 iterations : Training Loss =  0.010245275668934775; Validation Loss = 0.01923021596761059\n",
            "Cost after 298161 iterations : Training Loss =  0.010245267654572; Validation Loss = 0.019230212954501953\n",
            "Cost after 298162 iterations : Training Loss =  0.010245259640289987; Validation Loss = 0.01923020994143593\n",
            "Cost after 298163 iterations : Training Loss =  0.010245251626088747; Validation Loss = 0.01923020692841255\n",
            "Cost after 298164 iterations : Training Loss =  0.010245243611968267; Validation Loss = 0.01923020391543189\n",
            "Cost after 298165 iterations : Training Loss =  0.010245235597928613; Validation Loss = 0.019230200902494464\n",
            "Cost after 298166 iterations : Training Loss =  0.010245227583969785; Validation Loss = 0.019230197889599975\n",
            "Cost after 298167 iterations : Training Loss =  0.010245219570091686; Validation Loss = 0.019230194876748454\n",
            "Cost after 298168 iterations : Training Loss =  0.01024521155629439; Validation Loss = 0.019230191863939528\n",
            "Cost after 298169 iterations : Training Loss =  0.010245203542577904; Validation Loss = 0.019230188851173495\n",
            "Cost after 298170 iterations : Training Loss =  0.010245195528942055; Validation Loss = 0.019230185838450202\n",
            "Cost after 298171 iterations : Training Loss =  0.010245187515387237; Validation Loss = 0.019230182825769687\n",
            "Cost after 298172 iterations : Training Loss =  0.01024517950191301; Validation Loss = 0.01923017981313217\n",
            "Cost after 298173 iterations : Training Loss =  0.010245171488519523; Validation Loss = 0.019230176800537485\n",
            "Cost after 298174 iterations : Training Loss =  0.010245163475206924; Validation Loss = 0.019230173787985718\n",
            "Cost after 298175 iterations : Training Loss =  0.010245155461975087; Validation Loss = 0.019230170775476778\n",
            "Cost after 298176 iterations : Training Loss =  0.010245147448823988; Validation Loss = 0.019230167763010748\n",
            "Cost after 298177 iterations : Training Loss =  0.010245139435753598; Validation Loss = 0.01923016475058758\n",
            "Cost after 298178 iterations : Training Loss =  0.010245131422764097; Validation Loss = 0.019230161738206974\n",
            "Cost after 298179 iterations : Training Loss =  0.010245123409855346; Validation Loss = 0.019230158725869296\n",
            "Cost after 298180 iterations : Training Loss =  0.0102451153970273; Validation Loss = 0.01923015571357444\n",
            "Cost after 298181 iterations : Training Loss =  0.010245107384280054; Validation Loss = 0.019230152701322726\n",
            "Cost after 298182 iterations : Training Loss =  0.010245099371613577; Validation Loss = 0.01923014968911394\n",
            "Cost after 298183 iterations : Training Loss =  0.010245091359027932; Validation Loss = 0.01923014667694775\n",
            "Cost after 298184 iterations : Training Loss =  0.010245083346522955; Validation Loss = 0.019230143664824183\n",
            "Cost after 298185 iterations : Training Loss =  0.010245075334098738; Validation Loss = 0.019230140652743852\n",
            "Cost after 298186 iterations : Training Loss =  0.010245067321755338; Validation Loss = 0.019230137640706146\n",
            "Cost after 298187 iterations : Training Loss =  0.010245059309492667; Validation Loss = 0.01923013462871159\n",
            "Cost after 298188 iterations : Training Loss =  0.010245051297310728; Validation Loss = 0.019230131616759422\n",
            "Cost after 298189 iterations : Training Loss =  0.01024504328520961; Validation Loss = 0.01923012860485033\n",
            "Cost after 298190 iterations : Training Loss =  0.010245035273189249; Validation Loss = 0.019230125592984138\n",
            "Cost after 298191 iterations : Training Loss =  0.010245027261249613; Validation Loss = 0.019230122581160865\n",
            "Cost after 298192 iterations : Training Loss =  0.01024501924939077; Validation Loss = 0.01923011956937983\n",
            "Cost after 298193 iterations : Training Loss =  0.010245011237612612; Validation Loss = 0.019230116557642063\n",
            "Cost after 298194 iterations : Training Loss =  0.010245003225915211; Validation Loss = 0.019230113545947287\n",
            "Cost after 298195 iterations : Training Loss =  0.010244995214298596; Validation Loss = 0.01923011053429487\n",
            "Cost after 298196 iterations : Training Loss =  0.010244987202762816; Validation Loss = 0.019230107522685648\n",
            "Cost after 298197 iterations : Training Loss =  0.01024497919130765; Validation Loss = 0.01923010451111907\n",
            "Cost after 298198 iterations : Training Loss =  0.010244971179933329; Validation Loss = 0.01923010149959535\n",
            "Cost after 298199 iterations : Training Loss =  0.010244963168639659; Validation Loss = 0.019230098488114713\n",
            "Cost after 298200 iterations : Training Loss =  0.010244955157426872; Validation Loss = 0.01923009547667665\n",
            "Cost after 298201 iterations : Training Loss =  0.010244947146294803; Validation Loss = 0.01923009246528142\n",
            "Cost after 298202 iterations : Training Loss =  0.010244939135243407; Validation Loss = 0.019230089453929167\n",
            "Cost after 298203 iterations : Training Loss =  0.010244931124272803; Validation Loss = 0.01923008644261956\n",
            "Cost after 298204 iterations : Training Loss =  0.010244923113382877; Validation Loss = 0.019230083431353018\n",
            "Cost after 298205 iterations : Training Loss =  0.010244915102573804; Validation Loss = 0.01923008042012893\n",
            "Cost after 298206 iterations : Training Loss =  0.010244907091845352; Validation Loss = 0.019230077408948113\n",
            "Cost after 298207 iterations : Training Loss =  0.010244899081197719; Validation Loss = 0.01923007439780994\n",
            "Cost after 298208 iterations : Training Loss =  0.01024489107063081; Validation Loss = 0.019230071386714884\n",
            "Cost after 298209 iterations : Training Loss =  0.010244883060144535; Validation Loss = 0.01923006837566236\n",
            "Cost after 298210 iterations : Training Loss =  0.010244875049739201; Validation Loss = 0.01923006536465273\n",
            "Cost after 298211 iterations : Training Loss =  0.010244867039414478; Validation Loss = 0.01923006235368587\n",
            "Cost after 298212 iterations : Training Loss =  0.010244859029170546; Validation Loss = 0.01923005934276168\n",
            "Cost after 298213 iterations : Training Loss =  0.010244851019007215; Validation Loss = 0.01923005633188046\n",
            "Cost after 298214 iterations : Training Loss =  0.010244843008924752; Validation Loss = 0.019230053321042037\n",
            "Cost after 298215 iterations : Training Loss =  0.01024483499892298; Validation Loss = 0.019230050310246487\n",
            "Cost after 298216 iterations : Training Loss =  0.010244826989001978; Validation Loss = 0.019230047299493503\n",
            "Cost after 298217 iterations : Training Loss =  0.010244818979161656; Validation Loss = 0.01923004428878361\n",
            "Cost after 298218 iterations : Training Loss =  0.010244810969402043; Validation Loss = 0.019230041278116467\n",
            "Cost after 298219 iterations : Training Loss =  0.010244802959723208; Validation Loss = 0.01923003826749214\n",
            "Cost after 298220 iterations : Training Loss =  0.010244794950125051; Validation Loss = 0.01923003525691009\n",
            "Cost after 298221 iterations : Training Loss =  0.010244786940607655; Validation Loss = 0.01923003224637149\n",
            "Cost after 298222 iterations : Training Loss =  0.010244778931170984; Validation Loss = 0.019230029235875492\n",
            "Cost after 298223 iterations : Training Loss =  0.01024477092181508; Validation Loss = 0.019230026225422227\n",
            "Cost after 298224 iterations : Training Loss =  0.010244762912539849; Validation Loss = 0.019230023215012205\n",
            "Cost after 298225 iterations : Training Loss =  0.010244754903345315; Validation Loss = 0.019230020204644666\n",
            "Cost after 298226 iterations : Training Loss =  0.01024474689423153; Validation Loss = 0.01923001719431986\n",
            "Cost after 298227 iterations : Training Loss =  0.010244738885198423; Validation Loss = 0.019230014184038025\n",
            "Cost after 298228 iterations : Training Loss =  0.010244730876246116; Validation Loss = 0.019230011173798835\n",
            "Cost after 298229 iterations : Training Loss =  0.010244722867374467; Validation Loss = 0.01923000816360286\n",
            "Cost after 298230 iterations : Training Loss =  0.010244714858583564; Validation Loss = 0.01923000515344929\n",
            "Cost after 298231 iterations : Training Loss =  0.01024470684987338; Validation Loss = 0.019230002143338584\n",
            "Cost after 298232 iterations : Training Loss =  0.010244698841243896; Validation Loss = 0.01922999913327068\n",
            "Cost after 298233 iterations : Training Loss =  0.010244690832695092; Validation Loss = 0.019229996123245493\n",
            "Cost after 298234 iterations : Training Loss =  0.010244682824227056; Validation Loss = 0.019229993113263198\n",
            "Cost after 298235 iterations : Training Loss =  0.010244674815839624; Validation Loss = 0.019229990103323893\n",
            "Cost after 298236 iterations : Training Loss =  0.010244666807533075; Validation Loss = 0.019229987093427083\n",
            "Cost after 298237 iterations : Training Loss =  0.010244658799307069; Validation Loss = 0.019229984083573244\n",
            "Cost after 298238 iterations : Training Loss =  0.01024465079116188; Validation Loss = 0.01922998107376188\n",
            "Cost after 298239 iterations : Training Loss =  0.010244642783097368; Validation Loss = 0.019229978063993892\n",
            "Cost after 298240 iterations : Training Loss =  0.010244634775113524; Validation Loss = 0.01922997505426854\n",
            "Cost after 298241 iterations : Training Loss =  0.010244626767210462; Validation Loss = 0.019229972044585736\n",
            "Cost after 298242 iterations : Training Loss =  0.010244618759388003; Validation Loss = 0.019229969034945768\n",
            "Cost after 298243 iterations : Training Loss =  0.010244610751646304; Validation Loss = 0.019229966025348745\n",
            "Cost after 298244 iterations : Training Loss =  0.01024460274398533; Validation Loss = 0.019229963015794663\n",
            "Cost after 298245 iterations : Training Loss =  0.010244594736405025; Validation Loss = 0.019229960006282954\n",
            "Cost after 298246 iterations : Training Loss =  0.01024458672890541; Validation Loss = 0.019229956996814068\n",
            "Cost after 298247 iterations : Training Loss =  0.010244578721486512; Validation Loss = 0.01922995398738829\n",
            "Cost after 298248 iterations : Training Loss =  0.010244570714148346; Validation Loss = 0.019229950978005155\n",
            "Cost after 298249 iterations : Training Loss =  0.010244562706890814; Validation Loss = 0.019229947968664774\n",
            "Cost after 298250 iterations : Training Loss =  0.010244554699713993; Validation Loss = 0.01922994495936736\n",
            "Cost after 298251 iterations : Training Loss =  0.010244546692617847; Validation Loss = 0.019229941950112902\n",
            "Cost after 298252 iterations : Training Loss =  0.010244538685602404; Validation Loss = 0.01922993894090084\n",
            "Cost after 298253 iterations : Training Loss =  0.010244530678667691; Validation Loss = 0.019229935931731702\n",
            "Cost after 298254 iterations : Training Loss =  0.010244522671813706; Validation Loss = 0.019229932922605223\n",
            "Cost after 298255 iterations : Training Loss =  0.010244514665040253; Validation Loss = 0.019229929913521772\n",
            "Cost after 298256 iterations : Training Loss =  0.010244506658347648; Validation Loss = 0.019229926904480995\n",
            "Cost after 298257 iterations : Training Loss =  0.010244498651735672; Validation Loss = 0.019229923895483145\n",
            "Cost after 298258 iterations : Training Loss =  0.010244490645204399; Validation Loss = 0.019229920886527734\n",
            "Cost after 298259 iterations : Training Loss =  0.01024448263875378; Validation Loss = 0.01922991787761557\n",
            "Cost after 298260 iterations : Training Loss =  0.010244474632383871; Validation Loss = 0.01922991486874606\n",
            "Cost after 298261 iterations : Training Loss =  0.01024446662609468; Validation Loss = 0.019229911859919152\n",
            "Cost after 298262 iterations : Training Loss =  0.010244458619886128; Validation Loss = 0.01922990885113501\n",
            "Cost after 298263 iterations : Training Loss =  0.010244450613758177; Validation Loss = 0.0192299058423938\n",
            "Cost after 298264 iterations : Training Loss =  0.010244442607711055; Validation Loss = 0.019229902833695075\n",
            "Cost after 298265 iterations : Training Loss =  0.010244434601744538; Validation Loss = 0.019229899825039264\n",
            "Cost after 298266 iterations : Training Loss =  0.010244426595858681; Validation Loss = 0.019229896816426572\n",
            "Cost after 298267 iterations : Training Loss =  0.010244418590053561; Validation Loss = 0.019229893807856432\n",
            "Cost after 298268 iterations : Training Loss =  0.010244410584329074; Validation Loss = 0.019229890799328856\n",
            "Cost after 298269 iterations : Training Loss =  0.010244402578685243; Validation Loss = 0.01922988779084444\n",
            "Cost after 298270 iterations : Training Loss =  0.010244394573122088; Validation Loss = 0.019229884782402697\n",
            "Cost after 298271 iterations : Training Loss =  0.010244386567639636; Validation Loss = 0.019229881774003758\n",
            "Cost after 298272 iterations : Training Loss =  0.010244378562237817; Validation Loss = 0.019229878765647652\n",
            "Cost after 298273 iterations : Training Loss =  0.010244370556916817; Validation Loss = 0.019229875757334064\n",
            "Cost after 298274 iterations : Training Loss =  0.010244362551676316; Validation Loss = 0.019229872749063345\n",
            "Cost after 298275 iterations : Training Loss =  0.010244354546516617; Validation Loss = 0.0192298697408355\n",
            "Cost after 298276 iterations : Training Loss =  0.01024434654143751; Validation Loss = 0.019229866732650383\n",
            "Cost after 298277 iterations : Training Loss =  0.010244338536439069; Validation Loss = 0.019229863724507856\n",
            "Cost after 298278 iterations : Training Loss =  0.010244330531521288; Validation Loss = 0.01922986071640817\n",
            "Cost after 298279 iterations : Training Loss =  0.010244322526684134; Validation Loss = 0.0192298577083511\n",
            "Cost after 298280 iterations : Training Loss =  0.010244314521927717; Validation Loss = 0.01922985470033717\n",
            "Cost after 298281 iterations : Training Loss =  0.010244306517252022; Validation Loss = 0.019229851692365987\n",
            "Cost after 298282 iterations : Training Loss =  0.010244298512656793; Validation Loss = 0.01922984868443718\n",
            "Cost after 298283 iterations : Training Loss =  0.010244290508142321; Validation Loss = 0.019229845676551432\n",
            "Cost after 298284 iterations : Training Loss =  0.010244282503708632; Validation Loss = 0.019229842668708256\n",
            "Cost after 298285 iterations : Training Loss =  0.010244274499355456; Validation Loss = 0.019229839660907938\n",
            "Cost after 298286 iterations : Training Loss =  0.01024426649508293; Validation Loss = 0.019229836653150294\n",
            "Cost after 298287 iterations : Training Loss =  0.010244258490891065; Validation Loss = 0.01922983364543553\n",
            "Cost after 298288 iterations : Training Loss =  0.010244250486779993; Validation Loss = 0.019229830637763554\n",
            "Cost after 298289 iterations : Training Loss =  0.01024424248274945; Validation Loss = 0.01922982763013429\n",
            "Cost after 298290 iterations : Training Loss =  0.010244234478799637; Validation Loss = 0.01922982462254785\n",
            "Cost after 298291 iterations : Training Loss =  0.010244226474930403; Validation Loss = 0.019229821615004236\n",
            "Cost after 298292 iterations : Training Loss =  0.010244218471141802; Validation Loss = 0.019229818607503165\n",
            "Cost after 298293 iterations : Training Loss =  0.010244210467433977; Validation Loss = 0.019229815600044827\n",
            "Cost after 298294 iterations : Training Loss =  0.010244202463806685; Validation Loss = 0.019229812592629503\n",
            "Cost after 298295 iterations : Training Loss =  0.010244194460260035; Validation Loss = 0.019229809585256705\n",
            "Cost after 298296 iterations : Training Loss =  0.01024418645679413; Validation Loss = 0.019229806577926622\n",
            "Cost after 298297 iterations : Training Loss =  0.010244178453408755; Validation Loss = 0.01922980357063964\n",
            "Cost after 298298 iterations : Training Loss =  0.010244170450104114; Validation Loss = 0.01922980056339547\n",
            "Cost after 298299 iterations : Training Loss =  0.010244162446880076; Validation Loss = 0.01922979755619387\n",
            "Cost after 298300 iterations : Training Loss =  0.010244154443736676; Validation Loss = 0.019229794549034826\n",
            "Cost after 298301 iterations : Training Loss =  0.010244146440673976; Validation Loss = 0.01922979154191912\n",
            "Cost after 298302 iterations : Training Loss =  0.010244138437691834; Validation Loss = 0.019229788534845773\n",
            "Cost after 298303 iterations : Training Loss =  0.010244130434790414; Validation Loss = 0.01922978552781508\n",
            "Cost after 298304 iterations : Training Loss =  0.010244122431969583; Validation Loss = 0.019229782520827386\n",
            "Cost after 298305 iterations : Training Loss =  0.010244114429229416; Validation Loss = 0.019229779513882203\n",
            "Cost after 298306 iterations : Training Loss =  0.010244106426569902; Validation Loss = 0.019229776506979664\n",
            "Cost after 298307 iterations : Training Loss =  0.010244098423990919; Validation Loss = 0.019229773500120385\n",
            "Cost after 298308 iterations : Training Loss =  0.010244090421492673; Validation Loss = 0.019229770493303468\n",
            "Cost after 298309 iterations : Training Loss =  0.010244082419075031; Validation Loss = 0.01922976748652928\n",
            "Cost after 298310 iterations : Training Loss =  0.010244074416737983; Validation Loss = 0.019229764479798066\n",
            "Cost after 298311 iterations : Training Loss =  0.01024406641448166; Validation Loss = 0.019229761473109168\n",
            "Cost after 298312 iterations : Training Loss =  0.010244058412305865; Validation Loss = 0.019229758466463426\n",
            "Cost after 298313 iterations : Training Loss =  0.010244050410210748; Validation Loss = 0.019229755459860318\n",
            "Cost after 298314 iterations : Training Loss =  0.010244042408196312; Validation Loss = 0.019229752453299945\n",
            "Cost after 298315 iterations : Training Loss =  0.010244034406262323; Validation Loss = 0.019229749446782282\n",
            "Cost after 298316 iterations : Training Loss =  0.010244026404409166; Validation Loss = 0.019229746440307737\n",
            "Cost after 298317 iterations : Training Loss =  0.010244018402636532; Validation Loss = 0.019229743433875377\n",
            "Cost after 298318 iterations : Training Loss =  0.010244010400944541; Validation Loss = 0.01922974042748617\n",
            "Cost after 298319 iterations : Training Loss =  0.010244002399333103; Validation Loss = 0.01922973742113949\n",
            "Cost after 298320 iterations : Training Loss =  0.010243994397802452; Validation Loss = 0.019229734414835182\n",
            "Cost after 298321 iterations : Training Loss =  0.010243986396352264; Validation Loss = 0.019229731408574216\n",
            "Cost after 298322 iterations : Training Loss =  0.010243978394982863; Validation Loss = 0.01922972840235552\n",
            "Cost after 298323 iterations : Training Loss =  0.010243970393693942; Validation Loss = 0.019229725396180012\n",
            "Cost after 298324 iterations : Training Loss =  0.010243962392485728; Validation Loss = 0.01922972239004693\n",
            "Cost after 298325 iterations : Training Loss =  0.010243954391358043; Validation Loss = 0.019229719383956786\n",
            "Cost after 298326 iterations : Training Loss =  0.01024394639031099; Validation Loss = 0.019229716377909097\n",
            "Cost after 298327 iterations : Training Loss =  0.010243938389344573; Validation Loss = 0.01922971337190428\n",
            "Cost after 298328 iterations : Training Loss =  0.01024393038845872; Validation Loss = 0.019229710365942444\n",
            "Cost after 298329 iterations : Training Loss =  0.010243922387653584; Validation Loss = 0.019229707360022958\n",
            "Cost after 298330 iterations : Training Loss =  0.010243914386928952; Validation Loss = 0.01922970435414644\n",
            "Cost after 298331 iterations : Training Loss =  0.010243906386285046; Validation Loss = 0.01922970134831275\n",
            "Cost after 298332 iterations : Training Loss =  0.010243898385721636; Validation Loss = 0.019229698342521702\n",
            "Cost after 298333 iterations : Training Loss =  0.010243890385238928; Validation Loss = 0.019229695336773482\n",
            "Cost after 298334 iterations : Training Loss =  0.010243882384836733; Validation Loss = 0.019229692331067895\n",
            "Cost after 298335 iterations : Training Loss =  0.0102438743845152; Validation Loss = 0.019229689325404756\n",
            "Cost after 298336 iterations : Training Loss =  0.010243866384274321; Validation Loss = 0.019229686319784278\n",
            "Cost after 298337 iterations : Training Loss =  0.010243858384114055; Validation Loss = 0.019229683314206557\n",
            "Cost after 298338 iterations : Training Loss =  0.01024385038403427; Validation Loss = 0.019229680308671615\n",
            "Cost after 298339 iterations : Training Loss =  0.010243842384035123; Validation Loss = 0.019229677303179656\n",
            "Cost after 298340 iterations : Training Loss =  0.010243834384116628; Validation Loss = 0.019229674297730294\n",
            "Cost after 298341 iterations : Training Loss =  0.010243826384278669; Validation Loss = 0.01922967129232353\n",
            "Cost after 298342 iterations : Training Loss =  0.010243818384521361; Validation Loss = 0.01922966828695975\n",
            "Cost after 298343 iterations : Training Loss =  0.010243810384844619; Validation Loss = 0.019229665281638657\n",
            "Cost after 298344 iterations : Training Loss =  0.010243802385248482; Validation Loss = 0.019229662276360148\n",
            "Cost after 298345 iterations : Training Loss =  0.010243794385732957; Validation Loss = 0.019229659271124362\n",
            "Cost after 298346 iterations : Training Loss =  0.010243786386298066; Validation Loss = 0.01922965626593108\n",
            "Cost after 298347 iterations : Training Loss =  0.01024377838694375; Validation Loss = 0.019229653260780525\n",
            "Cost after 298348 iterations : Training Loss =  0.010243770387669946; Validation Loss = 0.019229650255673372\n",
            "Cost after 298349 iterations : Training Loss =  0.010243762388476829; Validation Loss = 0.01922964725060849\n",
            "Cost after 298350 iterations : Training Loss =  0.010243754389364203; Validation Loss = 0.01922964424558639\n",
            "Cost after 298351 iterations : Training Loss =  0.010243746390332245; Validation Loss = 0.019229641240607182\n",
            "Cost after 298352 iterations : Training Loss =  0.010243738391380777; Validation Loss = 0.019229638235670518\n",
            "Cost after 298353 iterations : Training Loss =  0.01024373039251006; Validation Loss = 0.01922963523077661\n",
            "Cost after 298354 iterations : Training Loss =  0.010243722393719835; Validation Loss = 0.019229632225925188\n",
            "Cost after 298355 iterations : Training Loss =  0.010243714395010237; Validation Loss = 0.019229629221116526\n",
            "Cost after 298356 iterations : Training Loss =  0.01024370639638112; Validation Loss = 0.019229626216350968\n",
            "Cost after 298357 iterations : Training Loss =  0.010243698397832648; Validation Loss = 0.01922962321162752\n",
            "Cost after 298358 iterations : Training Loss =  0.010243690399364817; Validation Loss = 0.019229620206946945\n",
            "Cost after 298359 iterations : Training Loss =  0.01024368240097753; Validation Loss = 0.019229617202309604\n",
            "Cost after 298360 iterations : Training Loss =  0.010243674402670796; Validation Loss = 0.019229614197714417\n",
            "Cost after 298361 iterations : Training Loss =  0.010243666404444626; Validation Loss = 0.019229611193162237\n",
            "Cost after 298362 iterations : Training Loss =  0.010243658406299094; Validation Loss = 0.01922960818865272\n",
            "Cost after 298363 iterations : Training Loss =  0.01024365040823409; Validation Loss = 0.019229605184185778\n",
            "Cost after 298364 iterations : Training Loss =  0.010243642410249661; Validation Loss = 0.019229602179761347\n",
            "Cost after 298365 iterations : Training Loss =  0.010243634412345852; Validation Loss = 0.019229599175379898\n",
            "Cost after 298366 iterations : Training Loss =  0.010243626414522701; Validation Loss = 0.019229596171041377\n",
            "Cost after 298367 iterations : Training Loss =  0.010243618416780008; Validation Loss = 0.019229593166745183\n",
            "Cost after 298368 iterations : Training Loss =  0.010243610419117795; Validation Loss = 0.019229590162491893\n",
            "Cost after 298369 iterations : Training Loss =  0.010243602421536298; Validation Loss = 0.019229587158281262\n",
            "Cost after 298370 iterations : Training Loss =  0.010243594424035322; Validation Loss = 0.019229584154112973\n",
            "Cost after 298371 iterations : Training Loss =  0.01024358642661492; Validation Loss = 0.01922958114998773\n",
            "Cost after 298372 iterations : Training Loss =  0.010243578429275032; Validation Loss = 0.019229578145905493\n",
            "Cost after 298373 iterations : Training Loss =  0.010243570432015788; Validation Loss = 0.019229575141865674\n",
            "Cost after 298374 iterations : Training Loss =  0.010243562434837079; Validation Loss = 0.019229572137868217\n",
            "Cost after 298375 iterations : Training Loss =  0.01024355443773897; Validation Loss = 0.019229569133913913\n",
            "Cost after 298376 iterations : Training Loss =  0.010243546440721379; Validation Loss = 0.019229566130002127\n",
            "Cost after 298377 iterations : Training Loss =  0.010243538443784365; Validation Loss = 0.019229563126132845\n",
            "Cost after 298378 iterations : Training Loss =  0.010243530446927829; Validation Loss = 0.01922956012230643\n",
            "Cost after 298379 iterations : Training Loss =  0.010243522450152016; Validation Loss = 0.01922955711852264\n",
            "Cost after 298380 iterations : Training Loss =  0.010243514453456651; Validation Loss = 0.019229554114781718\n",
            "Cost after 298381 iterations : Training Loss =  0.010243506456841852; Validation Loss = 0.019229551111083626\n",
            "Cost after 298382 iterations : Training Loss =  0.010243498460307728; Validation Loss = 0.01922954810742788\n",
            "Cost after 298383 iterations : Training Loss =  0.010243490463853953; Validation Loss = 0.019229545103815353\n",
            "Cost after 298384 iterations : Training Loss =  0.010243482467480929; Validation Loss = 0.019229542100244818\n",
            "Cost after 298385 iterations : Training Loss =  0.010243474471188353; Validation Loss = 0.01922953909671717\n",
            "Cost after 298386 iterations : Training Loss =  0.010243466474976346; Validation Loss = 0.019229536093232417\n",
            "Cost after 298387 iterations : Training Loss =  0.010243458478844916; Validation Loss = 0.01922953308979043\n",
            "Cost after 298388 iterations : Training Loss =  0.010243450482794092; Validation Loss = 0.019229530086391052\n",
            "Cost after 298389 iterations : Training Loss =  0.010243442486823677; Validation Loss = 0.019229527083034215\n",
            "Cost after 298390 iterations : Training Loss =  0.0102434344909339; Validation Loss = 0.019229524079719985\n",
            "Cost after 298391 iterations : Training Loss =  0.010243426495124597; Validation Loss = 0.01922952107644863\n",
            "Cost after 298392 iterations : Training Loss =  0.010243418499395966; Validation Loss = 0.019229518073219484\n",
            "Cost after 298393 iterations : Training Loss =  0.01024341050374777; Validation Loss = 0.01922951507003351\n",
            "Cost after 298394 iterations : Training Loss =  0.010243402508180217; Validation Loss = 0.019229512066890206\n",
            "Cost after 298395 iterations : Training Loss =  0.01024339451269312; Validation Loss = 0.019229509063789653\n",
            "Cost after 298396 iterations : Training Loss =  0.010243386517286504; Validation Loss = 0.019229506060731608\n",
            "Cost after 298397 iterations : Training Loss =  0.010243378521960568; Validation Loss = 0.019229503057716497\n",
            "Cost after 298398 iterations : Training Loss =  0.010243370526715137; Validation Loss = 0.01922950005474391\n",
            "Cost after 298399 iterations : Training Loss =  0.010243362531550228; Validation Loss = 0.01922949705181403\n",
            "Cost after 298400 iterations : Training Loss =  0.010243354536465885; Validation Loss = 0.019229494048926807\n",
            "Cost after 298401 iterations : Training Loss =  0.010243346541462034; Validation Loss = 0.019229491046082074\n",
            "Cost after 298402 iterations : Training Loss =  0.01024333854653869; Validation Loss = 0.01922948804328043\n",
            "Cost after 298403 iterations : Training Loss =  0.010243330551695969; Validation Loss = 0.019229485040521185\n",
            "Cost after 298404 iterations : Training Loss =  0.010243322556933759; Validation Loss = 0.01922948203780436\n",
            "Cost after 298405 iterations : Training Loss =  0.010243314562252087; Validation Loss = 0.01922947903513062\n",
            "Cost after 298406 iterations : Training Loss =  0.01024330656765095; Validation Loss = 0.01922947603249952\n",
            "Cost after 298407 iterations : Training Loss =  0.01024329857313039; Validation Loss = 0.019229473029910817\n",
            "Cost after 298408 iterations : Training Loss =  0.010243290578690166; Validation Loss = 0.01922947002736485\n",
            "Cost after 298409 iterations : Training Loss =  0.010243282584330651; Validation Loss = 0.019229467024861807\n",
            "Cost after 298410 iterations : Training Loss =  0.010243274590051626; Validation Loss = 0.019229464022401117\n",
            "Cost after 298411 iterations : Training Loss =  0.010243266595853074; Validation Loss = 0.019229461019983345\n",
            "Cost after 298412 iterations : Training Loss =  0.010243258601735139; Validation Loss = 0.019229458017608267\n",
            "Cost after 298413 iterations : Training Loss =  0.010243250607697625; Validation Loss = 0.019229455015275856\n",
            "Cost after 298414 iterations : Training Loss =  0.010243242613740747; Validation Loss = 0.0192294520129857\n",
            "Cost after 298415 iterations : Training Loss =  0.010243234619864297; Validation Loss = 0.019229449010738666\n",
            "Cost after 298416 iterations : Training Loss =  0.010243226626068374; Validation Loss = 0.019229446008534087\n",
            "Cost after 298417 iterations : Training Loss =  0.010243218632353095; Validation Loss = 0.019229443006372585\n",
            "Cost after 298418 iterations : Training Loss =  0.010243210638718175; Validation Loss = 0.01922944000425332\n",
            "Cost after 298419 iterations : Training Loss =  0.010243202645163863; Validation Loss = 0.019229437002176844\n",
            "Cost after 298420 iterations : Training Loss =  0.01024319465169006; Validation Loss = 0.019229434000143184\n",
            "Cost after 298421 iterations : Training Loss =  0.010243186658296715; Validation Loss = 0.01922943099815196\n",
            "Cost after 298422 iterations : Training Loss =  0.010243178664983947; Validation Loss = 0.019229427996203675\n",
            "Cost after 298423 iterations : Training Loss =  0.010243170671751671; Validation Loss = 0.019229424994297886\n",
            "Cost after 298424 iterations : Training Loss =  0.010243162678599965; Validation Loss = 0.019229421992434697\n",
            "Cost after 298425 iterations : Training Loss =  0.010243154685528751; Validation Loss = 0.019229418990614172\n",
            "Cost after 298426 iterations : Training Loss =  0.01024314669253797; Validation Loss = 0.01922941598883621\n",
            "Cost after 298427 iterations : Training Loss =  0.010243138699627752; Validation Loss = 0.01922941298710109\n",
            "Cost after 298428 iterations : Training Loss =  0.010243130706798059; Validation Loss = 0.01922940998540867\n",
            "Cost after 298429 iterations : Training Loss =  0.01024312271404884; Validation Loss = 0.019229406983758596\n",
            "Cost after 298430 iterations : Training Loss =  0.010243114721380167; Validation Loss = 0.01922940398215145\n",
            "Cost after 298431 iterations : Training Loss =  0.010243106728791885; Validation Loss = 0.019229400980586724\n",
            "Cost after 298432 iterations : Training Loss =  0.010243098736284198; Validation Loss = 0.01922939797906467\n",
            "Cost after 298433 iterations : Training Loss =  0.010243090743857056; Validation Loss = 0.019229394977585662\n",
            "Cost after 298434 iterations : Training Loss =  0.01024308275151041; Validation Loss = 0.019229391976149274\n",
            "Cost after 298435 iterations : Training Loss =  0.010243074759244247; Validation Loss = 0.01922938897475503\n",
            "Cost after 298436 iterations : Training Loss =  0.010243066767058552; Validation Loss = 0.019229385973403856\n",
            "Cost after 298437 iterations : Training Loss =  0.010243058774953412; Validation Loss = 0.019229382972095078\n",
            "Cost after 298438 iterations : Training Loss =  0.01024305078292866; Validation Loss = 0.019229379970828936\n",
            "Cost after 298439 iterations : Training Loss =  0.010243042790984537; Validation Loss = 0.019229376969605697\n",
            "Cost after 298440 iterations : Training Loss =  0.010243034799120861; Validation Loss = 0.01922937396842501\n",
            "Cost after 298441 iterations : Training Loss =  0.010243026807337683; Validation Loss = 0.019229370967287007\n",
            "Cost after 298442 iterations : Training Loss =  0.010243018815634965; Validation Loss = 0.019229367966191537\n",
            "Cost after 298443 iterations : Training Loss =  0.010243010824012784; Validation Loss = 0.019229364965138777\n",
            "Cost after 298444 iterations : Training Loss =  0.010243002832471008; Validation Loss = 0.019229361964128926\n",
            "Cost after 298445 iterations : Training Loss =  0.010242994841009807; Validation Loss = 0.01922935896316133\n",
            "Cost after 298446 iterations : Training Loss =  0.010242986849629113; Validation Loss = 0.01922935596223686\n",
            "Cost after 298447 iterations : Training Loss =  0.010242978858328929; Validation Loss = 0.01922935296135435\n",
            "Cost after 298448 iterations : Training Loss =  0.01024297086710914; Validation Loss = 0.01922934996051486\n",
            "Cost after 298449 iterations : Training Loss =  0.010242962875969947; Validation Loss = 0.019229346959718042\n",
            "Cost after 298450 iterations : Training Loss =  0.010242954884911127; Validation Loss = 0.01922934395896411\n",
            "Cost after 298451 iterations : Training Loss =  0.010242946893932885; Validation Loss = 0.01922934095825241\n",
            "Cost after 298452 iterations : Training Loss =  0.010242938903035037; Validation Loss = 0.01922933795758316\n",
            "Cost after 298453 iterations : Training Loss =  0.010242930912217703; Validation Loss = 0.01922933495695677\n",
            "Cost after 298454 iterations : Training Loss =  0.010242922921480836; Validation Loss = 0.019229331956373216\n",
            "Cost after 298455 iterations : Training Loss =  0.010242914930824519; Validation Loss = 0.019229328955832098\n",
            "Cost after 298456 iterations : Training Loss =  0.010242906940248594; Validation Loss = 0.019229325955333696\n",
            "Cost after 298457 iterations : Training Loss =  0.010242898949753245; Validation Loss = 0.01922932295487817\n",
            "Cost after 298458 iterations : Training Loss =  0.010242890959338289; Validation Loss = 0.019229319954464922\n",
            "Cost after 298459 iterations : Training Loss =  0.010242882969003848; Validation Loss = 0.019229316954094446\n",
            "Cost after 298460 iterations : Training Loss =  0.01024287497874992; Validation Loss = 0.019229313953766602\n",
            "Cost after 298461 iterations : Training Loss =  0.010242866988576484; Validation Loss = 0.019229310953481432\n",
            "Cost after 298462 iterations : Training Loss =  0.010242858998483378; Validation Loss = 0.019229307953238593\n",
            "Cost after 298463 iterations : Training Loss =  0.010242851008470865; Validation Loss = 0.01922930495303873\n",
            "Cost after 298464 iterations : Training Loss =  0.010242843018538766; Validation Loss = 0.019229301952881524\n",
            "Cost after 298465 iterations : Training Loss =  0.010242835028687178; Validation Loss = 0.019229298952767013\n",
            "Cost after 298466 iterations : Training Loss =  0.010242827038916021; Validation Loss = 0.019229295952694868\n",
            "Cost after 298467 iterations : Training Loss =  0.010242819049225393; Validation Loss = 0.019229292952665407\n",
            "Cost after 298468 iterations : Training Loss =  0.010242811059615171; Validation Loss = 0.019229289952678467\n",
            "Cost after 298469 iterations : Training Loss =  0.010242803070085486; Validation Loss = 0.019229286952734126\n",
            "Cost after 298470 iterations : Training Loss =  0.010242795080636182; Validation Loss = 0.019229283952832636\n",
            "Cost after 298471 iterations : Training Loss =  0.010242787091267348; Validation Loss = 0.01922928095297382\n",
            "Cost after 298472 iterations : Training Loss =  0.010242779101979028; Validation Loss = 0.0192292779531577\n",
            "Cost after 298473 iterations : Training Loss =  0.010242771112771167; Validation Loss = 0.019229274953383776\n",
            "Cost after 298474 iterations : Training Loss =  0.010242763123643786; Validation Loss = 0.01922927195365288\n",
            "Cost after 298475 iterations : Training Loss =  0.010242755134596831; Validation Loss = 0.019229268953964443\n",
            "Cost after 298476 iterations : Training Loss =  0.010242747145630268; Validation Loss = 0.019229265954318932\n",
            "Cost after 298477 iterations : Training Loss =  0.010242739156744275; Validation Loss = 0.019229262954715467\n",
            "Cost after 298478 iterations : Training Loss =  0.01024273116793869; Validation Loss = 0.01922925995515505\n",
            "Cost after 298479 iterations : Training Loss =  0.010242723179213582; Validation Loss = 0.019229256955637233\n",
            "Cost after 298480 iterations : Training Loss =  0.010242715190568916; Validation Loss = 0.01922925395616178\n",
            "Cost after 298481 iterations : Training Loss =  0.010242707202004707; Validation Loss = 0.019229250956728953\n",
            "Cost after 298482 iterations : Training Loss =  0.010242699213520931; Validation Loss = 0.01922924795733905\n",
            "Cost after 298483 iterations : Training Loss =  0.010242691225117595; Validation Loss = 0.019229244957991297\n",
            "Cost after 298484 iterations : Training Loss =  0.010242683236794796; Validation Loss = 0.019229241958686503\n",
            "Cost after 298485 iterations : Training Loss =  0.010242675248552317; Validation Loss = 0.019229238959424383\n",
            "Cost after 298486 iterations : Training Loss =  0.010242667260390328; Validation Loss = 0.019229235960204625\n",
            "Cost after 298487 iterations : Training Loss =  0.010242659272308937; Validation Loss = 0.019229232961027447\n",
            "Cost after 298488 iterations : Training Loss =  0.010242651284307822; Validation Loss = 0.019229229961893097\n",
            "Cost after 298489 iterations : Training Loss =  0.01024264329638719; Validation Loss = 0.019229226962801074\n",
            "Cost after 298490 iterations : Training Loss =  0.010242635308547014; Validation Loss = 0.019229223963752297\n",
            "Cost after 298491 iterations : Training Loss =  0.010242627320787276; Validation Loss = 0.019229220964745608\n",
            "Cost after 298492 iterations : Training Loss =  0.010242619333108079; Validation Loss = 0.01922921796578161\n",
            "Cost after 298493 iterations : Training Loss =  0.010242611345509167; Validation Loss = 0.019229214966860184\n",
            "Cost after 298494 iterations : Training Loss =  0.010242603357990722; Validation Loss = 0.01922921196798132\n",
            "Cost after 298495 iterations : Training Loss =  0.010242595370552736; Validation Loss = 0.019229208969145404\n",
            "Cost after 298496 iterations : Training Loss =  0.010242587383195181; Validation Loss = 0.019229205970351892\n",
            "Cost after 298497 iterations : Training Loss =  0.01024257939591812; Validation Loss = 0.0192292029716011\n",
            "Cost after 298498 iterations : Training Loss =  0.010242571408721484; Validation Loss = 0.019229199972892914\n",
            "Cost after 298499 iterations : Training Loss =  0.01024256342160521; Validation Loss = 0.01922919697422733\n",
            "Cost after 298500 iterations : Training Loss =  0.010242555434569495; Validation Loss = 0.019229193975604464\n",
            "Cost after 298501 iterations : Training Loss =  0.010242547447614174; Validation Loss = 0.019229190977023948\n",
            "Cost after 298502 iterations : Training Loss =  0.01024253946073919; Validation Loss = 0.019229187978485954\n",
            "Cost after 298503 iterations : Training Loss =  0.010242531473944721; Validation Loss = 0.019229184979990484\n",
            "Cost after 298504 iterations : Training Loss =  0.010242523487230608; Validation Loss = 0.019229181981537773\n",
            "Cost after 298505 iterations : Training Loss =  0.010242515500596974; Validation Loss = 0.019229178983127714\n",
            "Cost after 298506 iterations : Training Loss =  0.010242507514043778; Validation Loss = 0.019229175984760247\n",
            "Cost after 298507 iterations : Training Loss =  0.010242499527571029; Validation Loss = 0.019229172986435557\n",
            "Cost after 298508 iterations : Training Loss =  0.010242491541178695; Validation Loss = 0.01922916998815338\n",
            "Cost after 298509 iterations : Training Loss =  0.010242483554866802; Validation Loss = 0.01922916698991354\n",
            "Cost after 298510 iterations : Training Loss =  0.01024247556863517; Validation Loss = 0.019229163991716165\n",
            "Cost after 298511 iterations : Training Loss =  0.010242467582484212; Validation Loss = 0.01922916099356177\n",
            "Cost after 298512 iterations : Training Loss =  0.010242459596413424; Validation Loss = 0.019229157995450035\n",
            "Cost after 298513 iterations : Training Loss =  0.010242451610423258; Validation Loss = 0.01922915499738061\n",
            "Cost after 298514 iterations : Training Loss =  0.010242443624513367; Validation Loss = 0.019229151999354037\n",
            "Cost after 298515 iterations : Training Loss =  0.01024243563868403; Validation Loss = 0.019229149001369756\n",
            "Cost after 298516 iterations : Training Loss =  0.010242427652935045; Validation Loss = 0.019229146003428305\n",
            "Cost after 298517 iterations : Training Loss =  0.01024241966726648; Validation Loss = 0.019229143005529234\n",
            "Cost after 298518 iterations : Training Loss =  0.01024241168167834; Validation Loss = 0.01922914000767299\n",
            "Cost after 298519 iterations : Training Loss =  0.010242403696170603; Validation Loss = 0.019229137009859315\n",
            "Cost after 298520 iterations : Training Loss =  0.010242395710743318; Validation Loss = 0.019229134012088173\n",
            "Cost after 298521 iterations : Training Loss =  0.010242387725396398; Validation Loss = 0.019229131014359673\n",
            "Cost after 298522 iterations : Training Loss =  0.01024237974012984; Validation Loss = 0.019229128016673504\n",
            "Cost after 298523 iterations : Training Loss =  0.010242371754943793; Validation Loss = 0.01922912501903024\n",
            "Cost after 298524 iterations : Training Loss =  0.010242363769838073; Validation Loss = 0.019229122021429516\n",
            "Cost after 298525 iterations : Training Loss =  0.010242355784812808; Validation Loss = 0.01922911902387119\n",
            "Cost after 298526 iterations : Training Loss =  0.01024234779986791; Validation Loss = 0.01922911602635545\n",
            "Cost after 298527 iterations : Training Loss =  0.01024233981500352; Validation Loss = 0.01922911302888244\n",
            "Cost after 298528 iterations : Training Loss =  0.010242331830219388; Validation Loss = 0.01922911003145206\n",
            "Cost after 298529 iterations : Training Loss =  0.010242323845515789; Validation Loss = 0.019229107034064063\n",
            "Cost after 298530 iterations : Training Loss =  0.010242315860892559; Validation Loss = 0.019229104036718507\n",
            "Cost after 298531 iterations : Training Loss =  0.010242307876349683; Validation Loss = 0.019229101039415955\n",
            "Cost after 298532 iterations : Training Loss =  0.01024229989188724; Validation Loss = 0.019229098042155883\n",
            "Cost after 298533 iterations : Training Loss =  0.010242291907505264; Validation Loss = 0.019229095044938055\n",
            "Cost after 298534 iterations : Training Loss =  0.010242283923203555; Validation Loss = 0.019229092047763092\n",
            "Cost after 298535 iterations : Training Loss =  0.010242275938982285; Validation Loss = 0.019229089050630747\n",
            "Cost after 298536 iterations : Training Loss =  0.010242267954841447; Validation Loss = 0.019229086053540793\n",
            "Cost after 298537 iterations : Training Loss =  0.010242259970781039; Validation Loss = 0.019229083056493474\n",
            "Cost after 298538 iterations : Training Loss =  0.010242251986800944; Validation Loss = 0.019229080059488857\n",
            "Cost after 298539 iterations : Training Loss =  0.01024224400290127; Validation Loss = 0.019229077062526696\n",
            "Cost after 298540 iterations : Training Loss =  0.010242236019081948; Validation Loss = 0.019229074065607123\n",
            "Cost after 298541 iterations : Training Loss =  0.010242228035343144; Validation Loss = 0.01922907106873029\n",
            "Cost after 298542 iterations : Training Loss =  0.010242220051684651; Validation Loss = 0.019229068071895984\n",
            "Cost after 298543 iterations : Training Loss =  0.010242212068106452; Validation Loss = 0.01922906507510427\n",
            "Cost after 298544 iterations : Training Loss =  0.010242204084608818; Validation Loss = 0.019229062078355137\n",
            "Cost after 298545 iterations : Training Loss =  0.01024219610119144; Validation Loss = 0.019229059081648573\n",
            "Cost after 298546 iterations : Training Loss =  0.01024218811785448; Validation Loss = 0.019229056084984365\n",
            "Cost after 298547 iterations : Training Loss =  0.010242180134597958; Validation Loss = 0.019229053088362924\n",
            "Cost after 298548 iterations : Training Loss =  0.010242172151421819; Validation Loss = 0.01922905009178407\n",
            "Cost after 298549 iterations : Training Loss =  0.010242164168325968; Validation Loss = 0.01922904709524754\n",
            "Cost after 298550 iterations : Training Loss =  0.010242156185310568; Validation Loss = 0.019229044098753714\n",
            "Cost after 298551 iterations : Training Loss =  0.010242148202375553; Validation Loss = 0.019229041102302273\n",
            "Cost after 298552 iterations : Training Loss =  0.010242140219520869; Validation Loss = 0.01922903810589355\n",
            "Cost after 298553 iterations : Training Loss =  0.010242132236746613; Validation Loss = 0.01922903510952734\n",
            "Cost after 298554 iterations : Training Loss =  0.010242124254052763; Validation Loss = 0.019229032113203738\n",
            "Cost after 298555 iterations : Training Loss =  0.010242116271439257; Validation Loss = 0.019229029116922793\n",
            "Cost after 298556 iterations : Training Loss =  0.010242108288906012; Validation Loss = 0.019229026120683994\n",
            "Cost after 298557 iterations : Training Loss =  0.010242100306453254; Validation Loss = 0.019229023124488244\n",
            "Cost after 298558 iterations : Training Loss =  0.01024209232408099; Validation Loss = 0.019229020128334755\n",
            "Cost after 298559 iterations : Training Loss =  0.010242084341788837; Validation Loss = 0.019229017132223997\n",
            "Cost after 298560 iterations : Training Loss =  0.010242076359577268; Validation Loss = 0.01922901413615574\n",
            "Cost after 298561 iterations : Training Loss =  0.010242068377445946; Validation Loss = 0.0192290111401298\n",
            "Cost after 298562 iterations : Training Loss =  0.010242060395395063; Validation Loss = 0.01922900814414707\n",
            "Cost after 298563 iterations : Training Loss =  0.010242052413424504; Validation Loss = 0.019229005148206607\n",
            "Cost after 298564 iterations : Training Loss =  0.010242044431534375; Validation Loss = 0.019229002152308754\n",
            "Cost after 298565 iterations : Training Loss =  0.01024203644972455; Validation Loss = 0.019228999156453236\n",
            "Cost after 298566 iterations : Training Loss =  0.010242028467995079; Validation Loss = 0.01922899616064054\n",
            "Cost after 298567 iterations : Training Loss =  0.010242020486346023; Validation Loss = 0.019228993164870083\n",
            "Cost after 298568 iterations : Training Loss =  0.010242012504777312; Validation Loss = 0.019228990169142198\n",
            "Cost after 298569 iterations : Training Loss =  0.010242004523288937; Validation Loss = 0.01922898717345684\n",
            "Cost after 298570 iterations : Training Loss =  0.010241996541880937; Validation Loss = 0.01922898417781426\n",
            "Cost after 298571 iterations : Training Loss =  0.010241988560553394; Validation Loss = 0.01922898118221447\n",
            "Cost after 298572 iterations : Training Loss =  0.010241980579306118; Validation Loss = 0.019228978186656785\n",
            "Cost after 298573 iterations : Training Loss =  0.010241972598139163; Validation Loss = 0.01922897519114178\n",
            "Cost after 298574 iterations : Training Loss =  0.010241964617052608; Validation Loss = 0.01922897219566921\n",
            "Cost after 298575 iterations : Training Loss =  0.010241956636046443; Validation Loss = 0.01922896920023977\n",
            "Cost after 298576 iterations : Training Loss =  0.010241948655120599; Validation Loss = 0.019228966204852246\n",
            "Cost after 298577 iterations : Training Loss =  0.010241940674275097; Validation Loss = 0.019228963209507757\n",
            "Cost after 298578 iterations : Training Loss =  0.010241932693509908; Validation Loss = 0.01922896021420545\n",
            "Cost after 298579 iterations : Training Loss =  0.010241924712825128; Validation Loss = 0.019228957218945797\n",
            "Cost after 298580 iterations : Training Loss =  0.010241916732220704; Validation Loss = 0.01922895422372883\n",
            "Cost after 298581 iterations : Training Loss =  0.01024190875169664; Validation Loss = 0.019228951228554118\n",
            "Cost after 298582 iterations : Training Loss =  0.010241900771252917; Validation Loss = 0.019228948233421774\n",
            "Cost after 298583 iterations : Training Loss =  0.010241892790889509; Validation Loss = 0.019228945238332133\n",
            "Cost after 298584 iterations : Training Loss =  0.010241884810606434; Validation Loss = 0.019228942243285422\n",
            "Cost after 298585 iterations : Training Loss =  0.01024187683040373; Validation Loss = 0.01922893924828084\n",
            "Cost after 298586 iterations : Training Loss =  0.010241868850281406; Validation Loss = 0.01922893625331911\n",
            "Cost after 298587 iterations : Training Loss =  0.010241860870239374; Validation Loss = 0.0192289332583998\n",
            "Cost after 298588 iterations : Training Loss =  0.010241852890277785; Validation Loss = 0.0192289302635231\n",
            "Cost after 298589 iterations : Training Loss =  0.010241844910396473; Validation Loss = 0.01922892726868905\n",
            "Cost after 298590 iterations : Training Loss =  0.010241836930595447; Validation Loss = 0.019228924273897472\n",
            "Cost after 298591 iterations : Training Loss =  0.010241828950874762; Validation Loss = 0.019228921279148253\n",
            "Cost after 298592 iterations : Training Loss =  0.010241820971234558; Validation Loss = 0.019228918284441458\n",
            "Cost after 298593 iterations : Training Loss =  0.010241812991674615; Validation Loss = 0.019228915289777524\n",
            "Cost after 298594 iterations : Training Loss =  0.010241805012194887; Validation Loss = 0.01922891229515595\n",
            "Cost after 298595 iterations : Training Loss =  0.010241797032795588; Validation Loss = 0.019228909300576916\n",
            "Cost after 298596 iterations : Training Loss =  0.010241789053476644; Validation Loss = 0.019228906306040138\n",
            "Cost after 298597 iterations : Training Loss =  0.010241781074238015; Validation Loss = 0.01922890331154597\n",
            "Cost after 298598 iterations : Training Loss =  0.010241773095079737; Validation Loss = 0.019228900317094746\n",
            "Cost after 298599 iterations : Training Loss =  0.010241765116001767; Validation Loss = 0.019228897322685467\n",
            "Cost after 298600 iterations : Training Loss =  0.01024175713700409; Validation Loss = 0.019228894328318848\n",
            "Cost after 298601 iterations : Training Loss =  0.01024174915808678; Validation Loss = 0.0192288913339951\n",
            "Cost after 298602 iterations : Training Loss =  0.010241741179249928; Validation Loss = 0.019228888339713954\n",
            "Cost after 298603 iterations : Training Loss =  0.010241733200493193; Validation Loss = 0.01922888534547499\n",
            "Cost after 298604 iterations : Training Loss =  0.010241725221816827; Validation Loss = 0.019228882351278502\n",
            "Cost after 298605 iterations : Training Loss =  0.010241717243220854; Validation Loss = 0.019228879357124386\n",
            "Cost after 298606 iterations : Training Loss =  0.010241709264705167; Validation Loss = 0.019228876363013007\n",
            "Cost after 298607 iterations : Training Loss =  0.010241701286269769; Validation Loss = 0.019228873368944445\n",
            "Cost after 298608 iterations : Training Loss =  0.01024169330791481; Validation Loss = 0.019228870374918244\n",
            "Cost after 298609 iterations : Training Loss =  0.010241685329640036; Validation Loss = 0.019228867380934468\n",
            "Cost after 298610 iterations : Training Loss =  0.01024167735144567; Validation Loss = 0.0192288643869935\n",
            "Cost after 298611 iterations : Training Loss =  0.01024166937333157; Validation Loss = 0.019228861393094994\n",
            "Cost after 298612 iterations : Training Loss =  0.010241661395297813; Validation Loss = 0.01922885839923878\n",
            "Cost after 298613 iterations : Training Loss =  0.010241653417344336; Validation Loss = 0.019228855405425273\n",
            "Cost after 298614 iterations : Training Loss =  0.010241645439471292; Validation Loss = 0.01922885241165423\n",
            "Cost after 298615 iterations : Training Loss =  0.010241637461678449; Validation Loss = 0.019228849417925704\n",
            "Cost after 298616 iterations : Training Loss =  0.010241629483965942; Validation Loss = 0.019228846424239606\n",
            "Cost after 298617 iterations : Training Loss =  0.010241621506333749; Validation Loss = 0.019228843430595915\n",
            "Cost after 298618 iterations : Training Loss =  0.010241613528781866; Validation Loss = 0.019228840436995066\n",
            "Cost after 298619 iterations : Training Loss =  0.010241605551310363; Validation Loss = 0.019228837443436474\n",
            "Cost after 298620 iterations : Training Loss =  0.010241597573919101; Validation Loss = 0.01922883444992049\n",
            "Cost after 298621 iterations : Training Loss =  0.010241589596608118; Validation Loss = 0.01922883145644683\n",
            "Cost after 298622 iterations : Training Loss =  0.010241581619377509; Validation Loss = 0.019228828463015987\n",
            "Cost after 298623 iterations : Training Loss =  0.010241573642227194; Validation Loss = 0.019228825469627654\n",
            "Cost after 298624 iterations : Training Loss =  0.010241565665157121; Validation Loss = 0.019228822476281753\n",
            "Cost after 298625 iterations : Training Loss =  0.010241557688167383; Validation Loss = 0.019228819482978332\n",
            "Cost after 298626 iterations : Training Loss =  0.010241549711257994; Validation Loss = 0.019228816489717485\n",
            "Cost after 298627 iterations : Training Loss =  0.01024154173442881; Validation Loss = 0.019228813496498968\n",
            "Cost after 298628 iterations : Training Loss =  0.010241533757679944; Validation Loss = 0.019228810503322983\n",
            "Cost after 298629 iterations : Training Loss =  0.0102415257810115; Validation Loss = 0.019228807510189405\n",
            "Cost after 298630 iterations : Training Loss =  0.010241517804423301; Validation Loss = 0.019228804517098536\n",
            "Cost after 298631 iterations : Training Loss =  0.010241509827915328; Validation Loss = 0.019228801524050317\n",
            "Cost after 298632 iterations : Training Loss =  0.010241501851487705; Validation Loss = 0.019228798531044606\n",
            "Cost after 298633 iterations : Training Loss =  0.01024149387514041; Validation Loss = 0.019228795538080865\n",
            "Cost after 298634 iterations : Training Loss =  0.01024148589887332; Validation Loss = 0.019228792545160113\n",
            "Cost after 298635 iterations : Training Loss =  0.010241477922686567; Validation Loss = 0.01922878955228174\n",
            "Cost after 298636 iterations : Training Loss =  0.010241469946580163; Validation Loss = 0.019228786559445734\n",
            "Cost after 298637 iterations : Training Loss =  0.010241461970554009; Validation Loss = 0.01922878356665217\n",
            "Cost after 298638 iterations : Training Loss =  0.010241453994608187; Validation Loss = 0.019228780573901396\n",
            "Cost after 298639 iterations : Training Loss =  0.010241446018742564; Validation Loss = 0.019228777581193103\n",
            "Cost after 298640 iterations : Training Loss =  0.010241438042957206; Validation Loss = 0.019228774588527186\n",
            "Cost after 298641 iterations : Training Loss =  0.010241430067252272; Validation Loss = 0.019228771595903874\n",
            "Cost after 298642 iterations : Training Loss =  0.010241422091627548; Validation Loss = 0.019228768603323076\n",
            "Cost after 298643 iterations : Training Loss =  0.0102414141160831; Validation Loss = 0.019228765610784706\n",
            "Cost after 298644 iterations : Training Loss =  0.010241406140618954; Validation Loss = 0.019228762618288733\n",
            "Cost after 298645 iterations : Training Loss =  0.010241398165235107; Validation Loss = 0.019228759625835593\n",
            "Cost after 298646 iterations : Training Loss =  0.010241390189931544; Validation Loss = 0.01922875663342465\n",
            "Cost after 298647 iterations : Training Loss =  0.010241382214708213; Validation Loss = 0.019228753641056497\n",
            "Cost after 298648 iterations : Training Loss =  0.010241374239565225; Validation Loss = 0.019228750648730326\n",
            "Cost after 298649 iterations : Training Loss =  0.01024136626450245; Validation Loss = 0.01922874765644654\n",
            "Cost after 298650 iterations : Training Loss =  0.010241358289520034; Validation Loss = 0.019228744664205853\n",
            "Cost after 298651 iterations : Training Loss =  0.010241350314617773; Validation Loss = 0.019228741672007412\n",
            "Cost after 298652 iterations : Training Loss =  0.010241342339795841; Validation Loss = 0.019228738679851334\n",
            "Cost after 298653 iterations : Training Loss =  0.01024133436505424; Validation Loss = 0.019228735687737898\n",
            "Cost after 298654 iterations : Training Loss =  0.0102413263903929; Validation Loss = 0.019228732695666974\n",
            "Cost after 298655 iterations : Training Loss =  0.010241318415811815; Validation Loss = 0.019228729703638776\n",
            "Cost after 298656 iterations : Training Loss =  0.010241310441310922; Validation Loss = 0.019228726711652648\n",
            "Cost after 298657 iterations : Training Loss =  0.010241302466890374; Validation Loss = 0.019228723719709195\n",
            "Cost after 298658 iterations : Training Loss =  0.010241294492550136; Validation Loss = 0.019228720727808294\n",
            "Cost after 298659 iterations : Training Loss =  0.010241286518290085; Validation Loss = 0.019228717735949825\n",
            "Cost after 298660 iterations : Training Loss =  0.010241278544110331; Validation Loss = 0.019228714744133887\n",
            "Cost after 298661 iterations : Training Loss =  0.010241270570010879; Validation Loss = 0.019228711752360305\n",
            "Cost after 298662 iterations : Training Loss =  0.010241262595991736; Validation Loss = 0.019228708760629678\n",
            "Cost after 298663 iterations : Training Loss =  0.010241254622052665; Validation Loss = 0.019228705768940958\n",
            "Cost after 298664 iterations : Training Loss =  0.010241246648194114; Validation Loss = 0.019228702777294986\n",
            "Cost after 298665 iterations : Training Loss =  0.010241238674415662; Validation Loss = 0.019228699785691243\n",
            "Cost after 298666 iterations : Training Loss =  0.010241230700717434; Validation Loss = 0.01922869679412993\n",
            "Cost after 298667 iterations : Training Loss =  0.010241222727099476; Validation Loss = 0.01922869380261136\n",
            "Cost after 298668 iterations : Training Loss =  0.010241214753561899; Validation Loss = 0.019228690811135093\n",
            "Cost after 298669 iterations : Training Loss =  0.010241206780104546; Validation Loss = 0.019228687819701214\n",
            "Cost after 298670 iterations : Training Loss =  0.010241198806727407; Validation Loss = 0.019228684828310267\n",
            "Cost after 298671 iterations : Training Loss =  0.010241190833430523; Validation Loss = 0.019228681836961598\n",
            "Cost after 298672 iterations : Training Loss =  0.010241182860213902; Validation Loss = 0.019228678845655194\n",
            "Cost after 298673 iterations : Training Loss =  0.010241174887077565; Validation Loss = 0.01922867585439152\n",
            "Cost after 298674 iterations : Training Loss =  0.010241166914021459; Validation Loss = 0.019228672863170274\n",
            "Cost after 298675 iterations : Training Loss =  0.010241158941045572; Validation Loss = 0.019228669871991597\n",
            "Cost after 298676 iterations : Training Loss =  0.010241150968150052; Validation Loss = 0.019228666880854835\n",
            "Cost after 298677 iterations : Training Loss =  0.0102411429953346; Validation Loss = 0.01922866388976123\n",
            "Cost after 298678 iterations : Training Loss =  0.010241135022599521; Validation Loss = 0.019228660898709823\n",
            "Cost after 298679 iterations : Training Loss =  0.010241127049944702; Validation Loss = 0.019228657907700904\n",
            "Cost after 298680 iterations : Training Loss =  0.010241119077370051; Validation Loss = 0.019228654916734512\n",
            "Cost after 298681 iterations : Training Loss =  0.010241111104875678; Validation Loss = 0.0192286519258106\n",
            "Cost after 298682 iterations : Training Loss =  0.010241103132461522; Validation Loss = 0.019228648934929246\n",
            "Cost after 298683 iterations : Training Loss =  0.010241095160127679; Validation Loss = 0.01922864594409023\n",
            "Cost after 298684 iterations : Training Loss =  0.010241087187874092; Validation Loss = 0.01922864295329381\n",
            "Cost after 298685 iterations : Training Loss =  0.010241079215700692; Validation Loss = 0.019228639962539512\n",
            "Cost after 298686 iterations : Training Loss =  0.010241071243607514; Validation Loss = 0.01922863697182793\n",
            "Cost after 298687 iterations : Training Loss =  0.010241063271594616; Validation Loss = 0.019228633981158667\n",
            "Cost after 298688 iterations : Training Loss =  0.010241055299661928; Validation Loss = 0.01922863099053221\n",
            "Cost after 298689 iterations : Training Loss =  0.010241047327809456; Validation Loss = 0.019228627999947814\n",
            "Cost after 298690 iterations : Training Loss =  0.010241039356037318; Validation Loss = 0.019228625009405826\n",
            "Cost after 298691 iterations : Training Loss =  0.010241031384345243; Validation Loss = 0.0192286220189066\n",
            "Cost after 298692 iterations : Training Loss =  0.010241023412733563; Validation Loss = 0.019228619028449433\n",
            "Cost after 298693 iterations : Training Loss =  0.010241015441202116; Validation Loss = 0.01922861603803499\n",
            "Cost after 298694 iterations : Training Loss =  0.010241007469750789; Validation Loss = 0.019228613047662955\n",
            "Cost after 298695 iterations : Training Loss =  0.01024099949837976; Validation Loss = 0.01922861005733335\n",
            "Cost after 298696 iterations : Training Loss =  0.010240991527088942; Validation Loss = 0.019228607067046318\n",
            "Cost after 298697 iterations : Training Loss =  0.010240983555878365; Validation Loss = 0.019228604076802075\n",
            "Cost after 298698 iterations : Training Loss =  0.010240975584748081; Validation Loss = 0.019228601086599767\n",
            "Cost after 298699 iterations : Training Loss =  0.010240967613697898; Validation Loss = 0.019228598096440043\n",
            "Cost after 298700 iterations : Training Loss =  0.010240959642728024; Validation Loss = 0.019228595106322712\n",
            "Cost after 298701 iterations : Training Loss =  0.010240951671838359; Validation Loss = 0.019228592116248177\n",
            "Cost after 298702 iterations : Training Loss =  0.010240943701028935; Validation Loss = 0.01922858912621564\n",
            "Cost after 298703 iterations : Training Loss =  0.010240935730299681; Validation Loss = 0.019228586136225842\n",
            "Cost after 298704 iterations : Training Loss =  0.010240927759650749; Validation Loss = 0.019228583146278472\n",
            "Cost after 298705 iterations : Training Loss =  0.010240919789081947; Validation Loss = 0.019228580156373486\n",
            "Cost after 298706 iterations : Training Loss =  0.010240911818593332; Validation Loss = 0.019228577166510875\n",
            "Cost after 298707 iterations : Training Loss =  0.01024090384818503; Validation Loss = 0.01922857417669108\n",
            "Cost after 298708 iterations : Training Loss =  0.010240895877856891; Validation Loss = 0.019228571186913547\n",
            "Cost after 298709 iterations : Training Loss =  0.010240887907608997; Validation Loss = 0.01922856819717833\n",
            "Cost after 298710 iterations : Training Loss =  0.010240879937441258; Validation Loss = 0.019228565207485754\n",
            "Cost after 298711 iterations : Training Loss =  0.010240871967353777; Validation Loss = 0.01922856221783545\n",
            "Cost after 298712 iterations : Training Loss =  0.010240863997346535; Validation Loss = 0.01922855922822758\n",
            "Cost after 298713 iterations : Training Loss =  0.010240856027419481; Validation Loss = 0.019228556238662178\n",
            "Cost after 298714 iterations : Training Loss =  0.010240848057572652; Validation Loss = 0.019228553249139496\n",
            "Cost after 298715 iterations : Training Loss =  0.01024084008780598; Validation Loss = 0.019228550259658847\n",
            "Cost after 298716 iterations : Training Loss =  0.010240832118119594; Validation Loss = 0.019228547270221087\n",
            "Cost after 298717 iterations : Training Loss =  0.010240824148513376; Validation Loss = 0.01922854428082548\n",
            "Cost after 298718 iterations : Training Loss =  0.010240816178987313; Validation Loss = 0.019228541291472473\n",
            "Cost after 298719 iterations : Training Loss =  0.01024080820954159; Validation Loss = 0.01922853830216172\n",
            "Cost after 298720 iterations : Training Loss =  0.010240800240176005; Validation Loss = 0.019228535312893397\n",
            "Cost after 298721 iterations : Training Loss =  0.010240792270890553; Validation Loss = 0.01922853232366784\n",
            "Cost after 298722 iterations : Training Loss =  0.0102407843016854; Validation Loss = 0.019228529334484188\n",
            "Cost after 298723 iterations : Training Loss =  0.010240776332560422; Validation Loss = 0.019228526345343368\n",
            "Cost after 298724 iterations : Training Loss =  0.010240768363515642; Validation Loss = 0.019228523356244837\n",
            "Cost after 298725 iterations : Training Loss =  0.010240760394551017; Validation Loss = 0.01922852036718903\n",
            "Cost after 298726 iterations : Training Loss =  0.0102407524256667; Validation Loss = 0.019228517378175478\n",
            "Cost after 298727 iterations : Training Loss =  0.010240744456862514; Validation Loss = 0.01922851438920437\n",
            "Cost after 298728 iterations : Training Loss =  0.010240736488138467; Validation Loss = 0.019228511400275643\n",
            "Cost after 298729 iterations : Training Loss =  0.010240728519494719; Validation Loss = 0.019228508411389383\n",
            "Cost after 298730 iterations : Training Loss =  0.01024072055093104; Validation Loss = 0.019228505422545422\n",
            "Cost after 298731 iterations : Training Loss =  0.01024071258244772; Validation Loss = 0.019228502433744087\n",
            "Cost after 298732 iterations : Training Loss =  0.010240704614044539; Validation Loss = 0.019228499444984996\n",
            "Cost after 298733 iterations : Training Loss =  0.01024069664572147; Validation Loss = 0.01922849645626836\n",
            "Cost after 298734 iterations : Training Loss =  0.010240688677478613; Validation Loss = 0.019228493467594275\n",
            "Cost after 298735 iterations : Training Loss =  0.010240680709315953; Validation Loss = 0.019228490478962457\n",
            "Cost after 298736 iterations : Training Loss =  0.010240672741233463; Validation Loss = 0.019228487490373206\n",
            "Cost after 298737 iterations : Training Loss =  0.010240664773231293; Validation Loss = 0.019228484501826272\n",
            "Cost after 298738 iterations : Training Loss =  0.010240656805309185; Validation Loss = 0.01922848151332193\n",
            "Cost after 298739 iterations : Training Loss =  0.010240648837467226; Validation Loss = 0.019228478524859732\n",
            "Cost after 298740 iterations : Training Loss =  0.010240640869705522; Validation Loss = 0.019228475536439894\n",
            "Cost after 298741 iterations : Training Loss =  0.010240632902024078; Validation Loss = 0.019228472548062956\n",
            "Cost after 298742 iterations : Training Loss =  0.010240624934422644; Validation Loss = 0.01922846955972813\n",
            "Cost after 298743 iterations : Training Loss =  0.010240616966901548; Validation Loss = 0.01922846657143576\n",
            "Cost after 298744 iterations : Training Loss =  0.010240608999460575; Validation Loss = 0.01922846358318564\n",
            "Cost after 298745 iterations : Training Loss =  0.010240601032099677; Validation Loss = 0.01922846059497795\n",
            "Cost after 298746 iterations : Training Loss =  0.010240593064819132; Validation Loss = 0.019228457606813222\n",
            "Cost after 298747 iterations : Training Loss =  0.010240585097618587; Validation Loss = 0.019228454618690584\n",
            "Cost after 298748 iterations : Training Loss =  0.010240577130498371; Validation Loss = 0.019228451630610398\n",
            "Cost after 298749 iterations : Training Loss =  0.010240569163458366; Validation Loss = 0.0192284486425727\n",
            "Cost after 298750 iterations : Training Loss =  0.010240561196498316; Validation Loss = 0.019228445654577236\n",
            "Cost after 298751 iterations : Training Loss =  0.01024055322961861; Validation Loss = 0.01922844266662459\n",
            "Cost after 298752 iterations : Training Loss =  0.01024054526281902; Validation Loss = 0.019228439678713873\n",
            "Cost after 298753 iterations : Training Loss =  0.010240537296099595; Validation Loss = 0.019228436690845993\n",
            "Cost after 298754 iterations : Training Loss =  0.010240529329460351; Validation Loss = 0.019228433703020256\n",
            "Cost after 298755 iterations : Training Loss =  0.010240521362901303; Validation Loss = 0.019228430715236722\n",
            "Cost after 298756 iterations : Training Loss =  0.010240513396422433; Validation Loss = 0.01922842772749563\n",
            "Cost after 298757 iterations : Training Loss =  0.010240505430023718; Validation Loss = 0.01922842473979694\n",
            "Cost after 298758 iterations : Training Loss =  0.010240497463705136; Validation Loss = 0.019228421752140862\n",
            "Cost after 298759 iterations : Training Loss =  0.01024048949746669; Validation Loss = 0.019228418764527327\n",
            "Cost after 298760 iterations : Training Loss =  0.010240481531308441; Validation Loss = 0.01922841577695623\n",
            "Cost after 298761 iterations : Training Loss =  0.010240473565230387; Validation Loss = 0.019228412789427385\n",
            "Cost after 298762 iterations : Training Loss =  0.010240465599232475; Validation Loss = 0.019228409801940995\n",
            "Cost after 298763 iterations : Training Loss =  0.010240457633314728; Validation Loss = 0.019228406814496852\n",
            "Cost after 298764 iterations : Training Loss =  0.010240449667477104; Validation Loss = 0.01922840382709524\n",
            "Cost after 298765 iterations : Training Loss =  0.010240441701719656; Validation Loss = 0.01922840083973601\n",
            "Cost after 298766 iterations : Training Loss =  0.010240433736042427; Validation Loss = 0.019228397852419365\n",
            "Cost after 298767 iterations : Training Loss =  0.010240425770445276; Validation Loss = 0.019228394865144757\n",
            "Cost after 298768 iterations : Training Loss =  0.01024041780492833; Validation Loss = 0.01922839187791286\n",
            "Cost after 298769 iterations : Training Loss =  0.010240409839491564; Validation Loss = 0.019228388890723237\n",
            "Cost after 298770 iterations : Training Loss =  0.010240401874134813; Validation Loss = 0.019228385903576183\n",
            "Cost after 298771 iterations : Training Loss =  0.010240393908858307; Validation Loss = 0.01922838291647128\n",
            "Cost after 298772 iterations : Training Loss =  0.010240385943661993; Validation Loss = 0.019228379929408703\n",
            "Cost after 298773 iterations : Training Loss =  0.010240377978545764; Validation Loss = 0.019228376942388756\n",
            "Cost after 298774 iterations : Training Loss =  0.010240370013509683; Validation Loss = 0.01922837395541128\n",
            "Cost after 298775 iterations : Training Loss =  0.010240362048553829; Validation Loss = 0.019228370968475808\n",
            "Cost after 298776 iterations : Training Loss =  0.010240354083678028; Validation Loss = 0.019228367981583224\n",
            "Cost after 298777 iterations : Training Loss =  0.010240346118882438; Validation Loss = 0.01922836499473305\n",
            "Cost after 298778 iterations : Training Loss =  0.010240338154166933; Validation Loss = 0.019228362007924892\n",
            "Cost after 298779 iterations : Training Loss =  0.010240330189531594; Validation Loss = 0.01922835902115955\n",
            "Cost after 298780 iterations : Training Loss =  0.010240322224976384; Validation Loss = 0.019228356034436362\n",
            "Cost after 298781 iterations : Training Loss =  0.010240314260501412; Validation Loss = 0.019228353047755616\n",
            "Cost after 298782 iterations : Training Loss =  0.010240306296106502; Validation Loss = 0.019228350061116926\n",
            "Cost after 298783 iterations : Training Loss =  0.010240298331791724; Validation Loss = 0.019228347074521025\n",
            "Cost after 298784 iterations : Training Loss =  0.010240290367557078; Validation Loss = 0.019228344087967416\n",
            "Cost after 298785 iterations : Training Loss =  0.010240282403402579; Validation Loss = 0.01922834110145606\n",
            "Cost after 298786 iterations : Training Loss =  0.010240274439328217; Validation Loss = 0.019228338114987448\n",
            "Cost after 298787 iterations : Training Loss =  0.010240266475334056; Validation Loss = 0.01922833512856086\n",
            "Cost after 298788 iterations : Training Loss =  0.010240258511419947; Validation Loss = 0.01922833214217712\n",
            "Cost after 298789 iterations : Training Loss =  0.01024025054758603; Validation Loss = 0.019228329155835527\n",
            "Cost after 298790 iterations : Training Loss =  0.010240242583832174; Validation Loss = 0.01922832616953623\n",
            "Cost after 298791 iterations : Training Loss =  0.010240234620158488; Validation Loss = 0.019228323183279203\n",
            "Cost after 298792 iterations : Training Loss =  0.010240226656564898; Validation Loss = 0.01922832019706463\n",
            "Cost after 298793 iterations : Training Loss =  0.010240218693051431; Validation Loss = 0.019228317210892715\n",
            "Cost after 298794 iterations : Training Loss =  0.010240210729618207; Validation Loss = 0.019228314224762934\n",
            "Cost after 298795 iterations : Training Loss =  0.010240202766265057; Validation Loss = 0.01922831123867557\n",
            "Cost after 298796 iterations : Training Loss =  0.010240194802992012; Validation Loss = 0.019228308252630527\n",
            "Cost after 298797 iterations : Training Loss =  0.010240186839799166; Validation Loss = 0.019228305266628064\n",
            "Cost after 298798 iterations : Training Loss =  0.010240178876686257; Validation Loss = 0.019228302280667648\n",
            "Cost after 298799 iterations : Training Loss =  0.010240170913653573; Validation Loss = 0.01922829929475008\n",
            "Cost after 298800 iterations : Training Loss =  0.010240162950701053; Validation Loss = 0.019228296308874633\n",
            "Cost after 298801 iterations : Training Loss =  0.010240154987828662; Validation Loss = 0.019228293323041476\n",
            "Cost after 298802 iterations : Training Loss =  0.010240147025036363; Validation Loss = 0.019228290337250677\n",
            "Cost after 298803 iterations : Training Loss =  0.010240139062324153; Validation Loss = 0.019228287351502147\n",
            "Cost after 298804 iterations : Training Loss =  0.01024013109969209; Validation Loss = 0.019228284365796378\n",
            "Cost after 298805 iterations : Training Loss =  0.010240123137140074; Validation Loss = 0.019228281380132606\n",
            "Cost after 298806 iterations : Training Loss =  0.01024011517466822; Validation Loss = 0.019228278394511404\n",
            "Cost after 298807 iterations : Training Loss =  0.010240107212276537; Validation Loss = 0.019228275408932828\n",
            "Cost after 298808 iterations : Training Loss =  0.010240099249965003; Validation Loss = 0.019228272423396333\n",
            "Cost after 298809 iterations : Training Loss =  0.010240091287733451; Validation Loss = 0.019228269437902276\n",
            "Cost after 298810 iterations : Training Loss =  0.010240083325582017; Validation Loss = 0.019228266452450616\n",
            "Cost after 298811 iterations : Training Loss =  0.010240075363510816; Validation Loss = 0.019228263467041404\n",
            "Cost after 298812 iterations : Training Loss =  0.01024006740151963; Validation Loss = 0.019228260481674524\n",
            "Cost after 298813 iterations : Training Loss =  0.010240059439608564; Validation Loss = 0.01922825749634963\n",
            "Cost after 298814 iterations : Training Loss =  0.010240051477777623; Validation Loss = 0.01922825451106762\n",
            "Cost after 298815 iterations : Training Loss =  0.010240043516026831; Validation Loss = 0.01922825152582812\n",
            "Cost after 298816 iterations : Training Loss =  0.010240035554356137; Validation Loss = 0.019228248540630707\n",
            "Cost after 298817 iterations : Training Loss =  0.010240027592765417; Validation Loss = 0.019228245555475348\n",
            "Cost after 298818 iterations : Training Loss =  0.010240019631254864; Validation Loss = 0.01922824257036263\n",
            "Cost after 298819 iterations : Training Loss =  0.010240011669824479; Validation Loss = 0.01922823958529222\n",
            "Cost after 298820 iterations : Training Loss =  0.01024000370847415; Validation Loss = 0.019228236600264008\n",
            "Cost after 298821 iterations : Training Loss =  0.010239995747203938; Validation Loss = 0.019228233615278884\n",
            "Cost after 298822 iterations : Training Loss =  0.010239987786013852; Validation Loss = 0.019228230630335776\n",
            "Cost after 298823 iterations : Training Loss =  0.010239979824903763; Validation Loss = 0.019228227645434758\n",
            "Cost after 298824 iterations : Training Loss =  0.010239971863873798; Validation Loss = 0.019228224660575968\n",
            "Cost after 298825 iterations : Training Loss =  0.010239963902924013; Validation Loss = 0.019228221675760052\n",
            "Cost after 298826 iterations : Training Loss =  0.010239955942054275; Validation Loss = 0.019228218690986003\n",
            "Cost after 298827 iterations : Training Loss =  0.01023994798126466; Validation Loss = 0.019228215706254877\n",
            "Cost after 298828 iterations : Training Loss =  0.010239940020555092; Validation Loss = 0.01922821272156585\n",
            "Cost after 298829 iterations : Training Loss =  0.010239932059925665; Validation Loss = 0.019228209736918998\n",
            "Cost after 298830 iterations : Training Loss =  0.010239924099376222; Validation Loss = 0.019228206752314927\n",
            "Cost after 298831 iterations : Training Loss =  0.010239916138906931; Validation Loss = 0.019228203767753048\n",
            "Cost after 298832 iterations : Training Loss =  0.010239908178517741; Validation Loss = 0.019228200783233135\n",
            "Cost after 298833 iterations : Training Loss =  0.01023990021820873; Validation Loss = 0.01922819779875593\n",
            "Cost after 298834 iterations : Training Loss =  0.010239892257979687; Validation Loss = 0.019228194814321004\n",
            "Cost after 298835 iterations : Training Loss =  0.010239884297830766; Validation Loss = 0.019228191829928215\n",
            "Cost after 298836 iterations : Training Loss =  0.010239876337761888; Validation Loss = 0.019228188845577927\n",
            "Cost after 298837 iterations : Training Loss =  0.010239868377773102; Validation Loss = 0.01922818586126994\n",
            "Cost after 298838 iterations : Training Loss =  0.010239860417864432; Validation Loss = 0.01922818287700477\n",
            "Cost after 298839 iterations : Training Loss =  0.010239852458035825; Validation Loss = 0.019228179892781517\n",
            "Cost after 298840 iterations : Training Loss =  0.010239844498287295; Validation Loss = 0.019228176908600673\n",
            "Cost after 298841 iterations : Training Loss =  0.01023983653861888; Validation Loss = 0.019228173924462105\n",
            "Cost after 298842 iterations : Training Loss =  0.010239828579030536; Validation Loss = 0.019228170940366114\n",
            "Cost after 298843 iterations : Training Loss =  0.010239820619522277; Validation Loss = 0.019228167956312523\n",
            "Cost after 298844 iterations : Training Loss =  0.010239812660094015; Validation Loss = 0.019228164972301124\n",
            "Cost after 298845 iterations : Training Loss =  0.010239804700745972; Validation Loss = 0.01922816198833215\n",
            "Cost after 298846 iterations : Training Loss =  0.010239796741477916; Validation Loss = 0.019228159004405283\n",
            "Cost after 298847 iterations : Training Loss =  0.01023978878228991; Validation Loss = 0.019228156020520797\n",
            "Cost after 298848 iterations : Training Loss =  0.010239780823181966; Validation Loss = 0.01922815303667917\n",
            "Cost after 298849 iterations : Training Loss =  0.010239772864154196; Validation Loss = 0.019228150052879624\n",
            "Cost after 298850 iterations : Training Loss =  0.010239764905206348; Validation Loss = 0.019228147069122254\n",
            "Cost after 298851 iterations : Training Loss =  0.010239756946338689; Validation Loss = 0.01922814408540743\n",
            "Cost after 298852 iterations : Training Loss =  0.010239748987551067; Validation Loss = 0.019228141101734878\n",
            "Cost after 298853 iterations : Training Loss =  0.010239741028843506; Validation Loss = 0.019228138118104407\n",
            "Cost after 298854 iterations : Training Loss =  0.010239733070215988; Validation Loss = 0.019228135134516468\n",
            "Cost after 298855 iterations : Training Loss =  0.01023972511166851; Validation Loss = 0.019228132150970977\n",
            "Cost after 298856 iterations : Training Loss =  0.01023971715320118; Validation Loss = 0.01922812916746758\n",
            "Cost after 298857 iterations : Training Loss =  0.010239709194813873; Validation Loss = 0.019228126184006683\n",
            "Cost after 298858 iterations : Training Loss =  0.010239701236506606; Validation Loss = 0.019228123200587997\n",
            "Cost after 298859 iterations : Training Loss =  0.01023969327827943; Validation Loss = 0.01922812021721187\n",
            "Cost after 298860 iterations : Training Loss =  0.010239685320132214; Validation Loss = 0.01922811723387802\n",
            "Cost after 298861 iterations : Training Loss =  0.010239677362065283; Validation Loss = 0.019228114250586313\n",
            "Cost after 298862 iterations : Training Loss =  0.010239669404078212; Validation Loss = 0.019228111267336912\n",
            "Cost after 298863 iterations : Training Loss =  0.010239661446171295; Validation Loss = 0.01922810828413009\n",
            "Cost after 298864 iterations : Training Loss =  0.010239653488344441; Validation Loss = 0.019228105300965328\n",
            "Cost after 298865 iterations : Training Loss =  0.010239645530597561; Validation Loss = 0.01922810231784324\n",
            "Cost after 298866 iterations : Training Loss =  0.010239637572930789; Validation Loss = 0.01922809933476347\n",
            "Cost after 298867 iterations : Training Loss =  0.010239629615344097; Validation Loss = 0.01922809635172595\n",
            "Cost after 298868 iterations : Training Loss =  0.010239621657837372; Validation Loss = 0.01922809336873039\n",
            "Cost after 298869 iterations : Training Loss =  0.010239613700410781; Validation Loss = 0.019228090385777545\n",
            "Cost after 298870 iterations : Training Loss =  0.01023960574306416; Validation Loss = 0.01922808740286694\n",
            "Cost after 298871 iterations : Training Loss =  0.010239597785797638; Validation Loss = 0.019228084419998646\n",
            "Cost after 298872 iterations : Training Loss =  0.010239589828611086; Validation Loss = 0.019228081437172726\n",
            "Cost after 298873 iterations : Training Loss =  0.010239581871504658; Validation Loss = 0.019228078454388977\n",
            "Cost after 298874 iterations : Training Loss =  0.01023957391447827; Validation Loss = 0.019228075471647916\n",
            "Cost after 298875 iterations : Training Loss =  0.010239565957531957; Validation Loss = 0.01922807248894908\n",
            "Cost after 298876 iterations : Training Loss =  0.010239558000665575; Validation Loss = 0.019228069506292342\n",
            "Cost after 298877 iterations : Training Loss =  0.010239550043879385; Validation Loss = 0.0192280665236779\n",
            "Cost after 298878 iterations : Training Loss =  0.010239542087173174; Validation Loss = 0.01922806354110607\n",
            "Cost after 298879 iterations : Training Loss =  0.010239534130547024; Validation Loss = 0.019228060558576513\n",
            "Cost after 298880 iterations : Training Loss =  0.010239526174000813; Validation Loss = 0.019228057576089364\n",
            "Cost after 298881 iterations : Training Loss =  0.010239518217534714; Validation Loss = 0.01922805459364428\n",
            "Cost after 298882 iterations : Training Loss =  0.01023951026114871; Validation Loss = 0.019228051611241696\n",
            "Cost after 298883 iterations : Training Loss =  0.010239502304842637; Validation Loss = 0.019228048628881233\n",
            "Cost after 298884 iterations : Training Loss =  0.010239494348616597; Validation Loss = 0.019228045646563083\n",
            "Cost after 298885 iterations : Training Loss =  0.010239486392470692; Validation Loss = 0.01922804266428747\n",
            "Cost after 298886 iterations : Training Loss =  0.010239478436404752; Validation Loss = 0.01922803968205386\n",
            "Cost after 298887 iterations : Training Loss =  0.010239470480418804; Validation Loss = 0.019228036699862897\n",
            "Cost after 298888 iterations : Training Loss =  0.010239462524512983; Validation Loss = 0.019228033717714147\n",
            "Cost after 298889 iterations : Training Loss =  0.010239454568687044; Validation Loss = 0.019228030735607495\n",
            "Cost after 298890 iterations : Training Loss =  0.01023944661294128; Validation Loss = 0.01922802775354337\n",
            "Cost after 298891 iterations : Training Loss =  0.01023943865727555; Validation Loss = 0.019228024771521558\n",
            "Cost after 298892 iterations : Training Loss =  0.010239430701689851; Validation Loss = 0.019228021789542245\n",
            "Cost after 298893 iterations : Training Loss =  0.010239422746184109; Validation Loss = 0.019228018807604927\n",
            "Cost after 298894 iterations : Training Loss =  0.01023941479075843; Validation Loss = 0.019228015825709537\n",
            "Cost after 298895 iterations : Training Loss =  0.010239406835412716; Validation Loss = 0.019228012843857268\n",
            "Cost after 298896 iterations : Training Loss =  0.010239398880147008; Validation Loss = 0.019228009862046664\n",
            "Cost after 298897 iterations : Training Loss =  0.010239390924961405; Validation Loss = 0.019228006880278974\n",
            "Cost after 298898 iterations : Training Loss =  0.010239382969855736; Validation Loss = 0.019228003898553395\n",
            "Cost after 298899 iterations : Training Loss =  0.010239375014830228; Validation Loss = 0.019228000916870006\n",
            "Cost after 298900 iterations : Training Loss =  0.010239367059884541; Validation Loss = 0.019227997935228825\n",
            "Cost after 298901 iterations : Training Loss =  0.010239359105019018; Validation Loss = 0.019227994953630274\n",
            "Cost after 298902 iterations : Training Loss =  0.010239351150233474; Validation Loss = 0.019227991972073658\n",
            "Cost after 298903 iterations : Training Loss =  0.010239343195528; Validation Loss = 0.019227988990559275\n",
            "Cost after 298904 iterations : Training Loss =  0.010239335240902493; Validation Loss = 0.019227986009087285\n",
            "Cost after 298905 iterations : Training Loss =  0.010239327286356894; Validation Loss = 0.019227983027658\n",
            "Cost after 298906 iterations : Training Loss =  0.010239319331891449; Validation Loss = 0.019227980046270486\n",
            "Cost after 298907 iterations : Training Loss =  0.01023931137750596; Validation Loss = 0.01922797706492557\n",
            "Cost after 298908 iterations : Training Loss =  0.010239303423200532; Validation Loss = 0.019227974083623058\n",
            "Cost after 298909 iterations : Training Loss =  0.010239295468975033; Validation Loss = 0.01922797110236271\n",
            "Cost after 298910 iterations : Training Loss =  0.010239287514829616; Validation Loss = 0.01922796812114466\n",
            "Cost after 298911 iterations : Training Loss =  0.010239279560764147; Validation Loss = 0.019227965139968932\n",
            "Cost after 298912 iterations : Training Loss =  0.010239271606778731; Validation Loss = 0.019227962158835817\n",
            "Cost after 298913 iterations : Training Loss =  0.010239263652873283; Validation Loss = 0.0192279591777445\n",
            "Cost after 298914 iterations : Training Loss =  0.010239255699047862; Validation Loss = 0.019227956196695435\n",
            "Cost after 298915 iterations : Training Loss =  0.010239247745302367; Validation Loss = 0.019227953215688844\n",
            "Cost after 298916 iterations : Training Loss =  0.010239239791636975; Validation Loss = 0.019227950234724442\n",
            "Cost after 298917 iterations : Training Loss =  0.010239231838051555; Validation Loss = 0.01922794725380249\n",
            "Cost after 298918 iterations : Training Loss =  0.010239223884546098; Validation Loss = 0.019227944272922997\n",
            "Cost after 298919 iterations : Training Loss =  0.01023921593112072; Validation Loss = 0.019227941292085462\n",
            "Cost after 298920 iterations : Training Loss =  0.010239207977775274; Validation Loss = 0.019227938311290646\n",
            "Cost after 298921 iterations : Training Loss =  0.01023920002450981; Validation Loss = 0.019227935330537925\n",
            "Cost after 298922 iterations : Training Loss =  0.010239192071324366; Validation Loss = 0.0192279323498273\n",
            "Cost after 298923 iterations : Training Loss =  0.010239184118218956; Validation Loss = 0.019227929369158896\n",
            "Cost after 298924 iterations : Training Loss =  0.010239176165193526; Validation Loss = 0.019227926388533028\n",
            "Cost after 298925 iterations : Training Loss =  0.010239168212248032; Validation Loss = 0.01922792340794945\n",
            "Cost after 298926 iterations : Training Loss =  0.010239160259382622; Validation Loss = 0.019227920427408188\n",
            "Cost after 298927 iterations : Training Loss =  0.010239152306597079; Validation Loss = 0.019227917446909208\n",
            "Cost after 298928 iterations : Training Loss =  0.01023914435389167; Validation Loss = 0.01922791446645227\n",
            "Cost after 298929 iterations : Training Loss =  0.010239136401266143; Validation Loss = 0.019227911486037686\n",
            "Cost after 298930 iterations : Training Loss =  0.010239128448720611; Validation Loss = 0.019227908505665286\n",
            "Cost after 298931 iterations : Training Loss =  0.010239120496255085; Validation Loss = 0.019227905525335338\n",
            "Cost after 298932 iterations : Training Loss =  0.010239112543869514; Validation Loss = 0.0192279025450477\n",
            "Cost after 298933 iterations : Training Loss =  0.01023910459156399; Validation Loss = 0.01922789956480227\n",
            "Cost after 298934 iterations : Training Loss =  0.010239096639338426; Validation Loss = 0.01922789658459923\n",
            "Cost after 298935 iterations : Training Loss =  0.010239088687192801; Validation Loss = 0.019227893604438495\n",
            "Cost after 298936 iterations : Training Loss =  0.01023908073512721; Validation Loss = 0.019227890624319805\n",
            "Cost after 298937 iterations : Training Loss =  0.010239072783141592; Validation Loss = 0.01922788764424363\n",
            "Cost after 298938 iterations : Training Loss =  0.010239064831235954; Validation Loss = 0.01922788466420976\n",
            "Cost after 298939 iterations : Training Loss =  0.010239056879410229; Validation Loss = 0.019227881684218066\n",
            "Cost after 298940 iterations : Training Loss =  0.010239048927664576; Validation Loss = 0.01922787870426878\n",
            "Cost after 298941 iterations : Training Loss =  0.010239040975998872; Validation Loss = 0.01922787572436177\n",
            "Cost after 298942 iterations : Training Loss =  0.010239033024413159; Validation Loss = 0.019227872744496916\n",
            "Cost after 298943 iterations : Training Loss =  0.010239025072907311; Validation Loss = 0.019227869764674305\n",
            "Cost after 298944 iterations : Training Loss =  0.010239017121481541; Validation Loss = 0.019227866784894073\n",
            "Cost after 298945 iterations : Training Loss =  0.010239009170135718; Validation Loss = 0.019227863805155867\n",
            "Cost after 298946 iterations : Training Loss =  0.010239001218869915; Validation Loss = 0.019227860825460175\n",
            "Cost after 298947 iterations : Training Loss =  0.010238993267684014; Validation Loss = 0.019227857845806526\n",
            "Cost after 298948 iterations : Training Loss =  0.01023898531657809; Validation Loss = 0.01922785486619543\n",
            "Cost after 298949 iterations : Training Loss =  0.010238977365552153; Validation Loss = 0.019227851886626603\n",
            "Cost after 298950 iterations : Training Loss =  0.01023896941460608; Validation Loss = 0.0192278489070997\n",
            "Cost after 298951 iterations : Training Loss =  0.010238961463740124; Validation Loss = 0.019227845927615256\n",
            "Cost after 298952 iterations : Training Loss =  0.010238953512954117; Validation Loss = 0.019227842948173205\n",
            "Cost after 298953 iterations : Training Loss =  0.010238945562248051; Validation Loss = 0.019227839968773405\n",
            "Cost after 298954 iterations : Training Loss =  0.010238937611621875; Validation Loss = 0.01922783698941559\n",
            "Cost after 298955 iterations : Training Loss =  0.010238929661075685; Validation Loss = 0.019227834010100222\n",
            "Cost after 298956 iterations : Training Loss =  0.010238921710609509; Validation Loss = 0.019227831030827217\n",
            "Cost after 298957 iterations : Training Loss =  0.010238913760223305; Validation Loss = 0.019227828051596518\n",
            "Cost after 298958 iterations : Training Loss =  0.010238905809916954; Validation Loss = 0.019227825072407893\n",
            "Cost after 298959 iterations : Training Loss =  0.010238897859690645; Validation Loss = 0.019227822093261328\n",
            "Cost after 298960 iterations : Training Loss =  0.010238889909544245; Validation Loss = 0.019227819114157337\n",
            "Cost after 298961 iterations : Training Loss =  0.010238881959477832; Validation Loss = 0.01922781613509595\n",
            "Cost after 298962 iterations : Training Loss =  0.01023887400949141; Validation Loss = 0.0192278131560765\n",
            "Cost after 298963 iterations : Training Loss =  0.010238866059584918; Validation Loss = 0.019227810177099333\n",
            "Cost after 298964 iterations : Training Loss =  0.010238858109758301; Validation Loss = 0.01922780719816452\n",
            "Cost after 298965 iterations : Training Loss =  0.01023885016001174; Validation Loss = 0.019227804219271956\n",
            "Cost after 298966 iterations : Training Loss =  0.010238842210345026; Validation Loss = 0.01922780124042142\n",
            "Cost after 298967 iterations : Training Loss =  0.010238834260758388; Validation Loss = 0.01922779826161305\n",
            "Cost after 298968 iterations : Training Loss =  0.010238826311251574; Validation Loss = 0.019227795282847238\n",
            "Cost after 298969 iterations : Training Loss =  0.010238818361824838; Validation Loss = 0.01922779230412325\n",
            "Cost after 298970 iterations : Training Loss =  0.010238810412477949; Validation Loss = 0.019227789325441842\n",
            "Cost after 298971 iterations : Training Loss =  0.010238802463210918; Validation Loss = 0.019227786346802472\n",
            "Cost after 298972 iterations : Training Loss =  0.010238794514023972; Validation Loss = 0.019227783368205655\n",
            "Cost after 298973 iterations : Training Loss =  0.010238786564916988; Validation Loss = 0.0192277803896508\n",
            "Cost after 298974 iterations : Training Loss =  0.010238778615889895; Validation Loss = 0.01922777741113825\n",
            "Cost after 298975 iterations : Training Loss =  0.010238770666942702; Validation Loss = 0.019227774432668314\n",
            "Cost after 298976 iterations : Training Loss =  0.010238762718075506; Validation Loss = 0.019227771454240473\n",
            "Cost after 298977 iterations : Training Loss =  0.010238754769288257; Validation Loss = 0.019227768475854827\n",
            "Cost after 298978 iterations : Training Loss =  0.01023874682058087; Validation Loss = 0.0192277654975114\n",
            "Cost after 298979 iterations : Training Loss =  0.010238738871953512; Validation Loss = 0.019227762519210274\n",
            "Cost after 298980 iterations : Training Loss =  0.010238730923405992; Validation Loss = 0.01922775954095148\n",
            "Cost after 298981 iterations : Training Loss =  0.010238722974938486; Validation Loss = 0.019227756562734892\n",
            "Cost after 298982 iterations : Training Loss =  0.0102387150265509; Validation Loss = 0.019227753584560553\n",
            "Cost after 298983 iterations : Training Loss =  0.01023870707824317; Validation Loss = 0.019227750606427934\n",
            "Cost after 298984 iterations : Training Loss =  0.01023869913001542; Validation Loss = 0.019227747628337916\n",
            "Cost after 298985 iterations : Training Loss =  0.010238691181867716; Validation Loss = 0.019227744650290274\n",
            "Cost after 298986 iterations : Training Loss =  0.010238683233799837; Validation Loss = 0.0192277416722846\n",
            "Cost after 298987 iterations : Training Loss =  0.01023867528581192; Validation Loss = 0.019227738694321525\n",
            "Cost after 298988 iterations : Training Loss =  0.010238667337903769; Validation Loss = 0.019227735716400702\n",
            "Cost after 298989 iterations : Training Loss =  0.010238659390075707; Validation Loss = 0.019227732738521772\n",
            "Cost after 298990 iterations : Training Loss =  0.010238651442327577; Validation Loss = 0.01922772976068533\n",
            "Cost after 298991 iterations : Training Loss =  0.010238643494659307; Validation Loss = 0.019227726782891\n",
            "Cost after 298992 iterations : Training Loss =  0.010238635547070985; Validation Loss = 0.019227723805139037\n",
            "Cost after 298993 iterations : Training Loss =  0.010238627599562664; Validation Loss = 0.019227720827429358\n",
            "Cost after 298994 iterations : Training Loss =  0.010238619652134181; Validation Loss = 0.01922771784976196\n",
            "Cost after 298995 iterations : Training Loss =  0.010238611704785633; Validation Loss = 0.01922771487213677\n",
            "Cost after 298996 iterations : Training Loss =  0.010238603757516965; Validation Loss = 0.019227711894553693\n",
            "Cost after 298997 iterations : Training Loss =  0.010238595810328247; Validation Loss = 0.01922770891701259\n",
            "Cost after 298998 iterations : Training Loss =  0.010238587863219508; Validation Loss = 0.01922770593951402\n",
            "Cost after 298999 iterations : Training Loss =  0.010238579916190559; Validation Loss = 0.019227702962057773\n",
            "Cost after 299000 iterations : Training Loss =  0.010238571969241621; Validation Loss = 0.01922769998464358\n",
            "Cost after 299001 iterations : Training Loss =  0.010238564022372547; Validation Loss = 0.0192276970072717\n",
            "Cost after 299002 iterations : Training Loss =  0.010238556075583383; Validation Loss = 0.01922769402994223\n",
            "Cost after 299003 iterations : Training Loss =  0.010238548128874156; Validation Loss = 0.01922769105265497\n",
            "Cost after 299004 iterations : Training Loss =  0.010238540182244822; Validation Loss = 0.0192276880754099\n",
            "Cost after 299005 iterations : Training Loss =  0.010238532235695416; Validation Loss = 0.019227685098206968\n",
            "Cost after 299006 iterations : Training Loss =  0.010238524289225904; Validation Loss = 0.019227682121046287\n",
            "Cost after 299007 iterations : Training Loss =  0.01023851634283636; Validation Loss = 0.019227679143927538\n",
            "Cost after 299008 iterations : Training Loss =  0.010238508396526581; Validation Loss = 0.01922767616685119\n",
            "Cost after 299009 iterations : Training Loss =  0.010238500450296884; Validation Loss = 0.019227673189817215\n",
            "Cost after 299010 iterations : Training Loss =  0.010238492504146955; Validation Loss = 0.01922767021282539\n",
            "Cost after 299011 iterations : Training Loss =  0.010238484558076912; Validation Loss = 0.019227667235875708\n",
            "Cost after 299012 iterations : Training Loss =  0.010238476612086895; Validation Loss = 0.019227664258968363\n",
            "Cost after 299013 iterations : Training Loss =  0.010238468666176686; Validation Loss = 0.019227661282103167\n",
            "Cost after 299014 iterations : Training Loss =  0.010238460720346483; Validation Loss = 0.019227658305280077\n",
            "Cost after 299015 iterations : Training Loss =  0.010238452774596118; Validation Loss = 0.019227655328499356\n",
            "Cost after 299016 iterations : Training Loss =  0.010238444828925561; Validation Loss = 0.01922765235176108\n",
            "Cost after 299017 iterations : Training Loss =  0.010238436883334968; Validation Loss = 0.019227649375064847\n",
            "Cost after 299018 iterations : Training Loss =  0.010238428937824347; Validation Loss = 0.0192276463984108\n",
            "Cost after 299019 iterations : Training Loss =  0.0102384209923935; Validation Loss = 0.01922764342179895\n",
            "Cost after 299020 iterations : Training Loss =  0.01023841304704265; Validation Loss = 0.019227640445229364\n",
            "Cost after 299021 iterations : Training Loss =  0.010238405101771654; Validation Loss = 0.01922763746870176\n",
            "Cost after 299022 iterations : Training Loss =  0.010238397156580512; Validation Loss = 0.019227634492216445\n",
            "Cost after 299023 iterations : Training Loss =  0.010238389211469337; Validation Loss = 0.019227631515773322\n",
            "Cost after 299024 iterations : Training Loss =  0.01023838126643796; Validation Loss = 0.019227628539372665\n",
            "Cost after 299025 iterations : Training Loss =  0.010238373321486603; Validation Loss = 0.0192276255630143\n",
            "Cost after 299026 iterations : Training Loss =  0.010238365376615011; Validation Loss = 0.01922762258669772\n",
            "Cost after 299027 iterations : Training Loss =  0.010238357431823338; Validation Loss = 0.019227619610423713\n",
            "Cost after 299028 iterations : Training Loss =  0.010238349487111489; Validation Loss = 0.019227616634191813\n",
            "Cost after 299029 iterations : Training Loss =  0.010238341542479693; Validation Loss = 0.01922761365800185\n",
            "Cost after 299030 iterations : Training Loss =  0.010238333597927685; Validation Loss = 0.019227610681854627\n",
            "Cost after 299031 iterations : Training Loss =  0.010238325653455503; Validation Loss = 0.019227607705749314\n",
            "Cost after 299032 iterations : Training Loss =  0.010238317709063239; Validation Loss = 0.019227604729686338\n",
            "Cost after 299033 iterations : Training Loss =  0.010238309764750922; Validation Loss = 0.019227601753665557\n",
            "Cost after 299034 iterations : Training Loss =  0.01023830182051846; Validation Loss = 0.019227598777687024\n",
            "Cost after 299035 iterations : Training Loss =  0.010238293876365821; Validation Loss = 0.019227595801750555\n",
            "Cost after 299036 iterations : Training Loss =  0.010238285932293064; Validation Loss = 0.019227592825856506\n",
            "Cost after 299037 iterations : Training Loss =  0.010238277988300242; Validation Loss = 0.01922758985000424\n",
            "Cost after 299038 iterations : Training Loss =  0.01023827004438724; Validation Loss = 0.019227586874194312\n",
            "Cost after 299039 iterations : Training Loss =  0.010238262100554243; Validation Loss = 0.019227583898426652\n",
            "Cost after 299040 iterations : Training Loss =  0.010238254156800965; Validation Loss = 0.019227580922701003\n",
            "Cost after 299041 iterations : Training Loss =  0.010238246213127562; Validation Loss = 0.01922757794701793\n",
            "Cost after 299042 iterations : Training Loss =  0.010238238269534064; Validation Loss = 0.019227574971376817\n",
            "Cost after 299043 iterations : Training Loss =  0.01023823032602049; Validation Loss = 0.019227571995777945\n",
            "Cost after 299044 iterations : Training Loss =  0.0102382223825867; Validation Loss = 0.01922756902022152\n",
            "Cost after 299045 iterations : Training Loss =  0.010238214439232887; Validation Loss = 0.019227566044706965\n",
            "Cost after 299046 iterations : Training Loss =  0.010238206495958925; Validation Loss = 0.019227563069234673\n",
            "Cost after 299047 iterations : Training Loss =  0.010238198552764706; Validation Loss = 0.019227560093804184\n",
            "Cost after 299048 iterations : Training Loss =  0.010238190609650439; Validation Loss = 0.019227557118416297\n",
            "Cost after 299049 iterations : Training Loss =  0.010238182666615985; Validation Loss = 0.01922755414307059\n",
            "Cost after 299050 iterations : Training Loss =  0.0102381747236615; Validation Loss = 0.01922755116776703\n",
            "Cost after 299051 iterations : Training Loss =  0.010238166780786801; Validation Loss = 0.01922754819250606\n",
            "Cost after 299052 iterations : Training Loss =  0.010238158837991989; Validation Loss = 0.019227545217286913\n",
            "Cost after 299053 iterations : Training Loss =  0.010238150895277045; Validation Loss = 0.01922754224211021\n",
            "Cost after 299054 iterations : Training Loss =  0.010238142952641867; Validation Loss = 0.01922753926697532\n",
            "Cost after 299055 iterations : Training Loss =  0.010238135010086668; Validation Loss = 0.019227536291882886\n",
            "Cost after 299056 iterations : Training Loss =  0.010238127067611262; Validation Loss = 0.019227533316832537\n",
            "Cost after 299057 iterations : Training Loss =  0.010238119125215724; Validation Loss = 0.01922753034182435\n",
            "Cost after 299058 iterations : Training Loss =  0.010238111182900076; Validation Loss = 0.019227527366858427\n",
            "Cost after 299059 iterations : Training Loss =  0.010238103240664195; Validation Loss = 0.01922752439193467\n",
            "Cost after 299060 iterations : Training Loss =  0.010238095298508254; Validation Loss = 0.019227521417053077\n",
            "Cost after 299061 iterations : Training Loss =  0.01023808735643215; Validation Loss = 0.019227518442213883\n",
            "Cost after 299062 iterations : Training Loss =  0.0102380794144358; Validation Loss = 0.019227515467416623\n",
            "Cost after 299063 iterations : Training Loss =  0.010238071472519419; Validation Loss = 0.019227512492661875\n",
            "Cost after 299064 iterations : Training Loss =  0.010238063530682868; Validation Loss = 0.019227509517949132\n",
            "Cost after 299065 iterations : Training Loss =  0.010238055588926065; Validation Loss = 0.01922750654327877\n",
            "Cost after 299066 iterations : Training Loss =  0.010238047647249233; Validation Loss = 0.019227503568650616\n",
            "Cost after 299067 iterations : Training Loss =  0.010238039705652116; Validation Loss = 0.019227500594064157\n",
            "Cost after 299068 iterations : Training Loss =  0.010238031764135036; Validation Loss = 0.01922749761952002\n",
            "Cost after 299069 iterations : Training Loss =  0.010238023822697662; Validation Loss = 0.0192274946450184\n",
            "Cost after 299070 iterations : Training Loss =  0.010238015881340181; Validation Loss = 0.019227491670558962\n",
            "Cost after 299071 iterations : Training Loss =  0.010238007940062535; Validation Loss = 0.01922748869614112\n",
            "Cost after 299072 iterations : Training Loss =  0.010237999998864647; Validation Loss = 0.019227485721765827\n",
            "Cost after 299073 iterations : Training Loss =  0.010237992057746632; Validation Loss = 0.019227482747433026\n",
            "Cost after 299074 iterations : Training Loss =  0.01023798411670853; Validation Loss = 0.019227479773141886\n",
            "Cost after 299075 iterations : Training Loss =  0.010237976175750205; Validation Loss = 0.019227476798893212\n",
            "Cost after 299076 iterations : Training Loss =  0.010237968234871721; Validation Loss = 0.01922747382468667\n",
            "Cost after 299077 iterations : Training Loss =  0.010237960294073043; Validation Loss = 0.019227470850522253\n",
            "Cost after 299078 iterations : Training Loss =  0.010237952353354255; Validation Loss = 0.019227467876400117\n",
            "Cost after 299079 iterations : Training Loss =  0.010237944412715338; Validation Loss = 0.019227464902319667\n",
            "Cost after 299080 iterations : Training Loss =  0.010237936472156199; Validation Loss = 0.019227461928281588\n",
            "Cost after 299081 iterations : Training Loss =  0.010237928531676866; Validation Loss = 0.01922745895428604\n",
            "Cost after 299082 iterations : Training Loss =  0.010237920591277453; Validation Loss = 0.019227455980332358\n",
            "Cost after 299083 iterations : Training Loss =  0.01023791265095777; Validation Loss = 0.019227453006420894\n",
            "Cost after 299084 iterations : Training Loss =  0.010237904710717964; Validation Loss = 0.019227450032551757\n",
            "Cost after 299085 iterations : Training Loss =  0.010237896770558022; Validation Loss = 0.019227447058724624\n",
            "Cost after 299086 iterations : Training Loss =  0.010237888830477809; Validation Loss = 0.019227444084939913\n",
            "Cost after 299087 iterations : Training Loss =  0.010237880890477478; Validation Loss = 0.01922744111119728\n",
            "Cost after 299088 iterations : Training Loss =  0.01023787295055694; Validation Loss = 0.019227438137496413\n",
            "Cost after 299089 iterations : Training Loss =  0.010237865010716225; Validation Loss = 0.01922743516383839\n",
            "Cost after 299090 iterations : Training Loss =  0.010237857070955338; Validation Loss = 0.019227432190222014\n",
            "Cost after 299091 iterations : Training Loss =  0.010237849131274353; Validation Loss = 0.019227429216647895\n",
            "Cost after 299092 iterations : Training Loss =  0.010237841191673003; Validation Loss = 0.01922742624311601\n",
            "Cost after 299093 iterations : Training Loss =  0.010237833252151664; Validation Loss = 0.019227423269626082\n",
            "Cost after 299094 iterations : Training Loss =  0.010237825312710029; Validation Loss = 0.019227420296178747\n",
            "Cost after 299095 iterations : Training Loss =  0.010237817373348269; Validation Loss = 0.019227417322773152\n",
            "Cost after 299096 iterations : Training Loss =  0.010237809434066322; Validation Loss = 0.019227414349410117\n",
            "Cost after 299097 iterations : Training Loss =  0.010237801494864125; Validation Loss = 0.01922741137608867\n",
            "Cost after 299098 iterations : Training Loss =  0.010237793555741845; Validation Loss = 0.01922740840280987\n",
            "Cost after 299099 iterations : Training Loss =  0.010237785616699227; Validation Loss = 0.019227405429573078\n",
            "Cost after 299100 iterations : Training Loss =  0.010237777677736582; Validation Loss = 0.019227402456378582\n",
            "Cost after 299101 iterations : Training Loss =  0.010237769738853705; Validation Loss = 0.019227399483226236\n",
            "Cost after 299102 iterations : Training Loss =  0.010237761800050578; Validation Loss = 0.019227396510115832\n",
            "Cost after 299103 iterations : Training Loss =  0.010237753861327293; Validation Loss = 0.019227393537047947\n",
            "Cost after 299104 iterations : Training Loss =  0.01023774592268386; Validation Loss = 0.019227390564021802\n",
            "Cost after 299105 iterations : Training Loss =  0.010237737984120148; Validation Loss = 0.019227387591038\n",
            "Cost after 299106 iterations : Training Loss =  0.010237730045636278; Validation Loss = 0.019227384618096748\n",
            "Cost after 299107 iterations : Training Loss =  0.01023772210723226; Validation Loss = 0.0192273816451971\n",
            "Cost after 299108 iterations : Training Loss =  0.010237714168907959; Validation Loss = 0.01922737867233967\n",
            "Cost after 299109 iterations : Training Loss =  0.010237706230663541; Validation Loss = 0.01922737569952423\n",
            "Cost after 299110 iterations : Training Loss =  0.010237698292498898; Validation Loss = 0.019227372726751235\n",
            "Cost after 299111 iterations : Training Loss =  0.010237690354414028; Validation Loss = 0.01922736975401998\n",
            "Cost after 299112 iterations : Training Loss =  0.010237682416408971; Validation Loss = 0.01922736678133155\n",
            "Cost after 299113 iterations : Training Loss =  0.010237674478483718; Validation Loss = 0.0192273638086849\n",
            "Cost after 299114 iterations : Training Loss =  0.010237666540638225; Validation Loss = 0.01922736083608047\n",
            "Cost after 299115 iterations : Training Loss =  0.010237658602872585; Validation Loss = 0.019227357863517962\n",
            "Cost after 299116 iterations : Training Loss =  0.01023765066518668; Validation Loss = 0.019227354890997958\n",
            "Cost after 299117 iterations : Training Loss =  0.010237642727580607; Validation Loss = 0.019227351918519636\n",
            "Cost after 299118 iterations : Training Loss =  0.01023763479005435; Validation Loss = 0.019227348946083866\n",
            "Cost after 299119 iterations : Training Loss =  0.010237626852607817; Validation Loss = 0.019227345973690264\n",
            "Cost after 299120 iterations : Training Loss =  0.010237618915241154; Validation Loss = 0.01922734300133861\n",
            "Cost after 299121 iterations : Training Loss =  0.010237610977954224; Validation Loss = 0.019227340029029146\n",
            "Cost after 299122 iterations : Training Loss =  0.010237603040747133; Validation Loss = 0.019227337056762037\n",
            "Cost after 299123 iterations : Training Loss =  0.010237595103619754; Validation Loss = 0.01922733408453674\n",
            "Cost after 299124 iterations : Training Loss =  0.010237587166572217; Validation Loss = 0.01922733111235357\n",
            "Cost after 299125 iterations : Training Loss =  0.010237579229604488; Validation Loss = 0.01922732814021256\n",
            "Cost after 299126 iterations : Training Loss =  0.010237571292716496; Validation Loss = 0.01922732516811377\n",
            "Cost after 299127 iterations : Training Loss =  0.010237563355908261; Validation Loss = 0.019227322196057346\n",
            "Cost after 299128 iterations : Training Loss =  0.010237555419179896; Validation Loss = 0.01922731922404289\n",
            "Cost after 299129 iterations : Training Loss =  0.01023754748253125; Validation Loss = 0.019227316252070664\n",
            "Cost after 299130 iterations : Training Loss =  0.01023753954596238; Validation Loss = 0.019227313280140334\n",
            "Cost after 299131 iterations : Training Loss =  0.010237531609473311; Validation Loss = 0.01922731030825239\n",
            "Cost after 299132 iterations : Training Loss =  0.01023752367306406; Validation Loss = 0.019227307336406506\n",
            "Cost after 299133 iterations : Training Loss =  0.010237515736734606; Validation Loss = 0.019227304364602838\n",
            "Cost after 299134 iterations : Training Loss =  0.01023750780048479; Validation Loss = 0.01922730139284103\n",
            "Cost after 299135 iterations : Training Loss =  0.010237499864314778; Validation Loss = 0.019227298421121867\n",
            "Cost after 299136 iterations : Training Loss =  0.010237491928224682; Validation Loss = 0.019227295449444456\n",
            "Cost after 299137 iterations : Training Loss =  0.010237483992214259; Validation Loss = 0.01922729247780934\n",
            "Cost after 299138 iterations : Training Loss =  0.01023747605628357; Validation Loss = 0.019227289506216185\n",
            "Cost after 299139 iterations : Training Loss =  0.010237468120432717; Validation Loss = 0.019227286534664982\n",
            "Cost after 299140 iterations : Training Loss =  0.010237460184661634; Validation Loss = 0.01922728356315623\n",
            "Cost after 299141 iterations : Training Loss =  0.010237452248970314; Validation Loss = 0.01922728059168948\n",
            "Cost after 299142 iterations : Training Loss =  0.010237444313358672; Validation Loss = 0.01922727762026494\n",
            "Cost after 299143 iterations : Training Loss =  0.010237436377826896; Validation Loss = 0.01922727464888241\n",
            "Cost after 299144 iterations : Training Loss =  0.010237428442374904; Validation Loss = 0.019227271677542172\n",
            "Cost after 299145 iterations : Training Loss =  0.010237420507002652; Validation Loss = 0.019227268706244107\n",
            "Cost after 299146 iterations : Training Loss =  0.01023741257171009; Validation Loss = 0.019227265734988064\n",
            "Cost after 299147 iterations : Training Loss =  0.010237404636497336; Validation Loss = 0.019227262763773924\n",
            "Cost after 299148 iterations : Training Loss =  0.01023739670136439; Validation Loss = 0.019227259792602168\n",
            "Cost after 299149 iterations : Training Loss =  0.010237388766311181; Validation Loss = 0.0192272568214726\n",
            "Cost after 299150 iterations : Training Loss =  0.01023738083133776; Validation Loss = 0.0192272538503849\n",
            "Cost after 299151 iterations : Training Loss =  0.010237372896443999; Validation Loss = 0.0192272508793395\n",
            "Cost after 299152 iterations : Training Loss =  0.010237364961630092; Validation Loss = 0.019227247908336124\n",
            "Cost after 299153 iterations : Training Loss =  0.010237357026895834; Validation Loss = 0.01922724493737491\n",
            "Cost after 299154 iterations : Training Loss =  0.010237349092241464; Validation Loss = 0.019227241966455654\n",
            "Cost after 299155 iterations : Training Loss =  0.01023734115766675; Validation Loss = 0.019227238995578568\n",
            "Cost after 299156 iterations : Training Loss =  0.010237333223171866; Validation Loss = 0.019227236024743542\n",
            "Cost after 299157 iterations : Training Loss =  0.010237325288756672; Validation Loss = 0.019227233053950597\n",
            "Cost after 299158 iterations : Training Loss =  0.010237317354421247; Validation Loss = 0.01922723008320039\n",
            "Cost after 299159 iterations : Training Loss =  0.010237309420165605; Validation Loss = 0.019227227112491724\n",
            "Cost after 299160 iterations : Training Loss =  0.010237301485989711; Validation Loss = 0.019227224141825327\n",
            "Cost after 299161 iterations : Training Loss =  0.010237293551893517; Validation Loss = 0.019227221171200883\n",
            "Cost after 299162 iterations : Training Loss =  0.010237285617877035; Validation Loss = 0.019227218200618738\n",
            "Cost after 299163 iterations : Training Loss =  0.010237277683940414; Validation Loss = 0.019227215230078695\n",
            "Cost after 299164 iterations : Training Loss =  0.010237269750083534; Validation Loss = 0.019227212259580757\n",
            "Cost after 299165 iterations : Training Loss =  0.010237261816306303; Validation Loss = 0.019227209289124844\n",
            "Cost after 299166 iterations : Training Loss =  0.010237253882608937; Validation Loss = 0.019227206318710985\n",
            "Cost after 299167 iterations : Training Loss =  0.01023724594899117; Validation Loss = 0.01922720334833952\n",
            "Cost after 299168 iterations : Training Loss =  0.010237238015453246; Validation Loss = 0.01922720037801012\n",
            "Cost after 299169 iterations : Training Loss =  0.01023723008199495; Validation Loss = 0.01922719740772265\n",
            "Cost after 299170 iterations : Training Loss =  0.01023722214861649; Validation Loss = 0.019227194437477203\n",
            "Cost after 299171 iterations : Training Loss =  0.01023721421531779; Validation Loss = 0.019227191467274075\n",
            "Cost after 299172 iterations : Training Loss =  0.010237206282098819; Validation Loss = 0.019227188497112628\n",
            "Cost after 299173 iterations : Training Loss =  0.010237198348959578; Validation Loss = 0.019227185526993703\n",
            "Cost after 299174 iterations : Training Loss =  0.010237190415900021; Validation Loss = 0.01922718255691713\n",
            "Cost after 299175 iterations : Training Loss =  0.010237182482920215; Validation Loss = 0.019227179586882397\n",
            "Cost after 299176 iterations : Training Loss =  0.0102371745500201; Validation Loss = 0.019227176616889746\n",
            "Cost after 299177 iterations : Training Loss =  0.010237166617199771; Validation Loss = 0.019227173646939127\n",
            "Cost after 299178 iterations : Training Loss =  0.0102371586844592; Validation Loss = 0.01922717067703068\n",
            "Cost after 299179 iterations : Training Loss =  0.010237150751798317; Validation Loss = 0.01922716770716437\n",
            "Cost after 299180 iterations : Training Loss =  0.010237142819217185; Validation Loss = 0.019227164737340336\n",
            "Cost after 299181 iterations : Training Loss =  0.010237134886715758; Validation Loss = 0.019227161767557774\n",
            "Cost after 299182 iterations : Training Loss =  0.010237126954294018; Validation Loss = 0.01922715879781786\n",
            "Cost after 299183 iterations : Training Loss =  0.010237119021952085; Validation Loss = 0.019227155828119683\n",
            "Cost after 299184 iterations : Training Loss =  0.010237111089689825; Validation Loss = 0.019227152858463936\n",
            "Cost after 299185 iterations : Training Loss =  0.010237103157507293; Validation Loss = 0.019227149888850156\n",
            "Cost after 299186 iterations : Training Loss =  0.010237095225404518; Validation Loss = 0.019227146919278383\n",
            "Cost after 299187 iterations : Training Loss =  0.01023708729338143; Validation Loss = 0.019227143949748646\n",
            "Cost after 299188 iterations : Training Loss =  0.010237079361438129; Validation Loss = 0.019227140980261296\n",
            "Cost after 299189 iterations : Training Loss =  0.010237071429574463; Validation Loss = 0.019227138010815964\n",
            "Cost after 299190 iterations : Training Loss =  0.010237063497790607; Validation Loss = 0.019227135041412234\n",
            "Cost after 299191 iterations : Training Loss =  0.010237055566086421; Validation Loss = 0.01922713207205117\n",
            "Cost after 299192 iterations : Training Loss =  0.010237047634461935; Validation Loss = 0.01922712910273196\n",
            "Cost after 299193 iterations : Training Loss =  0.010237039702917108; Validation Loss = 0.019227126133454953\n",
            "Cost after 299194 iterations : Training Loss =  0.010237031771452108; Validation Loss = 0.01922712316422017\n",
            "Cost after 299195 iterations : Training Loss =  0.010237023840066759; Validation Loss = 0.01922712019502758\n",
            "Cost after 299196 iterations : Training Loss =  0.010237015908761122; Validation Loss = 0.01922711722587652\n",
            "Cost after 299197 iterations : Training Loss =  0.010237007977535266; Validation Loss = 0.019227114256767843\n",
            "Cost after 299198 iterations : Training Loss =  0.010237000046389014; Validation Loss = 0.01922711128770145\n",
            "Cost after 299199 iterations : Training Loss =  0.010236992115322542; Validation Loss = 0.01922710831867685\n",
            "Cost after 299200 iterations : Training Loss =  0.010236984184335775; Validation Loss = 0.019227105349694126\n",
            "Cost after 299201 iterations : Training Loss =  0.010236976253428698; Validation Loss = 0.01922710238075383\n",
            "Cost after 299202 iterations : Training Loss =  0.010236968322601273; Validation Loss = 0.019227099411855516\n",
            "Cost after 299203 iterations : Training Loss =  0.010236960391853664; Validation Loss = 0.019227096442999102\n",
            "Cost after 299204 iterations : Training Loss =  0.010236952461185774; Validation Loss = 0.019227093474185174\n",
            "Cost after 299205 iterations : Training Loss =  0.010236944530597411; Validation Loss = 0.019227090505413293\n",
            "Cost after 299206 iterations : Training Loss =  0.010236936600088911; Validation Loss = 0.01922708753668332\n",
            "Cost after 299207 iterations : Training Loss =  0.010236928669659971; Validation Loss = 0.019227084567995305\n",
            "Cost after 299208 iterations : Training Loss =  0.010236920739310791; Validation Loss = 0.019227081599349622\n",
            "Cost after 299209 iterations : Training Loss =  0.010236912809041392; Validation Loss = 0.019227078630745974\n",
            "Cost after 299210 iterations : Training Loss =  0.010236904878851698; Validation Loss = 0.019227075662184335\n",
            "Cost after 299211 iterations : Training Loss =  0.010236896948741532; Validation Loss = 0.019227072693664794\n",
            "Cost after 299212 iterations : Training Loss =  0.010236889018711191; Validation Loss = 0.019227069725187566\n",
            "Cost after 299213 iterations : Training Loss =  0.01023688108876061; Validation Loss = 0.019227066756752145\n",
            "Cost after 299214 iterations : Training Loss =  0.01023687315888961; Validation Loss = 0.019227063788358853\n",
            "Cost after 299215 iterations : Training Loss =  0.010236865229098233; Validation Loss = 0.019227060820007257\n",
            "Cost after 299216 iterations : Training Loss =  0.01023685729938671; Validation Loss = 0.01922705785169817\n",
            "Cost after 299217 iterations : Training Loss =  0.010236849369754742; Validation Loss = 0.01922705488343101\n",
            "Cost after 299218 iterations : Training Loss =  0.010236841440202574; Validation Loss = 0.019227051915205906\n",
            "Cost after 299219 iterations : Training Loss =  0.010236833510730032; Validation Loss = 0.019227048947022842\n",
            "Cost after 299220 iterations : Training Loss =  0.010236825581337273; Validation Loss = 0.019227045978882213\n",
            "Cost after 299221 iterations : Training Loss =  0.010236817652024026; Validation Loss = 0.019227043010783394\n",
            "Cost after 299222 iterations : Training Loss =  0.01023680972279061; Validation Loss = 0.019227040042726618\n",
            "Cost after 299223 iterations : Training Loss =  0.010236801793636837; Validation Loss = 0.01922703707471204\n",
            "Cost after 299224 iterations : Training Loss =  0.010236793864562754; Validation Loss = 0.019227034106739364\n",
            "Cost after 299225 iterations : Training Loss =  0.010236785935568262; Validation Loss = 0.019227031138808776\n",
            "Cost after 299226 iterations : Training Loss =  0.010236778006653536; Validation Loss = 0.019227028170920417\n",
            "Cost after 299227 iterations : Training Loss =  0.01023677007781858; Validation Loss = 0.019227025203074385\n",
            "Cost after 299228 iterations : Training Loss =  0.010236762149063192; Validation Loss = 0.019227022235269668\n",
            "Cost after 299229 iterations : Training Loss =  0.010236754220387452; Validation Loss = 0.01922701926750732\n",
            "Cost after 299230 iterations : Training Loss =  0.010236746291791483; Validation Loss = 0.01922701629978708\n",
            "Cost after 299231 iterations : Training Loss =  0.01023673836327513; Validation Loss = 0.019227013332109237\n",
            "Cost after 299232 iterations : Training Loss =  0.0102367304348384; Validation Loss = 0.019227010364473024\n",
            "Cost after 299233 iterations : Training Loss =  0.010236722506481452; Validation Loss = 0.019227007396878985\n",
            "Cost after 299234 iterations : Training Loss =  0.01023671457820414; Validation Loss = 0.019227004429327007\n",
            "Cost after 299235 iterations : Training Loss =  0.01023670665000647; Validation Loss = 0.019227001461817387\n",
            "Cost after 299236 iterations : Training Loss =  0.01023669872188856; Validation Loss = 0.01922699849434908\n",
            "Cost after 299237 iterations : Training Loss =  0.010236690793850256; Validation Loss = 0.019226995526923696\n",
            "Cost after 299238 iterations : Training Loss =  0.01023668286589156; Validation Loss = 0.019226992559539868\n",
            "Cost after 299239 iterations : Training Loss =  0.010236674938012629; Validation Loss = 0.019226989592198058\n",
            "Cost after 299240 iterations : Training Loss =  0.010236667010213304; Validation Loss = 0.019226986624898353\n",
            "Cost after 299241 iterations : Training Loss =  0.01023665908249365; Validation Loss = 0.01922698365764079\n",
            "Cost after 299242 iterations : Training Loss =  0.01023665115485364; Validation Loss = 0.019226980690425367\n",
            "Cost after 299243 iterations : Training Loss =  0.010236643227293373; Validation Loss = 0.01922697772325186\n",
            "Cost after 299244 iterations : Training Loss =  0.010236635299812766; Validation Loss = 0.01922697475612022\n",
            "Cost after 299245 iterations : Training Loss =  0.010236627372411784; Validation Loss = 0.01922697178903087\n",
            "Cost after 299246 iterations : Training Loss =  0.010236619445090429; Validation Loss = 0.01922696882198373\n",
            "Cost after 299247 iterations : Training Loss =  0.010236611517848713; Validation Loss = 0.019226965854978455\n",
            "Cost after 299248 iterations : Training Loss =  0.010236603590686746; Validation Loss = 0.019226962888014995\n",
            "Cost after 299249 iterations : Training Loss =  0.010236595663604473; Validation Loss = 0.019226959921093915\n",
            "Cost after 299250 iterations : Training Loss =  0.010236587736601733; Validation Loss = 0.019226956954214603\n",
            "Cost after 299251 iterations : Training Loss =  0.010236579809678683; Validation Loss = 0.019226953987377907\n",
            "Cost after 299252 iterations : Training Loss =  0.010236571882835362; Validation Loss = 0.019226951020582757\n",
            "Cost after 299253 iterations : Training Loss =  0.010236563956071563; Validation Loss = 0.01922694805383001\n",
            "Cost after 299254 iterations : Training Loss =  0.010236556029387493; Validation Loss = 0.019226945087118787\n",
            "Cost after 299255 iterations : Training Loss =  0.01023654810278307; Validation Loss = 0.019226942120450057\n",
            "Cost after 299256 iterations : Training Loss =  0.01023654017625839; Validation Loss = 0.019226939153823005\n",
            "Cost after 299257 iterations : Training Loss =  0.010236532249813284; Validation Loss = 0.019226936187238253\n",
            "Cost after 299258 iterations : Training Loss =  0.010236524323447753; Validation Loss = 0.01922693322069557\n",
            "Cost after 299259 iterations : Training Loss =  0.010236516397161863; Validation Loss = 0.01922693025419449\n",
            "Cost after 299260 iterations : Training Loss =  0.01023650847095571; Validation Loss = 0.019226927287735965\n",
            "Cost after 299261 iterations : Training Loss =  0.01023650054482919; Validation Loss = 0.01922692432131928\n",
            "Cost after 299262 iterations : Training Loss =  0.010236492618782397; Validation Loss = 0.019226921354944564\n",
            "Cost after 299263 iterations : Training Loss =  0.01023648469281502; Validation Loss = 0.019226918388611836\n",
            "Cost after 299264 iterations : Training Loss =  0.010236476766927445; Validation Loss = 0.019226915422321588\n",
            "Cost after 299265 iterations : Training Loss =  0.010236468841119471; Validation Loss = 0.019226912456072973\n",
            "Cost after 299266 iterations : Training Loss =  0.010236460915391196; Validation Loss = 0.019226909489866374\n",
            "Cost after 299267 iterations : Training Loss =  0.010236452989742522; Validation Loss = 0.019226906523702254\n",
            "Cost after 299268 iterations : Training Loss =  0.010236445064173521; Validation Loss = 0.019226903557579746\n",
            "Cost after 299269 iterations : Training Loss =  0.010236437138684072; Validation Loss = 0.019226900591499653\n",
            "Cost after 299270 iterations : Training Loss =  0.01023642921327427; Validation Loss = 0.019226897625461183\n",
            "Cost after 299271 iterations : Training Loss =  0.010236421287944086; Validation Loss = 0.019226894659464863\n",
            "Cost after 299272 iterations : Training Loss =  0.010236413362693627; Validation Loss = 0.01922689169351078\n",
            "Cost after 299273 iterations : Training Loss =  0.010236405437522774; Validation Loss = 0.01922688872759858\n",
            "Cost after 299274 iterations : Training Loss =  0.010236397512431572; Validation Loss = 0.019226885761728357\n",
            "Cost after 299275 iterations : Training Loss =  0.010236389587419924; Validation Loss = 0.019226882795900152\n",
            "Cost after 299276 iterations : Training Loss =  0.010236381662487913; Validation Loss = 0.01922687983011413\n",
            "Cost after 299277 iterations : Training Loss =  0.0102363737376356; Validation Loss = 0.019226876864370152\n",
            "Cost after 299278 iterations : Training Loss =  0.010236365812862882; Validation Loss = 0.019226873898668093\n",
            "Cost after 299279 iterations : Training Loss =  0.010236357888169775; Validation Loss = 0.019226870933008393\n",
            "Cost after 299280 iterations : Training Loss =  0.010236349963556306; Validation Loss = 0.019226867967390402\n",
            "Cost after 299281 iterations : Training Loss =  0.010236342039022453; Validation Loss = 0.019226865001814492\n",
            "Cost after 299282 iterations : Training Loss =  0.01023633411456822; Validation Loss = 0.01922686203628052\n",
            "Cost after 299283 iterations : Training Loss =  0.010236326190193622; Validation Loss = 0.019226859070788593\n",
            "Cost after 299284 iterations : Training Loss =  0.010236318265898704; Validation Loss = 0.019226856105338773\n",
            "Cost after 299285 iterations : Training Loss =  0.010236310341683284; Validation Loss = 0.019226853139930927\n",
            "Cost after 299286 iterations : Training Loss =  0.01023630241754763; Validation Loss = 0.019226850174565033\n",
            "Cost after 299287 iterations : Training Loss =  0.010236294493491487; Validation Loss = 0.01922684720924118\n",
            "Cost after 299288 iterations : Training Loss =  0.010236286569514927; Validation Loss = 0.019226844243959356\n",
            "Cost after 299289 iterations : Training Loss =  0.010236278645618095; Validation Loss = 0.019226841278719382\n",
            "Cost after 299290 iterations : Training Loss =  0.010236270721800863; Validation Loss = 0.0192268383135217\n",
            "Cost after 299291 iterations : Training Loss =  0.010236262798063129; Validation Loss = 0.019226835348365782\n",
            "Cost after 299292 iterations : Training Loss =  0.010236254874405095; Validation Loss = 0.019226832383252047\n",
            "Cost after 299293 iterations : Training Loss =  0.010236246950826738; Validation Loss = 0.019226829418180533\n",
            "Cost after 299294 iterations : Training Loss =  0.010236239027327856; Validation Loss = 0.019226826453150835\n",
            "Cost after 299295 iterations : Training Loss =  0.010236231103908748; Validation Loss = 0.01922682348816309\n",
            "Cost after 299296 iterations : Training Loss =  0.01023622318056913; Validation Loss = 0.019226820523217406\n",
            "Cost after 299297 iterations : Training Loss =  0.01023621525730912; Validation Loss = 0.01922681755831395\n",
            "Cost after 299298 iterations : Training Loss =  0.010236207334128698; Validation Loss = 0.019226814593452237\n",
            "Cost after 299299 iterations : Training Loss =  0.010236199411027989; Validation Loss = 0.01922681162863253\n",
            "Cost after 299300 iterations : Training Loss =  0.010236191488006793; Validation Loss = 0.01922680866385499\n",
            "Cost after 299301 iterations : Training Loss =  0.01023618356506516; Validation Loss = 0.019226805699119304\n",
            "Cost after 299302 iterations : Training Loss =  0.010236175642203192; Validation Loss = 0.019226802734425865\n",
            "Cost after 299303 iterations : Training Loss =  0.010236167719420883; Validation Loss = 0.019226799769774205\n",
            "Cost after 299304 iterations : Training Loss =  0.010236159796718141; Validation Loss = 0.019226796805164578\n",
            "Cost after 299305 iterations : Training Loss =  0.01023615187409501; Validation Loss = 0.019226793840596994\n",
            "Cost after 299306 iterations : Training Loss =  0.01023614395155144; Validation Loss = 0.0192267908760715\n",
            "Cost after 299307 iterations : Training Loss =  0.010236136029087435; Validation Loss = 0.019226787911587884\n",
            "Cost after 299308 iterations : Training Loss =  0.010236128106703107; Validation Loss = 0.01922678494714642\n",
            "Cost after 299309 iterations : Training Loss =  0.010236120184398326; Validation Loss = 0.019226781982747097\n",
            "Cost after 299310 iterations : Training Loss =  0.010236112262173134; Validation Loss = 0.0192267790183898\n",
            "Cost after 299311 iterations : Training Loss =  0.010236104340027542; Validation Loss = 0.019226776054074152\n",
            "Cost after 299312 iterations : Training Loss =  0.010236096417961556; Validation Loss = 0.019226773089800666\n",
            "Cost after 299313 iterations : Training Loss =  0.010236088495975113; Validation Loss = 0.01922677012556907\n",
            "Cost after 299314 iterations : Training Loss =  0.010236080574068362; Validation Loss = 0.01922676716137935\n",
            "Cost after 299315 iterations : Training Loss =  0.010236072652241128; Validation Loss = 0.019226764197231717\n",
            "Cost after 299316 iterations : Training Loss =  0.010236064730493521; Validation Loss = 0.01922676123312617\n",
            "Cost after 299317 iterations : Training Loss =  0.010236056808825477; Validation Loss = 0.019226758269062512\n",
            "Cost after 299318 iterations : Training Loss =  0.01023604888723698; Validation Loss = 0.019226755305040946\n",
            "Cost after 299319 iterations : Training Loss =  0.010236040965728123; Validation Loss = 0.01922675234106152\n",
            "Cost after 299320 iterations : Training Loss =  0.010236033044298844; Validation Loss = 0.019226749377124132\n",
            "Cost after 299321 iterations : Training Loss =  0.010236025122949141; Validation Loss = 0.019226746413228215\n",
            "Cost after 299322 iterations : Training Loss =  0.01023601720167908; Validation Loss = 0.019226743449374754\n",
            "Cost after 299323 iterations : Training Loss =  0.010236009280488454; Validation Loss = 0.019226740485563068\n",
            "Cost after 299324 iterations : Training Loss =  0.010236001359377493; Validation Loss = 0.01922673752179359\n",
            "Cost after 299325 iterations : Training Loss =  0.01023599343834612; Validation Loss = 0.019226734558065874\n",
            "Cost after 299326 iterations : Training Loss =  0.010235985517394386; Validation Loss = 0.019226731594379955\n",
            "Cost after 299327 iterations : Training Loss =  0.01023597759652218; Validation Loss = 0.019226728630736534\n",
            "Cost after 299328 iterations : Training Loss =  0.010235969675729508; Validation Loss = 0.019226725667134955\n",
            "Cost after 299329 iterations : Training Loss =  0.010235961755016452; Validation Loss = 0.019226722703575335\n",
            "Cost after 299330 iterations : Training Loss =  0.010235953834382933; Validation Loss = 0.019226719740057702\n",
            "Cost after 299331 iterations : Training Loss =  0.010235945913829097; Validation Loss = 0.019226716776582106\n",
            "Cost after 299332 iterations : Training Loss =  0.010235937993354687; Validation Loss = 0.019226713813148603\n",
            "Cost after 299333 iterations : Training Loss =  0.010235930072959903; Validation Loss = 0.019226710849756936\n",
            "Cost after 299334 iterations : Training Loss =  0.010235922152644695; Validation Loss = 0.01922670788640706\n",
            "Cost after 299335 iterations : Training Loss =  0.010235914232409047; Validation Loss = 0.019226704923099577\n",
            "Cost after 299336 iterations : Training Loss =  0.010235906312252964; Validation Loss = 0.019226701959833913\n",
            "Cost after 299337 iterations : Training Loss =  0.010235898392176517; Validation Loss = 0.019226698996609996\n",
            "Cost after 299338 iterations : Training Loss =  0.010235890472179496; Validation Loss = 0.019226696033428137\n",
            "Cost after 299339 iterations : Training Loss =  0.010235882552262146; Validation Loss = 0.01922669307028846\n",
            "Cost after 299340 iterations : Training Loss =  0.010235874632424351; Validation Loss = 0.01922669010719081\n",
            "Cost after 299341 iterations : Training Loss =  0.010235866712666098; Validation Loss = 0.019226687144134596\n",
            "Cost after 299342 iterations : Training Loss =  0.010235858792987438; Validation Loss = 0.0192266841811207\n",
            "Cost after 299343 iterations : Training Loss =  0.010235850873388247; Validation Loss = 0.019226681218149008\n",
            "Cost after 299344 iterations : Training Loss =  0.010235842953868727; Validation Loss = 0.01922667825521911\n",
            "Cost after 299345 iterations : Training Loss =  0.010235835034428717; Validation Loss = 0.019226675292331134\n",
            "Cost after 299346 iterations : Training Loss =  0.010235827115068336; Validation Loss = 0.019226672329485342\n",
            "Cost after 299347 iterations : Training Loss =  0.010235819195787348; Validation Loss = 0.01922666936668133\n",
            "Cost after 299348 iterations : Training Loss =  0.010235811276586057; Validation Loss = 0.019226666403919478\n",
            "Cost after 299349 iterations : Training Loss =  0.010235803357464266; Validation Loss = 0.019226663441199613\n",
            "Cost after 299350 iterations : Training Loss =  0.010235795438422044; Validation Loss = 0.019226660478521646\n",
            "Cost after 299351 iterations : Training Loss =  0.010235787519459352; Validation Loss = 0.019226657515885662\n",
            "Cost after 299352 iterations : Training Loss =  0.010235779600576303; Validation Loss = 0.019226654553291336\n",
            "Cost after 299353 iterations : Training Loss =  0.010235771681772695; Validation Loss = 0.01922665159073936\n",
            "Cost after 299354 iterations : Training Loss =  0.0102357637630486; Validation Loss = 0.019226648628229263\n",
            "Cost after 299355 iterations : Training Loss =  0.01023575584440421; Validation Loss = 0.01922664566576109\n",
            "Cost after 299356 iterations : Training Loss =  0.01023574792583927; Validation Loss = 0.019226642703334792\n",
            "Cost after 299357 iterations : Training Loss =  0.010235740007353857; Validation Loss = 0.01922663974095064\n",
            "Cost after 299358 iterations : Training Loss =  0.01023573208894806; Validation Loss = 0.019226636778608388\n",
            "Cost after 299359 iterations : Training Loss =  0.01023572417062174; Validation Loss = 0.01922663381630835\n",
            "Cost after 299360 iterations : Training Loss =  0.010235716252375036; Validation Loss = 0.01922663085405013\n",
            "Cost after 299361 iterations : Training Loss =  0.010235708334207769; Validation Loss = 0.019226627891833527\n",
            "Cost after 299362 iterations : Training Loss =  0.010235700416120165; Validation Loss = 0.019226624929659156\n",
            "Cost after 299363 iterations : Training Loss =  0.01023569249811196; Validation Loss = 0.019226621967526707\n",
            "Cost after 299364 iterations : Training Loss =  0.010235684580183448; Validation Loss = 0.019226619005436662\n",
            "Cost after 299365 iterations : Training Loss =  0.010235676662334403; Validation Loss = 0.019226616043387885\n",
            "Cost after 299366 iterations : Training Loss =  0.010235668744564877; Validation Loss = 0.019226613081381596\n",
            "Cost after 299367 iterations : Training Loss =  0.01023566082687492; Validation Loss = 0.019226610119416915\n",
            "Cost after 299368 iterations : Training Loss =  0.010235652909264486; Validation Loss = 0.01922660715749466\n",
            "Cost after 299369 iterations : Training Loss =  0.01023564499173359; Validation Loss = 0.019226604195613728\n",
            "Cost after 299370 iterations : Training Loss =  0.01023563707428224; Validation Loss = 0.01922660123377494\n",
            "Cost after 299371 iterations : Training Loss =  0.010235629156910375; Validation Loss = 0.019226598271978066\n",
            "Cost after 299372 iterations : Training Loss =  0.010235621239618046; Validation Loss = 0.019226595310223218\n",
            "Cost after 299373 iterations : Training Loss =  0.010235613322405317; Validation Loss = 0.019226592348510433\n",
            "Cost after 299374 iterations : Training Loss =  0.010235605405272056; Validation Loss = 0.019226589386839875\n",
            "Cost after 299375 iterations : Training Loss =  0.010235597488218314; Validation Loss = 0.019226586425211082\n",
            "Cost after 299376 iterations : Training Loss =  0.010235589571244152; Validation Loss = 0.019226583463623933\n",
            "Cost after 299377 iterations : Training Loss =  0.01023558165434947; Validation Loss = 0.01922658050207911\n",
            "Cost after 299378 iterations : Training Loss =  0.010235573737534285; Validation Loss = 0.01922657754057611\n",
            "Cost after 299379 iterations : Training Loss =  0.010235565820798695; Validation Loss = 0.01922657457911496\n",
            "Cost after 299380 iterations : Training Loss =  0.010235557904142621; Validation Loss = 0.019226571617695767\n",
            "Cost after 299381 iterations : Training Loss =  0.010235549987566072; Validation Loss = 0.019226568656318666\n",
            "Cost after 299382 iterations : Training Loss =  0.010235542071068957; Validation Loss = 0.019226565694983735\n",
            "Cost after 299383 iterations : Training Loss =  0.010235534154651478; Validation Loss = 0.019226562733690458\n",
            "Cost after 299384 iterations : Training Loss =  0.010235526238313477; Validation Loss = 0.019226559772438777\n",
            "Cost after 299385 iterations : Training Loss =  0.010235518322054958; Validation Loss = 0.019226556811229357\n",
            "Cost after 299386 iterations : Training Loss =  0.010235510405875952; Validation Loss = 0.019226553850061882\n",
            "Cost after 299387 iterations : Training Loss =  0.010235502489776506; Validation Loss = 0.019226550888936263\n",
            "Cost after 299388 iterations : Training Loss =  0.010235494573756575; Validation Loss = 0.019226547927852968\n",
            "Cost after 299389 iterations : Training Loss =  0.010235486657816087; Validation Loss = 0.019226544966811473\n",
            "Cost after 299390 iterations : Training Loss =  0.0102354787419552; Validation Loss = 0.01922654200581177\n",
            "Cost after 299391 iterations : Training Loss =  0.01023547082617377; Validation Loss = 0.019226539044853826\n",
            "Cost after 299392 iterations : Training Loss =  0.010235462910471855; Validation Loss = 0.019226536083938255\n",
            "Cost after 299393 iterations : Training Loss =  0.010235454994849473; Validation Loss = 0.019226533123064566\n",
            "Cost after 299394 iterations : Training Loss =  0.01023544707930659; Validation Loss = 0.019226530162232573\n",
            "Cost after 299395 iterations : Training Loss =  0.01023543916384319; Validation Loss = 0.019226527201442686\n",
            "Cost after 299396 iterations : Training Loss =  0.0102354312484593; Validation Loss = 0.01922652424069474\n",
            "Cost after 299397 iterations : Training Loss =  0.010235423333154992; Validation Loss = 0.01922652127998889\n",
            "Cost after 299398 iterations : Training Loss =  0.010235415417930127; Validation Loss = 0.019226518319324905\n",
            "Cost after 299399 iterations : Training Loss =  0.010235407502784786; Validation Loss = 0.019226515358702578\n",
            "Cost after 299400 iterations : Training Loss =  0.01023539958771893; Validation Loss = 0.019226512398122186\n",
            "Cost after 299401 iterations : Training Loss =  0.010235391672732548; Validation Loss = 0.01922650943758385\n",
            "Cost after 299402 iterations : Training Loss =  0.010235383757825695; Validation Loss = 0.01922650647708743\n",
            "Cost after 299403 iterations : Training Loss =  0.010235375842998325; Validation Loss = 0.019226503516633037\n",
            "Cost after 299404 iterations : Training Loss =  0.010235367928250489; Validation Loss = 0.019226500556220504\n",
            "Cost after 299405 iterations : Training Loss =  0.010235360013582146; Validation Loss = 0.019226497595850232\n",
            "Cost after 299406 iterations : Training Loss =  0.010235352098993263; Validation Loss = 0.019226494635521656\n",
            "Cost after 299407 iterations : Training Loss =  0.010235344184483837; Validation Loss = 0.019226491675234887\n",
            "Cost after 299408 iterations : Training Loss =  0.010235336270054067; Validation Loss = 0.019226488714990345\n",
            "Cost after 299409 iterations : Training Loss =  0.0102353283557037; Validation Loss = 0.019226485754787425\n",
            "Cost after 299410 iterations : Training Loss =  0.010235320441432721; Validation Loss = 0.019226482794626732\n",
            "Cost after 299411 iterations : Training Loss =  0.010235312527241334; Validation Loss = 0.01922647983450752\n",
            "Cost after 299412 iterations : Training Loss =  0.010235304613129482; Validation Loss = 0.01922647687443035\n",
            "Cost after 299413 iterations : Training Loss =  0.010235296699097033; Validation Loss = 0.019226473914395296\n",
            "Cost after 299414 iterations : Training Loss =  0.010235288785144096; Validation Loss = 0.019226470954402393\n",
            "Cost after 299415 iterations : Training Loss =  0.010235280871270647; Validation Loss = 0.019226467994451126\n",
            "Cost after 299416 iterations : Training Loss =  0.010235272957476688; Validation Loss = 0.019226465034541798\n",
            "Cost after 299417 iterations : Training Loss =  0.010235265043762175; Validation Loss = 0.019226462074674194\n",
            "Cost after 299418 iterations : Training Loss =  0.010235257130127276; Validation Loss = 0.019226459114848597\n",
            "Cost after 299419 iterations : Training Loss =  0.010235249216571667; Validation Loss = 0.01922645615506527\n",
            "Cost after 299420 iterations : Training Loss =  0.010235241303095678; Validation Loss = 0.01922645319532365\n",
            "Cost after 299421 iterations : Training Loss =  0.010235233389699082; Validation Loss = 0.01922645023562396\n",
            "Cost after 299422 iterations : Training Loss =  0.010235225476382085; Validation Loss = 0.019226447275965827\n",
            "Cost after 299423 iterations : Training Loss =  0.010235217563144454; Validation Loss = 0.019226444316350337\n",
            "Cost after 299424 iterations : Training Loss =  0.01023520964998639; Validation Loss = 0.019226441356776213\n",
            "Cost after 299425 iterations : Training Loss =  0.01023520173690769; Validation Loss = 0.01922643839724429\n",
            "Cost after 299426 iterations : Training Loss =  0.010235193823908629; Validation Loss = 0.019226435437754243\n",
            "Cost after 299427 iterations : Training Loss =  0.010235185910988858; Validation Loss = 0.019226432478306254\n",
            "Cost after 299428 iterations : Training Loss =  0.01023517799814865; Validation Loss = 0.019226429518900037\n",
            "Cost after 299429 iterations : Training Loss =  0.010235170085387957; Validation Loss = 0.019226426559535683\n",
            "Cost after 299430 iterations : Training Loss =  0.010235162172706638; Validation Loss = 0.01922642360021323\n",
            "Cost after 299431 iterations : Training Loss =  0.010235154260104856; Validation Loss = 0.019226420640932624\n",
            "Cost after 299432 iterations : Training Loss =  0.010235146347582559; Validation Loss = 0.019226417681693694\n",
            "Cost after 299433 iterations : Training Loss =  0.010235138435139717; Validation Loss = 0.01922641472249692\n",
            "Cost after 299434 iterations : Training Loss =  0.01023513052277632; Validation Loss = 0.019226411763342454\n",
            "Cost after 299435 iterations : Training Loss =  0.010235122610492382; Validation Loss = 0.019226408804229562\n",
            "Cost after 299436 iterations : Training Loss =  0.01023511469828791; Validation Loss = 0.019226405845158775\n",
            "Cost after 299437 iterations : Training Loss =  0.010235106786162882; Validation Loss = 0.019226402886129815\n",
            "Cost after 299438 iterations : Training Loss =  0.010235098874117356; Validation Loss = 0.019226399927142892\n",
            "Cost after 299439 iterations : Training Loss =  0.010235090962151283; Validation Loss = 0.019226396968197605\n",
            "Cost after 299440 iterations : Training Loss =  0.010235083050264705; Validation Loss = 0.019226394009294368\n",
            "Cost after 299441 iterations : Training Loss =  0.010235075138457561; Validation Loss = 0.019226391050433035\n",
            "Cost after 299442 iterations : Training Loss =  0.010235067226729861; Validation Loss = 0.019226388091613655\n",
            "Cost after 299443 iterations : Training Loss =  0.010235059315081627; Validation Loss = 0.019226385132835932\n",
            "Cost after 299444 iterations : Training Loss =  0.01023505140351281; Validation Loss = 0.01922638217410013\n",
            "Cost after 299445 iterations : Training Loss =  0.010235043492023565; Validation Loss = 0.019226379215406136\n",
            "Cost after 299446 iterations : Training Loss =  0.010235035580613694; Validation Loss = 0.019226376256754642\n",
            "Cost after 299447 iterations : Training Loss =  0.010235027669283229; Validation Loss = 0.019226373298144597\n",
            "Cost after 299448 iterations : Training Loss =  0.010235019758032277; Validation Loss = 0.019226370339576537\n",
            "Cost after 299449 iterations : Training Loss =  0.010235011846860735; Validation Loss = 0.019226367381050456\n",
            "Cost after 299450 iterations : Training Loss =  0.01023500393576868; Validation Loss = 0.01922636442256632\n",
            "Cost after 299451 iterations : Training Loss =  0.010234996024756079; Validation Loss = 0.0192263614641238\n",
            "Cost after 299452 iterations : Training Loss =  0.010234988113822976; Validation Loss = 0.01922635850572346\n",
            "Cost after 299453 iterations : Training Loss =  0.010234980202969222; Validation Loss = 0.019226355547364987\n",
            "Cost after 299454 iterations : Training Loss =  0.010234972292194933; Validation Loss = 0.01922635258904817\n",
            "Cost after 299455 iterations : Training Loss =  0.01023496438150017; Validation Loss = 0.019226349630773593\n",
            "Cost after 299456 iterations : Training Loss =  0.010234956470884826; Validation Loss = 0.019226346672540744\n",
            "Cost after 299457 iterations : Training Loss =  0.010234948560348862; Validation Loss = 0.019226343714349742\n",
            "Cost after 299458 iterations : Training Loss =  0.010234940649892409; Validation Loss = 0.019226340756200902\n",
            "Cost after 299459 iterations : Training Loss =  0.010234932739515366; Validation Loss = 0.019226337798093473\n",
            "Cost after 299460 iterations : Training Loss =  0.01023492482921778; Validation Loss = 0.019226334840027972\n",
            "Cost after 299461 iterations : Training Loss =  0.010234916918999571; Validation Loss = 0.019226331882004684\n",
            "Cost after 299462 iterations : Training Loss =  0.01023490900886083; Validation Loss = 0.01922632892402309\n",
            "Cost after 299463 iterations : Training Loss =  0.010234901098801525; Validation Loss = 0.019226325966083514\n",
            "Cost after 299464 iterations : Training Loss =  0.01023489318882169; Validation Loss = 0.019226323008185792\n",
            "Cost after 299465 iterations : Training Loss =  0.010234885278921304; Validation Loss = 0.019226320050330103\n",
            "Cost after 299466 iterations : Training Loss =  0.010234877369100318; Validation Loss = 0.01922631709251607\n",
            "Cost after 299467 iterations : Training Loss =  0.010234869459358706; Validation Loss = 0.019226314134744082\n",
            "Cost after 299468 iterations : Training Loss =  0.010234861549696642; Validation Loss = 0.019226311177014083\n",
            "Cost after 299469 iterations : Training Loss =  0.010234853640113966; Validation Loss = 0.01922630821932547\n",
            "Cost after 299470 iterations : Training Loss =  0.010234845730610682; Validation Loss = 0.019226305261679257\n",
            "Cost after 299471 iterations : Training Loss =  0.010234837821186836; Validation Loss = 0.019226302304074745\n",
            "Cost after 299472 iterations : Training Loss =  0.010234829911842484; Validation Loss = 0.01922629934651232\n",
            "Cost after 299473 iterations : Training Loss =  0.010234822002577453; Validation Loss = 0.01922629638899162\n",
            "Cost after 299474 iterations : Training Loss =  0.010234814093391923; Validation Loss = 0.019226293431512685\n",
            "Cost after 299475 iterations : Training Loss =  0.010234806184285824; Validation Loss = 0.019226290474075747\n",
            "Cost after 299476 iterations : Training Loss =  0.010234798275259138; Validation Loss = 0.01922628751668056\n",
            "Cost after 299477 iterations : Training Loss =  0.010234790366311782; Validation Loss = 0.01922628455932751\n",
            "Cost after 299478 iterations : Training Loss =  0.010234782457443994; Validation Loss = 0.019226281602016473\n",
            "Cost after 299479 iterations : Training Loss =  0.010234774548655583; Validation Loss = 0.019226278644746995\n",
            "Cost after 299480 iterations : Training Loss =  0.010234766639946546; Validation Loss = 0.019226275687519345\n",
            "Cost after 299481 iterations : Training Loss =  0.010234758731316955; Validation Loss = 0.019226272730333976\n",
            "Cost after 299482 iterations : Training Loss =  0.010234750822766774; Validation Loss = 0.01922626977318998\n",
            "Cost after 299483 iterations : Training Loss =  0.010234742914296028; Validation Loss = 0.019226266816088133\n",
            "Cost after 299484 iterations : Training Loss =  0.010234735005904674; Validation Loss = 0.01922626385902795\n",
            "Cost after 299485 iterations : Training Loss =  0.010234727097592647; Validation Loss = 0.0192262609020098\n",
            "Cost after 299486 iterations : Training Loss =  0.010234719189360251; Validation Loss = 0.019226257945033274\n",
            "Cost after 299487 iterations : Training Loss =  0.010234711281207225; Validation Loss = 0.01922625498809911\n",
            "Cost after 299488 iterations : Training Loss =  0.010234703373133477; Validation Loss = 0.019226252031206564\n",
            "Cost after 299489 iterations : Training Loss =  0.010234695465139188; Validation Loss = 0.019226249074356088\n",
            "Cost after 299490 iterations : Training Loss =  0.0102346875572244; Validation Loss = 0.019226246117547495\n",
            "Cost after 299491 iterations : Training Loss =  0.01023467964938885; Validation Loss = 0.019226243160780584\n",
            "Cost after 299492 iterations : Training Loss =  0.01023467174163284; Validation Loss = 0.0192262402040553\n",
            "Cost after 299493 iterations : Training Loss =  0.010234663833956248; Validation Loss = 0.019226237247372224\n",
            "Cost after 299494 iterations : Training Loss =  0.010234655926358914; Validation Loss = 0.019226234290730747\n",
            "Cost after 299495 iterations : Training Loss =  0.01023464801884113; Validation Loss = 0.019226231334131216\n",
            "Cost after 299496 iterations : Training Loss =  0.01023464011140274; Validation Loss = 0.019226228377573533\n",
            "Cost after 299497 iterations : Training Loss =  0.010234632204043729; Validation Loss = 0.019226225421057865\n",
            "Cost after 299498 iterations : Training Loss =  0.010234624296764054; Validation Loss = 0.019226222464584167\n",
            "Cost after 299499 iterations : Training Loss =  0.010234616389563805; Validation Loss = 0.019226219508152127\n",
            "Cost after 299500 iterations : Training Loss =  0.01023460848244303; Validation Loss = 0.019226216551762144\n",
            "Cost after 299501 iterations : Training Loss =  0.010234600575401603; Validation Loss = 0.01922621359541373\n",
            "Cost after 299502 iterations : Training Loss =  0.010234592668439544; Validation Loss = 0.01922621063910756\n",
            "Cost after 299503 iterations : Training Loss =  0.01023458476155688; Validation Loss = 0.019226207682842875\n",
            "Cost after 299504 iterations : Training Loss =  0.010234576854753645; Validation Loss = 0.019226204726620465\n",
            "Cost after 299505 iterations : Training Loss =  0.010234568948029866; Validation Loss = 0.019226201770439558\n",
            "Cost after 299506 iterations : Training Loss =  0.010234561041385348; Validation Loss = 0.019226198814300634\n",
            "Cost after 299507 iterations : Training Loss =  0.010234553134820365; Validation Loss = 0.019226195858203548\n",
            "Cost after 299508 iterations : Training Loss =  0.010234545228334706; Validation Loss = 0.0192261929021483\n",
            "Cost after 299509 iterations : Training Loss =  0.010234537321928368; Validation Loss = 0.019226189946134675\n",
            "Cost after 299510 iterations : Training Loss =  0.01023452941560149; Validation Loss = 0.01922618699016334\n",
            "Cost after 299511 iterations : Training Loss =  0.01023452150935403; Validation Loss = 0.019226184034233422\n",
            "Cost after 299512 iterations : Training Loss =  0.01023451360318585; Validation Loss = 0.0192261810783459\n",
            "Cost after 299513 iterations : Training Loss =  0.010234505697097199; Validation Loss = 0.019226178122499795\n",
            "Cost after 299514 iterations : Training Loss =  0.010234497791087776; Validation Loss = 0.019226175166695766\n",
            "Cost after 299515 iterations : Training Loss =  0.010234489885157894; Validation Loss = 0.019226172210933798\n",
            "Cost after 299516 iterations : Training Loss =  0.010234481979307281; Validation Loss = 0.019226169255213237\n",
            "Cost after 299517 iterations : Training Loss =  0.010234474073536084; Validation Loss = 0.019226166299534428\n",
            "Cost after 299518 iterations : Training Loss =  0.010234466167844269; Validation Loss = 0.019226163343897963\n",
            "Cost after 299519 iterations : Training Loss =  0.010234458262231799; Validation Loss = 0.01922616038830339\n",
            "Cost after 299520 iterations : Training Loss =  0.010234450356698758; Validation Loss = 0.019226157432750392\n",
            "Cost after 299521 iterations : Training Loss =  0.010234442451245131; Validation Loss = 0.019226154477239032\n",
            "Cost after 299522 iterations : Training Loss =  0.01023443454587081; Validation Loss = 0.019226151521769895\n",
            "Cost after 299523 iterations : Training Loss =  0.010234426640575936; Validation Loss = 0.01922614856634229\n",
            "Cost after 299524 iterations : Training Loss =  0.010234418735360313; Validation Loss = 0.019226145610957035\n",
            "Cost after 299525 iterations : Training Loss =  0.010234410830224193; Validation Loss = 0.01922614265561313\n",
            "Cost after 299526 iterations : Training Loss =  0.010234402925167394; Validation Loss = 0.019226139700311444\n",
            "Cost after 299527 iterations : Training Loss =  0.010234395020189958; Validation Loss = 0.01922613674505136\n",
            "Cost after 299528 iterations : Training Loss =  0.010234387115291883; Validation Loss = 0.01922613378983289\n",
            "Cost after 299529 iterations : Training Loss =  0.010234379210473267; Validation Loss = 0.019226130834656584\n",
            "Cost after 299530 iterations : Training Loss =  0.010234371305733904; Validation Loss = 0.01922612787952211\n",
            "Cost after 299531 iterations : Training Loss =  0.010234363401073938; Validation Loss = 0.01922612492442939\n",
            "Cost after 299532 iterations : Training Loss =  0.010234355496493427; Validation Loss = 0.019226121969378587\n",
            "Cost after 299533 iterations : Training Loss =  0.010234347591992111; Validation Loss = 0.019226119014369485\n",
            "Cost after 299534 iterations : Training Loss =  0.010234339687570244; Validation Loss = 0.019226116059402304\n",
            "Cost after 299535 iterations : Training Loss =  0.010234331783227787; Validation Loss = 0.019226113104477235\n",
            "Cost after 299536 iterations : Training Loss =  0.010234323878964646; Validation Loss = 0.01922611014959357\n",
            "Cost after 299537 iterations : Training Loss =  0.010234315974780877; Validation Loss = 0.019226107194751724\n",
            "Cost after 299538 iterations : Training Loss =  0.010234308070676532; Validation Loss = 0.019226104239952108\n",
            "Cost after 299539 iterations : Training Loss =  0.010234300166651396; Validation Loss = 0.019226101285194072\n",
            "Cost after 299540 iterations : Training Loss =  0.010234292262705736; Validation Loss = 0.01922609833047786\n",
            "Cost after 299541 iterations : Training Loss =  0.010234284358839382; Validation Loss = 0.01922609537580375\n",
            "Cost after 299542 iterations : Training Loss =  0.010234276455052479; Validation Loss = 0.01922609242117112\n",
            "Cost after 299543 iterations : Training Loss =  0.010234268551344825; Validation Loss = 0.019226089466580384\n",
            "Cost after 299544 iterations : Training Loss =  0.010234260647716509; Validation Loss = 0.019226086512031556\n",
            "Cost after 299545 iterations : Training Loss =  0.010234252744167654; Validation Loss = 0.019226083557524615\n",
            "Cost after 299546 iterations : Training Loss =  0.010234244840698088; Validation Loss = 0.01922608060305963\n",
            "Cost after 299547 iterations : Training Loss =  0.010234236937307847; Validation Loss = 0.01922607764863645\n",
            "Cost after 299548 iterations : Training Loss =  0.01023422903399704; Validation Loss = 0.019226074694254403\n",
            "Cost after 299549 iterations : Training Loss =  0.010234221130765463; Validation Loss = 0.019226071739915175\n",
            "Cost after 299550 iterations : Training Loss =  0.010234213227613413; Validation Loss = 0.019226068785617327\n",
            "Cost after 299551 iterations : Training Loss =  0.010234205324540517; Validation Loss = 0.019226065831361418\n",
            "Cost after 299552 iterations : Training Loss =  0.010234197421547069; Validation Loss = 0.019226062877147187\n",
            "Cost after 299553 iterations : Training Loss =  0.010234189518632928; Validation Loss = 0.01922605992297466\n",
            "Cost after 299554 iterations : Training Loss =  0.010234181615798064; Validation Loss = 0.019226056968844357\n",
            "Cost after 299555 iterations : Training Loss =  0.010234173713042694; Validation Loss = 0.019226054014755484\n",
            "Cost after 299556 iterations : Training Loss =  0.01023416581036659; Validation Loss = 0.019226051060708984\n",
            "Cost after 299557 iterations : Training Loss =  0.010234157907769874; Validation Loss = 0.019226048106704023\n",
            "Cost after 299558 iterations : Training Loss =  0.010234150005252478; Validation Loss = 0.01922604515274045\n",
            "Cost after 299559 iterations : Training Loss =  0.010234142102814401; Validation Loss = 0.019226042198819033\n",
            "Cost after 299560 iterations : Training Loss =  0.010234134200455592; Validation Loss = 0.019226039244939607\n",
            "Cost after 299561 iterations : Training Loss =  0.010234126298176247; Validation Loss = 0.019226036291101724\n",
            "Cost after 299562 iterations : Training Loss =  0.01023411839597614; Validation Loss = 0.019226033337305964\n",
            "Cost after 299563 iterations : Training Loss =  0.010234110493855433; Validation Loss = 0.019226030383551997\n",
            "Cost after 299564 iterations : Training Loss =  0.010234102591814032; Validation Loss = 0.01922602742983973\n",
            "Cost after 299565 iterations : Training Loss =  0.010234094689851994; Validation Loss = 0.019226024476169164\n",
            "Cost after 299566 iterations : Training Loss =  0.010234086787969266; Validation Loss = 0.019226021522540673\n",
            "Cost after 299567 iterations : Training Loss =  0.010234078886165883; Validation Loss = 0.019226018568953795\n",
            "Cost after 299568 iterations : Training Loss =  0.010234070984441835; Validation Loss = 0.019226015615408727\n",
            "Cost after 299569 iterations : Training Loss =  0.010234063082797016; Validation Loss = 0.019226012661905538\n",
            "Cost after 299570 iterations : Training Loss =  0.010234055181231693; Validation Loss = 0.019226009708444142\n",
            "Cost after 299571 iterations : Training Loss =  0.010234047279745574; Validation Loss = 0.019226006755024456\n",
            "Cost after 299572 iterations : Training Loss =  0.010234039378338746; Validation Loss = 0.019226003801646463\n",
            "Cost after 299573 iterations : Training Loss =  0.010234031477011343; Validation Loss = 0.01922600084831067\n",
            "Cost after 299574 iterations : Training Loss =  0.01023402357576318; Validation Loss = 0.019225997895016427\n",
            "Cost after 299575 iterations : Training Loss =  0.01023401567459439; Validation Loss = 0.019225994941764177\n",
            "Cost after 299576 iterations : Training Loss =  0.010234007773504965; Validation Loss = 0.01922599198855339\n",
            "Cost after 299577 iterations : Training Loss =  0.010233999872494808; Validation Loss = 0.01922598903538478\n",
            "Cost after 299578 iterations : Training Loss =  0.01023399197156399; Validation Loss = 0.01922598608225778\n",
            "Cost after 299579 iterations : Training Loss =  0.010233984070712451; Validation Loss = 0.019225983129172605\n",
            "Cost after 299580 iterations : Training Loss =  0.010233976169940285; Validation Loss = 0.019225980176129462\n",
            "Cost after 299581 iterations : Training Loss =  0.010233968269247433; Validation Loss = 0.019225977223127903\n",
            "Cost after 299582 iterations : Training Loss =  0.010233960368633777; Validation Loss = 0.019225974270168134\n",
            "Cost after 299583 iterations : Training Loss =  0.010233952468099517; Validation Loss = 0.01922597131725057\n",
            "Cost after 299584 iterations : Training Loss =  0.010233944567644582; Validation Loss = 0.019225968364374128\n",
            "Cost after 299585 iterations : Training Loss =  0.010233936667269046; Validation Loss = 0.019225965411539918\n",
            "Cost after 299586 iterations : Training Loss =  0.010233928766972744; Validation Loss = 0.019225962458747293\n",
            "Cost after 299587 iterations : Training Loss =  0.010233920866755732; Validation Loss = 0.019225959505996683\n",
            "Cost after 299588 iterations : Training Loss =  0.01023391296661806; Validation Loss = 0.019225956553287703\n",
            "Cost after 299589 iterations : Training Loss =  0.01023390506655963; Validation Loss = 0.0192259536006209\n",
            "Cost after 299590 iterations : Training Loss =  0.010233897166580559; Validation Loss = 0.019225950647995493\n",
            "Cost after 299591 iterations : Training Loss =  0.010233889266680785; Validation Loss = 0.019225947695411756\n",
            "Cost after 299592 iterations : Training Loss =  0.010233881366860329; Validation Loss = 0.019225944742870454\n",
            "Cost after 299593 iterations : Training Loss =  0.010233873467119096; Validation Loss = 0.019225941790370456\n",
            "Cost after 299594 iterations : Training Loss =  0.010233865567457333; Validation Loss = 0.019225938837912095\n",
            "Cost after 299595 iterations : Training Loss =  0.010233857667874767; Validation Loss = 0.01922593588549591\n",
            "Cost after 299596 iterations : Training Loss =  0.01023384976837146; Validation Loss = 0.019225932933121363\n",
            "Cost after 299597 iterations : Training Loss =  0.010233841868947586; Validation Loss = 0.01922592998078888\n",
            "Cost after 299598 iterations : Training Loss =  0.010233833969602922; Validation Loss = 0.019225927028497885\n",
            "Cost after 299599 iterations : Training Loss =  0.010233826070337506; Validation Loss = 0.019225924076248502\n",
            "Cost after 299600 iterations : Training Loss =  0.010233818171151403; Validation Loss = 0.0192259211240413\n",
            "Cost after 299601 iterations : Training Loss =  0.010233810272044643; Validation Loss = 0.01922591817187568\n",
            "Cost after 299602 iterations : Training Loss =  0.010233802373017227; Validation Loss = 0.019225915219751907\n",
            "Cost after 299603 iterations : Training Loss =  0.010233794474069009; Validation Loss = 0.01922591226767017\n",
            "Cost after 299604 iterations : Training Loss =  0.010233786575200148; Validation Loss = 0.019225909315629664\n",
            "Cost after 299605 iterations : Training Loss =  0.010233778676410555; Validation Loss = 0.01922590636363154\n",
            "Cost after 299606 iterations : Training Loss =  0.010233770777700196; Validation Loss = 0.019225903411674877\n",
            "Cost after 299607 iterations : Training Loss =  0.010233762879069208; Validation Loss = 0.01922590045976006\n",
            "Cost after 299608 iterations : Training Loss =  0.010233754980517526; Validation Loss = 0.019225897507887203\n",
            "Cost after 299609 iterations : Training Loss =  0.01023374708204509; Validation Loss = 0.019225894556055827\n",
            "Cost after 299610 iterations : Training Loss =  0.010233739183651952; Validation Loss = 0.019225891604266448\n",
            "Cost after 299611 iterations : Training Loss =  0.010233731285338008; Validation Loss = 0.019225888652518747\n",
            "Cost after 299612 iterations : Training Loss =  0.010233723387103471; Validation Loss = 0.01922588570081272\n",
            "Cost after 299613 iterations : Training Loss =  0.01023371548894815; Validation Loss = 0.019225882749148937\n",
            "Cost after 299614 iterations : Training Loss =  0.010233707590872079; Validation Loss = 0.019225879797526442\n",
            "Cost after 299615 iterations : Training Loss =  0.010233699692875433; Validation Loss = 0.019225876845945625\n",
            "Cost after 299616 iterations : Training Loss =  0.010233691794957983; Validation Loss = 0.019225873894406827\n",
            "Cost after 299617 iterations : Training Loss =  0.010233683897119757; Validation Loss = 0.01922587094290971\n",
            "Cost after 299618 iterations : Training Loss =  0.010233675999360868; Validation Loss = 0.019225867991454277\n",
            "Cost after 299619 iterations : Training Loss =  0.010233668101681262; Validation Loss = 0.019225865040040976\n",
            "Cost after 299620 iterations : Training Loss =  0.010233660204080858; Validation Loss = 0.01922586208866948\n",
            "Cost after 299621 iterations : Training Loss =  0.010233652306559846; Validation Loss = 0.01922585913733944\n",
            "Cost after 299622 iterations : Training Loss =  0.01023364440911808; Validation Loss = 0.019225856186051164\n",
            "Cost after 299623 iterations : Training Loss =  0.010233636511755564; Validation Loss = 0.019225853234804996\n",
            "Cost after 299624 iterations : Training Loss =  0.010233628614472341; Validation Loss = 0.01922585028360044\n",
            "Cost after 299625 iterations : Training Loss =  0.010233620717268325; Validation Loss = 0.01922584733243787\n",
            "Cost after 299626 iterations : Training Loss =  0.010233612820143694; Validation Loss = 0.01922584438131653\n",
            "Cost after 299627 iterations : Training Loss =  0.010233604923098187; Validation Loss = 0.019225841430237745\n",
            "Cost after 299628 iterations : Training Loss =  0.01023359702613197; Validation Loss = 0.01922583847920028\n",
            "Cost after 299629 iterations : Training Loss =  0.010233589129245173; Validation Loss = 0.01922583552820457\n",
            "Cost after 299630 iterations : Training Loss =  0.010233581232437486; Validation Loss = 0.01922583257725039\n",
            "Cost after 299631 iterations : Training Loss =  0.010233573335709154; Validation Loss = 0.019225829626338106\n",
            "Cost after 299632 iterations : Training Loss =  0.010233565439060028; Validation Loss = 0.01922582667546798\n",
            "Cost after 299633 iterations : Training Loss =  0.010233557542490198; Validation Loss = 0.019225823724639064\n",
            "Cost after 299634 iterations : Training Loss =  0.010233549645999561; Validation Loss = 0.019225820773852275\n",
            "Cost after 299635 iterations : Training Loss =  0.010233541749588293; Validation Loss = 0.01922581782310722\n",
            "Cost after 299636 iterations : Training Loss =  0.010233533853256257; Validation Loss = 0.019225814872403813\n",
            "Cost after 299637 iterations : Training Loss =  0.010233525957003364; Validation Loss = 0.019225811921742136\n",
            "Cost after 299638 iterations : Training Loss =  0.010233518060829845; Validation Loss = 0.01922580897112249\n",
            "Cost after 299639 iterations : Training Loss =  0.010233510164735598; Validation Loss = 0.019225806020544446\n",
            "Cost after 299640 iterations : Training Loss =  0.01023350226872051; Validation Loss = 0.019225803070007885\n",
            "Cost after 299641 iterations : Training Loss =  0.01023349437278474; Validation Loss = 0.01922580011951347\n",
            "Cost after 299642 iterations : Training Loss =  0.010233486476928233; Validation Loss = 0.019225797169060938\n",
            "Cost after 299643 iterations : Training Loss =  0.010233478581150963; Validation Loss = 0.01922579421864975\n",
            "Cost after 299644 iterations : Training Loss =  0.010233470685452961; Validation Loss = 0.019225791268280567\n",
            "Cost after 299645 iterations : Training Loss =  0.010233462789834195; Validation Loss = 0.01922578831795293\n",
            "Cost after 299646 iterations : Training Loss =  0.010233454894294646; Validation Loss = 0.019225785367667638\n",
            "Cost after 299647 iterations : Training Loss =  0.010233446998834496; Validation Loss = 0.01922578241742351\n",
            "Cost after 299648 iterations : Training Loss =  0.010233439103453344; Validation Loss = 0.019225779467221333\n",
            "Cost after 299649 iterations : Training Loss =  0.010233431208151629; Validation Loss = 0.019225776517060932\n",
            "Cost after 299650 iterations : Training Loss =  0.010233423312929067; Validation Loss = 0.019225773566942116\n",
            "Cost after 299651 iterations : Training Loss =  0.010233415417785835; Validation Loss = 0.019225770616865385\n",
            "Cost after 299652 iterations : Training Loss =  0.010233407522721814; Validation Loss = 0.019225767666830363\n",
            "Cost after 299653 iterations : Training Loss =  0.010233399627736964; Validation Loss = 0.019225764716836992\n",
            "Cost after 299654 iterations : Training Loss =  0.010233391732831445; Validation Loss = 0.019225761766885417\n",
            "Cost after 299655 iterations : Training Loss =  0.010233383838005159; Validation Loss = 0.019225758816975122\n",
            "Cost after 299656 iterations : Training Loss =  0.01023337594325803; Validation Loss = 0.01922575586710704\n",
            "Cost after 299657 iterations : Training Loss =  0.010233368048590152; Validation Loss = 0.019225752917280382\n",
            "Cost after 299658 iterations : Training Loss =  0.010233360154001574; Validation Loss = 0.019225749967495518\n",
            "Cost after 299659 iterations : Training Loss =  0.01023335225949222; Validation Loss = 0.019225747017752565\n",
            "Cost after 299660 iterations : Training Loss =  0.010233344365062092; Validation Loss = 0.019225744068051588\n",
            "Cost after 299661 iterations : Training Loss =  0.010233336470711167; Validation Loss = 0.019225741118392214\n",
            "Cost after 299662 iterations : Training Loss =  0.01023332857643954; Validation Loss = 0.019225738168774372\n",
            "Cost after 299663 iterations : Training Loss =  0.010233320682247058; Validation Loss = 0.01922573521919832\n",
            "Cost after 299664 iterations : Training Loss =  0.010233312788133856; Validation Loss = 0.01922573226966441\n",
            "Cost after 299665 iterations : Training Loss =  0.010233304894099925; Validation Loss = 0.019225729320171806\n",
            "Cost after 299666 iterations : Training Loss =  0.010233297000145207; Validation Loss = 0.01922572637072141\n",
            "Cost after 299667 iterations : Training Loss =  0.010233289106269668; Validation Loss = 0.0192257234213124\n",
            "Cost after 299668 iterations : Training Loss =  0.010233281212473432; Validation Loss = 0.019225720471944787\n",
            "Cost after 299669 iterations : Training Loss =  0.010233273318756324; Validation Loss = 0.019225717522619465\n",
            "Cost after 299670 iterations : Training Loss =  0.010233265425118513; Validation Loss = 0.019225714573335825\n",
            "Cost after 299671 iterations : Training Loss =  0.010233257531559955; Validation Loss = 0.019225711624094012\n",
            "Cost after 299672 iterations : Training Loss =  0.01023324963808054; Validation Loss = 0.01922570867489355\n",
            "Cost after 299673 iterations : Training Loss =  0.01023324174468041; Validation Loss = 0.019225705725735114\n",
            "Cost after 299674 iterations : Training Loss =  0.010233233851359458; Validation Loss = 0.019225702776618316\n",
            "Cost after 299675 iterations : Training Loss =  0.010233225958117665; Validation Loss = 0.019225699827542894\n",
            "Cost after 299676 iterations : Training Loss =  0.010233218064955257; Validation Loss = 0.019225696878509647\n",
            "Cost after 299677 iterations : Training Loss =  0.01023321017187195; Validation Loss = 0.019225693929517763\n",
            "Cost after 299678 iterations : Training Loss =  0.010233202278867816; Validation Loss = 0.019225690980568203\n",
            "Cost after 299679 iterations : Training Loss =  0.010233194385942985; Validation Loss = 0.019225688031660047\n",
            "Cost after 299680 iterations : Training Loss =  0.010233186493097388; Validation Loss = 0.01922568508279336\n",
            "Cost after 299681 iterations : Training Loss =  0.010233178600330947; Validation Loss = 0.019225682133968757\n",
            "Cost after 299682 iterations : Training Loss =  0.01023317070764374; Validation Loss = 0.019225679185185925\n",
            "Cost after 299683 iterations : Training Loss =  0.01023316281503571; Validation Loss = 0.019225676236444986\n",
            "Cost after 299684 iterations : Training Loss =  0.010233154922506989; Validation Loss = 0.01922567328774545\n",
            "Cost after 299685 iterations : Training Loss =  0.010233147030057392; Validation Loss = 0.0192256703390878\n",
            "Cost after 299686 iterations : Training Loss =  0.010233139137687048; Validation Loss = 0.019225667390471716\n",
            "Cost after 299687 iterations : Training Loss =  0.010233131245395837; Validation Loss = 0.019225664441897546\n",
            "Cost after 299688 iterations : Training Loss =  0.010233123353183882; Validation Loss = 0.019225661493364864\n",
            "Cost after 299689 iterations : Training Loss =  0.010233115461051163; Validation Loss = 0.019225658544874374\n",
            "Cost after 299690 iterations : Training Loss =  0.010233107568997627; Validation Loss = 0.019225655596425435\n",
            "Cost after 299691 iterations : Training Loss =  0.010233099677023268; Validation Loss = 0.019225652648017812\n",
            "Cost after 299692 iterations : Training Loss =  0.010233091785128103; Validation Loss = 0.019225649699651827\n",
            "Cost after 299693 iterations : Training Loss =  0.01023308389331222; Validation Loss = 0.019225646751328027\n",
            "Cost after 299694 iterations : Training Loss =  0.0102330760015754; Validation Loss = 0.01922564380304574\n",
            "Cost after 299695 iterations : Training Loss =  0.010233068109917927; Validation Loss = 0.019225640854805465\n",
            "Cost after 299696 iterations : Training Loss =  0.010233060218339574; Validation Loss = 0.01922563790660643\n",
            "Cost after 299697 iterations : Training Loss =  0.01023305232684041; Validation Loss = 0.01922563495844957\n",
            "Cost after 299698 iterations : Training Loss =  0.010233044435420502; Validation Loss = 0.019225632010334004\n",
            "Cost after 299699 iterations : Training Loss =  0.010233036544079708; Validation Loss = 0.019225629062260444\n",
            "Cost after 299700 iterations : Training Loss =  0.010233028652818206; Validation Loss = 0.019225626114228588\n",
            "Cost after 299701 iterations : Training Loss =  0.010233020761635883; Validation Loss = 0.019225623166238517\n",
            "Cost after 299702 iterations : Training Loss =  0.010233012870532718; Validation Loss = 0.01922562021829012\n",
            "Cost after 299703 iterations : Training Loss =  0.01023300497950875; Validation Loss = 0.019225617270383258\n",
            "Cost after 299704 iterations : Training Loss =  0.010232997088563985; Validation Loss = 0.019225614322518136\n",
            "Cost after 299705 iterations : Training Loss =  0.010232989197698302; Validation Loss = 0.019225611374694914\n",
            "Cost after 299706 iterations : Training Loss =  0.010232981306911868; Validation Loss = 0.01922560842691303\n",
            "Cost after 299707 iterations : Training Loss =  0.010232973416204666; Validation Loss = 0.01922560547917336\n",
            "Cost after 299708 iterations : Training Loss =  0.010232965525576633; Validation Loss = 0.019225602531475185\n",
            "Cost after 299709 iterations : Training Loss =  0.01023295763502786; Validation Loss = 0.019225599583818576\n",
            "Cost after 299710 iterations : Training Loss =  0.010232949744558146; Validation Loss = 0.0192255966362038\n",
            "Cost after 299711 iterations : Training Loss =  0.010232941854167605; Validation Loss = 0.019225593688630648\n",
            "Cost after 299712 iterations : Training Loss =  0.010232933963856335; Validation Loss = 0.019225590741099412\n",
            "Cost after 299713 iterations : Training Loss =  0.0102329260736242; Validation Loss = 0.019225587793609774\n",
            "Cost after 299714 iterations : Training Loss =  0.010232918183471254; Validation Loss = 0.019225584846162002\n",
            "Cost after 299715 iterations : Training Loss =  0.010232910293397486; Validation Loss = 0.019225581898755707\n",
            "Cost after 299716 iterations : Training Loss =  0.010232902403402947; Validation Loss = 0.01922557895139125\n",
            "Cost after 299717 iterations : Training Loss =  0.010232894513487514; Validation Loss = 0.019225576004068393\n",
            "Cost after 299718 iterations : Training Loss =  0.010232886623651336; Validation Loss = 0.019225573056787366\n",
            "Cost after 299719 iterations : Training Loss =  0.010232878733894262; Validation Loss = 0.019225570109547906\n",
            "Cost after 299720 iterations : Training Loss =  0.01023287084421632; Validation Loss = 0.01922556716235026\n",
            "Cost after 299721 iterations : Training Loss =  0.010232862954617615; Validation Loss = 0.019225564215194384\n",
            "Cost after 299722 iterations : Training Loss =  0.0102328550650981; Validation Loss = 0.019225561268080057\n",
            "Cost after 299723 iterations : Training Loss =  0.010232847175657736; Validation Loss = 0.019225558321007457\n",
            "Cost after 299724 iterations : Training Loss =  0.010232839286296522; Validation Loss = 0.019225555373976414\n",
            "Cost after 299725 iterations : Training Loss =  0.010232831397014508; Validation Loss = 0.019225552426986904\n",
            "Cost after 299726 iterations : Training Loss =  0.010232823507811609; Validation Loss = 0.01922554948003954\n",
            "Cost after 299727 iterations : Training Loss =  0.010232815618688029; Validation Loss = 0.01922554653313391\n",
            "Cost after 299728 iterations : Training Loss =  0.010232807729643438; Validation Loss = 0.01922554358626995\n",
            "Cost after 299729 iterations : Training Loss =  0.010232799840678146; Validation Loss = 0.019225540639447584\n",
            "Cost after 299730 iterations : Training Loss =  0.010232791951791935; Validation Loss = 0.019225537692666737\n",
            "Cost after 299731 iterations : Training Loss =  0.01023278406298485; Validation Loss = 0.019225534745927787\n",
            "Cost after 299732 iterations : Training Loss =  0.010232776174256995; Validation Loss = 0.019225531799230502\n",
            "Cost after 299733 iterations : Training Loss =  0.010232768285608347; Validation Loss = 0.019225528852574777\n",
            "Cost after 299734 iterations : Training Loss =  0.010232760397038692; Validation Loss = 0.019225525905961046\n",
            "Cost after 299735 iterations : Training Loss =  0.010232752508548286; Validation Loss = 0.01922552295938859\n",
            "Cost after 299736 iterations : Training Loss =  0.010232744620137115; Validation Loss = 0.01922552001285824\n",
            "Cost after 299737 iterations : Training Loss =  0.010232736731805019; Validation Loss = 0.019225517066369706\n",
            "Cost after 299738 iterations : Training Loss =  0.010232728843552075; Validation Loss = 0.01922551411992231\n",
            "Cost after 299739 iterations : Training Loss =  0.010232720955378298; Validation Loss = 0.01922551117351676\n",
            "Cost after 299740 iterations : Training Loss =  0.010232713067283659; Validation Loss = 0.01922550822715291\n",
            "Cost after 299741 iterations : Training Loss =  0.01023270517926828; Validation Loss = 0.019225505280830776\n",
            "Cost after 299742 iterations : Training Loss =  0.010232697291331914; Validation Loss = 0.019225502334550164\n",
            "Cost after 299743 iterations : Training Loss =  0.010232689403474776; Validation Loss = 0.019225499388311744\n",
            "Cost after 299744 iterations : Training Loss =  0.010232681515696786; Validation Loss = 0.019225496442114843\n",
            "Cost after 299745 iterations : Training Loss =  0.010232673627997877; Validation Loss = 0.01922549349595944\n",
            "Cost after 299746 iterations : Training Loss =  0.010232665740378136; Validation Loss = 0.01922549054984604\n",
            "Cost after 299747 iterations : Training Loss =  0.010232657852837578; Validation Loss = 0.019225487603774162\n",
            "Cost after 299748 iterations : Training Loss =  0.010232649965376128; Validation Loss = 0.019225484657743677\n",
            "Cost after 299749 iterations : Training Loss =  0.010232642077993865; Validation Loss = 0.01922548171175513\n",
            "Cost after 299750 iterations : Training Loss =  0.010232634190690721; Validation Loss = 0.01922547876580822\n",
            "Cost after 299751 iterations : Training Loss =  0.01023262630346674; Validation Loss = 0.01922547581990295\n",
            "Cost after 299752 iterations : Training Loss =  0.010232618416321894; Validation Loss = 0.019225472874039334\n",
            "Cost after 299753 iterations : Training Loss =  0.010232610529256145; Validation Loss = 0.019225469928217644\n",
            "Cost after 299754 iterations : Training Loss =  0.010232602642269547; Validation Loss = 0.019225466982437357\n",
            "Cost after 299755 iterations : Training Loss =  0.010232594755362158; Validation Loss = 0.019225464036698815\n",
            "Cost after 299756 iterations : Training Loss =  0.010232586868533834; Validation Loss = 0.01922546109100236\n",
            "Cost after 299757 iterations : Training Loss =  0.010232578981784762; Validation Loss = 0.0192254581453469\n",
            "Cost after 299758 iterations : Training Loss =  0.010232571095114608; Validation Loss = 0.019225455199733527\n",
            "Cost after 299759 iterations : Training Loss =  0.010232563208523712; Validation Loss = 0.019225452254161713\n",
            "Cost after 299760 iterations : Training Loss =  0.010232555322011986; Validation Loss = 0.019225449308631525\n",
            "Cost after 299761 iterations : Training Loss =  0.010232547435579307; Validation Loss = 0.019225446363142922\n",
            "Cost after 299762 iterations : Training Loss =  0.01023253954922584; Validation Loss = 0.019225443417696223\n",
            "Cost after 299763 iterations : Training Loss =  0.010232531662951442; Validation Loss = 0.01922544047229136\n",
            "Cost after 299764 iterations : Training Loss =  0.01023252377675625; Validation Loss = 0.019225437526927677\n",
            "Cost after 299765 iterations : Training Loss =  0.01023251589064017; Validation Loss = 0.019225434581605926\n",
            "Cost after 299766 iterations : Training Loss =  0.010232508004603102; Validation Loss = 0.019225431636325976\n",
            "Cost after 299767 iterations : Training Loss =  0.010232500118645309; Validation Loss = 0.01922542869108759\n",
            "Cost after 299768 iterations : Training Loss =  0.010232492232766613; Validation Loss = 0.019225425745891003\n",
            "Cost after 299769 iterations : Training Loss =  0.010232484346966862; Validation Loss = 0.019225422800735838\n",
            "Cost after 299770 iterations : Training Loss =  0.010232476461246436; Validation Loss = 0.01922541985562262\n",
            "Cost after 299771 iterations : Training Loss =  0.010232468575605095; Validation Loss = 0.01922541691055082\n",
            "Cost after 299772 iterations : Training Loss =  0.010232460690042832; Validation Loss = 0.019225413965520743\n",
            "Cost after 299773 iterations : Training Loss =  0.010232452804559684; Validation Loss = 0.019225411020532265\n",
            "Cost after 299774 iterations : Training Loss =  0.01023244491915569; Validation Loss = 0.019225408075585367\n",
            "Cost after 299775 iterations : Training Loss =  0.010232437033830817; Validation Loss = 0.019225405130680346\n",
            "Cost after 299776 iterations : Training Loss =  0.010232429148585038; Validation Loss = 0.019225402185817135\n",
            "Cost after 299777 iterations : Training Loss =  0.010232421263418367; Validation Loss = 0.019225399240995134\n",
            "Cost after 299778 iterations : Training Loss =  0.010232413378330805; Validation Loss = 0.019225396296215048\n",
            "Cost after 299779 iterations : Training Loss =  0.010232405493322369; Validation Loss = 0.019225393351476647\n",
            "Cost after 299780 iterations : Training Loss =  0.01023239760839304; Validation Loss = 0.01922539040677992\n",
            "Cost after 299781 iterations : Training Loss =  0.010232389723542852; Validation Loss = 0.01922538746212477\n",
            "Cost after 299782 iterations : Training Loss =  0.010232381838771792; Validation Loss = 0.019225384517511518\n",
            "Cost after 299783 iterations : Training Loss =  0.010232373954079799; Validation Loss = 0.019225381572939897\n",
            "Cost after 299784 iterations : Training Loss =  0.010232366069466899; Validation Loss = 0.0192253786284096\n",
            "Cost after 299785 iterations : Training Loss =  0.010232358184933146; Validation Loss = 0.019225375683921103\n",
            "Cost after 299786 iterations : Training Loss =  0.01023235030047847; Validation Loss = 0.01922537273947419\n",
            "Cost after 299787 iterations : Training Loss =  0.010232342416102932; Validation Loss = 0.019225369795068843\n",
            "Cost after 299788 iterations : Training Loss =  0.010232334531806463; Validation Loss = 0.019225366850705475\n",
            "Cost after 299789 iterations : Training Loss =  0.010232326647589085; Validation Loss = 0.01922536390638346\n",
            "Cost after 299790 iterations : Training Loss =  0.010232318763450876; Validation Loss = 0.019225360962103283\n",
            "Cost after 299791 iterations : Training Loss =  0.010232310879391693; Validation Loss = 0.01922535801786487\n",
            "Cost after 299792 iterations : Training Loss =  0.010232302995411634; Validation Loss = 0.019225355073667846\n",
            "Cost after 299793 iterations : Training Loss =  0.010232295111510739; Validation Loss = 0.019225352129512648\n",
            "Cost after 299794 iterations : Training Loss =  0.010232287227688897; Validation Loss = 0.019225349185398824\n",
            "Cost after 299795 iterations : Training Loss =  0.010232279343946145; Validation Loss = 0.01922534624132683\n",
            "Cost after 299796 iterations : Training Loss =  0.010232271460282478; Validation Loss = 0.019225343297296697\n",
            "Cost after 299797 iterations : Training Loss =  0.010232263576697907; Validation Loss = 0.01922534035330795\n",
            "Cost after 299798 iterations : Training Loss =  0.010232255693192445; Validation Loss = 0.019225337409360978\n",
            "Cost after 299799 iterations : Training Loss =  0.010232247809766127; Validation Loss = 0.019225334465455476\n",
            "Cost after 299800 iterations : Training Loss =  0.010232239926418822; Validation Loss = 0.01922533152159184\n",
            "Cost after 299801 iterations : Training Loss =  0.010232232043150555; Validation Loss = 0.019225328577770047\n",
            "Cost after 299802 iterations : Training Loss =  0.010232224159961521; Validation Loss = 0.019225325633989578\n",
            "Cost after 299803 iterations : Training Loss =  0.010232216276851488; Validation Loss = 0.019225322690250887\n",
            "Cost after 299804 iterations : Training Loss =  0.010232208393820634; Validation Loss = 0.01922531974655393\n",
            "Cost after 299805 iterations : Training Loss =  0.010232200510868735; Validation Loss = 0.019225316802898452\n",
            "Cost after 299806 iterations : Training Loss =  0.010232192627995927; Validation Loss = 0.019225313859284367\n",
            "Cost after 299807 iterations : Training Loss =  0.010232184745202327; Validation Loss = 0.019225310915712505\n",
            "Cost after 299808 iterations : Training Loss =  0.010232176862487672; Validation Loss = 0.019225307972181933\n",
            "Cost after 299809 iterations : Training Loss =  0.010232168979852194; Validation Loss = 0.019225305028692505\n",
            "Cost after 299810 iterations : Training Loss =  0.010232161097295826; Validation Loss = 0.01922530208524521\n",
            "Cost after 299811 iterations : Training Loss =  0.010232153214818504; Validation Loss = 0.01922529914183998\n",
            "Cost after 299812 iterations : Training Loss =  0.010232145332420221; Validation Loss = 0.019225296198475465\n",
            "Cost after 299813 iterations : Training Loss =  0.010232137450101026; Validation Loss = 0.019225293255153313\n",
            "Cost after 299814 iterations : Training Loss =  0.010232129567861004; Validation Loss = 0.019225290311872558\n",
            "Cost after 299815 iterations : Training Loss =  0.010232121685699933; Validation Loss = 0.01922528736863314\n",
            "Cost after 299816 iterations : Training Loss =  0.010232113803618005; Validation Loss = 0.019225284425435536\n",
            "Cost after 299817 iterations : Training Loss =  0.010232105921615053; Validation Loss = 0.019225281482279546\n",
            "Cost after 299818 iterations : Training Loss =  0.01023209803969129; Validation Loss = 0.019225278539165457\n",
            "Cost after 299819 iterations : Training Loss =  0.01023209015784659; Validation Loss = 0.019225275596092935\n",
            "Cost after 299820 iterations : Training Loss =  0.010232082276080924; Validation Loss = 0.019225272653062116\n",
            "Cost after 299821 iterations : Training Loss =  0.010232074394394367; Validation Loss = 0.019225269710072705\n",
            "Cost after 299822 iterations : Training Loss =  0.010232066512786842; Validation Loss = 0.01922526676712472\n",
            "Cost after 299823 iterations : Training Loss =  0.010232058631258364; Validation Loss = 0.019225263824218863\n",
            "Cost after 299824 iterations : Training Loss =  0.010232050749809034; Validation Loss = 0.019225260881354615\n",
            "Cost after 299825 iterations : Training Loss =  0.01023204286843868; Validation Loss = 0.01922525793853137\n",
            "Cost after 299826 iterations : Training Loss =  0.010232034987147389; Validation Loss = 0.01922525499575022\n",
            "Cost after 299827 iterations : Training Loss =  0.010232027105935253; Validation Loss = 0.019225252053010754\n",
            "Cost after 299828 iterations : Training Loss =  0.010232019224802111; Validation Loss = 0.01922524911031269\n",
            "Cost after 299829 iterations : Training Loss =  0.01023201134374808; Validation Loss = 0.019225246167656335\n",
            "Cost after 299830 iterations : Training Loss =  0.0102320034627731; Validation Loss = 0.019225243225041683\n",
            "Cost after 299831 iterations : Training Loss =  0.010231995581877169; Validation Loss = 0.019225240282468588\n",
            "Cost after 299832 iterations : Training Loss =  0.010231987701060315; Validation Loss = 0.019225237339937116\n",
            "Cost after 299833 iterations : Training Loss =  0.01023197982032246; Validation Loss = 0.0192252343974476\n",
            "Cost after 299834 iterations : Training Loss =  0.010231971939663707; Validation Loss = 0.01922523145499949\n",
            "Cost after 299835 iterations : Training Loss =  0.01023196405908402; Validation Loss = 0.019225228512592875\n",
            "Cost after 299836 iterations : Training Loss =  0.010231956178583402; Validation Loss = 0.01922522557022782\n",
            "Cost after 299837 iterations : Training Loss =  0.01023194829816177; Validation Loss = 0.019225222627904284\n",
            "Cost after 299838 iterations : Training Loss =  0.010231940417819245; Validation Loss = 0.019225219685622658\n",
            "Cost after 299839 iterations : Training Loss =  0.010231932537555737; Validation Loss = 0.01922521674338249\n",
            "Cost after 299840 iterations : Training Loss =  0.010231924657371293; Validation Loss = 0.019225213801183978\n",
            "Cost after 299841 iterations : Training Loss =  0.010231916777265938; Validation Loss = 0.019225210859027012\n",
            "Cost after 299842 iterations : Training Loss =  0.010231908897239569; Validation Loss = 0.01922520791691183\n",
            "Cost after 299843 iterations : Training Loss =  0.01023190101729232; Validation Loss = 0.019225204974837914\n",
            "Cost after 299844 iterations : Training Loss =  0.010231893137424113; Validation Loss = 0.019225202032805734\n",
            "Cost after 299845 iterations : Training Loss =  0.01023188525763487; Validation Loss = 0.019225199090815485\n",
            "Cost after 299846 iterations : Training Loss =  0.0102318773779247; Validation Loss = 0.019225196148866756\n",
            "Cost after 299847 iterations : Training Loss =  0.010231869498293631; Validation Loss = 0.01922519320695954\n",
            "Cost after 299848 iterations : Training Loss =  0.01023186161874155; Validation Loss = 0.01922519026509407\n",
            "Cost after 299849 iterations : Training Loss =  0.010231853739268547; Validation Loss = 0.01922518732327027\n",
            "Cost after 299850 iterations : Training Loss =  0.010231845859874503; Validation Loss = 0.019225184381487747\n",
            "Cost after 299851 iterations : Training Loss =  0.010231837980559605; Validation Loss = 0.019225181439746875\n",
            "Cost after 299852 iterations : Training Loss =  0.010231830101323714; Validation Loss = 0.019225178498047862\n",
            "Cost after 299853 iterations : Training Loss =  0.0102318222221668; Validation Loss = 0.019225175556389997\n",
            "Cost after 299854 iterations : Training Loss =  0.010231814343089033; Validation Loss = 0.019225172614774157\n",
            "Cost after 299855 iterations : Training Loss =  0.010231806464090276; Validation Loss = 0.019225169673199535\n",
            "Cost after 299856 iterations : Training Loss =  0.01023179858517048; Validation Loss = 0.01922516673166674\n",
            "Cost after 299857 iterations : Training Loss =  0.010231790706329793; Validation Loss = 0.019225163790175488\n",
            "Cost after 299858 iterations : Training Loss =  0.010231782827568068; Validation Loss = 0.01922516084872591\n",
            "Cost after 299859 iterations : Training Loss =  0.010231774948885388; Validation Loss = 0.01922515790731804\n",
            "Cost after 299860 iterations : Training Loss =  0.010231767070281797; Validation Loss = 0.019225154965951667\n",
            "Cost after 299861 iterations : Training Loss =  0.01023175919175723; Validation Loss = 0.019225152024627046\n",
            "Cost after 299862 iterations : Training Loss =  0.010231751313311587; Validation Loss = 0.019225149083343794\n",
            "Cost after 299863 iterations : Training Loss =  0.010231743434945087; Validation Loss = 0.01922514614210223\n",
            "Cost after 299864 iterations : Training Loss =  0.01023173555665754; Validation Loss = 0.019225143200902226\n",
            "Cost after 299865 iterations : Training Loss =  0.010231727678449069; Validation Loss = 0.019225140259743746\n",
            "Cost after 299866 iterations : Training Loss =  0.010231719800319573; Validation Loss = 0.01922513731862695\n",
            "Cost after 299867 iterations : Training Loss =  0.010231711922269196; Validation Loss = 0.0192251343775517\n",
            "Cost after 299868 iterations : Training Loss =  0.010231704044297818; Validation Loss = 0.019225131436518054\n",
            "Cost after 299869 iterations : Training Loss =  0.010231696166405384; Validation Loss = 0.019225128495526246\n",
            "Cost after 299870 iterations : Training Loss =  0.010231688288591976; Validation Loss = 0.01922512555457607\n",
            "Cost after 299871 iterations : Training Loss =  0.010231680410857615; Validation Loss = 0.019225122613667316\n",
            "Cost after 299872 iterations : Training Loss =  0.010231672533202297; Validation Loss = 0.019225119672800082\n",
            "Cost after 299873 iterations : Training Loss =  0.010231664655625961; Validation Loss = 0.01922511673197428\n",
            "Cost after 299874 iterations : Training Loss =  0.010231656778128699; Validation Loss = 0.019225113791190386\n",
            "Cost after 299875 iterations : Training Loss =  0.010231648900710391; Validation Loss = 0.01922511085044824\n",
            "Cost after 299876 iterations : Training Loss =  0.01023164102337115; Validation Loss = 0.019225107909747158\n",
            "Cost after 299877 iterations : Training Loss =  0.01023163314611084; Validation Loss = 0.019225104969087917\n",
            "Cost after 299878 iterations : Training Loss =  0.010231625268929654; Validation Loss = 0.019225102028470116\n",
            "Cost after 299879 iterations : Training Loss =  0.010231617391827343; Validation Loss = 0.01922509908789424\n",
            "Cost after 299880 iterations : Training Loss =  0.01023160951480419; Validation Loss = 0.01922509614735961\n",
            "Cost after 299881 iterations : Training Loss =  0.010231601637859924; Validation Loss = 0.019225093206866793\n",
            "Cost after 299882 iterations : Training Loss =  0.010231593760994728; Validation Loss = 0.019225090266415414\n",
            "Cost after 299883 iterations : Training Loss =  0.010231585884208541; Validation Loss = 0.019225087326005637\n",
            "Cost after 299884 iterations : Training Loss =  0.010231578007501385; Validation Loss = 0.019225084385637574\n",
            "Cost after 299885 iterations : Training Loss =  0.010231570130873134; Validation Loss = 0.019225081445311022\n",
            "Cost after 299886 iterations : Training Loss =  0.010231562254323956; Validation Loss = 0.019225078505026087\n",
            "Cost after 299887 iterations : Training Loss =  0.010231554377853723; Validation Loss = 0.019225075564782777\n",
            "Cost after 299888 iterations : Training Loss =  0.010231546501462635; Validation Loss = 0.01922507262458076\n",
            "Cost after 299889 iterations : Training Loss =  0.010231538625150432; Validation Loss = 0.019225069684420754\n",
            "Cost after 299890 iterations : Training Loss =  0.010231530748917276; Validation Loss = 0.019225066744302056\n",
            "Cost after 299891 iterations : Training Loss =  0.010231522872763094; Validation Loss = 0.019225063804225152\n",
            "Cost after 299892 iterations : Training Loss =  0.010231514996687938; Validation Loss = 0.019225060864189666\n",
            "Cost after 299893 iterations : Training Loss =  0.01023150712069172; Validation Loss = 0.01922505792419607\n",
            "Cost after 299894 iterations : Training Loss =  0.010231499244774593; Validation Loss = 0.01922505498424369\n",
            "Cost after 299895 iterations : Training Loss =  0.010231491368936328; Validation Loss = 0.019225052044333134\n",
            "Cost after 299896 iterations : Training Loss =  0.010231483493177148; Validation Loss = 0.019225049104464157\n",
            "Cost after 299897 iterations : Training Loss =  0.01023147561749694; Validation Loss = 0.01922504616463667\n",
            "Cost after 299898 iterations : Training Loss =  0.010231467741895787; Validation Loss = 0.01922504322485051\n",
            "Cost after 299899 iterations : Training Loss =  0.010231459866373532; Validation Loss = 0.01922504028510612\n",
            "Cost after 299900 iterations : Training Loss =  0.010231451990930333; Validation Loss = 0.019225037345402943\n",
            "Cost after 299901 iterations : Training Loss =  0.010231444115565992; Validation Loss = 0.01922503440574166\n",
            "Cost after 299902 iterations : Training Loss =  0.01023143624028078; Validation Loss = 0.01922503146612194\n",
            "Cost after 299903 iterations : Training Loss =  0.010231428365074489; Validation Loss = 0.019225028526543845\n",
            "Cost after 299904 iterations : Training Loss =  0.010231420489947286; Validation Loss = 0.019225025587007543\n",
            "Cost after 299905 iterations : Training Loss =  0.010231412614899021; Validation Loss = 0.01922502264751215\n",
            "Cost after 299906 iterations : Training Loss =  0.010231404739929664; Validation Loss = 0.019225019708058795\n",
            "Cost after 299907 iterations : Training Loss =  0.010231396865039311; Validation Loss = 0.019225016768646883\n",
            "Cost after 299908 iterations : Training Loss =  0.010231388990227996; Validation Loss = 0.019225013829276577\n",
            "Cost after 299909 iterations : Training Loss =  0.010231381115495573; Validation Loss = 0.019225010889947682\n",
            "Cost after 299910 iterations : Training Loss =  0.010231373240842225; Validation Loss = 0.019225007950660674\n",
            "Cost after 299911 iterations : Training Loss =  0.010231365366267791; Validation Loss = 0.0192250050114152\n",
            "Cost after 299912 iterations : Training Loss =  0.010231357491772394; Validation Loss = 0.019225002072211065\n",
            "Cost after 299913 iterations : Training Loss =  0.01023134961735583; Validation Loss = 0.01922499913304869\n",
            "Cost after 299914 iterations : Training Loss =  0.010231341743018438; Validation Loss = 0.01922499619392773\n",
            "Cost after 299915 iterations : Training Loss =  0.010231333868759915; Validation Loss = 0.01922499325484853\n",
            "Cost after 299916 iterations : Training Loss =  0.010231325994580385; Validation Loss = 0.019224990315810587\n",
            "Cost after 299917 iterations : Training Loss =  0.010231318120479851; Validation Loss = 0.019224987376814242\n",
            "Cost after 299918 iterations : Training Loss =  0.010231310246458238; Validation Loss = 0.01922498443785958\n",
            "Cost after 299919 iterations : Training Loss =  0.010231302372515612; Validation Loss = 0.019224981498946362\n",
            "Cost after 299920 iterations : Training Loss =  0.010231294498651925; Validation Loss = 0.019224978560074893\n",
            "Cost after 299921 iterations : Training Loss =  0.010231286624867282; Validation Loss = 0.019224975621244835\n",
            "Cost after 299922 iterations : Training Loss =  0.010231278751161596; Validation Loss = 0.019224972682456484\n",
            "Cost after 299923 iterations : Training Loss =  0.01023127087753483; Validation Loss = 0.01922496974370978\n",
            "Cost after 299924 iterations : Training Loss =  0.010231263003987077; Validation Loss = 0.019224966805004295\n",
            "Cost after 299925 iterations : Training Loss =  0.010231255130518238; Validation Loss = 0.01922496386634036\n",
            "Cost after 299926 iterations : Training Loss =  0.010231247257128404; Validation Loss = 0.019224960927718072\n",
            "Cost after 299927 iterations : Training Loss =  0.010231239383817503; Validation Loss = 0.01922495798913714\n",
            "Cost after 299928 iterations : Training Loss =  0.01023123151058563; Validation Loss = 0.01922495505059819\n",
            "Cost after 299929 iterations : Training Loss =  0.010231223637432642; Validation Loss = 0.019224952112100666\n",
            "Cost after 299930 iterations : Training Loss =  0.010231215764358622; Validation Loss = 0.01922494917364464\n",
            "Cost after 299931 iterations : Training Loss =  0.0102312078913636; Validation Loss = 0.01922494623523039\n",
            "Cost after 299932 iterations : Training Loss =  0.010231200018447484; Validation Loss = 0.019224943296857477\n",
            "Cost after 299933 iterations : Training Loss =  0.010231192145610425; Validation Loss = 0.019224940358525663\n",
            "Cost after 299934 iterations : Training Loss =  0.010231184272852203; Validation Loss = 0.019224937420235715\n",
            "Cost after 299935 iterations : Training Loss =  0.010231176400173; Validation Loss = 0.01922493448198756\n",
            "Cost after 299936 iterations : Training Loss =  0.010231168527572714; Validation Loss = 0.01922493154378073\n",
            "Cost after 299937 iterations : Training Loss =  0.010231160655051356; Validation Loss = 0.019224928605615605\n",
            "Cost after 299938 iterations : Training Loss =  0.01023115278260894; Validation Loss = 0.019224925667491843\n",
            "Cost after 299939 iterations : Training Loss =  0.010231144910245576; Validation Loss = 0.019224922729409833\n",
            "Cost after 299940 iterations : Training Loss =  0.01023113703796112; Validation Loss = 0.01922491979136965\n",
            "Cost after 299941 iterations : Training Loss =  0.010231129165755621; Validation Loss = 0.01922491685337075\n",
            "Cost after 299942 iterations : Training Loss =  0.010231121293629011; Validation Loss = 0.019224913915413328\n",
            "Cost after 299943 iterations : Training Loss =  0.010231113421581357; Validation Loss = 0.01922491097749753\n",
            "Cost after 299944 iterations : Training Loss =  0.010231105549612709; Validation Loss = 0.019224908039622984\n",
            "Cost after 299945 iterations : Training Loss =  0.010231097677723044; Validation Loss = 0.019224905101790153\n",
            "Cost after 299946 iterations : Training Loss =  0.010231089805912115; Validation Loss = 0.019224902163998824\n",
            "Cost after 299947 iterations : Training Loss =  0.010231081934180392; Validation Loss = 0.019224899226248978\n",
            "Cost after 299948 iterations : Training Loss =  0.010231074062527434; Validation Loss = 0.019224896288540867\n",
            "Cost after 299949 iterations : Training Loss =  0.01023106619095342; Validation Loss = 0.019224893350873972\n",
            "Cost after 299950 iterations : Training Loss =  0.010231058319458424; Validation Loss = 0.019224890413248662\n",
            "Cost after 299951 iterations : Training Loss =  0.010231050448042315; Validation Loss = 0.019224887475664813\n",
            "Cost after 299952 iterations : Training Loss =  0.010231042576705197; Validation Loss = 0.019224884538122624\n",
            "Cost after 299953 iterations : Training Loss =  0.010231034705446902; Validation Loss = 0.019224881600622346\n",
            "Cost after 299954 iterations : Training Loss =  0.010231026834267654; Validation Loss = 0.019224878663163105\n",
            "Cost after 299955 iterations : Training Loss =  0.01023101896316727; Validation Loss = 0.0192248757257458\n",
            "Cost after 299956 iterations : Training Loss =  0.010231011092145824; Validation Loss = 0.01922487278836946\n",
            "Cost after 299957 iterations : Training Loss =  0.010231003221203312; Validation Loss = 0.01922486985103509\n",
            "Cost after 299958 iterations : Training Loss =  0.01023099535033974; Validation Loss = 0.019224866913742176\n",
            "Cost after 299959 iterations : Training Loss =  0.010230987479555174; Validation Loss = 0.019224863976490987\n",
            "Cost after 299960 iterations : Training Loss =  0.01023097960884945; Validation Loss = 0.019224861039281275\n",
            "Cost after 299961 iterations : Training Loss =  0.010230971738222691; Validation Loss = 0.019224858102112847\n",
            "Cost after 299962 iterations : Training Loss =  0.010230963867674865; Validation Loss = 0.019224855164986118\n",
            "Cost after 299963 iterations : Training Loss =  0.010230955997205908; Validation Loss = 0.01922485222790111\n",
            "Cost after 299964 iterations : Training Loss =  0.01023094812681589; Validation Loss = 0.019224849290857035\n",
            "Cost after 299965 iterations : Training Loss =  0.01023094025650484; Validation Loss = 0.019224846353854814\n",
            "Cost after 299966 iterations : Training Loss =  0.01023093238627266; Validation Loss = 0.01922484341689448\n",
            "Cost after 299967 iterations : Training Loss =  0.010230924516119485; Validation Loss = 0.019224840479974987\n",
            "Cost after 299968 iterations : Training Loss =  0.010230916646045154; Validation Loss = 0.01922483754309742\n",
            "Cost after 299969 iterations : Training Loss =  0.01023090877604989; Validation Loss = 0.019224834606261225\n",
            "Cost after 299970 iterations : Training Loss =  0.01023090090613328; Validation Loss = 0.01922483166946684\n",
            "Cost after 299971 iterations : Training Loss =  0.01023089303629583; Validation Loss = 0.019224828732713698\n",
            "Cost after 299972 iterations : Training Loss =  0.010230885166537163; Validation Loss = 0.01922482579600245\n",
            "Cost after 299973 iterations : Training Loss =  0.010230877296857415; Validation Loss = 0.019224822859332375\n",
            "Cost after 299974 iterations : Training Loss =  0.010230869427256659; Validation Loss = 0.019224819922704015\n",
            "Cost after 299975 iterations : Training Loss =  0.010230861557734713; Validation Loss = 0.01922481698611696\n",
            "Cost after 299976 iterations : Training Loss =  0.010230853688291734; Validation Loss = 0.019224814049571295\n",
            "Cost after 299977 iterations : Training Loss =  0.010230845818927626; Validation Loss = 0.019224811113067158\n",
            "Cost after 299978 iterations : Training Loss =  0.010230837949642501; Validation Loss = 0.019224808176604997\n",
            "Cost after 299979 iterations : Training Loss =  0.010230830080436308; Validation Loss = 0.019224805240183874\n",
            "Cost after 299980 iterations : Training Loss =  0.010230822211308924; Validation Loss = 0.019224802303804283\n",
            "Cost after 299981 iterations : Training Loss =  0.0102308143422605; Validation Loss = 0.019224799367466513\n",
            "Cost after 299982 iterations : Training Loss =  0.01023080647329096; Validation Loss = 0.01922479643117017\n",
            "Cost after 299983 iterations : Training Loss =  0.010230798604400342; Validation Loss = 0.019224793494915182\n",
            "Cost after 299984 iterations : Training Loss =  0.010230790735588682; Validation Loss = 0.019224790558701736\n",
            "Cost after 299985 iterations : Training Loss =  0.010230782866855826; Validation Loss = 0.019224787622530264\n",
            "Cost after 299986 iterations : Training Loss =  0.010230774998201953; Validation Loss = 0.019224784686400088\n",
            "Cost after 299987 iterations : Training Loss =  0.010230767129626868; Validation Loss = 0.01922478175031082\n",
            "Cost after 299988 iterations : Training Loss =  0.010230759261130751; Validation Loss = 0.019224778814263738\n",
            "Cost after 299989 iterations : Training Loss =  0.010230751392713544; Validation Loss = 0.019224775878258042\n",
            "Cost after 299990 iterations : Training Loss =  0.010230743524375308; Validation Loss = 0.019224772942294008\n",
            "Cost after 299991 iterations : Training Loss =  0.010230735656115866; Validation Loss = 0.019224770006370785\n",
            "Cost after 299992 iterations : Training Loss =  0.010230727787935387; Validation Loss = 0.01922476707048956\n",
            "Cost after 299993 iterations : Training Loss =  0.010230719919833731; Validation Loss = 0.019224764134649786\n",
            "Cost after 299994 iterations : Training Loss =  0.010230712051811036; Validation Loss = 0.019224761198851407\n",
            "Cost after 299995 iterations : Training Loss =  0.010230704183867168; Validation Loss = 0.019224758263094936\n",
            "Cost after 299996 iterations : Training Loss =  0.010230696316002217; Validation Loss = 0.019224755327379616\n",
            "Cost after 299997 iterations : Training Loss =  0.010230688448216186; Validation Loss = 0.01922475239170604\n",
            "Cost after 299998 iterations : Training Loss =  0.010230680580508968; Validation Loss = 0.019224749456073522\n",
            "Cost after 299999 iterations : Training Loss =  0.010230672712880718; Validation Loss = 0.019224746520482738\n",
            "Cost after 300000 iterations : Training Loss =  0.010230664845331352; Validation Loss = 0.01922474358493356\n",
            "Training Complete : min_loss_achieved = 0.01922474358493356; A,B,C = (0.8585086017205897, -1.4081778399486164, -0.06391288182802626)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EcM656tQWFA9",
        "outputId": "4dfc74b1-8f5b-4557-e192-5069bd73bb0e"
      },
      "source": [
        "# Visualizing Individual Results :\n",
        "\n",
        "W,B,_,_ = lin_results[0]\n",
        "y = lambda x : W*x + B\n",
        "plt.plot(lin_data['test_x'], lin_data['test_y'], 'ro')\n",
        "plt.plot(x, y_lin, 'b')\n",
        "plt.plot(x, y(x), 'g')\n",
        "plt.legend(['test_data', 'ground_truth', 'estimated_line'])\n",
        "plt.title('Linear Regression For Loss = |y-z|')\n",
        "plt.show()\n",
        "\n",
        "W,B,_,_ = lin_results[1]\n",
        "y = lambda x : W*x + B\n",
        "plt.plot(lin_data['test_x'], lin_data['test_y'], 'ro')\n",
        "plt.plot(x, y_lin, 'b')\n",
        "plt.plot(x, y(x), 'g')\n",
        "plt.legend(['test_data', 'ground_truth', 'estimated_line'])\n",
        "plt.title('Linear Regression For Loss = |y-z|^3')\n",
        "plt.show()\n",
        "\n",
        "A,B,C,_,_ = quad_results[0]\n",
        "y = lambda x : A*(x**2) + B*x + C\n",
        "plt.plot(quad_data['test_x'], quad_data['test_y'], 'ro')\n",
        "plt.plot(x, y_quad, 'b')\n",
        "plt.plot(x, y(x), 'g')\n",
        "plt.legend(['test_data', 'ground_truth', 'estimated_curve'])\n",
        "plt.title('Quadratic Regression For Loss = |y-z|^4')\n",
        "plt.show()\n",
        "\n",
        "A,B,C,_,_ = quad_results[1]\n",
        "y = lambda x : A*(x**2) + B*x + C\n",
        "plt.plot(quad_data['test_x'], quad_data['test_y'], 'ro')\n",
        "plt.plot(x, y_quad, 'b')\n",
        "plt.plot(x, y(x), 'g')\n",
        "plt.legend(['test_data', 'ground_truth', 'estimated_curve'])\n",
        "plt.title('Quadratic Regression For Loss = |y-z|^7')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gUVffA8e9JCAkhdAKCpGBDEKQYREDp0kURERAVFEQFAX+vyisvKqhgL6AICtKJCIINRWlSBAsEDKIgPYn00Ekve39/zCZsQhrJJptNzud59snuzJ2Zs7Nw9u6dO/eKMQallFLux8PVASillMofTeBKKeWmNIErpZSb0gSulFJuShO4Ukq5KU3gSinlpjSBF1MiEiwiRkTKOHGf/xORT521P3cgIoEiEiMinq6OpSQTEaf3RxaRCSIyIZt1Q0RkuLOP6W40gV8BERksIjtFJE5EjovINBGp5Oq4siIi7UTksOMyY8xrxpih+djXehFJsCfCUyLypYjUcl60hccYE2WM8TPGpDp73/Yv2Fj7eYkRkXNO2u8EEVnojH2VRCLyHPAy8IyITMxifUymR6qIfFj0kRY+TeB5JCLPAG8CzwGVgNuAYGCViHgVcSwiIkX92T1ljPEDrgP8gHecfQBn/tooQo3tXxB+xpjKV7qxm75nlxGRQcCTQBv7o4+IPOVYxuHz8AOuAuKBL4o82CKgCTwPRKQi1jf+SGPMj8aYZGNMBHA/cA3wgL3cXMcaQeZasIg8LyIHROSiiOwSkd4O6zxF5B17Dfcg0CNTDOtFZJKIbAbigGtE5BER2W3f30ERedxetjzwA1DboRZSO3PNTkRuF5FfROSciPwrIoNzOxfGmHPA10ATh/3cKCKrReSMiOwRkfsd1lUTkeUickFEtorIRBHZ5LDeiMgIEdkH7LMv6yki4fa4fhGRmx3K/1dEjtjf8x4R6WhffquIhNmPc0JE3rMvz9AUZT8P39pj3S8ijznse4KILBGR+fb9/y0iIbmdk8xEpJJ9H9EiEikiL6R94dp/xW0WkfdF5DQw4Qr33cse1zn7v4n6+T03hUFEytrPbSOHZTXE+tXqn0X5qZlqyymSfbNJD+C/QFtjzEFjzBGgLTBURPpmE1If4CTwc0HfW7FkjNFHLg+gK5AClMli3Twg1P58LjDRYV074LDD675Abawvzn5ALFDLvu4J4B8gAKgKrANM2jGB9UAUcBNQBvDCSvLXAoL1DzkOaJbVse3LJgAL7c+DgIvAAPu+qgFNsnn/64Gh9ufVgDXAN/bX5YF/gUfscTUFTgEN7Os/tz98gQb2spsc9m2A1fb3XM6+/UmgBeAJDAIiAG+gnn372vZtg4Fr7c9/BR6yP/cDbnMo43geNwLTAB+sL6FooIPD+UkAutuP/TrwWw7/LgxwXRbL5wPfABXsx98LDLGvG4z1b2mk/XyVy2L79M8p0/IbsP7N3Gn/zMYA+4Gy+Tk3Wew/EDiXw+OBnM6Fw/NpwJsOr0cDy/Pw/yzt82jqcB4mFPD/7k8F3Udxfrg8AHd4AA8Cx7NZ9wawyv58Ljkk8Cy2DQfutj//CXjCYV1nLk/gr+QS59fA6OyOTcYEPhb4Ko/vfz3Wl8N5e0zhQKB9XT/g50zlPwHGYyXBZKCew7qJXJ7AOzi8ng68mml/e7C+oK7DSu6dAK9MZTZi/Uqqnml5cNp5xPpyTAUqOKx/HZjrcH7WOKxrAMTncF4McMEhwX1gf89J2L/A7OUeB9bbnw8GonI53+mfU6blLwJLHF57AEfsn/UVnxsn/x8xDs9bYFU2xP46DLg/l+39sb6o+2c6DxMKEFOQ/fOuW1jv29UPbULJm1NA9WzaK2vZ1+dKRB52aBo4BzQEqttX18aqQaWJzGIXjusRkW4i8pv9J+s5rJpj9Sy2y0oAcCCPZQFGGWMqATcDVYA69uVBQIu092SPYyBW26M/VuJ0jDvDe8hiWRDWxSnH/QVg1Sz3A09j/cc+KSKfi0ht+3ZDsGqo/9ibanpmcZzawBljzEWHZZHA1Q6vjzs8jwN8svnc0zQzxlS2P0ZhnX8vMn5+mY+R1TnIi9qO+zXG2Oz7utoJ58ZpjDG/Y527diJyI9aXy7dw2QXGQPsyL2Ap8Jkx5nMnhvIQVmXhkBP3WaxoAs+bX4FE4F7HhSLiB3TDqqGC9fPW16HIVQ5lg4CZwFNANWNd8PoLq/kD4BhWokoTmEUc6V21RMQbWIZ1MbGmfX8rHPaXW7euf7GaX66IMWYnVi36IxER+342OCSxysa6gPQk1s/hFC4le8j4Hi97X/b9Tcq0P19jzCL78T8zxtyOlegN1oVljDH7jDEDgBr2ZUvFuhbg6ChQVUQqOCwLxKrFOssprF8dQTkcI79d7o467td+/gPS9l3Ac+PY5TK7x8AriHUe1i/Xh4ClxpgEeyx+Do8oe9kPsX7JvHAlJyMPHrbHUWJpAs8DY8x5rJ+gH4pIVxHxEpFgYAnWf9hQe9FwoLuIVBWRq7BqRGnKY/2nigYQkUewauBplgCjRKSOiFQBns8lrLJY7cLRQIqIdMNqdklzAqgm2XdzDAU6icj9IlJGrIuNTbIpm9k8oCbQC/gOuEFEHrKfFy8RaS4i9Y3Vde9LYIKI+NprYw/nsu+ZwBMi0kIs5UWkh4hUEJF6ItLB/uWVgNW7wAYgIg+KiL+9VprWnc/muGNjzL/AL8DrIuIj1sXRIYDTuuzZ3/MSYJI95iDgP/k4hoc9xrSHt32/PUSko73W+gxWxeKXgp4be+xRmRJs5kdo5m1ysBDojZXE52dXSKwL722Bgfb4nEJEWmH96imRvU/SaALPI2PMW8D/sGq8F4FDWLXtTsaYWHuxBcAOrLa8VcBih+13Ae9i1eZPAI2AzQ6HmAmstG+/HSvx5RTPRWAU1n/qs1g9Yb51WP8PsAg4aG+KqJ1p+yisJpdngDNYXz6N83gukoApwIv2ODoD/bFqiMexanne9uJPYXW7PI51fhZhJZ3s9h0GPAZMtb+v/Vjtxtj3+QbWl+ZxrBrlWPu6rsDfIhJjj62/MSY+i0MMwGoXPwp8BYw3xqzJy/u+AiOxfo0dBDYBnwGzr3AfA7CScNrjgDFmD1ZC/BDrHNwF3GX/PJxxbpzG/mW5HavSklMPkAFYPbmOOtT0/+eEEAYBX2ZqLitx0i4yqCtkr0G/ArR2+CmociEibwJXGWMGuToW5TwiYowxkmnZbOCoMSZfTSNp3QmNMRMKHGAJpTcR5JMxZo6IpACtsK64qyzYm03KAjuB5lhNFld8N6hyL/YmxnuxuoWqQqIJvACMMQtcHYMbqIDVbFIbq+noXaw+0qpkeTntiYi8Cvwf8HoBe4CsL2hQJZ02oSillJvSi5hKKeWmirQJpXr16iY4OLgoD6mUUm5v27Ztp4wxl40lU6QJPDg4mLCwsKI8pFJKuT0RyerObG1CUUopd6UJXCml3JQmcKWUclMu7weenJzM4cOHSUhIcHUoyol8fHyoU6cOXl5FOlmRUqWKyxP44cOHqVChAsHBwViDqyl3Z4zh9OnTHD58mLp167o6HKVKLJc3oSQkJFCtWjVN3iWIiFCtWjX9VaVUIXN5Agc0eZdA+pkqVfiKRQJXSqmSKj4eHht9muPHcy97pTSBK6VUIRr+6jY+9Qtk1s/fO33fuSZwEZktIidF5K8s1j0jIkZE8joPY8GFhkJwMHh4WH9Dr2SSkMudO3eOadOm5WvbyZMnExcXl+fyc+fO5amnnsqxzPr16/nll1/yFY9SqhhwyFFhdbow9+yjlPOoxIi7Wjv9UHmpgc/FmtEjAxEJwJqJpejGwg4NhWHDIDISjLH+DhtWoCRelAk8LzSBK+XGHHJUqhHuubYuXPUnn1R7kMo+lZ1+uFwTuDFmI9aUW5m9D4wh/xO0Xrlx4yBzwoyLs5bn0/PPP8+BAwdo0qQJzz33HG+//TbNmzfn5ptvZvz48QDExsbSo0cPGjduTMOGDVm8eDEffPABR48epX379rRv3z7b/c+ZM4cbbriBW2+9lc2bL82gtnz5clq0aEHTpk3p1KkTJ06cICIigo8//pj333+fJk2a8PPPP2dZTilVTDnkqHH+D3CkzRxa/hXAQ5OXFM7xjDG5PrDmEPzL4fXdwBT78wigel72c8stt5jMdu3addmybIkYY9W9Mz6s6Zzy5dChQ+amm24yxhizcuVK89hjjxmbzWZSU1NNjx49zIYNG8zSpUvN0KFD07c5d+6cMcaYoKAgEx0dne2+jx49agICAszJkydNYmKiadWqlRkxYoQxxpgzZ84Ym81mjDFm5syZ5j//+Y8xxpjx48ebt99+O30f2ZVzB1f02SpVEthz1F4JNh5DbjVeYyqaE74Fy1HGGAOEmSxy6hXfyCMivliT+3bOray9/DBgGEBgYOCVHi6jwECr2SSr5U6watUqVq1aRdOm1ixQMTEx7Nu3jzvuuINnnnmG//73v/Ts2ZM77rgjT/v7/fffadeuHf7+1iiQ/fr1Y+/evYB1A1O/fv04duwYSUlJ2d7wktdySqliIDAQExlJjxbdsAVMZ/KyqtSIA4Kck6Myy08vlGuBusAOEYkA6gDbReSqrAobY2YYY0KMMSFpiSzfJk0CX9+My3x9reVOYIxh7NixhIeHEx4ezv79+xkyZAg33HAD27dvp1GjRrzwwgu88sorBT7WyJEjeeqpp9i5cyeffPJJtje95LWcUspFHDtWxMTwZrX+7Oswj4Z7A3lq5xmn5qjMrjiBG2N2GmNqGGOCjTHBwGGgmTGmEHo5ZjJwIMyYAUFBIGL9nTHDWp5PFSpU4OLFiwB06dKF2bNnExMTA8CRI0c4efIkR48exdfXlwcffJDnnnuO7du3X7ZtVlq0aMGGDRs4ffo0ycnJfPHFF+nrzp8/z9VXXw3AvHnzsownp3JKqWIgU8eKI6e9eKHHUTxtwvff/Ys4IUflJNcmFBFZBLQDqovIYWC8MWZWoUSTFwMHOvVkVKtWjdatW9OwYUO6devGAw88QMuWLQHw8/Nj4cKF7N+/n+eeew4PDw+8vLyYPn06AMOGDaNr167Url2bdevWXbbvWrVqMWHCBFq2bEnlypVp0qRJ+roJEybQt29fqlSpQocOHTh0yJr79a677uK+++7jm2++4cMPP8y2nFKqGHC4aGmA7rf0JvWa6by68XoCz+8t9MMX6aTGISEhJvOMPLt376Z+/fpFFoMqOvrZqhLPw8PqSgF8WLkHo57cwPWHq7JnYRRic15uFZFtxpiQyw7vtCMopVRpY+9AEU1Vnul1AQ9s/PDtYSQwqEgO7/LhZEuKFi1akJiYmGHZggULaNSokYsiUkoVukmTYNgwejToRfI1c3lpeS2uTabQLlpmpgncSX7//XdXh6CUKmoDBzIrXNhadhjXHLiGCadSCvWiZWaawJVSKp/OnrMx/MKnePh7sPLddYh/4fT3zo4mcKWUyqder3xMUu11jG04g+uKOHmDXsRUSql8WfTDITaVG0NQSmcm3TvUJTFoAldKqSt04aKNR795FA88WTXiU5fNQKUJ3MUmTJjAO++8k+36uXPncvTo0Sveb3h4OCtWrMjzcZRSeddzwnQSaq3nucbvccNVAS6LQxN4HqSkpLjs2Dkl8NTU1Gy3y5zAlVLOseC7g/zsM4bglK683vdRl8ZSrC5iPv00hIc7d59NmsDkyTmXefXVV1m4cCH+/v4EBARwyy238N1339GkSRM2bdrEgAEDaNKkCc8++ywpKSk0b96c6dOn4+3tTXBwMGFhYVSvXp2wsDCeffZZ1q9fz4QJE4iKiuLgwYNERUXx9NNPM2rUKAAmTZrEvHnzqFGjRvrxsrJ06VLCwsIYOHAg5cqV49dff6V+/fr069eP1atXM2bMGD7++GPeeecdQkJCOHXqFCEhIezdu5eXXnqJ+Ph4Nm3axNixYwHYtWsX7dq1uywepVTenDtv47HvHsWjWhlWjZzp8sm7S30NfOvWrSxbtowdO3bwww8/4Hirf1JSEmFhYYwYMYLBgwezePFidu7cSUpKSvp4KDn5559/WLlyJVu2bOHll18mOTmZbdu28fnnn6fXkLdu3Zrt9vfddx8hISGEhoYSHh5OuXLlAGv8lu3bt9O/f/8stytbtiyvvPIK/fr1Izw8nH79+mUbj1Iq77pOmEJirQ083/R9rq9Rx9XhFK8aeG415cKwefNm7r77bnx8fPDx8eGuu+5KX5eW+Pbs2UPdunW54YYbABg0aBAfffQRTz/9dI777tGjB97e3nh7e1OjRg1OnDjBzz//TO/evfG1D4vbq1evK445La4rlVU8deq4/h+hUu7g42V/87vfWK5L7cXEPo+4OhxAa+A5Kl++fK5lypQpg81mA7hsrG5vb+/0556enk5rS3eMK6fjZ1ZY8ShV0p04lcSo9Q/hmVKRtU/PcHnTSZpSn8Bbt27N8uXLSUhIICYmhu++++6yMvXq1SMiIoL9+/cD1hgnbdu2BSA4OJht27YBsGzZslyP16ZNG77++mvi4+O5ePEiy5cvz7F8bmOOOx5/6dKled5OKZV3d056heTqf/BG6xkEVq3p6nDSlfoE3rx5c3r16sXNN99Mt27daNSoEZUqVcpQxsfHhzlz5tC3b18aNWqEh4cHTzzxBADjx49n9OjRhISE4OnpmevxmjVrRr9+/WjcuDHdunWjefPmOZYfPHgwTzzxBE2aNCE+Pv6y9c8++yzTp0+nadOmnDp1Kn15+/bt2bVrF02aNGHx4sV5ORVKqSy8veg3dlZ8nZttg3m25z2uDicDHQ8ca+5LPz8/4uLiaNOmDTNmzKBZs2YujakkKA6frVIFEXU8lmveaoJHmWSOvvgn1StUdEkc2Y0HXqwuYrrKsGHD2LVrFwkJCQwaNEiTt1IKgI5vPEdq5QN8fPs6lyXvnGgCBz777DNXh8CIESPYvHlzhmWjR4/mkUeKx9VupUqbF+b+yP4q02nFfxjaqa2rw8lSXubEnA30BE4aYxral70N3AUkAQeAR4wx5woz0JLuo48+cnUISim7vw+d5rW/H8XHdhM/TiyayRnyIy8XMecCXTMtWw00NMbcDOwFxjo5LqWUcgmbzdDp/eGYcqcIvW8BFcr5uDqkbOWawI0xG4EzmZatMsakdSL+DdC7QZRSJcLQKZ9zvNoSevpN4N6WTV0dTo6c0Y3wUeCH7FaKyDARCRORsOjoaCccTimlCseGP/5lzsnhVL7Yki+fGePqcHJVoAQuIuOAFCA0uzLGmBnGmBBjTIi/v39BDqeUUoUmITGVu+Y8CB4pfD90AV6exb+PR74TuIgMxrq4OdAUZWdyF8o8tOvQoUPZtWtXgfcbERGRr54wgwcPznD3pVIq/3q8/iYXq21kRN2ptLrxWleHkyf5SuAi0hUYA/QyxsQ5N6TiK3MC//TTT2nQoEGB95vfBF6YchprXKmSZvbKLfxkG0/QxX58OORhV4eTZ3npRrgIaAdUF5HDwHisXifewGr7oC6/GWOeKGgwT//4NOHHnTsgeJOrmjC5a87DHC5cuJAPPviApKQkWrRowbRp0xgyZAhhYWGICI8++igBAQGXjc3drVu39LG4/fz8ePLJJ1mxYgW1atXitddeY8yYMURFRTF58mR69epFREQEDz30ELGxsQBMnTqVVq1a8fzzz7N7926aNGnCoEGDGDVqFM8//zzr168nMTGRESNG8Pjjj2OMYeTIkaxevZqAgADKli2b4/vaunUro0ePJjY2Fm9vb9auXcuyZcsICwtj6tSpAPTs2ZNnn32Wdu3a4efnx+OPP86aNWvo27cvO3bs4IsvvgBg/fr1vPPOO3z33XesWrWK8ePHk5iYyLXXXsucOXPw8/NzwqelVNE7fvYiT6x+AE+pzYbnPi42A1XlRa4J3BgzIIvFswohFpfYvXs3ixcvZvPmzXh5eTF8+HAmTpzIkSNH+OuvvwA4d+4clStXZurUqekJO7PY2Fg6dOjA22+/Te/evXnhhRdYvXo1u3btYtCgQfTq1YsaNWqwevVqfHx82LdvHwMGDCAsLIw33ngjPTkCzJgxg0qVKrF161YSExNp3bo1nTt35o8//mDPnj3s2rWLEydO0KBBAx59NOsZQZKSkujXrx+LFy+mefPmXLhwIX088ezExsbSokUL3n33XVJSUrjmmmuIjY2lfPnyLF68mP79+3Pq1CkmTpzImjVrKF++PG+++SbvvfceL730UgE/CaVco90bo0kuf4gpzdYRVLOyq8O5IsWqlT63mnJhWLt2Ldu2bUsfVCo+Pp6uXbty8OBBRo4cSY8ePejcuXOu+ylbtixdu1rd5Rs1aoS3tzdeXl40atSIiIgIAJKTk3nqqacIDw/H09OTvXv3ZrmvVatW8eeff6a3b58/f559+/axceNGBgwYgKenJ7Vr16ZDhw7ZxrNnzx5q1aqV/r4qVsz9NmBPT0/69OkDWMPUdu3aleXLl3Pffffx/fff89Zbb7FhwwZ27dpF69atAeuLomXLlrnuW6niaOyCL9jjO4fbksYx6u42rg7nihWrBO4KxhgGDRrE66+/nmH5pEmTWLlyJR9//DFLlixh9uzZOe7Hy8sr/aeXh4dH+tjbHh4e6eNuv//++9SsWZMdO3Zgs9nw8cn6BgFjDB9++CFdunTJsNwZc1w6jh8OGccQ9/HxyTCiYv/+/Zk6dSpVq1YlJCSEChUqYIzhzjvvZNGiRQWORSlX+uNgFG/uHoZvbAtWvz7e1eHkS6kfTrZjx44sXbqUkydPAnDmzBkiIyOx2Wz06dOHiRMnsn37dqDgY2yfP3+eWrVq4eHhwYIFC9IvFGbeb5cuXZg+fXr6lGd79+4lNjaWNm3asHjxYlJTUzl27Bjr1q3L9lj16tXj2LFj6VO2Xbx4kZSUFIKDgwkPD8dms/Hvv/+yZcuWbPfRtm1btm/fzsyZM9Onb7vtttvYvHlz+tjosbGx2f6SUKq4SklNpdO0hzCk8EX/UPx8vVwdUr6U+hp4gwYNmDhxIp07d8Zms+Hl5cV7771H796902uqabXztLG50y5iXqnhw4fTp08f5s+fT9euXdNn1rn55pvx9PSkcePGDB48mNGjRxMREUGzZs0wxuDv78/XX39N7969+emnn2jQoAGBgYE5Nl2ULVuWxYsXM3LkSOLj4ylXrhxr1qyhdevW1K1blwYNGlC/fv0cR1709PSkZ8+ezJ07l3nz5gHg7+/P3LlzGTBgAImJiQBMnDgxfbo5pdxB73ff5EyFjQz0nUv3lu7RZTArOh64KjT62ariKHT9Fh78qTW1zvXh8ORFeHgU/14nOh64UqrUO372Ao989wAe1GL9c9PdInnnpNS3gZcEvXv3pkmTJhkeK1eudHVYShUrZuFC7niyC8nlI3h7dX1u2OjQKSA0FIKDwcPD+hua7eggxUqxqIEbY9yq83xx89VXX7k6hMuUktEVlLsIDWXEB1PY3yOM29d24T9/roRhmy6tHzYM4uw3lUdGWq8BBg4s+livgMvbwA8dOkSFChWoVq2aJvESwhjD6dOnuXjxInXr1nV1OEqxptHN3Hn3fipENeLEwu2USxsNOyjI+hsZeflGQUFgv4fD1YptG3idOnU4fPgwOtRsyeLj40OdOjpMvHK9iwlx3N3mAiT68eNXJy8lb4CoqOw3zGldMeHyBO7l5aW1NKVUoWn/1iji/KN4dkEnWsWszrgyMND6m1UNPG1dMebyBK6UUoXl5S8/Y5uZRYODw3nr2NyMK319YZJ9vkvHNvDM64ox7YWilCqRtkfs4+Xtj+N9ojUbp0xBZs6w2rVFrL8zZlgXKQcOtJ5nta6Yc/lFTKWUcraE5ERqvdiScyaSr7qGc0/7AFeHVCDF9iKmUko5W7f3n+NcuT942Otbt0/eOdEErpQqUd5d8RXr4z8k+NjTzJl2l6vDKVSawJVSJcaOqIOM2fwIXudD2Pzym3iU8Kt8ub49EZktIidF5C+HZVVFZLWI7LP/rVK4YSqlVM7ikxNoN/0+bKnCwruXULtmzlMOlgR5+X6aC3TNtOx5YK0x5npgrf21Ukq5TOf3R3HO5w8erjCf++8sHfeW5JrAjTEbgTOZFt8NzLM/nwfc4+S4lFIqz175dh6b4mdS98jzzBlbstu9HeW3haimMeaY/flxoGZ2BUVkmIiEiUiY3i6vlHK2Xw/uZMLWJyl7tB2bX321xLd7OyrwWzVWR/JsO5MbY2YYY0KMMSH+/v4FPZxSSqU7n3CBzp/2wcRXZsn9i6hVs3T1y8hvAj8hIrUA7H9POi8kpZTKnTGG9pMfJcbrIMOqfs7dHa9ydUhFLr8J/FtgkP35IOAb54SjlFJ588zSKfyRuIx6/77O9OfbuDocl8hLN8JFwK9APRE5LCJDgDeAO0VkH9DJ/loppQqPw6w5K0Ku5/2dz1Eu8h42vfVsqWr3dpRrg5ExZkA2qzo6ORallMpaaGj6iIHHygv3tj0L5wP5PvgBqlcvvRPBlNLvLaVUsec4T+WgQRAXR4oHtO4TTKJvDC8uaUz72c+5OkqX0gSulCp+0mrckZFgDKSmAtCvY10OXXOI1t89wMvHv7LWu+FkxM5SuvrcKKXcw7hxGSdYAD5oWJ0vWx+i+pa+rA5fhIA1fnfabDpuNBmxs2gNXClV/GSaj3JrTS/+r1cMnlEt+HnlVsqRYCXvzPMZxMVZyb+U0ASulCp+HOajPFMOOvarjC2xCnO+qMSNtkhr1pzsJqNxg8mInUUTuFKq+Jk0CXx9SRVoe+/VXKx0jkFL7+Gh6Q+DzQYREVYSz4obTEbsLJrAlVLFj32eyqF3Xcdf1x+h/qpRzHr19oxt2/Ykn4GbTEbsLHoRUylVLM2+phxzm+3Hb89jbPzuHTyrZyqQlszHjbOaTQIDreRdSi5ggk5qrJQqhnYc3UWz6S0wJ2/it2EbuPUWb1eH5FI6qbFSyi2cSzhH2+m9sSWUZ9ody0p98s6JJnClVLGRYkuh9fv9OC+HeMBrLU8+eLWrQyrWNIErpYqNgXOfY1fSKm6KmMn8OXe4OpxiTxO4UqpYeOenWSz5dxjjO/MAAB33SURBVDKVdo/m5+lD8fR0dUTFnyZwpZTLrd2/iTEbnsQz6k42jHuHKlVcHZF70ASulHKpiLOR9Jx/L+ZsXeZ0X0zjRpqW8kpv5FFKuUxMUgytp/YiITmJ4VW/5aG+WvW+EvpVp5RyCZux0eXjhzma8hetjvzAh3PruTokt6MJXCnlEsOXjOeXs19R++/3WTm3c6mdFq0gCnTKROT/RORvEflLRBaJiI+zAlNKlVyf/LKIT/6ZSLndj/Lre6Px83N1RO4p3wlcRK4GRgEhxpiGgCfQ31mBKaVKpnUHNvPkysHIv3ewavQ0AgNL75yWBVXQHy1lgHIiUgbwBY4WPCSlVEm1//QBus27B3M2iE9W3sDtrcuVyqnQnCXfCdwYcwR4B4gCjgHnjTGrMpcTkWEiEiYiYdHR0fmPVCnl1s7En6HVRz1ITLTx1Oc9eOzwLGtShrSp0DSJX7GCNKFUAe4G6gK1gfIi8mDmcsaYGcaYEGNMiL+/f/4jVUq5raTUJNpO60N08iHafz2GKdFTMhYoZVOhOUtBmlA6AYeMMdHGmGTgS6CVc8JSSpUUxhj6zh/GXzHrqbtzFt/tfQUPshjGuhRNheYsBUngUcBtIuIrIgJ0BHY7JyylVEnx/Pev8W3UPCpsm8CmaQ/iG5TNL/FSNBWasxSkDfx3YCmwHdhp39cMJ8WllHJXoaHWhUkPD+a0q8Nb216gzN8PsuHll6hdG50KzYkKdCOPMWY8MN5JsSil3F1oqHVBMi6OjQGeDL3jJETezpd1B9K0qb27oE6F5jR6J6ZSynnGjYO4OP6pBp0H+GA7X4v3Pw/irqSe8LYtY7LWhF1gevOqUsp5oqI47getHqxEovHlyYV38XR8KKSmapfBQqAJXCnlNBevqUPLgVU4Wz6FLqEj+ejs+5cX0i6DTqNNKEopp0hKTaLDQH8iOErDzybxzdGXyPYmee0y6BRaA1dKFZgxhvsWDCHMYztXbZzCxoRQvCWZbOdF0y6DTqEJXClVYKO+/R/LIxdS/vdX+WX2CKr8+yfYbDBvnnYZLESawJVSBfL2hqlMDX+DMuGP89OEcdSt67By4ECYMQOCgkDE+jtjhvZAcRJtA1dK5dvnO75kzLpRyL5efPvEVG69NYtWb+0yWGg0gSul8mXtgfUM/PIBONKC2d0X0a2LppOipk0oSqkrFnZkG93m98J26lom3fQdgwf65r6RcjpN4EqpK/LPqX9oO7MryReqMqLiKv73f9UyFnAYC0Unayhc+ptHKZVnUeejaDn9TuJiPegTt5oP37s6YwGHsVCAS3degraDFwKtgSul8iQ6NpqW0zpzLu4CbaJW8vlH1yOZr1nax0LJQO+8LDRaA1dK5epC4gVaT+/K0dgoGu1cxQ9LmlAmq+yR3R2WeudlodAauFIqR/HJ8bT9uBf7LvxJ0JalrJ9/+2X35qTL7g5LvfOyUGgCV0plK8WWQrdZ/Qk/u5Grfp3P7wu6U7VqDhvoZA1FShO4UipLqbZU7p77MBtOfEuVX6fy+6wB1KyZy0Z652WRKlACF5HKIrJURP4Rkd0i0tJZgSmlXMDeBdDmIfR7sDYr/l2E329vsOWj4XlvBRk4ECIirLFQIiI0eReigl7EnAL8aIy5T0TKAtqbXyl3Ze8CaOLiGNSjPMvqnaTcujH82vUmrrvO1cGprOS7Bi4ilYA2wCwAY0ySMeacswJTShWxceMwcXE83rUcC5vHUvbn/2PDhjU0/PgpV0emslGQJpS6QDQwR0T+EJFPRaR85kIiMkxEwkQkLDo6ugCHU0o5ncNdkyYykv/r5M3M2+Lx/G04q9duoTnbrZtxVLFUkAReBmgGTDfGNAVigeczFzLGzDDGhBhjQvz9/QtwOKWUU6XdNRkZCcbwv7beTLk9EY+tj/Hdj/tow2arnIjeDl9MFSSBHwYOG2N+t79eipXQlVLuwOGuyZdbl+WN9onIHw+zdMVxurL6Ujlj9E7KYirfCdwYcxz4V0Tq2Rd1BHY5JSqlVOGz3x35xm1lmXBnErKzH0u+PU9vszzbsqp4KWgvlJFAqL0HykHgkYKHpJQqEoGBvF77GP/rkgS77uXzrxK4z3yTbVlV/BQogRtjwoEQJ8WilCpCE5++nRfPh8LffVi0zMb9tm/Ay8tq805KulRQ76QstvROTKVKoVd+eoMXz4cif/cldEN1+puvrbsm58yB2bP1Tko3oaMRKlXKjF87iVc2vQA7B7Cgz3weWFIG+DhjIU3YbkFr4EqVIi+sftVK3n8OZH7v+QwcoHU4d6afnlKlxNgfX+aN3ycgfz5EaN85DOjv6eqQVAFpAleqhDPG8NwP43l366vIjsEsefBT7rtXk3dJoAlcqRLMGMOTXz/LJ3++h8eOR/lmyEx69tCW05JCE7hSJVSqLZWHlzzBZ3s+pcy2kawYNZk7O2nyLkn001SqBEpKTaL3woF8tudTvDY/z9otR7mzcxlr4Cod16TE0Bq4UiVMfHI83ef2Zf3R7/FZP4l1v67itsQN1srISGsAK9CugiWA1sCVKkEuJl6kzSfdWX9kBX7rp7P5n82XkneauDgdnKqE0ASuVAlxJv4MLT7qRFj0z1TbsIBtnzxBsxM/ZF1YB6cqEbQJRakS4HjMcW57qzWR5jB1vpjMrzEfUmcr1iBUWU3IoINTlQhaA1fKze07vY+b37yFyNQT3PDZu/yxZwJ1jvxutXV3724NRuVIB6cqMTSBK+XGth7ZStOPWhGdmETzuW8QdnAs1TltrYyLgxUrrMGodHCqEkkTuFJu6sd9K2k9sz2xZyrQZdaL/Hz0GSoQk7FQZCQ89JD1fMECiIjQ5F2CaAJXyg3N3b6Q7qE9ST5+PQ8m/sL3fpPxJinrwsZc6j6ofcBLFE3gSrmZiT+9wyPLH8IcasO4qzcwf9pVeL726uVt3Zlp98ESp8C9UETEEwgDjhhjehY8JKVUVmzGxuPLnuXTv99H/r6fGd3nM3Swt7UyrVlk3Diri6AxWe9Euw+WKM6ogY8GdjthP0qpbMQlx9H50758+vf7eG0fxQ9DF11K3mkGDrTauG0262JlVrT7YIlSoAQuInWAHsCnzglHKZXZiZgT3PJhe9Ye+YqKv7zPlpcn06VzLv91J03S7oOlQEFr4JOBMYDNCbEopTLZFb2LhlNa8M/pvwj45St2znyaJk0k9w0HDtTug6VAvtvARaQncNIYs01E2uVQbhgwDCBQf74plWerD6zlrgV9SIwpxy17N7BmSQiVK1/BDgYO1IRdwhWkBt4a6CUiEcDnQAcRWZi5kDFmhjEmxBgT4u/vX4DDKVV6TP9tNl0WdCXxZAD9Y3/j12VXmLxVqZDvBG6MGWuMqWOMCQb6Az8ZYx50WmRKlUI2Y+Opr//H8JVDMAfb89r1m/hsWhBeXq6OTBVHOpiVUsXEhcQL9Jz9ID+fXI7XjmEsfXQqvXpq5lbZc0oCN8asB9Y7Y19KlUYHzhyg3Se9OJywh6pbprLh7eE0bJiHi5WqVNMauFIutmr/Wu5e2JeEeKHh7lX8NL8DerlI5YXeSq+UixhjeHPdVLou7EJCdG36XdhC2BeavFXeaQ1cKRdISk3igYVPsSxiJrKvF1PuWMioJyq4OizlZjSBK1XEDl84TIfpfdmX8Bt+28fx439foXUr/TGsrpwmcKWK0Jr96+i1oB/xyfHcuOcL1n1yH1dd5eqolLvSr32lioAxhhd/fIs7F3Yi/nR1Hojdyo5FmrxVwWgNXKlCdiHxAj1mDmbT6a/w3NOXqZ1n8cQj2t6tCk5r4EoVoj+P/801bzRnU/S31Ph1En9sOMsTQypBcLDOjqMKTGvgShWSDzfO4+k1w7HFVaTTP6F8tXEkfvHR1sq0Kc5AB5xS+aY1cKWuRGioVXv28Mi2Fn0x8SJdPn6YUesGw+Fbefv6baza+d9LyTuNTnGmCkhr4ErlVWioVWuOi7NeZ1GL3nr4D7rN6sdp2wGq/fUyq18aR9PGnvBkNlOZ6RRnqgC0Bq5UXo0bdyl5p7HXoo0xTFjxIS1m3sbpC7F0PPITh+a+ZCVvyH4qMx0jXxWAJnCl8iqb2vLpk5E0f/deXt46Cs+IO5neeAdrPm1LBceOJjrFmSoEmsCVyqssasvf1i1PwJPV2Hbhe+rufZe9E5bzxMPVL99WpzhThUATuFJ55VCLTigD93e5mrsHxRKfVI0nym5m3/z/ULduDkPAOs4aHxGhyVsVmF7EVCqv7An397fH0aN1AqdrHKHizsdY/sxk2rT0zWVjpZxPa+BKOcqhm2CqLZURcoTbeh3ldDmh2+kfODJ3hiZv5TJaA1cqTQ7dBHd3aEXnaYM5XGYj5f69lwX3f0Kfblm0dStVhPKdwEUkAJgP1AQMMMMYM8VZgSlV5LLoJmiLj+PZOW8xefcBjM2DdjFz+WbKw1SsqNOdKdcrSA08BXjGGLNdRCoA20RktTFml5NiU6poZeomGF7Nh569anEk6E98Dndh1j0zeKCH9ttWxUe+E7gx5hhwzP78oojsBq4GNIEr9xQYCJGRpAoMa1mPOe0jMClnaffDML5Z9bHWulWx45SLmCISDDQFfs9i3TARCRORsOjo6MyrlSo+Jk1iXZ061BhyHbM776H8gdZ8+VE91v0+g4o319XRA1WxU+CLmCLiBywDnjbGXMi83hgzA5gBEBISYgp6PKUKQ0xCPPcf3cMPj5yAhErc/0Vv5v/9Hd4kWwV09EBVDBWoBi4iXljJO9QY86VzQlKqCDh0F/zw1lup/kIDfoh7latO389vD+1iccz2S8k7jY4eqIqZfCdwERFgFrDbGPOe80JSKp+y68M9fDiUKWPdwl6mDHTqBMOG8eeZC1xzXzNG9dhKSqInz5/5gKNTF9KikX/2owTq6IGqGBFj8teqISK3Az8DOwGbffH/jDErstsmJCTEhIWF5et4SuUocx9usG57b9kS1q7NUDTBw4MHmrfmqw7h4JlEq43t+WrzZmrUqWrd4g7WF0Bk5OXHCQq6VEapIiIi24wxIZmXF6QXyiZAL8ur4iG7oV4zJe/JgU0Z2y2ehFo/U21fCKErztDl7I/WyqiYSwUnTcr6C0FHD1TFiN6JqUqGXJo2Vle+lkF3XsWxmzbjeb4Wzy4J4c1dYRnbEB1HG0y7UDlunLXvwEAreesFTFWMaAJXJYO9D3dm+8vWoO/ttxDe6icwR+m47nYW/7KVasnHMhbMqnY9cKAmbFWs6WBWqmTo3t26SGl3XspzT7M+3DDSg/A2P1Bv102Ef+jNmg2bqJacCB076tjcyu1pDVy5n9DQjE0b3bvDvHlgDAl4859rujGz00FSai+janxzPo7qRt9v5kNqKnh6Wm3b06a5+l0oVWD57oWSH9oLRRVYVr1NREg0XrxYuxdTOh0n6ZpNeJ+ryfPbrualDVvxEP2hqdyb03uhKOUSmXqbJOHFm1Xv4fUOscTftJQysZUY8cN1vBu2H2/bSdDkrUowTeDKvdh7myTgzeSK9/JaG8PFZl/gkezNg+vrMvXXQ1RKPG+VDdKRA1XJptUTVbzkMCMOwIU6DRhbaRhVe/Rj7OilxDRdSq8tdTj6QRwL1h+iUqK9oPbZVqWA1sBV8ZHDjDgnOg3k5SmH+DSkBckN5yAGem3354NNRwlKjoZBT8KKFdpnW5UqmsBV8ZHF3ZR/xdVl4uiDLL3jEVIbLcSjkQd9Pbrw7pI/CNh9FAKDNFmrUksTuCo+7O3bqXjwHT15NaAj21qtgxvH45lShoe8u/L6yI+5uuLVMN7FsSpVDGgCV8XG2asbMutIR96uV4+TrRZA4GjKxfvw5M/ejPk9gZrVd8K4q10dplLFhiZw5VLGwMaNMH1WDMuufpyUez6E6pOpcdaX538QHvsjAb8ke+E4HcpVKUeawJVLnDhh3Tw5bfEeImtMg6Zz4doL1JfreGmZcN9fcZSxZdooULsFKuXILboRLl0KL7wAf//t6khUrjJPnjB8ePqq+HhYsgTuvieV2u2/5b9/dSay14143jadfk3u4tchv/L3i3vpP2YBZXx8M+5XuwUqdRm3qIGHhcHbb1v/fxs1ggEDoH9/qFvX1ZGpDIYPh+nTL71OTSVl+gzWRtXns2ojWbr2AHH15uDRbB62poe5qlwdnrptIkObDaWmX81L2+lQrkrliduMhXLiBHzxBSxaBL/8Yi1r2dJK5Pfco7+ui4UyZSA1lWTKsJE2fEVvlnh1J7rBJjxDZpMasAEPPOhyXVeGNhtCr3q9KOPhFnUIpVwqu7FQCpTARaQrMAXwBD41xryRU3lnDWYVEQGff24l8z//tJY1aWIl8rvvhsaNM4wsqgpDphEBY198gx+HfsHX3MNy6cb54B14NlqA3LSEFO94rq1yHUOaPsrDjR+2ugEqpfLM6QlcRDyBvcCdwGFgKzDAGLMru20KYzTCvXvhm2+sxy+/GIwRgojgrgrr6TIsmHYT2uHn59RDlmyZh2rNqukiNBTz2DD2xtdhFZ1ZSRfW0IHEgHB8Gs6Bm5aQ4HcBv0TosxuGhHtw+8EURL9VlcqX7BI4xph8PYCWwEqH12OBsTltc8stt5hCs3ChOV4u2HzKo+YuvjE+xBkwxsszxbRpY8yrrxrz++/GpKRk3MYEBRkjYv1duLBAx3favlxl4UJjfH2NsXr3WQ9f3/T3cuaMMV98YcxjfqEmiEMGSTEEbDKV7xxiKjxdxTAB4/0Cps/9mC8aYOLK2Pfx5JMufmNKuTcgzGSVh7NamJcHcB9Ws0na64eAqVmUGwaEAWGBgYH5Cn71gdXmk7BPTNS5qOwLBQVlSDzxeJs1dDBjKk43TZteWlW5sjE9ehjz2v1/mA3ed5o4fDImqyefvPJEnEvic4m0LxQwxtPT+pvb+8l0DqOoYz6jv3nSb75p1Mg6JXjFmnL1PjNBvTqYCs/5GCZgvF7EdBuImd8Yc374kEvH8/TU5K2UE2SXwAvShHIf0NUYM9T++iGghTHmqey2yW8TyrDlw5i5fSYADWs0pNt13eh+fXdaB7TGy9PLKuThYaWdywMFm43oaGuC8jVrYPNm+Ocfa7UXSdzCNlqzmRDCaEo417MXD8yl7Y2xpt3KridEcHCW8zESFGQ12DvKSxNFfjjut2pVuHABkpMvL+frm+X0YRcuwB+V2rGdpoQRwmZaE0kwYPCtGk5gz1/gmlUc8lhNoi2eignQfR/cvQe67cMaBTCr96uUKrDCaANvCUwwxnSxvx4LYIx5Pbtt8pvAjTHsPrWbFftW8MP+H/g58meSbclUKFuBjtd0pH1we9o+8QaNdhzDI/PbySapnBJ/fuU2NnE7m2nNVpqThDcAflykMTtoyh805Q9u4m/qsYfKvsmXkp9jwszuHNq/PNJlNZtMNgn1imS132yk4sGh2rfzzycb2LULtm+3Hvv2XSpzVbm/CKo7B49rVxJx7T6OVbZuhQyuHEyP63tw9xE/2v7nA8rGxDv3fSilslQYCbwM1kXMjsARrIuYDxhjsr3dxlkXMS8mXuSnQz+xYt8KVh1cRcS5CACqxkObCGgbCW0joNHFcpT5ZGbGpJKWeDPVmJPwYhcN7CnbeoTThBgqpJepyXHqeUdyY6uq1Ns0i7rJewjgXwKJwp9oLrtEl/nL40pq6pnjTatZA5w5c6n2DjBokDXfo10C3vxLAFEEEkkQEQTzDzeym/rs43oS8UkvGxBoaHBbFBVu2kRc0mIOnvuRf/ytmnvFBOgY5cmdrR/izv7juLbKtZcuRBbWLwml1GUKqxthd2AyVjfC2caYHG+VK6w5MSPPRbIhcgMb1s5m/ZHNHKyYAkA5KcstAbdya+1baVGnBbduPUrQiP8hcfG57NFiQzjAteymPnuoxx7q8Q83ssejPqds1TKU9SaBOhwmgH+5iuNU9zhLtbtvp1r7m6leHapVg0pdbqMccfgSRzniKUc8vsThRQqkpKRX5NP+piz8nNgRY4iNF+LwJZbyxOHLBSpyiupEe9biVGoVoqlONP6coCZRBHKCqzLE5kEqdTlEfXYT7LMdn3q7iB/TnKOEsfXEZg5fOAxARe+KtPIMpvWmSDr+cZ7mnoGUmfiaJmalXKxQEviVKqpJjaPOR7EpahNbjmxhy5EtbD+2ncRUa6oW/1i4+QQ0OgGNTkLDk3DTSSifRXNxloKCICqKM6YykQQRRSD/EpBe4/2XAE5Qk9NSnbOmSuG9SbuyJOJPNP5EU4OTBBJFIFFc7RFB2cp7iPOP5KT/KXbUsrG9FhysemnbwEqBtApoxe0Bt3N74O00rNEQTw/PQo9ZKXVlSlUCzywpNYmdJ3ay5Z4QttaGnTXg7xoQb7/+KQauOQvXn4Zrz8K1Z+x/z8I1Z6Bcin1Hae28WTTBXEaElCQbZ8/C6dNw6hRc/HYdcVNmEp/kQRy+xFOOOK/KJPe6F2ncOG2z9L+eL4ylPDGUJxZf4ihPLOWJxY8Y/Immkkc05yrGElUZIivBwSqw2x92V4e91SDJ4SbHa87ALed9aRbSk1u6DaVpraZU963u3BOtlCoUpTqBp3Nog04VOFTFSuZ/1YS//GF/VThQFc77ZNyseizUTvSiVp0bqX1DCLUjz1B78QqqnU+mcgJUSYDKDo+yqWTfrp1N27ExhqTUJGKSYriYdJGYpBjO9ezEyZgTnCxPhscxP4isDEcqgM1hODIPG9Q9B/WjoX6LHtRv04f6/vWpX70+lXwqFdZZVUoVMk3gkH0vkEGD0udTNIEBnHllLPs7NOXA2QMcOHOAoxePcjTmKEcvHuXYxWMcjzlOqknN9jCeNvD29Kasdzm8Pb3xLuNNWc+yeIgHqbZUUk1q+l+bsZGYksjFpIuk2FKy3SdAlXioEQs1YyD4HASdh6Bzl54HnAfvVKwG91OnnHTSlFKull0CL10jCeVhlDsBqtkfLeq0yHI3qbZUouOiORN/hnMJ5zj3w1ec/XwO52JPc/aqSsR3bEvijdeTlJpEYkoiSTbrr8HgIR54iieeHp54iice4oG3pzcVvCvgV9aPCmWtv35l/ajkU4maG8Lwf3sa1fcdoWxl+4XTtF4o3bvDrFmQlHQpuLJlYcqUQjl9SqnipXTVwEsi7c6nVImnNfCSauBATdhKlVJuMSOPUkqpy2kCV0opN6UJXCml3JQmcKWUclOawJVSyk1pAldKKTelCVwppdxUkd7IIyLRQC6jQGWrOlDa7g/X91w66HsuHQrynoOMMf6ZFxZpAi8IEQnL6k6kkkzfc+mg77l0KIz3rE0oSinlpjSBK6WUm3KnBD7D1QG4gL7n0kHfc+ng9PfsNm3gSimlMnKnGrhSSikHmsCVUspNuUUCF5GuIrJHRPaLyPOujqewichsETkpIn+5OpaiIiIBIrJORHaJyN8iMtrVMRU2EfERkS0issP+nl92dUxFQUQ8ReQPEfnO1bEUFRGJEJGdIhIuIk6b1abYt4GLiCewF7gTOAxsBQYYY3a5NLBCJCJtgBhgvjGmoavjKQoiUguoZYzZLiIVgG3APSX8cxagvDEmRkS8gE3AaGPMby4OrVCJyH+AEKCiMaanq+MpCiISAYQYY5x685I71MBvBfYbYw4aY5KAz4G7XRxToTLGbATOuDqOomSMOWaM2W5/fhHYDVzt2qgKl7HE2F962R/Fu0ZVQCJSB+gBfOrqWEoCd0jgVwP/Orw+TAn/j13aiUgw0BT43bWRFD57c0I4cBJYbYwp6e95MjAGsLk6kCJmgFUisk1Ehjlrp+6QwFUpIiJ+wDLgaWPMBVfHU9iMManGmCZAHeBWESmxTWYi0hM4aYzZ5upYXOB2Y0wzoBswwt5MWmDukMCPAAEOr+vYl6kSxt4OvAwINcZ86ep4ipIx5hywDujq6lgKUWugl709+HOgg4gsdG1IRcMYc8T+9yTwFVbTcIG5QwLfClwvInVFpCzQH/jWxTEpJ7Nf0JsF7DbGvOfqeIqCiPiLSGX783JYF+r/cW1UhccYM9YYU8cYE4z1//gnY8yDLg6r0IlIefuFeUSkPNAZcEoPs2KfwI0xKcBTwEqsC1tLjDF/uzaqwiUii4BfgXoiclhEhrg6piLQGngIq1YWbn90d3VQhawWsE5E/sSqqKw2xpSarnWlSE1gk4jsALYA3xtjfnTGjot9N0KllFJZK/Y1cKWUUlnTBK6UUm5KE7hSSrkpTeBKKeWmNIErpZSb0gSulFJuShO4Ukq5qf8HYOfD2h/CfrAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "Ih9DidT0ZExC",
        "outputId": "baa2d0db-d863-4e3c-b8ce-f9634aa5864d"
      },
      "source": [
        "# Comparing the Estimated Curves\n",
        "\n",
        "W,B,_,_ = lin_results[0]\n",
        "W1,B1,_,_ = lin_results[1]\n",
        "y = lambda x : W*x + B\n",
        "g = lambda x : W1*x + B1\n",
        "plt.plot(lin_data['test_x'], lin_data['test_y'], 'ro')\n",
        "plt.plot(x, y(x), 'g')\n",
        "plt.plot(x, g(x), 'b')\n",
        "plt.legend(['test_data', 'line using loss=|y-z|', 'line using loss=|y-z|^3'])\n",
        "plt.title('Linear Regression Results')\n",
        "plt.show()\n",
        "\n",
        "A,B,C,_,_ = quad_results[0]\n",
        "A1,B1,C1,_,_ = quad_results[1]\n",
        "y = lambda x : A*(x**2) + B*x + C\n",
        "g = lambda x : A1*(x**2) + B1*x + C1\n",
        "plt.plot(quad_data['test_x'], quad_data['test_y'], 'ro')\n",
        "plt.plot(x, y(x), 'g')\n",
        "plt.plot(x, g(x), 'b')\n",
        "plt.legend(['test_data', 'curve using loss=|y-z|^4', 'curve using loss=|y-z|^7'])\n",
        "plt.title('Quadratic Regression Results')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "PeKg7P-9Ab-L",
        "outputId": "d2dfb3e2-96de-4c68-fda9-7111d59f6455"
      },
      "source": [
        "# Comparing Linear Regression Loss Curves \n",
        "\n",
        "_,_,train_loss,test_loss = lin_results[0]\n",
        "_,_,train_loss1,test_loss1 = lin_results[1] \n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.plot(train_loss1)\n",
        "plt.legend(['for loss=|y-z|','for loss=|y-z|^3'])\n",
        "plt.title('Training Loss for Linear Regression')\n",
        "plt.ylim((0,30))\n",
        "plt.show()\n",
        "\n",
        "plt.plot(test_loss)\n",
        "plt.plot(test_loss1)\n",
        "plt.legend(['for loss=|y-z|','for loss=|y-z|^3'])\n",
        "plt.title('Validation Loss for Linear Regression')\n",
        "plt.ylim((0,30))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "KBATmtfiC7h4",
        "outputId": "63c07e7e-dbb8-4dbf-9acf-c4ffa22e67aa"
      },
      "source": [
        "# Comparing Quadratic Regression Loss Curves \n",
        "\n",
        "_,_,_,train_loss,test_loss = quad_results[0]\n",
        "_,_,_,train_loss1,test_loss1 = quad_results[1] \n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.plot(train_loss1)\n",
        "plt.legend(['for loss=|y-z|','for loss=|y-z|^4'])\n",
        "plt.title('Training Loss for Quadratic Regression')\n",
        "plt.ylim((0,4))\n",
        "plt.show()\n",
        "\n",
        "plt.plot(test_loss)\n",
        "plt.plot(test_loss1)\n",
        "plt.legend(['for loss=|y-z|','for loss=|y-z|^7'])\n",
        "plt.title('Validation Loss for Quadratic Regression')\n",
        "plt.ylim((0,4))\n",
        "plt.xlim((-2500,50000))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}